[{"question": "1991\u5e74\u6211\u56fd\u52a0\u5165\u300a\u5173\u4e8e\u5411\u56fd\u5916\u9001\u8fbe\u6c11\u4e8b\u6216\u5546\u4e8b\u53f8\u6cd5\u6587\u4e66\u548c\u53f8\u6cd5\u5916\u6587\u4e66\u516c\u7ea6\u300b\u548c1997\u5e74\u52a0\u5165\u300a\u5173\u4e8e\u4ece\u56fd\u5916\u8c03\u53d6\u6c11\u4e8b\u6216\u5546\u4e8b\u8bc1\u636e\u7684\u516c\u7ea6\u300b\u65f6\uff0c\u5747\u6307\u5b9a\u6211\u56fd\u4e13\u95e8\u8d1f\u8d23\u53f8\u6cd5\u534f\u52a9\u548c\u6709\u6743\u63a5\u6536\u5916\u56fd\u901a\u8fc7\u9886\u4e8b\u9014\u5f84\u8f6c\u9012\u7684\u4e2d\u592e\u673a\u5173\u662f\nA. \u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u6700\u9ad8\u4eba\u6c11\u6cd5\u9662\nB. \u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u53f8\u6cd5\u90e8\nC. \u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u516c\u5b89\u90e8\nD. \u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u5916\u4ea4\u90e8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6636262490502093, "meta-math/MetaMath-Mistral-7B": 0.9552778526789922, "itpossible/Chinese-Mistral-7B-v0.1": 0.43589455442585096, "HuggingFaceH4/zephyr-7b-beta": 0.9988314765401928, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7934332213536612, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e59\u516c\u53f8\u5bf9\u7532\u516c\u53f8\u53d1\u4ef7\u7684\u63a5\u53d7\u901a\u77e5\u4e8e8\u67085\u65e5\u4ece\u4e59\u5730\u53d1\u51fa\uff0c8\u67089\u65e5\u5230\u8fbe\u7532\u516c\u53f8\u6240\u5728\u5730\uff0c8\u670810\u65e5\u4e0b\u5348\u5230\u8fbe\u7532\u516c\u53f8\u4f20\u8fbe\u5ba4\uff0c8\u670811\u65e5\u4e0a\u5348\u7532\u516c\u53f8\u7ecf\u7406\u9605\u53ca\u6b64\u901a\u77e5\u3002\u4f9d\u300a\u56fd\u9645\u8d27\u7269\u4e70\u5356\u5408\u540c\u516c\u7ea6\u300b\uff0c\u4e59\u516c\u53f8\u63a5\u53d7\u7684\u751f\u6548\u65f6\u95f4\u662f\nA. 10/08\nB. 11/08\nC. 05/08\nD. 09/08\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u5173\u4e8e\u54f2\u5b66\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f\nA. \u54f2\u5b66\u548c\u81ea\u7136\u79d1\u5b66\u5b58\u5728\u754c\u9650\nB. \u54f2\u5b66\u662f\u4e00\u95e8\u79d1\u5b66\nC. \u54f2\u5b66\u4e3a\u6211\u4eec\u786e\u7acb\u4e86\u4ef7\u503c\u53d6\u5411\nD. \u54f2\u5b66\u6ca1\u6709\u7edf\u4e00\u7684\u6807\u51c6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4300308859387476, "meta-math/MetaMath-Mistral-7B": 0.70176950042759, "itpossible/Chinese-Mistral-7B-v0.1": 0.40181812173932907, "HuggingFaceH4/zephyr-7b-beta": 0.9986815827672729, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9848506269111381, "meta-llama/Meta-Llama-3-8B": 0.8497303233167147, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6140786521147283}}, {"question": "1972\u5e743\u67081\u65e5\uff0c\u7f8e\u56fd\u300a\u592a\u9633\u62a5\u300b\u8f7d\uff1a\u201c\u7531\u4e8e\u5728\u8bbf\u95ee\u7ed3\u675f\u65f6\u53d1\u8868\u4e86\u516c\u62a5\uff0c\u5c3c\u514b\u677e\u548c\u4e2d\u56fd\u9886\u5bfc\u4eba\u4f3c\u4e4e\u5b8c\u6210\u4e86\u8fd9\u79cd\u4e0d\u53ef\u80fd\u7684\u4e8b\u3002\u201d\u5bf9\u8fd9\u4e00\u8bc4\u8bba\u7684\u5206\u6790\uff0c\u7b26\u5408\u53f2\u5b9e\u7684\u662f\nA. \u7f8e\u56fd\u627f\u8ba4\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u653f\u5e9c\u662f\u4e2d\u56fd\u7684\u552f\u4e00\u5408\u6cd5\u653f\u5e9c\nB. \u8bbf\u95ee\u7ed3\u675f\u65f6\u53d1\u8868\u7684\u516c\u62a5\u5ba3\u544a\u4e2d\u7f8e\u4e24\u56fd\u6b63\u5f0f\u5efa\u4ea4\nC. \u201c\u4e0d\u53ef\u80fd\u7684\u4e8b\u201d\u6307\u4e2d\u7f8e\u4e24\u56fd\u5173\u7cfb\u7531\u654c\u5bf9\u5f00\u59cb\u8d70\u5411\u5408\u4f5c\nD. \u8bc4\u8bba\u9488\u5bf9\u7684\u662f\u4e2d\u7f8e\u4e24\u56fd\u9886\u5bfc\u4eba\u5728\u5317\u4eac\u7b7e\u7f72\u8054\u5408\u516c\u62a5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4569079496902297, "meta-math/MetaMath-Mistral-7B": 0.8052074097506847, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6348420303292784, "meta-llama/Meta-Llama-3-8B": 0.5463490701810393, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.852407303963947}}, {"question": "\u4e2d\u5171\u5341\u516b\u5927\u62a5\u544a\u4e2d\u6307\u51fa:\u201c\u5168\u9762\u6b63\u786e\u8d2f\u5f7b\u843d\u5b9e\u515a\u7684\u6c11\u65cf\u653f\u7b56\uff0c\u575a\u6301\u548c\u5b8c\u5584\u6c11\u65cf\u533a\u57df\u81ea\u6cbb\u5236\u5ea6\uff0c\u6df1\u5165\u5f00\u5c55\u6c11\u65cf\u56e2\u7ed3\u8fdb\u6b65\u6559\u80b2\uff0c\u52a0\u5feb\u6c11\u65cf\u5730\u533a\u53d1\u5c55\uff0c\u4fdd\u969c\u5c11\u6570\u6c11\u65cf\u5408\u6cd5\u6743\u76ca\uff0c\u5de9\u56fa\u548c\u53d1\u5c55\u5e73\u7b49\u56e2\u7ed3\u4e92\u52a9\u548c\u8c10\u7684\u793e\u4f1a\u4e3b\u4e49\u6c11\u65cf\u5173\u7cfb\uff0c\u4fc3\u8fdb\u5404\u6c11\u65cf\u548c\u7766\u76f8\u5904\u3001\u548c\u8877\u5171\u6d4e\u3001\u548c\u8c10\u53d1\u5c55\u3002\u201d\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u6c11\u65cf\u533a\u57df\u81ea\u6cbb\u5236\u5ea6\u6709\u5229\u4e8e\u5404\u6c11\u65cf\u5171\u540c\u7e41\u8363\nB. \u6c11\u65cf\u533a\u57df\u81ea\u6cbb\u5236\u5ea6\u662f\u6211\u56fd\u7684\u57fa\u5c42\u6c11\u4e3b\u5236\u5ea6\nC. \u6c11\u65cf\u533a\u57df\u81ea\u6cbb\u5236\u5ea6\u662f\u6211\u56fd\u7684\u6839\u672c\u653f\u6cbb\u5236\u5ea6\nD. \u6c11\u65cf\u533a\u57df\u81ea\u6cbb\u5236\u5ea6\u88ab\u786e\u7acb\u4e3a\u6211\u56fd\u7684\u4e00\u9879\u57fa\u672c\u653f\u6cbb\u5236\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f5b\u6559\u56db\u5927\u5929\u738b\u4e2d\uff0c\u624b\u6301\u5b9d\u5251\u7684\u662f\nA. \u5317\u65b9\u591a\u95fb\u5929\u738b\nB. \u4e1c\u65b9\u6301\u56fd\u5929\u738b\nC. \u897f\u65b9\u5e7f\u76ee\u5929\u738b\nD. \u5357\u65b9\u589e\u957f\u5929\u738b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.32545507259594497, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u519c\u4e1a\u751f\u4ea7\u4e3a\u4e3b\u8981\u8c0b\u751f\u624b\u6bb5\u7684\u4eba\u4eec\u4e3a\u4e3b\u4f53\u6784\u6210\u7684\u540c\u8d28\u6027\u8f83\u9ad8\u7684\u5730\u57df\u6027\u751f\u6d3b\u5171\u540c\u4f53\uff0c\u88ab\u79f0\u4e3a\nA. \u96c6\u9547\u793e\u533a\nB. \u519c\u6751\u793e\u533a\nC. \u57ce\u5e02\u793e\u533a\nD. \u5c0f\u57ce\u9547\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8282898540314527, "meta-math/MetaMath-Mistral-7B": 0.9333847678391244, "itpossible/Chinese-Mistral-7B-v0.1": 0.8073348881139204, "HuggingFaceH4/zephyr-7b-beta": 0.9783797518785405, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9866454007445181, "meta-llama/Meta-Llama-3-8B": 0.9039720609586457, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9401817390050472}}, {"question": "\u751f\u4ea7\u5316\u80a5\u7684\u4f01\u4e1a\u53c8\u6295\u8d44\u519c\u836f\u9879\u76ee\uff0c\u8fd9\u79cd\u591a\u5143\u5316\u589e\u957f\u65b9\u5f0f\u5c5e\u4e8e\nA. \u5782\u76f4\u591a\u5143\u5316\nB. \u6c34\u5e73\u591a\u5143\u5316\nC. \u540c\u5fc3\u591a\u5143\u5316\nD. \u96c6\u56e2\u591a\u5143\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3342400036303519, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.35804867662249235, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u7269\u8d28\u53ef\u7528\u4f5c\u751f\u7269\u6218\u5242\u7684\u662f\uff08\uff09\nA. \u7a92\u606f\u6027\u6bd2\u5242\nB. \u4e8c\u6c27\u5316\u78b3\nC. \u70ad\u75bd\u6746\u83cc\nD. \u7cdc\u70c2\u6027\u6bd2\u5242\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5257223981187984, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7030405852819478, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7417470382222742}}, {"question": "\u5df4\u91d1\u7684\u300a\u6000\u5ff5\u8427\u73ca\u300b\u4e00\u6587\uff0c\u51fa\u81ea\u4ed6\u7684\u6563\u6587\u96c6\nA. \u300a\u968f\u60f3\u5f55\u300b\nB. \u300a\u82f1\u96c4\u7684\u6545\u4e8b\u300b\nC. \u300a\u518d\u601d\u5f55\u300b\nD. \u300a\u65b0\u58f0\u96c6\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3354456100429363, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4777312483975283, "meta-llama/Meta-Llama-3-8B": 0.5899675810610455, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6613527492320387}}, {"question": "\u4f01\u4e1a\u5e94\u5f53\u63a5\u53d7\u4e00\u4e2a\u6295\u8d44\u65b9\u6848\uff0c\u53ea\u8981\u5b83\u7684\u56de\u62a5\u7387\nA. \u7b49\u4e8e\u5b83\u7684\u5e73\u5747\u8d44\u91d1\u6210\u672c\nB. \u5927\u4e8e\u5b83\u7684\u8fb9\u9645\u8d44\u91d1\u6210\u672c\nC. \u5927\u4e8e\u5b83\u7684\u5e73\u5747\u8d44\u91d1\u6210\u672c\nD. \u5927\u4e8e\u5b83\u5728\u80a1\u7968\u5e02\u573a\u5c06\u6765\u53ef\u80fd\u7684\u56de\u62a5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.35540643906208824, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5270891094993299}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u4e49\u7406\u4e4b\u5b66\u63cf\u8ff0\u6b63\u786e\u7684\u662f\uff1a\nA. \u8bb2\u6c42\u7ecf\u4e49\u4e0e\u63a2\u7a76\u660e\u7406\u7684\u5b66\u95ee\nB. \u4ee5\u4e0a\u90fd\u6709\nC. \u666e\u904d\u7686\u5b9c\u7684\u5b66\u672f\u9053\u7406\nD. \u8bba\u7a76\u8a00\u8bba\u6587\u7ae0\u53ca\u5176\u5185\u5bb9\u9053\u7406\u7684\u5b66\u95ee\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7699976799111661, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.685656430705724}}, {"question": "\u5728\u6211\u56fd\u65b0\u95fb\u754c\uff0c\u65b0\u95fb\u4ef7\u503c\u8fd9\u4e00\u6982\u5ff5\u6700\u65e9\u662f\u4ece\u7f8e\u56fd\u548c\u65e5\u672c\u5f15\u5165\u7684\u3002\u662f\u8c01\u5f15\u5165\u7684\nA. \u5f90\u5b9d\u749c\nB. \u90b9\u97ec\u594b\nC. \u90b5\u98d8\u840d\nD. \u6208\u516c\u632f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4209626922472216, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.46683855109474853, "meta-llama/Meta-Llama-3-8B": 0.3834792504939983, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4136219774353547}}, {"question": "\u4e0b\u5217\u5bf9\u4e8e\u5b9e\u9a8c\u7684\u76f8\u5173\u53d9\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u201c\u4f4e\u6e29\u8bf1\u5bfc\u67d3\u8272\u4f53\u52a0\u500d\u201d\u5b9e\u9a8c\u4e2d\u9009\u7528\u7d2b\u8272\u6d0b\u8471\u9cde\u7247\u53f6\u7684\u5916\u8868\u76ae\u66f4\u6613\u4e8e\u89c2\u5bdf\nB. \u8272\u7d20\u7684\u63d0\u53d6\u548c\u5206\u79bb\u5b9e\u9a8c\u4e2d\uff0c\u53ef\u7528\u4e19\u916e\u4ee3\u66ff\u65e0\u6c34\u4e59\u9187\u4f5c\u6709\u673a\u6eb6\u5242\u63d0\u53d6\u8272\u7d20\nC. \u201c\u89c2\u5bdfDNA\u548cRNA\u5728\u7ec6\u80de\u4e2d\u7684\u5206\u5e03\u201d\u5b9e\u9a8c\uff0c\u76d0\u9178\u7684\u4f5c\u7528\u662f\u4f7f\u7ec6\u80de\u5206\u6563\u5f00\uff0c\u4fbf\u4e8e\u89c2\u5bdf\nD. \u89c2\u5bdf\u690d\u7269\u7ec6\u80de\u6709\u4e1d\u5206\u88c2\u7684\u5b9e\u9a8c\u64cd\u4f5c\u4e3a\uff1a\u89e3\u79bb\u2192\u67d3\u8272\u2192\u6f02\u6d17\u2192\u5236\u7247\u2192\u89c2\u5bdf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ba1\u7406\u7684\u4e8c\u91cd\u6027\u662f\u6307\nA. \u57fa\u7840\u6027\u4e0e\u666e\u904d\u6027\nB. \u521b\u9020\u6027\u4e0e\u53d8\u9769\u6027\nC. \u79d1\u5b66\u6027\u4e0e\u827a\u672f\u6027\nD. \u81ea\u7136\u6027\u4e0e\u793e\u4f1a\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31740786589267667, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6095080521381203, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9690410123166319}}, {"question": "\u4eba\u5458\u672c\u571f\u5316\u9996\u5148\u8981\u505a\u7684\u662f\u5bf9\u672c\u571f\u5458\u5de5\u7684\u57f9\u8bad\uff0c\u5305\u62ec\u5bf9\u4ed6\u4eec\u4e1a\u52a1\u80fd\u529b\u7684\u57f9\u8bad\u548c\nA. \u4f26\u7406\u89c2\u57f9\u8bad\nB. \u4e16\u754c\u89c2\u57f9\u8bad\nC. \u4ef7\u503c\u89c2\u57f9\u8bad\nD. \u4f01\u4e1a\u6587\u5316\u57f9\u8bad\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3893866500811934, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u793e\u4f1a\u5236\u5ea6\u7684\u6784\u6210\u8981\u7d20\u4e2d\uff0c\u793e\u4f1a\u5236\u5ea6\u8fd0\u884c\u7684\u7269\u8d28\u57fa\u7840\u662f\nA. \u7ec4\u7ec7\u8981\u7d20\nB. \u8bbe\u5907\u8981\u7d20\nC. \u4ef7\u503c\u8981\u7d20\nD. \u89c4\u8303\u8981\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8052153476776515, "meta-math/MetaMath-Mistral-7B": 0.698858328991183, "itpossible/Chinese-Mistral-7B-v0.1": 0.7515162101487689, "HuggingFaceH4/zephyr-7b-beta": 0.5511358654480591, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.618100704978697, "meta-llama/Meta-Llama-3-8B": 0.6774678264487255, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7991656293879681}}, {"question": "\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u78b3\u5355\u4f4d\u6765\u6e90\u7684\u6c28\u57fa\u9178\u662f\nA. \u4e19\u6c28\u9178\nB. \u4e1d\u6c28\u9178\nC. \u7532\u786b\u6c28\u9178\nD. \u4eae\u6c28\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3060136256597631, "meta-math/MetaMath-Mistral-7B": 0.43913053677182107, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.3756998283262494, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.2866128117777278, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u4e8c\u5c16\u74e3\u72ed\u7a84\u5fc3\u5c16\u90e8\u8212\u5f20\u671f\u6742\u97f3\u542c\u8bca\u7279\u70b9\u7684\u53d9\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u5411\u5251\u7a81\u65b9\u5411\u4f20\u5bfc\nB. \u591a\u4e3a\u9012\u589e\u9012\u51cf\u578b\nC. \u5f3a\u5ea6\u4e0d\u53d7\u547c\u5438\u5f71\u54cd\nD. \u4e3a\u5168\u8212\u5f20\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u5386\u53f2\u6545\u4e8b\uff0c\u4e0e\u79e6\u59cb\u7687\u6709\u5173\u7684\u662f\nA. \u6307\u9e7f\u4e3a\u9a6c\nB. \u56fe\u7a77\u5315\u89c1\nC. \u671b\u6885\u6b62\u6e34\nD. \u4e09\u987e\u8305\u5e90\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6496565745267675, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.45072239710256407, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u51fa\u79df\u6c7d\u8f66\u53f8\u673a\u7532\u56e0\u88ab\u5355\u4f4d\u9886\u5bfc\u6279\u8bc4\uff0c\u4e3a\u53d1\u6cc4\u4e0d\u6ee1\u3001\u79c1\u6124\uff0c\u9a7e\u8f66\u9a76\u5165\u95f9\u5e02\u533a\u5411\u5bc6\u96c6\u7684\u4eba\u7fa4\u51b2\u53bb\uff0c\u5f53\u573a\u8f67\u6b7b5\u4eba\uff0c\u649e\u4f2419\u4eba\u3002\u5bf9\u7532\u7684\u884c\u4e3a\u5e94\u8ba4\u5b9a\u4e3a\nA. \u4ee5\u5371\u9669\u65b9\u6cd5\u5371\u5bb3\u516c\u5171\u5b89\u5168\u7f6a\nB. \u4ea4\u901a\u8087\u4e8b\u7f6a\nC. \u6545\u610f\u6740\u4eba\u7f6a\nD. \u91cd\u5927\u8d23\u4efb\u4e8b\u6545\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0d\u5c5e\u4e8e\u843d\u6795\u7684\u75c5\u56e0\u662f\nA. \u9888\u90e8\u6d3b\u52a8\u8fc7\u591a\nB. \u7761\u65f6\u611f\u53d7\u98ce\u5bd2\nC. \u6795\u5934\u8fc7\u4f4e\nD. \u7761\u59ff\u4e0d\u5f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u7684\u8ba4\u8bc6\u7684\u503e\u5411\u6027\u662f\nA. \u5174\u8da3\nB. \u9700\u8981\nC. \u7406\u60f3\nD. \u52a8\u673a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u628a20\u5347\u7684\u7eaf\u725b\u5976\u559d\u63892\u5347\uff0c\u7528\u6c34\u6dfb\u81f3\u548c\u539f\u6765\u4e00\u6837\u591a\uff0c\u5219\u725b\u5976\u6d53\u5ea6\u4e3a\nA. 95%\nB. 85%\nC. 90%\nD. 10%\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3254553863584653, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8043288707383772, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53d8\u538b\u5668\u4e2d\u6027\u70b9\u7ecf\u6d88\u5f27\u7ebf\u5708\u63a5\u5730\u662f\u4e3a\u4e86\nA. \u8865\u507f\u7535\u7f51\u7cfb\u7edf\u5355\u76f8\u63a5\u5730\u65f6\u7684\u7535\u5bb9\u7535\u6d41\nB. \u6d88\u9664\u201c\u6f5c\u4f9b\u7535\u6d41\u201d\nC. \u9650\u5236\u53d8\u538b\u5668\u6545\u969c\u7535\u6d41\nD. \u63d0\u9ad8\u7535\u7f51\u7684\u7535\u538b\u6c34\u5e73\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3994863020756128, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4710683612826208}}, {"question": "\u4f01\u4e1a\u6309\u7167\u6d88\u8d39\u8005\u7684\u5e74\u9f84\u3001\u6027\u522b\u3001\u6536\u5165\u3001\u804c\u4e1a\u3001\u53d7\u6559\u80b2\u7a0b\u5ea6\u7b49\u56e0\u7d20\u5bf9\u6d88\u8d39\u8005\u5e02\u573a\u8fdb\u884c\u7ec6\u5206\uff0c\u8fd9\u5c5e\u4e8e\nA. \u5730\u7406\u7ec6\u5206\nB. \u884c\u4e3a\u7ec6\u5206\nC. \u4eba\u53e3\u7ec6\u5206\nD. \u5fc3\u7406\u7ec6\u5206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.852997018913579, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.9383336739449903, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5501609761495956, "meta-llama/Meta-Llama-3-8B": 0.5326190845563334, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7582001842621149}}, {"question": "2011\u5e743\u6708\uff0c\u4e2d\u56fd\u652f\u6301\u5b89\u7406\u4f1a\u91c7\u53d6\u9002\u5f53\u548c\u5fc5\u8981\u7684\u884c\u52a8\u7a33\u5b9a\u5229\u6bd4\u4e9a\u5c40\u52bf\uff0c\u4f46\u5bf9\u652f\u6301\u5b89\u7406\u4f1a\u51b3\u8bae\u90e8\u5206\u5185\u5bb9\u6709\u56f0\u96be\uff0c\u56e0\u6b64\u6295\u4e86\u5f03\u6743\u7968\uff1b2012\u5e742\u6708\uff0c\u5b89\u7406\u4f1a\u5c31\u53d9\u5229\u4e9a\u95ee\u9898\u51b3\u8bae\u8349\u6848\u8fdb\u884c\u8868\u51b3\uff0c\u8003\u8651\u5230\u63d0\u6848\u56fd\u5728\u5404\u65b9\u4ecd\u6709\u4e25\u91cd\u5206\u6b67\u7684\u60c5\u51b5\u4e0b\u5f3a\u884c\u63a8\u52a8\u8868\u51b3\u65e0\u52a9\u4e8e\u7ef4\u62a4\u5b89\u7406\u4f1a\u7684\u56e2\u7ed3\u548c\u6743\u5a01\uff0c\u65e0\u52a9\u4e8e\u95ee\u9898\u7684\u59a5\u5584\u89e3\u51b3\uff0c\u4e2d\u56fd\u6295\u4e86\u53cd\u5bf9\u7968\u3002\u8fd9\u8868\u660e\u4e2d\u56fd\u5728\u65b0\u65f6\u671f\uff1aa\u5f00\u5c55\u4ee5\u8054\u5408\u56fd\u4e3a\u4e2d\u5fc3\u7684\u5916\u4ea4\u6d3b\u52a8\uff1bb\u79ef\u6781\u7ef4\u62a4\u5730\u533a\u7a33\u5b9a\u548c\u4e16\u754c\u548c\u5e73\uff1bc\u53c2\u4e0e\u7b26\u5408\u8054\u5408\u56fd\u5baa\u7ae0\u7684\u7ef4\u548c\u884c\u52a8\uff1bd\u5949\u884c\u72ec\u7acb\u81ea\u4e3b\u7684\u548c\u5e73\u5916\u4ea4\u653f\u7b56\nA. bd\nB. abc\nC. abd\nD. ad\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3564675601642158, "meta-math/MetaMath-Mistral-7B": 0.34031470637689565, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4160513129390438, "meta-llama/Meta-Llama-3-8B": 0.44432870984215717, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.36617644066136085}}, {"question": "\u516c\u5143\u524d1046\u5e74\uff0c\u6b66\u738b\u7387\u5175\u5728\u7267\u91ce\u6253\u8d25\u5546\u519b\uff0c\u5efa\u7acb\u5468\u671d\u3002\u8fd9\u4e00\u5e74\u5904\u5728\nA. \u516c\u5143\u524d10\u4e16\u7eaa\u4e0b\u534a\u53f6\nB. \u516c\u5143\u524dl1\u4e16\u7eaa\u4e0b\u534a\u53f6\nC. \u516c\u5143\u524d11\u4e16\u7eaa\u4e0a\u534a\u53f6\nD. \u516c\u5143\u524d10\u4e16\u7eaa\u4e0a\u534a\u53f6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6309\u7528\u9014\u548c\u7ed3\u6784\u5206\u7c7b\uff0c\u201c\u5728\u9014\u7269\u8d44\u201d\u8d26\u6237\u5c5e\u4e8e\nA. \u8c03\u6574\u8d26\u6237\nB. \u8d44\u4ea7\u8d26\u6237\nC. \u6210\u672c\u8ba1\u7b97\u8d26\u6237\nD. \u76d8\u5b58\u8d26\u6237\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ea4\u611f\u795e\u7ecf\u7684\u4f4e\u7ea7\u4e2d\u67a2\u4f4d\u4e8e\u810a\u9ad3\u7684\nA. \u98881-\u80f83\u8282\nB. \u81701-\u9ab62\u8282\nC. \u9ab62-4\u8282\nD. \u80f81-\u81703\u8282\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u5b50\uff0c\u540d\u8bcd\u7528\u4f5c\u4e00\u822c\u52a8\u8bcd\u7684\u4e00\u53e5\u662f\nA. \u8346\u4eba\u6b32\u8972\u5b8b\uff0c\u4f7f\u4eba\u5148\u8868\u6ee9\u6c34\u3002\nB. \u4e94\u755d\u4e4b\u5b85\uff0c\u6a39\u4e4b\u4ee5\u6851\u3002\nC. \u5f97\u8005\u4e0d\u6562\u8f12\u98df\uff0c\u4e43\u6b77\u6578\u5bb6\u3002\nD. \u6b64\u4e0d\u77e5\u5176\u7f6a\u800c\u6b7b\uff0c\u81e3\u7232\u541b\u6578\u4e4b\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3060136256597631, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4713948540807262}}, {"question": "\u8bbe\u6709\u4e24\u4e2a\u51cf\u56e0\uff0c\u5176\u8870\u51cf\u529b\u5747\u4e3a\u5e38\u6570\uff0c\u4e14$q_x^{(1)}=q_x^{(2)}=\\frac{12}{49}$\uff0c\u5219\u8054\u5408\u5355\u6de2\u56e0\u6a21\u578b\u4e2d\u7684$q_x^{\\prime(1)}=$( )\u3002\nA. $2 / 7$\nB. $2 / 5$\nC. $3 / 7$\nD. $3 / 5$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3528619536675028, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4760074206231705, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.32479161563275305, "meta-llama/Meta-Llama-3-8B": 0.2801288226217134, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b66\u4e60\"\u5706\u7684\u76f4\u5f84\u662f\u5b83\u7684\u534a\u5f84\u7684\u4e24\u500d\u301e\u662f\u3002\nA. \u7b26\u53f7\u5b66\u4e60\nB. \u4e0b\u4f4d\u5b66\u4e60\nC. \u547d\u9898\u5b66\u4e60\nD. \u6982\u5ff5\u5b66\u4e60\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c56\u5c81\uff0c2\u5929\u524d\u7a81\u53d1\u6301\u7eed\u4e0a\u8179\u75db\uff0c\u9635\u53d1\u52a0\u5267\uff0c\u5e76\u8170\u80cc\u90e8\u80c0\u75db\uff0c\u6076\u5fc3\uff0c\u5455\u5410\uff0c\u6025\u8bca\u5165\u9662\u3002\u65e2\u5f80\u6709\u80c6\u56ca\u7ed3\u77f3\u75c5\u53f23\u5e74\uff0c\u67e5\u4f53\uff1aT36.9\u2103\uff0cP104\u6b21\u6bcf\u5206\uff0cR20\u6b21\u6bcf\u5206\uff0cBP132/82mmHg\uff0c\u5de9\u819c\u65e0\u9ec4\u67d3\uff0c\u4e0a\u8179\u8f83\u5f6d\u9686\uff0c\u538b\u75db\uff0c\u8f7b\u5ea6\u808c\u7d27\u5f20\u53ca\u53cd\u8df3\u75db\uff0c\u80a0\u9e23\u97f3\u5f31\u3002\u5316\u9a8c\uff1aHb128g/L\uff0cWBC16.7\u00d7109/L\uff0c\u8840\u6dc0\u7c89\u9176786U/L\uff0c\u5c3f\u6dc0\u7c89\u91761600U/L.\u4e3a\u660e\u786e\u8bca\u65ad\uff0c\u6700\u6709\u6548\u7684\u68c0\u67e5\u65b9\u6cd5\u662f\nA. \u8179\u90e8X\u7ebf\u7247\nB. \u8179\u90e8CT\nC. ERCP\nD. \u8179\u90e8B\u8d85\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.40417522991757476, "meta-llama/Meta-Llama-3-8B": 0.45912274086952476, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8803148418234701}}, {"question": "\u57fa\u7763\u6559\u4f1a\u6839\u636e\u300a\u5723\u7ecf\u300b\u4e2d\u5173\u4e8e\u519c\u7267\u4ea7\u54c1\u7684\u5341\u5206\u4e4b\u4e00\u201c\u5c5e\u4e8e\u4e0a\u5e1d\u201d\u7684\u8bf4\u6cd5\u5411\u4fe1\u6559\u7684\u6c11\u4f17\u6536\u53d6\nA. \u4f9b\u7a0e\nB. \u9644\u52a0\u7a0e\nC. \u4ec0\u4e00\u7a0e\nD. \u519c\u4ea7\u54c1\u7a0e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7681202491227135, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9871071751836547, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9968750641145591}}, {"question": "\u8457\u540d\u5bfc\u6f14\u9648\u51ef\u6b4c\u7684\u7535\u5f71\u4ee3\u8868\u662f\nA. \u300a\u8299\u84c9\u9547\u300b\nB. \u300a\u7ea2\u9ad8\u7cb1\u300b\nC. \u300a\u9738\u738b\u522b\u59ec\u300b\nD. \u300a\u591c\u5bb4\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37754687503072853, "meta-math/MetaMath-Mistral-7B": 0.5175034254922011, "itpossible/Chinese-Mistral-7B-v0.1": 0.7631055143496998, "HuggingFaceH4/zephyr-7b-beta": 0.8073348881139203, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45352234650356354, "meta-llama/Meta-Llama-3-8B": 0.6274250672970438, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.49929075513756205}}, {"question": "\u4f01\u4e1a\u5728\u8425\u9500\u4e2d\u4fe1\u5b88\u8bfa\u8a00\uff0c\u5c65\u884c\u5408\u7ea6\uff0c\u907f\u514d\u6b3a\u9a97\u548c\u8bef\u5bfc\u6027\u5ba3\u4f20\uff0c\u5bf9\u4e8e\u8fc7\u5931\u4e88\u4ee5\u8865\u6551\uff0c\u4f7f\u4ea7\u54c1\u6216\u670d\u52a1\u9002\u5408\u6d88\u8d39\u8005\u7684\u9884\u671f\u8981\u6c42\uff0c\u5728\u7f57\u65af\u63d0\u51fa\u7684\u201c\u663e\u8981\u4e49\u52a1\u201d\u4e2d\u6307\u7684\u662f\nA. \u8bda\u5b9e\nB. \u611f\u6069\nC. \u884c\u5584\nD. \u516c\u6b63\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.888942450793241, "meta-math/MetaMath-Mistral-7B": 0.9956112757251334, "itpossible/Chinese-Mistral-7B-v0.1": 0.9394451237383601, "HuggingFaceH4/zephyr-7b-beta": 0.9998144590169615, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9744072984240499, "meta-llama/Meta-Llama-3-8B": 0.7998655551252825, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9195300214355251}}, {"question": "\u7ecf\u6d4e\u5229\u6da6\u662f\u6307\nA. \u4f01\u4e1a\u603b\u6536\u5165\u51cf\u53bb\u4f1a\u8ba1\u5229\u6da6\nB. \u9500\u552e\u6536\u5165\u51cf\u53bb\u4f1a\u8ba1\u5229\u6da6\nC. \u4f01\u4e1a\u603b\u6536\u5165\u51cf\u53bb\u4f1a\u8ba1\u6210\u672c\nD. \u9500\u552e\u6536\u5165\u51cf\u53bb\u673a\u4f1a\u6210\u672c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6297\u6218\u7206\u53d1\u540e\uff0c\u56fd\u6c11\u515a\u5185\u4e00\u90e8\u5206\u4eba\u8ba4\u4e3a\u201c\u4e2d\u56fd\u6b66\u5668\u4e0d\u5982\u4eba\uff0c\u6218\u5fc5\u8d25\uff0c\u518d\u6218\u5fc5\u4ea1\u201d\uff1b\u8fd8\u6709\u4e00\u90e8\u5206\u4eba\u5bc4\u5e0c\u671b\u4e8e\u5916\u90e8\u5e72\u6d89\uff0c\u8ba4\u4e3a\u201c\u53ea\u8981\u575a\u6301\u6253\u4e09\u4e2a\u6708\uff0c\u56fd\u9645\u5c40\u52bf\u4e00\u5b9a\u4f1a\u53d1\u751f\u53d8\u5316\u201d\u3002\u5728\u4e2d\u56fd\u5171\u4ea7\u515a\u5185\uff0c\u4e5f\u6709\u5c11\u6570\u4eba\u5bf9\u6218\u4e89\u7684\u957f\u671f\u6027\u548c\u8270\u82e6\u6027\u7f3a\u4e4f\u51c6\u786e\u5224\u65ad\u548c\u7cbe\u795e\u51c6\u5907\u3002\u4e3a\u6b64\uff0c\u6bdb\u6cfd\u4e1c\u53d1\u8868\u4e86\nA. \u300a\u8bba\u6301\u4e45\u6218\u300b\nB. \u300a\u8bba\u8054\u5408\u653f\u5e9c\u300b\nC. \u300a\u5bf9\u65e5\u5bc7\u7684\u6700\u540e\u4e00\u6218\u300b\nD. \u300a\u5173\u4e8e\u6b63\u786e\u5904\u7406\u4eba\u6c11\u5185\u90e8\u77db\u76fe\u7684\u95ee\u9898\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8506102703540974, "meta-math/MetaMath-Mistral-7B": 0.9968699956471508, "itpossible/Chinese-Mistral-7B-v0.1": 0.8346126184670675, "HuggingFaceH4/zephyr-7b-beta": 0.9978814305770533, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9392918319425011, "meta-llama/Meta-Llama-3-8B": 0.975425806576939, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9712792751221231}}, {"question": "\u5217\u7269\u8d28\u7684\u4f7f\u7528\u4e0d\u6d89\u53ca\u5316\u5b66\u53d8\u5316\u7684\u662f\nA. \u660e\u77fe\u7528\u4f5c\u51c0\u6c34\u5242\nB. \u751f\u77f3\u7070\u4f5c\u5e72\u71e5\u5242\nC. \u6db2\u6c28\u7528\u4f5c\u5236\u51b7\u5242\nD. \u6c22\u6c1f\u9178\u523b\u8680\u73bb\u7483\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u543e\u7231\u543e\u5e08\uff0c\u543e\u66f4\u7231\u771f\u7406\u3002\u201d\u51fa\u81ea\u54ea\u4f4d\u54f2\u5b66\u5bb6\u53e3\u4e2d\uff1f\nA. \u67cf\u62c9\u56fe\nB. \u82cf\u683c\u62c9\u5e95\nC. \u9ed1\u683c\u5c14\nD. \u4e9a\u91cc\u58eb\u591a\u5fb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6309\u7167\u4f20\u7edf\u7684\u201c\u516d\u4e66\u201d\u4f53\u4f8b\uff0c\u201c\u54c0\u201d\u5b57\u5e94\u5c5e\nA. \u8c61\u5f62\nB. \u6307\u4e8b\nC. \u4f1a\u610f\nD. \u5f62\u58f0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.34686667406054084, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4182952272499101, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5646804920991356}}, {"question": "\u4ee5\u4e0b\u4e0d\u5c5e\u4e8e\u4e00\u4e2a\u79d1\u5b66\u7684\u5b66\u4e1a\u751f\u6daf\u89c4\u5212\u5e94\u8be5\u6ee1\u8db3\u7684\u6807\u51c6\u7684\u662f\nA. \u672c\u4eba\u4e50\u610f\u5168\u8eab\u5fc3\u6295\u5165\nB. \u672c\u4eba\u70ed\u5207\u5e0c\u671b\u7684\u5b66\u4e1a\u751f\u6daf\u89c4\u5212\nC. \u4e0d\u8fdd\u6cd5\u4e71\u7eaa\uff0c\u4e0d\u8fdd\u80cc\u9053\u5fb7\nD. \u4e0e\u957f\u8fdc\u89c4\u5212\u6ca1\u6709\u8054\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.889410549389542, "meta-math/MetaMath-Mistral-7B": 0.9870659734870525, "itpossible/Chinese-Mistral-7B-v0.1": 0.8904414076390536, "HuggingFaceH4/zephyr-7b-beta": 0.9998966070827052, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9800332332401328, "meta-llama/Meta-Llama-3-8B": 0.9517984232274106, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9075203009119951}}, {"question": "\u53d1\u8fbe\u56fd\u5bb6\u2f08\u2f1d\u51fa\u2f63\u7387\u8f83\u4f4e\uff0c\u5173\u4e8e\u5176\u539f\u56e0\u7684\u6b63\u786e\u53d9\u8ff0\u662f\nA. \u65e9\u5a5a\u4e4b\u2edb\u76db\u2f8f\uff0c\u521d\u5a5a\u5e74\u9f84\u504f\u4f4e\nB. \u5d07\u5c1a\u591a\u80b2\u591a\u2f26\u3001\u91cd\u7537\u8f7b\u2f25\u7684\u4f20\u7edf\u89c2\u5ff5\nC. \u53d1\u8fbe\u56fd\u5bb6\u7684\u5b97\u6559\u2f00\u822c\u90fd\u53cd\u5bf9\u2f08\u2f1d\u589e\u6b96\nD. \u5a5a\u59fb\u5173\u7cfb\u4e0d\u7a33\u5b9a\uff0c\u665a\u5a5a\u3001\u4e0d\u5a5a\u3001\u4e0d\u80b2\u73b0\u8c61\u591a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7764516480648214, "meta-math/MetaMath-Mistral-7B": 0.8975436024511466, "itpossible/Chinese-Mistral-7B-v0.1": 0.8165867624675759, "HuggingFaceH4/zephyr-7b-beta": 0.9829728956315346, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8383320095843005, "meta-llama/Meta-Llama-3-8B": 0.8048645158983191, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe\u5f53 $x \\rightarrow 0$ \u65f6\uff0c $\\arctan x-(ax+bx^3)$ \u662f\u6bd4 $x(1-\\cos x)$ \u9ad8\u9636\u7684\u65e0\u7a77\u5c0f\u91cf\uff0c \u5219\uff08\uff09\nA. $a=1\uff0c b=-\\frac{1}{6}$\nB. $a=1\uff0c b=-\\frac{1}{3}$\nC. $a=1\uff0c b=\\frac{1}{3}$\nD. $a=1\uff0c b=\\frac{1}{6}$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.29863342676099575}}, {"question": "\u6fc0\u5149\u5236\u5bfc\u70b8\u5f39\u6700\u5148\u5728\u54ea\u573a\u6218\u4e89\u4e2d\u88ab\u4f7f\u7528\uff1f\nA. \u6d77\u6e7e\u6218\u4e89\nB. \u7b2c\u56db\u6b21\u4e2d\u4e1c\u6218\u4e89\nC. \u671d\u9c9c\u6218\u4e89\nD. \u8d8a\u5357\u6218\u4e89\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3492104466579257, "meta-math/MetaMath-Mistral-7B": 0.46683856593036355, "itpossible/Chinese-Mistral-7B-v0.1": 0.3148300531811561, "HuggingFaceH4/zephyr-7b-beta": 0.851974764972423, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.40067967579776836, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f8e\u56fd\u91c7\u53d6\u6743\u529b\u5236\u8861\u7684\u7ec4\u7ec7\u5f62\u5f0f\uff0c\u5176\u4e2d\u7acb\u6cd5\u6743\u5c5e\u4e8e\nA. \u56fd\u4f1a\nB. \u6700\u9ad8\u68c0\u5bdf\u9662\nC. \u6700\u9ad8\u6cd5\u9662\nD. \u603b\u7edf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.886038489453499, "itpossible/Chinese-Mistral-7B-v0.1": 0.7927968284673035, "HuggingFaceH4/zephyr-7b-beta": 0.9780002919785495, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8929304744872081, "meta-llama/Meta-Llama-3-8B": 0.9586384559268841, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9064355492662831}}, {"question": "\u6211\u56fd\u79ef\u6781\u53c2\u4e0e\u7ecf\u6d4e\u5168\u7403\u5316\u7684\u91cd\u8981\u9014\u5f84\u662f\u5efa\u7acb\u8de8\u56fd\u516c\u53f8\uff0c\u8fd9\u8868\u660e\u8de8\u56fd\u516c\u53f8\nA. \u5df2\u53d6\u4ee3\u5404\u56fd\u6216\u5730\u533a\u4e4b\u95f4\u7684\u8fdb\u51fa\u53e3\u8d38\u6613\nB. \u662f\u8fdb\u884c\u56fd\u9645\u8d38\u6613\u7684\u5927\u516c\u53f8\nC. \u662f\u7ecf\u6d4e\u5168\u7403\u5316\u8fc5\u731b\u53d1\u5c55\u7684\u6839\u672c\u539f\u56e0\nD. \u662f\u4ece\u4e8b\u56fd\u9645\u5316\u751f\u4ea7\u548c\u7ecf\u8425\u7684\u5927\u4f01\u4e1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8106981461848106, "meta-math/MetaMath-Mistral-7B": 0.9527756513294718, "itpossible/Chinese-Mistral-7B-v0.1": 0.6635567847237877, "HuggingFaceH4/zephyr-7b-beta": 0.9991290659186327, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8696703584684062, "meta-llama/Meta-Llama-3-8B": 0.9723725765703626, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.954516073442274}}, {"question": "\u5728Word\u7a97\u53e3\u5de5\u4f5c\u533a\u4e2d\uff0c\u95ea\u70c1\u7684\u5782\u76f4\u5149\u6761\u8868\u793a\nA. \u6807\u9898\u7684\u4f4d\u7f6e\nB. \u63d2\u5165\u70b9\u7684\u4f4d\u7f6e\nC. \u952e\u76d8\u7684\u4f4d\u7f6e\nD. \u9f20\u6807\u7684\u4f4d\u7f6e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6614654544709192, "meta-math/MetaMath-Mistral-7B": 0.6708737032475309, "itpossible/Chinese-Mistral-7B-v0.1": 0.7987683722169249, "HuggingFaceH4/zephyr-7b-beta": 0.9868738651707435, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8915150843649884, "meta-llama/Meta-Llama-3-8B": 0.6575142593399674, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9359455986372723}}, {"question": "\u8861\u91cf\u98df\u4e0d\u8fc7\u91cf\u7684\u6700\u597d\u7684\u6307\u6807\u4e3a\nA. \u4f53\u91cd\nB. \u7cd6\u5c3f\u75c5\u7684\u53d1\u75c5\u7387\nC. \u9ad8\u8840\u8102\u7684\u53d1\u751f\u7387\nD. \u80fd\u91cf\u7684\u63a8\u8350\u6444\u5165\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u4e2a\u540c\u6e90\u4e09\u500d\u4f53\u7684\u5355\u5f0f\u6742\u5408\u4f53(Aaa)\uff0c\u5982\u679c\u6309\u67d3\u8272\u4f53\u968f\u673a\u5206\u79bb\uff0c\u4e0d\u8003\u8651\u914d\u5b50\u80b2\u6027\u7b49\u5176\u5b83\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u5f62\u6210a\u914d\u5b50\u7684\u6bd4\u4f8b\u4e3a\nA. 1/3\nB. 1/12\nC. 1/6\nD. 1/4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3300364758848944, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4218340807091267}}, {"question": "\u63cf\u8ff0\u9519\u8bef\u7684\u662f\nA. \u4e00\u4e2a\u4eba\u8f6c\u8eab\u53ef\u4ee5\u8bf4\u5176\u5728\u6c34\u5e73\u9762\u8fd0\u52a8\nB. \u4e00\u4e2a\u4eba\u524d\u884c\u53ef\u4ee5\u8bf4\u5176\u5728\u6c34\u5e73\u9762\u8fd0\u52a8\nC. \u63cf\u8ff0\u4eba\u4f53\u7a7a\u95f4\u5185\u7684\u8fd0\u52a8\uff0c\u901a\u5e38\u7528\u77e2\u72b6\u9762\u3001\u51a0\u72b6\u9762\u3001\u6c34\u5e73\u9762\u6765\u8868\u793a\nD. \u63cf\u8ff0\u4eba\u4f53\u7a7a\u95f4\u8fd0\u52a8\u7684\u4e09\u4e2a\u5e73\u9762\u4e92\u76f8\u95f4\u5448\u5782\u76f4\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbeA\u4e3an\u9636\u65b9\u9635\uff0c \u4e14 $|A| \\neq 0$\uff0c \u5219\uff08\uff09.\nA. \u4ee5\u4e0a\u90fd\u4e0d\u5bf9\nB. \u7531AX=BA\uff0c \u53ef\u5f97X=B\nC. \u5f53$(A\uff0cE)$\u7ecf\u6709\u9650\u6b21\u521d\u7b49\u53d8\u6362\u53d8\u4e3a$(E\uff0cB)$\u65f6\uff0c \u6709$A^{-1}=B$\nD. A\u7ecf\u521d\u7b49\u5217\u53d8\u6362\u53ef\u53d8\u4e3a\u5355\u4f4d\u9635$E$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u6545\u610f\u6740\u4eba\u540e\u754f\u7f6a\u6f5c\u9003\uff0c\u5f52\u6848\u540e\u6cd5\u9662\u4f9d\u6cd5\u5224\u5904\u7532\u65e0\u671f\u5f92\u5211\uff0c\u5265\u593a\u653f\u6cbb\u6743\u5229\u7ec8\u8eab\u3002\u6839\u636e\u6cd5\u7684\u4f5c\u7528\u7406\u8bba\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u6cd5\u9662\u7684\u5224\u51b3\u65e2\u4f53\u73b0\u4e86\u6cd5\u7684\u89c4\u8303\u4f5c\u7528\uff0c\u4e5f\u4f53\u73b0\u4e86\u6cd5\u7684\u793e\u4f1a\u4f5c\u7528\nB. \u7532\u754f\u7f6a\u6f5c\u9003\u4f53\u73b0\u4e86\u6cd5\u7684\u6307\u5f15\u4f5c\u7528\nC. \u6709\u4eba\u8ba4\u4e3a\u7532\u754f\u7f6a\u6f5c\u9003\u5e94\u7f6a\u52a0\u4e00\u7b49\uff0c\u8be5\u89c2\u70b9\u4f53\u73b0\u4e86\u6cd5\u7684\u9884\u6d4b\u800c\u975e\u8bc4\u4ef7\u4f5c\u7528\nD. \u6cd5\u9662\u7684\u5224\u51b3\u4f53\u73b0\u4e86\u6cd5\u7684\u5f3a\u5236\u4f5c\u7528\uff0c\u800c\u975e\u6cd5\u7684\u6559\u80b2\u4f5c\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6079137175595951, "itpossible/Chinese-Mistral-7B-v0.1": 0.34478590299221606, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7514749964572092, "meta-llama/Meta-Llama-3-8B": 0.8383749405950683, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7592652503621798}}, {"question": "\u767d\u732b\u4e0e\u9ed1\u732b\u4ea4\u914d\uff0cF1\u90fd\u662f\u767d\u732b\u3002F1\u76f8\u4e92\u4ea4\u914d\uff0cF2\u4e2d\u767d\u3001\u9ed1\u3001\u68d5\u4e09\u79cd\u5c0f\u732b\u7684\u6bd4\u4f8b\u4e3a12:3:1\uff0c\u662f\u4e0b\u5217\u54ea\u4e00\u79cd\u539f\u56e0\u5f15\u8d77\u7684\nA. \u663e\u6027\u4e0a\u4f4d\u4f5c\u7528\nB. \u9690\u6027\u4e0a\u4f4d\u4f5c\u7528\nC. \u6291\u5236\u4f5c\u7528\nD. \u4e92\u8865\u4f5c\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5823334666195904, "itpossible/Chinese-Mistral-7B-v0.1": 0.32214924591241917, "HuggingFaceH4/zephyr-7b-beta": 0.998822851626713, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6144193877589753, "meta-llama/Meta-Llama-3-8B": 0.4167500531591177, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53\u81ea\u5df1\u7684\u5408\u6cd5\u6743\u5229\u53d7\u5230\u4e0d\u6cd5\u884c\u4e3a\u4fb5\u5bb3\u65f6\uff0c\u516c\u6c11\u5e94\u5148\u9009\u62e9\nA. \u8fd0\u7528\u6cd5\u5f8b\u6b66\u5668\u3001\u5bfb\u6c42\u53f8\u6cd5\u6551\u6d4e\nB. \u4f9d\u9760\u597d\u53cb\u5e2e\u52a9\u3001\u5bfb\u6c42\u79c1\u529b\u6551\u6d4e\nC. \u8fd0\u7528\u9053\u5fb7\u8c34\u8d23\u3001\u5bfb\u6c42\u8206\u8bba\u6551\u6d4e\nD. \u4f9d\u9760\u5355\u4f4d\u652f\u6301\u3001\u5bfb\u6c42\u7ec4\u7ec7\u6551\u6d4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9134117960432512, "meta-math/MetaMath-Mistral-7B": 0.9964373756381338, "itpossible/Chinese-Mistral-7B-v0.1": 0.9892044069161876, "HuggingFaceH4/zephyr-7b-beta": 0.9999715861667257, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9919852999116346, "meta-llama/Meta-Llama-3-8B": 0.8383749264610662, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.996055298583606}}, {"question": "\u201c\u7ed3\u53d1\u201d\u5728\u53e4\u65f6\u662f\u6307\u7ed3\u5a5a\u65f6\nA. \u59bb\u5b50\u628a\u5934\u53d1\u675f\u8d77\u6765\nB. \u592b\u59bb\u5206\u522b\u628a\u5934\u53d1\u675f\u8d77\u6765\nC. \u4e08\u592b\u628a\u5934\u53d1\u675f\u8d77\u6765\nD. \u628a\u592b\u59bb\u5934\u53d1\u675f\u5728\u4e00\u8d77\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4478423674559495, "HuggingFaceH4/zephyr-7b-beta": 0.9919464234721448, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.42886486864995427, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0e\u4e1d\u7ef8\u6469\u64e6\u540e\u7684\u73bb\u7483\u68d2\u5e26\u4e0a\u4e86\u6b63\u7535\u8377\uff0c\u8fd9\u662f\u56e0\u4e3a\nA. \u4e1d\u7ef8\u4e0a\u6709\u4e9b\u7535\u5b50\u8f6c\u79fb\u5230\u4e86\u73bb\u7483\u68d2\u4e0a\nB. \u73bb\u7483\u68d2\u4e0a\u6709\u4e9b\u7535\u5b50\u8f6c\u79fb\u5230\u4e86\u4e1d\u7ef8\u4e0a\nC. \u73bb\u7483\u68d2\u4e0a\u6709\u4e9b\u6b63\u7535\u8377\u8f6c\u79fb\u5230\u4e86\u4e1d\u7ef8\u4e0a\nD. \u4e1d\u7ef8\u4e0a\u6709\u4e9b\u6b63\u7535\u8377\u8f6c\u79fb\u5230\u4e86\u73bb\u7483\u68d2\u4e0a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ed6\u5f8b\u4e0e\u81ea\u5f8b\u7684\u7edf\u4e00\u5b9e\u8d28\u4e0a\u662f\nA. \u516c\u5fb7\nB. \u9053\u5fb7\nC. \u6cd5\u5f8b\nD. \u4f26\u7406\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8fc7\u5ea6\u81ea\u4fe1\u662f\u6307\u4e2a\u4f53\u57fa\u4e8e\u4e8b\u7269\u672a\u6765\u53d1\u5c55\u8fc7\u9ad8\u5730\u4f30\u8ba1\u81ea\u8eab\u5224\u65ad\u7684\u7cbe\u786e\u5ea6\uff0c\u8fdb\u800c\u504f\u79bb\u6821\u51c6\u7684\u4e00\u79cd\u5f62\u5f0f\u3002\u8fd9\u91cc\u7684\u6821\u51c6\u7a0b\u5ea6\u53d6\u51b3\u4e8e\u4fe1\u5fc3\u4e0e\u76f8\u5173\u76ee\u6807\u4e8b\u4ef6\u53d1\u751f\u7684\u76f8\u5bf9\u9891\u7387\u7684\u5339\u914d\u7a0b\u5ea6\u3002\u6839\u636e\u4e0a\u8ff0\u5b9a\u4e49\uff0c\u4e0b\u5217\u5c5e\u4e8e\u8fc7\u5ea6\u81ea\u4fe1\u7684\u662f\nA. \u67d0\u80a1\u7968\u5206\u6790\u5e08\u9884\u6d4b\u80a1\u7968\u6307\u6570\u4f1a\u4e0a\u6da8\uff0c\u7ed3\u679c\u80a1\u7968\u5374\u4e0b\u8dcc\u4e86\nB. \u67d0\u516c\u53f8\u6839\u636e\u76f8\u5173\u7edf\u8ba1\u6570\u636e\u8c03\u9ad8\u4e86\u4e0a\u5b63\u5ea6\u7684\u8425\u6536\u589e\u957f\u7387\nC. \u67d0\u8db3\u7403\u8bc4\u8bba\u5458\u8bf4\u660e\u5929\u7684\u6bd4\u8d5b\u7532\u961f\u83b7\u80dc\u7684\u53ef\u80fd\u6027\u662f80%\uff0c\u7136\u800c\u4ee5\u5f80\u6570\u636e\u663e\u793a\uff0c\u8be5\u8bc4\u8bba\u5458\u7684\u9884\u6d4b\u51c6\u786e\u7387\u4ec5\u4e3a30%\nD. \u5c0f\u5f20\u8ba4\u4e3a\u81ea\u5df1\u671f\u672b\u8003\u8bd5\u80fd\u800390\u5206\u4ee5\u4e0a\uff0c\u7ed3\u679c\u5374\u53ea\u8003\u4e8670\u5206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df4\u65af\u5c06\u4f20\u64ad\u5a92\u4ecb\u7684\u628a\u5173\u884c\u52a8\u5206\u4e3a\u524d\u540e\u76f8\u8fde\u7684\u4e24\u4e2a\u9636\u6bb5\uff0c\u5176\u4e2d\u7b2c\u4e8c\u9636\u6bb5\u7684\u628a\u5173\u4eba\u4e3b\u8981\u662f\nA. \u65b0\u95fb\u63d0\u4f9b\u8005\nB. \u7279\u7ea6\u8bc4\u8bba\u5458\nC. \u8bb0\u8005\nD. \u7f16\u8f91\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5666420019997775, "meta-math/MetaMath-Mistral-7B": 0.6906256970246039, "itpossible/Chinese-Mistral-7B-v0.1": 0.7536686356854363, "HuggingFaceH4/zephyr-7b-beta": 0.9989558324184786, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9759640628365989, "meta-llama/Meta-Llama-3-8B": 0.6912943320436838, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u5b66\u751f\u7684\u60c5\u7eea\u7279\u70b9\u662f\nA. \u60c5\u7eea\u7684\u4e24\u6781\u6027\u51cf\u5c11\nB. \u60c5\u7eea\u60c5\u611f\u7684\u793e\u4f1a\u6027\u6210\u5206\u5f00\u59cb\u589e\u52a0\nC. \u60c5\u7eea\u7684\u51b2\u52a8\u6027\u51cf\u5f31\nD. \u60c5\u7eea\u6fc0\u8361\uff0c\u6613\u52a8\u611f\u60c5\uff0c\u4e5f\u5bb9\u6613\u88ab\u6fc0\u60f9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4553706119235167, "meta-math/MetaMath-Mistral-7B": 0.4479780633493925, "itpossible/Chinese-Mistral-7B-v0.1": 0.6248486901872963, "HuggingFaceH4/zephyr-7b-beta": 0.9900333621422985, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8602231425537237, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8569448226302793}}, {"question": "\u4e00\u4e2a\u4eba\u5728\u56f0\u96be\u548c\u632b\u6298\u9762\u524d\u4e0d\u6c14\u9981\u3001\u4e0d\u9000\u7f29\uff0c\u59cb\u7ec8\u62b1\u5b9a\u4e00\u79cd\u79ef\u6781\u8fdb\u53d6\u3001\u4e50\u89c2\u5411\u4e0a\u7684\u4eba\u751f\u6001\u5ea6\uff0c\u662f\u56e0\u4e3a\u5177\u6709\u660e\u786e\u7684\nA. \u4eba\u751f\u76ee\u7684\nB. \u4eba\u751f\u4ef7\u503c\nC. \u4eba\u751f\u89c4\u5212\nD. \u4eba\u751f\u72b6\u6001\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4678003933087132, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.534381655142306, "meta-llama/Meta-Llama-3-8B": 0.6280021468755848, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u53d1\u5c55\u7ecf\u6d4e\u4e0d\u5e94\u4ee5\u7834\u574f\u73af\u5883\u4e3a\u4ee3\u4ef7\u201d\u5df2\u7ecf\u6210\u4e3a\u4eba\u7c7b\u7684\u5171\u8bc6\uff0c\u8981\u4fdd\u62a4\u751f\u6001\u7cfb\u7edf\uff0c\u5e94\u8d70\u4e00\u6761\u53ef\u6301\u7eed\u53d1\u5c55\u7684\u9053\u8def\u3002\u4e3a\u6b64\uff0c\u4eba\u4eec\u5728\u5e7f\u5927\u7684\u5c71\u533a\u8fdb\u884c\u4eba\u5de5\u690d\u6811\u9020\u6797\uff0c\u5728\u8bb8\u591a\u57ce\u5e02\u5efa\u9020\u4e86\u6e7f\u5730\u516c\u56ed\u3002\u4e0b\u5217\u53d9\u8ff0\u6b63\u786e\u7684\u662f\nA. \u9020\u6210\u4eba\u5de5\u6797\u4e2d\u7fa4\u843d\u5782\u76f4\u5206\u5e03\u7684\u4e3b\u8981\u539f\u56e0\u4e3a\u9633\u5149\nB. \u6625\u5929\u6e7f\u5730\u516c\u56ed\u7fa4\u843d\u4e2d\u7684\u9752\u86d9\u5448\u201cJ\u201d\u578b\u66f2\u7ebf\u589e\u957f\nC. \u6e7f\u5730\u516c\u56ed\u7684\u89c2\u8d4f\u4ef7\u503c\u4f53\u73b0\u4e86\u751f\u7269\u591a\u6837\u6027\u7684\u95f4\u63a5\u4ef7\u503c\nD. \u5728\u4e00\u7247\u5c71\u6797\u5730\u4e0a\u901a\u8fc7\u4eba\u5de5\u7ba1\u7406\u63d0\u9ad8\u67d0\u79cd\u901f\u751f\u6811\u7684\u6570\u91cf\u5e76\u6e05\u9664\u5176\u4ed6\u6811\u79cd\u540e\uff0c\u5176\u62b5\u6297\u529b\u7a33\u5b9a\u6027\u63d0\u9ad8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\u201c\u83ab\u201d\u7684\u7528\u6cd5\u4e0e\u5176\u5b83\u4e09\u4e2a\u4e0d\u540c\u7684\u662f\nA. \u543e\u6709\u8001\u7236\uff0c\u8eab\u6b7b\u83ab\u4e4b\u517b\u4e5f\u3002\nB. \u4fdd\u6c11\u800c\u738b\uff0c\u83ab\u4e4b\u80fd\u5fa1\u4e5f\u3002\nC. \u8fc7\u800c\u80fd\u6539\uff0c\u5584\u83ab\u5927\u7109\u3002\nD. \u541b\u6709\u75be\u75c5\u89c1\u4e8e\u9762\uff0c\u83ab\u591a\u996e\u9152\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.29863342676099575, "HuggingFaceH4/zephyr-7b-beta": 0.8439828662411932, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.29863342676099575, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c60\u5c81\u3002\u5de6\u624b\u9ebb\u6728\u534a\u5e74\uff0c\u53cc\u4e0b\u80a2\u4e4f\u529b\uff0c\u884c\u8d70\u4e0d\u7a333\u4e2a\u6708\u3002\u67e5\u4f53:\u5de6\u4e0a\u80a2\u6861\u9aa8\u819c\u53cd\u5c04\u51cf\u5f31\uff0c\u5de6\u624b\u62c7\u6307\u9488\u523a\u89c9\u51cf\u9000\uff0c\u53cc\u4e0b\u80a2\u8171\u53cd\u5c04\u4ea2\u8fdb\uff0c\u53cc\u4fa7Babinski\u5f81\uff08+\uff09\u3002\u521d\u6b65\u8bca\u65ad\u4e3a\u9888\u690e\u75c5\u3002\u9888\u690e\u75c5\u53d8\u7684\u5e73\u9762\u6700\u53ef\u80fd\u4f4d\u4e8e\nA. \u98885-6\nB. \u98887-\u80f81\nC. \u98886-7\nD. \u98884-5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.2801288226217134, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u5c5e\u4e8e\u7edd\u5bf9\u6cd5\u5f8b\u5173\u7cfb\u7684\u662f\nA. \u5a5a\u59fb\u5173\u7cfb\nB. \u503a\u6743\u5173\u7cfb\nC. \u6240\u6709\u6743\u5173\u7cfb\nD. \u8bc9\u8bbc\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41010460035012436, "meta-math/MetaMath-Mistral-7B": 0.8557348237577896, "itpossible/Chinese-Mistral-7B-v0.1": 0.47776353426970547, "HuggingFaceH4/zephyr-7b-beta": 0.9922300957789868, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.645053484054453, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5306219217567856}}, {"question": "\u201c\u9ad8\u6295\u8d44\u56de\u62a5\uff0c\u4f01\u4e1a\u7a33\u5b9a\u53d1\u5c55\u201d\u7684\u8981\u6c42\u6240\u5c5e\u7684\u5229\u76ca\u76f8\u5173\u8005\u662f\nA. \u4f9b\u5e94\u5546\nB. \u503a\u6743\u4eba\nC. \u6d88\u8d39\u8005\nD. \u80a1\u4e1c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5192459804749188, "meta-math/MetaMath-Mistral-7B": 0.7631055143496996, "itpossible/Chinese-Mistral-7B-v0.1": 0.6061547480899561, "HuggingFaceH4/zephyr-7b-beta": 0.9993291518702014, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9366505185620957, "meta-llama/Meta-Llama-3-8B": 0.73360783562793, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9578927464862863}}, {"question": "\u5bf9\u7269\u4f53\uff08\uff09\u6240\u4ea7\u751f\u7684\u77e5\u89c9\u53eb\u8fd0\u52a8\u77e5\u89c9\u3002\nA. \u65f6\u95f4\u7279\u6027\nB. \u5ef6\u7eed\u6027\u548c\u987a\u5e8f\u6027\nC. \u8ddd\u79bb\u548c\u65b9\u4f4d\u7279\u6027\nD. \u5728\u7a7a\u95f4\u4e2d\u7684\u4f4d\u79fb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6963628805329574, "meta-math/MetaMath-Mistral-7B": 0.7358914639715698, "itpossible/Chinese-Mistral-7B-v0.1": 0.5530762562180136, "HuggingFaceH4/zephyr-7b-beta": 0.9994740994687878, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8911591401476833, "meta-llama/Meta-Llama-3-8B": 0.9300922947816103, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6538270840087329}}, {"question": "\u827a\u672f\u521b\u4f5c\u7684\u601d\u7ef4\u65b9\u5f0f\u662f\nA. \u5f62\u8c61\u601d\u7ef4\u6392\u65a5\u601d\u7ef4\nB. \u62bd\u8c61\u601d\u7ef4\u4e3a\u4e3b\u5bfc\nC. \u5f62\u8c61\u601d\u7ef4\u548c\u62bd\u8c61\u601d\u7ef4\u8d77\u540c\u6837\u4f5c\u7528\nD. \u5f62\u8c61\u601d\u7ef4\u4e3a\u4e3b\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3216592693751747, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4577567506435653, "HuggingFaceH4/zephyr-7b-beta": 0.9867871723758811, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7560395924508768, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c0f\u5706\u76f4\u5f84\u4e3aa\u5398\u7c73\uff0c\u5927\u5706\u7684\u534a\u5f84\u4e3aa\u5398\u7c73\uff0c\u5219\u5c0f\u5706\u9762\u79ef\u4e0e\u5927\u5706\u9762\u79ef\u7684\u6bd4\u662f\nA. 2:1\nB. 1:4\nC. 3.14\nD. 4:1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.30817677628857404, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u6885\u82b1\u4e09\u5f04\u300b\u662f\nA. \u7b1b\u5b50\u66f2\nB. \u7b5d\u66f2\nC. \u7434\u66f2\nD. \u7435\u7436\u66f2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33660726463163315, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.34031470637689565, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u80c3\u5185\u7684\u9178\u6027\u73af\u5883\u662f\u901a\u8fc7\u8d28\u5b50\u6cf5\u7ef4\u6301\u7684\uff0c\u8d28\u5b50\u6cf5\u50ac\u53161\u5206\u5b50\u7684ATP\u6c34\u89e3\u6240\u91ca\u653e\u7684\u80fd\u91cf\uff0c\u53ef\u9a71\u52a81\u4e2aH\uff0b\u4ece\u80c3\u58c1\u7ec6\u80de\u8fdb\u5165\u80c3\u8154\u548c1\u4e2aK\uff0b\u4ece\u80c3\u8154\u8fdb\u5165\u80c3\u58c1\u7ec6\u80de\uff0cK\uff0b\u53c8\u53ef\u7ecf\u901a\u9053\u86cb\u767d\u987a\u6d53\u5ea6\u8fdb\u5165\u80c3\u8154\uff0c\u4e0b\u5217\u76f8\u5173\u53d9\u8ff0\u9519\u8bef\u7684\u662f\nA. H\uff0b\u4ece\u80c3\u58c1\u7ec6\u80de\u8fdb\u5165\u80c3\u8154\u7684\u65b9\u5f0f\u662f\u4e3b\u52a8\u8fd0\u8f93\nB. \u8d28\u5b50\u6cf5\u7684\u5316\u5b66\u672c\u8d28\u53ef\u80fd\u662f\u86cb\u767d\u8d28\nC. K\uff0b\u8fdb\u51fa\u80c3\u58c1\u7ec6\u80de\u7684\u8de8\u819c\u8fd0\u8f93\u65b9\u5f0f\u662f\u76f8\u540c\u7684\nD. \u80c3\u58c1\u7ec6\u80de\u5185K\uff0b\u542b\u91cf\u5f71\u54cd\u7ec6\u80de\u5185\u6db2\u6e17\u900f\u538b\u7684\u5927\u5c0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28595819752074036, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.32871459371036227, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u80c3\u540e\u58c1\u6e83\u75a1\u7a7f\u5b54\u540e\u5185\u5bb9\u7269\u9996\u5148\u7a3d\u7559\u4e8e\nA. \u53f3\u809d\u4e0b\u524d\u95f4\u9699\nB. \u80de\u819c\u540e\u95f4\u9699\nC. \u5de6\u809d\u4e0b\u540e\u95f4\u9699\nD. \u5de6\u809d\u4e0b\u524d\u95f4\u9699\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2994509091275924, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.38562010373619576, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.308176776288574, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u6d3b\u6cfc\u597d\u52a8\u3001\u884c\u52a8\u654f\u6377\u201d\u662f\u4eba\u7684() \u7279\u5f81\u7684\u8868\u73b0\u3002\nA. \u5174\u8da3\nB. \u6027\u683c\nC. \u80fd\u529b\nD. \u6c14\u8d28\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4748690587630942, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9137365670997053}}, {"question": "6\u5c81\u7684\u5c0f\u51e1\u8eab\u9ad81.05\u7c73\uff0c\u67d0\u65e5\u4ed6\u7792\u7740\u7236\u6bcd\u72ec\u81ea\u53bb\u5546\u573a\uff0c\u8d2d\u4e70\u4e86\u4e00\u53f0\u4ef7\u503c4000\u5143\u7684\u5e73\u677f\u7535\u8111\u3002\u5f53\u5929\uff0c\u5176\u7236\u6bcd\u5c06\u4e00\u5207\u5b8c\u597d\u7684\u5e73\u677f\u7535\u8111\u5e26\u56de\u5546\u573a\u8981\u6c42\u9000\u8fd8\u5168\u6b3e\u3002\u5546\u573a\u4ee5\u201c\u7535\u5b50\u5546\u54c1\u4e00\u7ecf\u4f7f\u7528\uff0c\u53ea\u6362\u4e0d\u9000\u201d\u4e3a\u7531\u62d2\u7edd\u3002\u6839\u636e\u76f8\u5173\u6cd5\u5f8b\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u5546\u573a\u505a\u6cd5\u6b63\u786e\uff0c\u7535\u5b50\u4ea7\u54c1\u53ea\u6362\u4e0d\u9000\nB. \u5546\u573a\u65e2\u4e0d\u7528\u9000\u8d27\u9000\u6b3e\uff0c\u4e5f\u4e0d\u7528\u6362\u8d27\nC. \u5546\u573a\u505a\u6cd5\u7f3a\u5c11\u6cd5\u5f8b\u4f9d\u636e\uff0c\u5e94\u5168\u989d\u9000\u6b3e\nD. \u5546\u573a\u5e94\u6536\u53d6\u5e73\u677f\u7535\u8111\u539f\u4ef750%\u7684\u6298\u65e7\u8d39\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6821881009294873, "meta-math/MetaMath-Mistral-7B": 0.8588287969810737, "itpossible/Chinese-Mistral-7B-v0.1": 0.44202477219447817, "HuggingFaceH4/zephyr-7b-beta": 0.9867001780999037, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7920585331191055, "meta-llama/Meta-Llama-3-8B": 0.5759518962845397, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9488\u5bf9\u53f3\u4fa7\u8f93\u5c3f\u7ba1\u4e0a\u6bb5 1.0\u00d70.8cm \u7684\u7ed3\u77f3\uff0c\u9009\u62e9\u7684\u6cbb\u7597\u65b9\u6cd5\u662f\nA. \u8179\u8154\u955c\u8f93\u5c3f\u7ba1\u5207\u5f00\u53d6\u77f3\nB. \u836f\u7269\u6392\u77f3\nC. ESWL\nD. \u8f93\u5c3f\u7ba1\u955c\u788e\u77f3\u53d6\u77f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.48383753684450176, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6700\u65e9\u521b\u7acb\u6d3b\u4f5b\u8f6c\u4e16\u5236\u5ea6\u7684\u662f()\u7684\u9ed1\u5e3d\u7cfb\u3002\nA. \u5676\u4e3e\u6d3e\nB. \u8428\u8fe6\u6d3e\nC. \u5b81\u739b\u6d3e\nD. \u683c\u9c81\u6d3e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.264634220591857, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3973062521595393, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u628a\u6cd5\u5f8b\u5206\u4e3a\u6839\u672c\u6cd5\u548c\u666e\u901a\u6cd5\u7684\u4e3b\u8981\u4f9d\u636e\u662f\nA. \u5236\u5b9a\u548c\u5b9e\u65bd\u7684\u4e3b\u4f53\u4e0d\u540c\nB. \u5236\u5b9a\u548c\u8868\u8fbe\u7684\u65b9\u5f0f\u4e0d\u540c\nC. \u9002\u7528\u8303\u56f4\u4e0d\u540c\nD. \u89c4\u5b9a\u7684\u5185\u5bb9\u3001\u6cd5\u5f8b\u5730\u4f4d\u548c\u5236\u5b9a\u7684\u7a0b\u5e8f\u4e0d\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8784727502703447, "meta-math/MetaMath-Mistral-7B": 0.9313686922897267, "itpossible/Chinese-Mistral-7B-v0.1": 0.8185299922456161, "HuggingFaceH4/zephyr-7b-beta": 0.99929991143245, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8372881095704224, "meta-llama/Meta-Llama-3-8B": 0.8472389610772516, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9734800607394957}}, {"question": "\u4e0b\u5217\u80a0\u6897\u963b\u8868\u73b0\u4e2d\uff0c\u63ed\u793a\u53d1\u751f\u80a0\u7ede\u7a84\u53ef\u80fd\u6027\u8f83\u5c0f\u7684\u662f\nA. \u53d1\u75c5\u6025\uff0c\u75bc\u75db\u91cd\u800c\u6301\u7eed\nB. \u8179\u819c\u523a\u6fc0\u5f81\u660e\u663e\uff0c\u6709\u4f11\u514b\u8868\u73b0\nC. \u5455\u5410\u7269\u4e3a\u8840\u6027\u6db2\u4f53\nD. \u6709\u591a\u6b21\u8179\u90e8\u624b\u672f\u53f2\uff0c\u53cd\u590d\u53d1\u4f5c\u8179\u75db\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.474069820068834, "HuggingFaceH4/zephyr-7b-beta": 0.6334848184130204, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4542143982076038, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee55'-A-A-T-C-C-G-C-A-T-3'\u4e3a\u6a21\u677f\u8f6c\u5f55\u5408\u6210\u7684RNA\u94fe\u7684\u5e8f\u5217\u4e3a\nA. 5'-U-U-A-G-G-C-G-T-A-3'\nB. 5'-A-U-G-C-G-G-A-U-U-3'\nC. 5'-A-T-G-C-G-G-A-T-T-3'\nD. 5'-T-T-A-G-G-C-G-T-A-3'\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.333183235354062, "HuggingFaceH4/zephyr-7b-beta": 0.453846664275464, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe\u4e09\u9636\u65b9\u7a0b$A=[A_{1},A_{2},A_{3}]$\uff0c\u5176\u4e2d$a_[i], i=1,2,3$\u4e3aA\u7684\u5217\u5411\u91cf\uff0c\u4e14$|A|=2$\uff0c\u5219$|B|=|[a_{1}+3a_{2},a_{2},a_{3}]|=$\nA. 6\nB. 0\nC. -2\nD. 2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9762\u5411\u6570\u636e\u6316\u6398\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u4e3b\u8981\u89e3\u51b3\u9ad8\u5c42\u5e94\u7528\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u81f4\u529b\u4e8e\u7814\u7a76\u5982\u4f55\u6839\u636e\u4e0d\u540c\u6570\u636e\u6316\u6398\u64cd\u4f5c\u7684\u7279\u5f81\u6765\u5b9e\u73b0\u5bf9\u9690\u79c1\u7684\u4fdd\u62a4\u3002\u4ece\u6570\u636e\u6316\u6398\u7684\u89d2\u5ea6\uff0c\u4e0d\u5c5e\u4e8e\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u7684\u662f\nA. \u57fa\u4e8e\u6570\u636e\u533f\u540d\u5316\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\nB. \u57fa\u4e8e\u6570\u636e\u52a0\u5bc6\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\nC. \u57fa\u4e8e\u6570\u636e\u5931\u771f\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\nD. \u57fa\u4e8e\u6570\u636e\u5206\u6790\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6248383566620674, "meta-math/MetaMath-Mistral-7B": 0.8451639160972241, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9932799326351368, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9514293957423988, "meta-llama/Meta-Llama-3-8B": 0.5349680218432725, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u540c\u4e00\u7269\u79cd\u7684\u4e24\u7c7b\u7ec6\u80de\u5404\u4ea7\u751f\u4e00\u79cd\u5206\u6ccc\u86cb\u767d\uff0c\u7ec4\u6210\u8fd9\u4e24\u79cd\u86cb\u767d\u8d28\u7684\u5404\u79cd\u6c28\u57fa\u9178\u542b\u91cf\u76f8\u540c\uff0c\u4f46\u6392\u5217\u987a\u5e8f\u4e0d\u540c\u3002\u5176\u539f\u56e0\u662f\u53c2\u4e0e\u8fd9\u4e24\u79cd\u86cb\u767d\u8d28\u5408\u6210\u7684\nA. tRNA\u79cd\u7c7b\u4e0d\u540c\nB. \u540c\u4e00\u5bc6\u7801\u5b50\u6240\u51b3\u5b9a\u7684\u6c28\u57fa\u9178\u4e0d\u540c\nC. mRNA\u78b1\u57fa\u5e8f\u5217\u4e0d\u540c\nD. \u6838\u7cd6\u4f53\u6210\u5206\u4e0d\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7298963692779062, "meta-math/MetaMath-Mistral-7B": 0.8484333545374207, "itpossible/Chinese-Mistral-7B-v0.1": 0.4324117087934554, "HuggingFaceH4/zephyr-7b-beta": 0.9912554475998031, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8720451464495983, "meta-llama/Meta-Llama-3-8B": 0.5594256355999565, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6140798508216314}}, {"question": "\u4ece2003\u5e74\u8d77\uff0c\u6b63\u5f0f\u5c06\u6bcf\u5e74\u7684\uff08\uff09\u5b9a\u4e3a\u4e2d\u56fd\u516c\u5173\u4eba\u81ea\u5df1\u7684\u8282\u65e5\uff1f\nA. 11/11\nB. 01/10\nC. \u5143\u67081\u65e5\nD. 20/12\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u4e2a\u6c11\u65cf\u51e0\u4e4e\u5168\u6c11\u4fe1\u4ef0\u4f5b\u6559\nA. \u8d6b\u54f2\u65cf\nB. \u85cf\u65cf\nC. \u4e1c\u4e61\u65cf\nD. \u6ee1\u65cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8488868562160409, "meta-math/MetaMath-Mistral-7B": 0.987696043182122, "itpossible/Chinese-Mistral-7B-v0.1": 0.90846762123232, "HuggingFaceH4/zephyr-7b-beta": 0.996893739019552, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.837824576074427, "meta-llama/Meta-Llama-3-8B": 0.9191740442037385, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9913380728680505}}, {"question": "\u8d85\u65b0\u661f\u7206\u53d1\u540e\u5149\u5ea6\u4f1a\u7f13\u6162\u4e0b\u964d\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u5149\u5ea6\u80fd\u91cf\u7684\u4e3b\u8981\u6765\u6e90\u662f\nA. \u6838\u88c2\u53d8\nB. \u653e\u5c04\u6027\u5143\u7d20\u8870\u53d8\nC. \u5468\u56f4\u7269\u8d28\u6563\u5c04\u8d85\u65b0\u661f\u7206\u53d1\u65f6\u7684\u5149\nD. \u6838\u805a\u53d8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5469986124821937, "meta-math/MetaMath-Mistral-7B": 0.6355675882042212, "itpossible/Chinese-Mistral-7B-v0.1": 0.8741434515316832, "HuggingFaceH4/zephyr-7b-beta": 0.9971877337539395, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7335183430658654, "meta-llama/Meta-Llama-3-8B": 0.41209692555709065, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4576402824148822}}, {"question": "\u201c\u4e3b\u8425\u4e1a\u52a1\u6536\u5165\u201d\u660e\u7ec6\u5206\u7c7b\u8d26\u8d26\u9875\u683c\u5f0f\u9002\u4e8e\u91c7\u7528\nA. \u501f\u65b9\u591a\u680f\u5f0f\nB. \u8d37\u65b9\u591a\u680f\u5f0f\nC. \u6570\u91cf\u91d1\u989d\u5f0f\nD. \u4e09\u680f\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u884c\u8f66\u4e2d\u9047\u8001\u5e74\u4eba\u9a91\u81ea\u884c\u8f66\u65f6\uff0c\u5e94\nA. \u4e34\u8fd1\u65f6\u9e23\u5587\u53ed\u793a\u610f\u5176\u8ba9\u9053\nB. \u63d0\u524d\u9e23\u5587\u53ed\uff0c\u51cf\u901f\u907f\u8ba9\nC. \u8fc5\u901f\u7ed5\u8fc7\nD. \u7d27\u968f\u5176\u540e\u884c\u9a76\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5542576078985689, "meta-math/MetaMath-Mistral-7B": 0.748969413060793, "itpossible/Chinese-Mistral-7B-v0.1": 0.9212790429621988, "HuggingFaceH4/zephyr-7b-beta": 0.5924886201857836, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8271204379980519, "meta-llama/Meta-Llama-3-8B": 0.8954861561120134, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.994918887880589}}, {"question": "1990\u5e7411\u6708\uff0c\u8054\u5408\u56fd\u5b89\u7406\u4f1a\u901a\u8fc7\u7b2c678\u53f7\u51b3\u8bae\uff0c\u201c\u6388\u6743\u540c\u79d1\u5a01\u7279\u653f\u5e9c\u5408\u4f5c\u7684\u4f1a\u5458\u56fd\uff0c\u9664\u975e\u4f0a\u62c9\u514b\u57281991\u5e741\u670815\u65e5\u6216\u4e4b\u524d\u2026\u2026\u5b8c\u5168\u6267\u884c\u4e0a\u8ff0\u5404\u51b3\u8bae\uff08\u6307\u65e0\u6761\u4ef6\u64a4\u519b\uff09\uff0c\u5426\u5219\u53ef\u4ee5\u4f7f\u7528\u4e00\u5207\u5fc5\u8981\u624b\u6bb5\u2026\u2026\u6062\u590d\u8be5\u5730\u533a\u7684\u56fd\u9645\u548c\u5e73\u4e0e\u5b89\u5168\u201d\u3002\u4f9d\u6b64\u51b3\u8bae\uff0c\u591a\u56fd\u90e8\u961f\u5bf9\u4f0a\u62c9\u514b\u91c7\u53d6\u4e86\u519b\u4e8b\u884c\u52a8\u3002\u4e2d\u56fd\u653f\u5e9c\u5bf9\u8be5\u51b3\u8bae\u6295\u4e86\u5f03\u6743\u7968\u3002\u5bf9\u6b64\u5206\u6790\u6b63\u786e\u7684\u662f\nA. \u4e2d\u56fd\u653f\u5e9c\u53cd\u5bf9\u519b\u4e8b\u5e72\u9884\u89e3\u51b3\u4e89\u7aef\nB. \u8054\u5408\u56fd\u5927\u4f1a\u804c\u8d23\u662f\u7ef4\u62a4\u4e16\u754c\u548c\u5e73\u4e0e\u5b89\u5168\nC. \u4e2d\u56fd\u653f\u5e9c\u53cd\u5bf9\u8054\u5408\u56fd\u5b89\u7406\u4f1a\u51b3\u8bae\nD. \u8054\u5408\u56fd\u7684\u519b\u4e8b\u884c\u52a8\u8fdd\u80cc\u4e86\u5927\u56fd\u4e00\u81f4\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3095294731489224, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5125245957881588, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.47020082125287926, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f18\u79c0\u7684\u516c\u5171\u5173\u7cfb\u4eba\u5458\u5728\u6027\u683c\u4e0a\u5e94\u5177\u5907\u7684\u7279\u5f81\u662f\nA. \u516c\u6b63\u65e0\u79c1\nB. \u521b\u65b0\u80fd\u529b\nC. \u5b9e\u4e8b\u6c42\u662f\nD. \u5f00\u6717\u3001\u6709\u8010\u5fc3\u3001\u80fd\u5bbd\u5bb9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8854882459240249, "meta-math/MetaMath-Mistral-7B": 0.9741854135730971, "itpossible/Chinese-Mistral-7B-v0.1": 0.38733771487116486, "HuggingFaceH4/zephyr-7b-beta": 0.994671975081245, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8897419498436991, "meta-llama/Meta-Llama-3-8B": 0.8182401747215049, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6561504983967962}}, {"question": "\u5df2\u7ecf\u8003\u8651\u5230\u5c06\u4e0e\u67d0\u4e00\u793e\u4f1a\u7ec4\u7ec7\u53d1\u751f\u8054\u7cfb\u4e0e\u4ea4\u5f80\uff0c\u4f46\u5c1a\u672a\u4ed8\u8bf8\u5b9e\u9645\u884c\u52a8\u7684\u516c\u4f17\uff0c\u53ef\u79f0\u4e3a\nA. \u8fb9\u7f18\u516c\u4f17\nB. \u884c\u52a8\u516c\u4f17\nC. \u77e5\u6653\u516c\u4f17\nD. \u6f5c\u5728\u516c\u4f17\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6673016741250293, "meta-math/MetaMath-Mistral-7B": 0.9067618213281409, "itpossible/Chinese-Mistral-7B-v0.1": 0.6172709020938322, "HuggingFaceH4/zephyr-7b-beta": 0.9999001691150197, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6307955466197845, "meta-llama/Meta-Llama-3-8B": 0.8154888023206258, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.556674671672004}}, {"question": "RAM\u7684\u7279\u70b9\u662f\nA. \u5bb9\u91cf\u5927\u4f46\u5b58\u53d6\u901f\u5ea6\u6162\nB. \u5b58\u50a8\u5728\u5176\u5185\u7684\u6570\u636e\u5c06\u6c38\u4e45\u4fdd\u5b58\nC. \u65ad\u7535\u540e\uff0c\u5b58\u50a8\u5728\u5176\u5185\u7684\u6570\u636e\u5c06\u4f1a\u4e22\u5931\nD. \u7528\u6237\u53ea\u80fd\u8bfb\u51fa\u6570\u636e\uff0c\u4f46\u4e0d\u80fd\u968f\u673a\u5199\u5165\u6570\u636e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9624109782318722, "meta-math/MetaMath-Mistral-7B": 0.9914624469250377, "itpossible/Chinese-Mistral-7B-v0.1": 0.9507271397898517, "HuggingFaceH4/zephyr-7b-beta": 0.9987564649803188, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9757978441675311, "meta-llama/Meta-Llama-3-8B": 0.9326709647461884, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8914703713232792}}, {"question": "\u5bf9\u4e8e\u628a\u52b3\u52a8\u548c\u793e\u4f1a\u4fdd\u969c\u6cd5\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684\u6cd5\u5f8b\u90e8\u95e8\u7684\u539f\u56e0\uff0c\u4e0b\u5217\u8868\u8ff0\u4e0d\u6b63\u786e\u7684\u6709\nA. \u5176\u6709\u533a\u522b\u4e8e\u5176\u4ed6\u6cd5\u5f8b\u90e8\u95e8\u7684\u7279\u5b9a\u8c03\u6574\u65b9\u6cd5\nB. \u8fd9\u79cd\u5212\u5206\u7b26\u5408\u6cd5\u5f8b\u90e8\u95e8\u5212\u5206\u539f\u5219\u4e2d\u7684\u5408\u76ee\u7684\u6027\u539f\u5219\nC. \u5176\u6709\u533a\u522b\u4e8e\u5176\u4ed6\u6cd5\u5f8b\u90e8\u95e8\u7684\u7279\u5b9a\u8c03\u6574\u5bf9\u8c61\nD. \u8fd9\u79cd\u5212\u5206\u7b26\u5408\u6cd5\u5f8b\u90e8\u95e8\u5212\u5206\u539f\u5219\u4e2d\u7684\u76f8\u5bf9\u7a33\u5b9a\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.34695068109082333}}, {"question": "\u6ccc\u5c3f\u7cfb\u8349\u9178\u9499\u7ed3\u77f3\u7684\u7279\u70b9\u662f\nA. \u5149\u6ed1\u3001\u6de1\u9ec4\u81f3\u9ec4\u68d5\u8272\u3001\u8721\u6837\u5916\u89c2\nB. X\u7ebf\u4e0d\u88ab\u663e\u793a\nC. \u8d28\u786c\u7c97\u7cd9\u3001\u4e0d\u89c4\u5219\u3001\u5e38\u5448\u6851\u6939\u6837\uff0c\u68d5\u8910\u8272\nD. \u6613\u788e\u7c97\u7cd9\u3001\u4e0d\u89c4\u5219\uff0c\u5448\u7070\u767d\u8272\u3001\u9ec4\u8272\u6216\u68d5\u8272\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3468666740605408, "meta-math/MetaMath-Mistral-7B": 0.450388688906963, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.48794714262410466, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6522291226369561, "meta-llama/Meta-Llama-3-8B": 0.44787546435624775, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9403723883250307}}, {"question": "WTO\u6295\u7968\u6743\u7684\u5206\u914d\u65b9\u5f0f\u662f\nA. \u52a0\u6743\u7968\u5236\nB. \u6839\u636e\u5404\u6210\u5458\u8d38\u6613\u603b\u989d\u786e\u5b9a\u6295\u7968\u6743\nC. \u57fa\u672c\u7968\u4e0e\u52a0\u6743\u7968\u76f8\u7ed3\u5408\u5236\nD. \u4e00\u6210\u5458\u4e00\u7968\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0d\u5c5e\u4e8e\u201c\u9633\u76db\u5219\u5916\u70ed\u201c\u673a\u7406\u7684\u662f\nA. \u70ed\u6c14\u718f\u80f8\u4e2d\nB. \u4e0a\u7126\u4e0d\u901a\u5229\nC. \u8160\u7406\u95ed\u585e\nD. \u76ae\u80a4\u81f4\u5bc6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u57284.3\u7684\u672b\u5c3e\u6dfb\u4e0a\u4e00\u4e2a\u96f6\u540e\uff0c\u5c0f\u6570\u7684\u8ba1\u6570\u5355\u4f4d\u662f\nA. \u767e\u5206\u4f4d\nB. 0.01 \nC. 0.1 \nD. \u5341\u5206\u4f4d \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.28850952576306876, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u786e\u7acb\u4e86\u201c\u4e00\u9879\u53d1\u660e\u4e00\u6b21\u7533\u8bf7\u5236\u5ea6\u201d\u7684\u516c\u7ea6\u662f\nA. \u300a\u6b27\u6d32\u5171\u540c\u4f53\u4e13\u5229\u516c\u7ea6\u300b\nB. \u300a\u4f2f\u5c14\u5c3c\u516c\u7ea6\u300b\nC. \u300a\u4e13\u5229\u5408\u4f5c\u6761\u7ea6\u300b\nD. \u300a\u6b27\u6d32\u4e13\u5229\u516c\u7ea6\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3918190504855606}}, {"question": "\u4e0b\u5217\u540d\u5173\u54ea\u4e00\u5ea7\u88ab\u79f0\u4e3a\u201c\u5929\u4e0b\u7b2c\u4e00\u5173\u201d\nA. \u5c45\u5eb8\u5173\nB. \u5c71\u6d77\u5173\nC. \u5a18\u5b50\u5173\nD. \u6f7c\u95e8\u5173\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.36717002281588484, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.998463355785033, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3875481250600586, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3527809287490976}}, {"question": "\u4e0b\u5217\u6709\u5173\u56fa\u5b9a\u5316\u9176\u548c\u56fa\u5b9a\u5316\u7ec6\u80de\u7684\u53d9\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u56fa\u5b9a\u5316\u7ec6\u80de\u53ef\u4ee5\u50ac\u5316\u5404\u79cd\u53cd\u5e94\u5e95\u7269\u7684\u4e00\u7cfb\u5217\u53cd\u5e94\nB. \u53cd\u5e94\u4ea7\u7269\u5bf9\u56fa\u5b9a\u5316\u9176\u7684\u6d3b\u6027\u6ca1\u6709\u5f71\u54cd\nC. \u8461\u8404\u7cd6\u5f02\u6784\u9176\u56fa\u5b9a\u524d\u540e\u4e13\u4e00\u6027\u4e0d\u540c\nD. \u53ef\u7528\u5305\u57cb\u6cd5\u5236\u5907\u56fa\u5b9a\u5316\u9175\u6bcd\u7ec6\u80de\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6012849662205093, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.34366150880343027, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6569479100112833}}, {"question": "\u4e0e\u8bed\u8a00\u3001\u58f0\u97f3\u3001\u547c\u5438\u7684\u5f3a\u5f31\u6709\u5173\u7684\u662f\nA. \u5b97\u6c14\nB. \u8425\u6c14\nC. \u536b\u6c14\nD. \u5143\u6c14\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35092266417261364, "meta-math/MetaMath-Mistral-7B": 0.7173140856724309, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.43921759124068155, "meta-llama/Meta-Llama-3-8B": 0.26463422059185693, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u5b66\u751f\u738b\u67d0\u4e0a\u8bfe\u73a9\uff0c\u88ab\u73ed\u4e3b\u4efb\u674e\u67d0\u5f53\u573a\u6ca1\u6536\u3002\u738b\u67d0\u8bfe\u540e\u5411\u674e\u67d0\u627f\u8ba4\u9519\u8bef\u5e76\u8981\u6c42\u5f52\u8fd8\u5176\uff0c\u88ab\u674e\u67d0\u4ee5\u738b\u67d0\u8fdd\u53cd\u6821\u89c4\u4e3a\u7531\u62d2\u7edd\u3002\u674e\u67d0\u7684\u505a\u6cd5\nA. \u6b63\u786e\uff0c\u6559\u5e08\u6709\u60e9\u6210\u5b66\u751f\u7684\u6743\u5229\nB. \u4e0d\u6b63\u786e\uff0c\u5e94\u4e0a\u4ea4\u5b66\u6821\u9500\u6bc1\nC. \u6b63\u786e\uff0c\u5b66\u6821\u89c4\u7ae0\u5e94\u8be5\u4eba\u4eba\u9075\u5b88\nD. \u4e0d\u6b63\u786e\uff0c\u4fb5\u72af\u4e86\u5b66\u751f\u7684\u8d22\u4ea7\u6743\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5000062673271941, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6355675644345774, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6694333256303844, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6071453710643068}}, {"question": "\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u5c06\u77f3\u8721\u6cb9(\u6db2\u6001\u70f7\u70c3\u6df7\u5408\u7269)\u52a0\u5f3a\u70ed\u5206\u89e3\u751f\u6210\u7684\u6c14\u4f53\u90fd\u662f\u70f7\u70c3\nB. \u7532\u82ef\u80fd\u591f\u4f7f\u6eb4\u7684\u56db\u6c2f\u5316\u78b3\u6eb6\u6db2\u548c\u9178\u6027\u9ad8\u9530\u9178\u94be\u6eb6\u6db2\u892a\u8272\nC. \u4e59\u9187\u80fd\u591f\u88ab\u9178\u6027\u7684\u9ad8\u9530\u9178\u94be\u6eb6\u6db2\u76f4\u63a5\u6c27\u5316\u6210\u4e59\u9178\nD. \u4e59\u70f7\u4e0e\u6c2f\u6c14\u5728\u5149\u7167\u6761\u4ef6\u4e0b\u53d1\u751f\u52a0\u6210\u53cd\u5e94\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c0f\u6797\u603b\u662f\u4e0a\u5b66\u8fdf\u5230\uff0c\u4e3a\u6b64\u6559\u5e08\u53d6\u6d88\u5c0f\u6797\u4e00\u6b21\u6625\u6e38\u7279\u6743\uff0c\u4ee5\u540e\u4ed6\u5f88\u5c11\u8fdf\u5230\u3002\u8fd9\u662f\u8fd0\u7528\u4e86\u884c\u4e3a\u539f\u7406\u3002\nA. \u5448\u73b0\u6027\u60e9\u7f5a\nB. \u8d1f\u5f3a\u5316\nC. \u53d6\u6d88\u6027\u60e9\u7f5a\nD. \u6b63\u5f3a\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.48876097445502614, "meta-math/MetaMath-Mistral-7B": 0.7648976275336151, "itpossible/Chinese-Mistral-7B-v0.1": 0.7640928028302983, "HuggingFaceH4/zephyr-7b-beta": 0.93151766783156, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5110391347635618, "meta-llama/Meta-Llama-3-8B": 0.44238635558418604, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u79cd\u75be\u75c5\u4e0d\u662f\u4eba\u755c\u5171\u60a3\u75be\u75c5\nA. \u4f24\u5bd2\nB. \u7ed3\u6838\u75c5\nC. \u963f\u7c73\u5df4\u75e2\u75be\nD. \u94a9\u7aef\u87ba\u65cb\u4f53\u75c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u665a\u6e05\u65f6\u671f\u6e05\u653f\u5e9c\u5b9e\u884c\u4e13\u5229\u5236\u5ea6\uff0c\u5141\u8bb8\u4f01\u4e1a\u4eab\u6709\u957f\u65f6\u95f4\u751f\u4ea7\u7ecf\u8425\u5784\u65ad\u6743\u30021912\u5e74\u5317\u4eac\u653f\u5e9c\u89c4\u5b9a\u4e13\u5229\u4fdd\u62a4\u671f\u9650\u6700\u9ad8\u4e3a5\u5e74\u3002\u540e\u8d22\u653f\u603b\u957f\u5468\u5b66\u7199\u7b49\u4e3a\u5176\u6240\u8bbe\u516c\u53f8\u7533\u8bf730\u5e74\u7684\u4e13\u5229\u6743\u65f6\uff0c\u672a\u83b7\u6279\u51c6\u3002\u8fd9\u53cd\u6620\u4e86\u6c11\u56fd\u521d\u671f\nA. \u4e2d\u592e\u653f\u5e9c\u9650\u5236\u5b98\u8425\u5546\u4e1a\u7684\u53d1\u5c55\nB. \u5efa\u7acb\u4e86\u7cfb\u7edf\u5b8c\u5584\u7684\u7ecf\u6d4e\u6cd5\u5236\u4f53\u7cfb\nC. \u7ecf\u6d4e\u6cd5\u89c4\u5f97\u4ee5\u5b8c\u5168\u9075\u7167\u6267\u884c\nD. \u7ecf\u6d4e\u7acb\u6cd5\u9f13\u52b1\u5de5\u5546\u4e1a\u81ea\u7531\u7ade\u4e89\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4eba\u7c7b\u6587\u660e\u53f2\u4e0a\uff0c\u8bb8\u591a\u601d\u60f3\u5bb6\u5bf9\u540e\u4e16\u4ea7\u751f\u5f71\u54cd\u5e38\u5e38\u662f\u901a\u8fc7\u6559\u80b2\u7684\u9014\u5f84\u3002\u4e0b\u5217\u601d\u60f3\u5bb6\u4e2d\u901a\u8fc7\u79c1\u5b66\u57f9\u517b\u4eba\u624d\u7684\u6709\uff1aa\u5b54\u5b50\uff1bb\u97e9\u975e\uff1bc\u6731\u71b9\uff1bd\u4e9a\u91cc\u58eb\u591a\u5fb7\nA. acd\nB. abc\nC. bcd\nD. abd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31740786589267667, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.31110386686052405, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45505421914265876, "meta-llama/Meta-Llama-3-8B": 0.3827714827491934, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.41184365349514407}}, {"question": "\u8bbe$\\bigtriangleup ABC$\u7684\u5185\u89d2A\uff0cB\uff0cC\u6240\u5bf9\u8fb9\u7684\u957f\u5206\u522b\u662fa\uff0cb\uff0cc\uff0c\u82e5$\\angle C=120^{\\circ }, c=\\sqrt{2}a$\uff0c\u5219\nA. A\u4e0eB\u7684\u5927\u5c0f\u5173\u7cfb\u4e0d\u80fd\u786e\u5b9a\nB. $A\\gt B$\nC. A=B\nD. $A\\lt B$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2904324311152723, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.29972404264597124, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.32205625344145955}}, {"question": "\u98df\u54c1\u4e2d\u7684\u6c34\u5206\u6d3b\u5ea6\u503c\u5e94\nA. \u22641\nB. \u22651\nC. \u22640.1\nD. 10\uff5e100\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36909242554999977, "meta-math/MetaMath-Mistral-7B": 0.8508068662803109, "itpossible/Chinese-Mistral-7B-v0.1": 0.37480846975763354, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.35686329776860143, "meta-llama/Meta-Llama-3-8B": 0.3949756055962341, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4e8e\u54c1\u724c\u5dee\u5f02\u4e0d\u5927\uff0c\u6d88\u8d39\u8005\u4e0d\u7ecf\u5e38\u8d2d\u4e70\uff0c\u800c\u8d2d\u4e70\u65f6\u53c8\u6709\u4e00\u5b9a\u98ce\u9669\u7684\u4ea7\u54c1\uff0c\u6d88\u8d39\u8005\u4e00\u822c\u4f1a\u8d27\u6bd4\u4e09\u5bb6\u540e\u51b3\u5b9a\u8d2d\u4e70\uff0c\u6d88\u8d39\u8005\u7684\u8fd9\u79cd\u8d2d\u4e70\u884c\u4e3a\u5c5e\u4e8e\nA. \u53d8\u6362\u578b\u8d2d\u4e70\u884c\u4e3a\nB. \u534f\u8c03\u578b\u8d2d\u4e70\u884c\u4e3a\nC. \u4e60\u60ef\u578b\u8d2d\u4e70\u884c\u4e3a\nD. \u590d\u6742\u578b\u8d2d\u4e70\u884c\u4e3a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e24\u4e2a\u6837\u672c\u5747\u6570\u7684\u5047\u8bbe\u68c0\u9a8c\u4e2d\uff0c\u82e5\u8981\u540c\u65f6\u51cf\u5c0fI\u578b\u9519\u8bef\u548cII\u578b\u9519\u8bef\uff0c\u5219\u5fc5\u987b\nA. A\u548cC\nB. \u51cf\u5c0f\u5bb9\u8bb8\u8bef\u5dee\nC. \u589e\u52a0\u6837\u672c\u542b\u91cf\nD. \u51cf\u5c0f\u603b\u4f53\u6807\u51c6\u5dee\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44279150962423947, "meta-math/MetaMath-Mistral-7B": 0.4573230284412956, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.968414912402556, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5789143476601383, "meta-llama/Meta-Llama-3-8B": 0.37286418926012416, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9437327528424053}}, {"question": "1971\u5e74\uff0c\u4e54\u51a0\u534e\u7387\u4e2d\u56fd\u4ee3\u8868\u56e2\u53c2\u77e5\u4e86\u7b2c26\u5c4a\u8054\u5927\uff0c\u5728\u8fd9\u6b21\u4f1a\u8bae\u4e0a\uff0c\u4e54\u51a0\u534e\u5f00\u6000\u5927\u7b11\uff0c\u897f\u65b9\u8bb0\u8005\u628a\u4e54\u51a0\u534e\u7684\u7b11\u63cf\u8ff0\u4e3a \u201c\u9707\u788e\u4e86\u8bae\u4f1a\u5927\u53a6\u7684\u73bb\u7483\u201d\u3002\u4e54\u51a0\u534e\u5f00\u6000\u5927\u7b11\u662f\u56e0\u4e3a\nA. \u4e2d\u56fd\u4fc3\u6210\u4e86\u4e94\u5927\u5e38\u4efb\u7406\u4e8b\u56fd\u9996\u8111\u7684\u4f1a\u6664\nB. \u4e2d\u56fd\u9996\u6b21\u63d0\u51fa\u548c\u5e73\u5171\u5904\u4e94\u9879\u539f\u5219\nC. \u4e2d\u56fd\u6062\u590d\u5728\u8054\u5408\u56fd\u7684\u5408\u6cd5\u5e2d\u4f4d\nD. \u4e2d\u56fd\u63d0\u51fa\u7684\u201c\u6c42\u540c\u5b58\u5f02\u201d\u65b9\u9488\uff0c\u4fc3\u6210\u4e86\u4f1a\u8bae\u7684\u6210\u529f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6198311194825779, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4967046218212403}}, {"question": "\u4e0b\u5217\u4e0d\u5c5e\u4e8e\u9632\u5fa1\u6218\u6597\u57fa\u672c\u4efb\u52a1\u7684\u662f\nA. \u963b\u654c\u589e\u63f4\u3001\u7a81\u56f4\u6216\u9000\u5374\nB. \u653b\u6b7c\u9a7b\u6b62\u3001\u8fd0\u52a8\u4e4b\u654c\nC. \u63a9\u62a4\u4e3b\u529b\u96c6\u4e2d\u3001\u673a\u52a8\u6216\u4f11\u6574\nD. \u4fdd\u536b\u91cd\u8981\u76ee\u7684\u6216\u5730\u533a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7358120032483787, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.46147264940900035, "meta-llama/Meta-Llama-3-8B": 0.303006867405966, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f8b\u5916\u539f\u5219\uff1a\u6307\u4f01\u4e1a\u9ad8\u7ea7\u7ba1\u7406\u4eba\u5458\u628a\u4e00\u822c\u65e5\u5e38\u4e8b\u52a1\u6388\u6743\u7ed9\u4e0b\u5c5e\u7ba1\u7406\u4eba\u5458\u8d1f\u8d23\u5904\u7406\uff0c\u800c\u81ea\u5df1\u4fdd\u7559\u5bf9\u4f8b\u5916\u4e8b\u9879\uff0c\u4e00\u822c\u4e5f\u662f\u91cd\u8981\u4e8b\u9879\u7684\u51b3\u7b56\u548c\u63a7\u5236\u6743\u3002\u4e0b\u5217\u5c5e\u4e8e\u4f8b\u5916\u539f\u5219\u7684\u662f\nA. \u67d0\u516c\u53f8\u7684\u8463\u4e8b\u957f\u4e3b\u8981\u8d1f\u8d23\u516c\u53f8\u5185\u7684\u4e00\u5207\u4e8b\u52a1\uff0c\u800c\u603b\u7ecf\u7406\u5219\u8d1f\u8d23\u7ef4\u62a4\u8ddf\u5408\u4f5c\u5355\u4f4d\u7684\u5173\u7cfb\nB. \u5728\u6bcf\u5468\u7684\u4f8b\u4f1a\u4e0a\uff0c\u5404\u4e2d\u5c42\u7ba1\u7406\u4eba\u5458\u90fd\u8981\u5c06\u672c\u5468\u5de5\u4f5c\u603b\u7ed3\u548c\u4e0b\u5468l2\u4f5c\u8ba1\u5212\u5411\u603b\u7ecf\u7406\u6c47\u62a5\nC. \u67d0\u516c\u53f8\u62a5\u8d26\u5236\u5ea6\u89c4\u5b9a\uff0c5000\u5143\u4ee5\u4e0b\u7684\u7531\u90e8\u95e8\u7ecf\u7406\u5ba1\u6279\uff0c5000\u5143\u53ca\u4ee5\u4e0a\u7684\u9700\u8981\u603b\u7ecf\u7406\u5ba1\u6279\nD. \u67d0\u7ba1\u7406\u4e2d\u5fc3\u517c\u804c\u4eba\u5458\u7684\u786e\u5b9a\uff0c\u603b\u7ecf\u7406\u6709\u51b3\u5b9a\u6743\uff1b\u4e13\u804c\u4eba\u5458\u7684\u786e\u5b9a\uff0c\u7531\u8463\u4e8b\u4f1a\u51b3\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.48996474925778133, "meta-math/MetaMath-Mistral-7B": 0.7187312093969307, "itpossible/Chinese-Mistral-7B-v0.1": 0.6232574485083003, "HuggingFaceH4/zephyr-7b-beta": 0.9723019136454939, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6628229530222163, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9171899478618665}}, {"question": "\u67d0\u5973\uff0c3\u5c81\u3002\u60a3\u201c\u4e59\u8111\u201c\u75c58\u5929\uff0c\u4f4e\u70ed\u4e0d\u9000\uff0c\u98a7\u7ea2\u76d7\u6c57\uff0c\u5931\u8bed\uff0c\u624b\u8db3\u8815\u52a8\uff0c\u820c\u7ea2\u7edb\u82d4\u65e0\uff0c\u8109\u7ec6\u6570\u3002\u4e34\u5e8a\u8bca\u65ad\u6700\u53ef\u80fd\u662f\nA. \u809d\u80be\u9634\u865a\nB. \u80be\u9634\u865a\nC. \u809d\u9634\u865a\nD. \u9634\u865a\u52a8\u98ce\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35934459430649096, "meta-math/MetaMath-Mistral-7B": 0.453609445271565, "itpossible/Chinese-Mistral-7B-v0.1": 0.5686604016727166, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3568632977686014, "meta-llama/Meta-Llama-3-8B": 0.36385828438381157, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u7a83\u53d6\u3001\u6536\u4e70\u3001\u975e\u6cd5\u63d0\u4f9b\u4fe1\u7528\u5361\u4fe1\u606f\u7f6a\u7684\u8bf4\u6cd5\uff0c\u9519\u8bef\u7684\u6709\nA. \u94f6\u884c\u7684\u5de5\u4f5c\u4eba\u5458\u5229\u7528\u804c\u52a1\u4e0a\u7684\u4fbf\u5229\u5b9e\u65bd\u7a83\u53d6\u3001\u6536\u4e70\u3001\u975e\u6cd5\u63d0\u4f9b\u4fe1\u7528\u5361\u4fe1\u606f\u7f6a\u7684\uff0c\u5e94\u5f53\u4ece\u91cd\u5904\u7f5a\nB. \u884c\u4e3a\u4eba\u7a83\u53d6\u3001\u6536\u4e70\u3001\u975e\u6cd5\u63d0\u4f9b\u4ed6\u4eba\u4fe1\u7528\u5361\u4fe1\u606f\u8d44\u6599\u4e4b\u540e\uff0c\u5c1a\u672a\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u8d44\u6599\u4f2a\u9020\u4ed6\u4eba\u4fe1\u7528\u5361\u4e4b\u524d\u6848\u53d1\u7684\uff0c\u5219\u6309\u7167\u7a83\u53d6\u3001\u6536\u4e70\u3001\u975e\u6cd5\u63d0\u4f9b\u4fe1\u7528\u5361\u4fe1\u606f\u7f6a\u5b9a\u7f6a\u5904\u7f5a\nC. \u884c\u4e3a\u4eba\u7a83\u53d6\u3001\u6536\u4e70\u4ed6\u4eba\u4fe1\u7528\u5361\u4fe1\u606f\u540e\u53c8\u4f2a\u9020\u4e86\u4fe1\u7528\u5361\u5e76\u7528\u8be5\u4fe1\u7528\u5361\u9a97\u53d6\u94b1\u8d22\u7684\uff0c\u5219\u5e94\u5f53\u6309\u7167\u4fe1\u7528\u5361\u8bc8\u9a97\u7f6a\u5b9a\u7f6a\u5904\u7f5a\nD. \u884c\u4e3a\u4eba\u975e\u6cd5\u6301\u6709\u4ed6\u4eba\u4fe1\u7528\u5361\uff0c\u6570\u91cf\u8f83\u5927\u7684\uff0c\u6784\u6210\u975e\u6cd5\u63d0\u4f9b\u4fe1\u7528\u5361\u4fe1\u606f\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5549235922343088, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u7edf\u8ba1\u6a21\u5f0f\u5206\u7c7b\u95ee\u9898\u4e2d\uff0c\u5f53\u5148\u9a8c\u6982\u7387\u672a\u77e5\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\nA. N-P\u5224\u51b3\nB. \u6700\u5c0f\u6700\u5927\u635f\u5931\u51c6\u5219\nC. \u6700\u5c0f\u635f\u5931\u51c6\u5219\nD. \u6700\u5c0f\u8bef\u5224\u6982\u7387\u51c6\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4343996999782232, "HuggingFaceH4/zephyr-7b-beta": 0.9623836201768152, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\uff0c28\u5c81\uff0c\u53f3\u9acb\u5173\u8282\u75bc\u75db\u4f34\u80bf\u72693\u4e2a\u6708\uff0c\u4f4e\u70ed\u3001\u54b3\u55fd1\u4e2a\u6708\u3002\u67e5\u4f53\uff1a\u53f3\u9acb\u5173\u8282\u5448\u5c48\u66f2\u7578\u5f62\uff0c\u53ef\u89e6\u53ca\u4e00\u76f4\u5f84\u7ea65cm\u5de6\u53f3\u7684\u80bf\u7269\uff0c\u5c48\u4f38\u6d3b\u52a8\u53d7\u9650\uff0cThomas\u5f81\uff08+\uff09\uff0c\u8840\u6c8958mm/h\u3002X\u7ebf\uff1a\u53f3\u9acb\u5173\u8282\u95f4\u9699\u53d8\u7a84\uff0c\u5173\u8282\u9762\u6709\u866b\u8680\u6837\u9aa8\u8d28\u7834\u574f\uff0c\u53f3\u9acb\u81fc\u6709\u7ea62cm\u5927\u5c0f\u7684\u7a7a\u6d1e\u3002\u6700\u53ef\u80fd\u7684\u8bca\u65ad\u662f\nA. \u9acb\u5173\u8282\u9aa8\u5173\u8282\u708e\nB. \u9acb\u5173\u8282\u5316\u8113\u6027\u5173\u8282\u708e\nC. \u5168\u9acb\u5173\u8282\u7ed3\u6838\nD. \u80ba\u764c\u9aa8\u8f6c\u79fb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.530621905332792}}, {"question": "\u540e\u5b66\u4e60\u7684\u6750\u6599\u5bf9\u5148\u5b66\u4e60\u7684\u6750\u6599\u53d1\u751f\u5e72\u6270\u4f5c\u7528\u800c\u9020\u6210\u7684\u9057\u5fd8\u662f\nA. \u9057\u5fd8\u89c4\u5f8b\nB. \u540e\u6444\u6291\u5236\nC. \u524d\u6444\u6291\u5236\nD. \u63a5\u8fd1\u5f8b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.7942494217715753, "itpossible/Chinese-Mistral-7B-v0.1": 0.8617082651907976, "HuggingFaceH4/zephyr-7b-beta": 0.9594409190843529, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8635684906523784, "meta-llama/Meta-Llama-3-8B": 0.6655611949330891, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7888975165521291}}, {"question": "\u5728\u767d\u65cf\u59d1\u5a18\u7684\u5934\u9970\u4e0a\uff0c\u8574\u542b\u7740\u4e00\u4e2a\u5b8c\u7f8e\u719f\u6089\u7684\u8bcd\u8bed\uff0c\u5b83\u5c31\u662f\nA. \u6885\u5170\u7af9\u83ca\nB. \u9634\u6674\u5706\u7f3a\nC. \u98ce\u82b1\u96ea\u6708\nD. \u6625\u590f\u79cb\u51ac\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.42667047600356006, "HuggingFaceH4/zephyr-7b-beta": 0.6561511996920351, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6051261620231373}}, {"question": "\u7537\u6027\uff0c82\u5c81\u3002\u4f53\u578b\u8f83\u6d88\u7626\uff0c3 \u4e2a\u6708\u524d\u53e3\u670d\u8461\u8404\u7cd6\u8010\u91cf\u8bd5\u9a8c\u8bca\u4e3a\u7cd6\u5c3f\u75c5\uff0c\u5e73\u65f6\u7a7a\u8179\u8840\u7cd66.5~7.2mmol/L\uff0c\u9910\u540e2\u5c0f\u65f6\u8840\u7cd612~14 mmol/L\uff0c\u6709\u51a0\u5fc3\u75c5\u5fc3\u8870\u75c5\u53f210\u5e74\uff0c\u7ed3\u80a0\u764c\u672f\u540e 5\u5e74\u3002\u4e3a\u63a7\u5236\u8840\u7cd6\uff0c\u5e94\u9996\u9009\u7684\u836f\u7269\u662f\nA. \u80f0\u5c9b\u7d20\nB. \u90a3\u683c\u5217\u5948\nC. \u963f\u5361\u6ce2\u7cd6\nD. \u4e8c\u7532\u53cc\u808c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.594032755698128, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8445024177787926, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45038868890696304, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u901a\u7528\u7535\u6c14\u516c\u53f8\u6cd5\u5bf9\u4f01\u4e1a\u7684\u6218\u7565\u4e1a\u52a1\u5355\u4f4d\u8fdb\u884c\u5206\u6790\u548c\u8bc4\u4ef7\u65f6\u91c7\u7528\u7684\u662f\nA. \u591a\u56e0\u7d20\u6295\u8d44\u7ec4\u5408\u77e9\u9635\nB. PEST\u5206\u6790\u77e9\u9635\nC. \u6ce2\u58eb\u987f\u77e9\u9635\nD. SWOT\u5206\u6790\u77e9\u9635\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u516c\u94a5\u5bc6\u7801\u4f53\u5236\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f\nA. \u516c\u94a5\u5bc6\u7801\u4f53\u5236\u4e2d\u7684\u79c1\u94a5\u53ef\u4ee5\u7528\u6765\u8fdb\u884c\u6570\u5b57\u7b7e\u540d\nB. \u516c\u94a5\u5bc6\u7801\u4f53\u5236\u4e2d\u4ec5\u6839\u636e\u5bc6\u7801\u7b97\u6cd5\u548c\u52a0\u5bc6\u5bc6\u94a5\u6765\u786e\u5b9a\u89e3\u5bc6\u5bc6\u94a5\u5728\u8ba1\u7b97\u4e0a\u662f\u53ef\u884c\u7684\nC. \u5728\u4e00\u4e2a\u516c\u94a5\u5bc6\u7801\u4f53\u5236\u4e2d\uff0c\u4e00\u822c\u5b58\u5728\u516c\u94a5\u548c\u79c1\u94a5\u4e24\u4e2a\u5bc6\u94a5\nD. \u516c\u94a5\u5bc6\u7801\u4f53\u5236\u4e2d\u4ec5\u6839\u636e\u5bc6\u7801\u7b97\u6cd5\u548c\u52a0\u5bc6\u5bc6\u94a5\u6765\u786e\u5b9a\u89e3\u5bc6\u5bc6\u94a5\u5728\u8ba1\u7b97\u4e0a\u662f\u4e0d\u53ef\u884c\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8904158714034173, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u5bf9\u7f51\u7edc\u7684\u4f9d\u8d56\u6027\u6700\u9ad8\u7684\u65f6\u4ee3\u662f\nA. \u591a\u7f51\u5408\u4e00\u65f6\u4ee3\nB. PC\u65f6\u4ee3\nC. \u4e13\u7f51\u65f6\u4ee3\nD. \u4e3b\u673a\u65f6\u4ee3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.34671647917417475, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.48148341211631945, "meta-llama/Meta-Llama-3-8B": 0.6633024574336022, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6137569264764302}}, {"question": "\u2f53\u4f53\u5185\u80fd\u662f\u6240\u6709\u2f53\u4f53\u5206\u2f26\u70ed\u8fd0\u52a8\u52a8\u80fd\u548c\u52bf\u80fd\u7684\u603b\u548c\uff0c\u5176\u2f24\u2f29\u4e0e\u2f53\u4f53\u7684\u72b6\u6001\u6709\u5173\uff0c\u5206\u2f26\u70ed\u8fd0\u52a8\u7684\u5e73\u5747\u52a8\u80fd\u4e0e\u5206\u2f26\u95f4\u52bf\u80fd\u5206\u522b\u53d6\u51b3\u4e8e\u2f53\u4f53\u7684\nA. \u538b\u5f3a\u548c\u6e29\u5ea6\nB. \u4f53\u79ef\u548c\u538b\u5f3a\nC. \u6e29\u5ea6\u548c\u538b\u5f3a\nD. \u6e29\u5ea6\u548c\u4f53\u79ef\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "{\u8bed\u2f42\uff0c\u6570\u5b66\uff0c\u82f1\u8bed\uff0c\u4f53\u80b2}$\\cup${\u8bed\u2f42\uff0c\u6570\u5b66\uff0c\u82f1\u8bed\uff0c\u5386\u53f2\uff0c\u5730\u7406}\u662f\nA. {\u8bed\u2f42\uff0c\u6570\u5b66\uff0c\u82f1\u8bed}\nB. \u7a7a\u96c6 \nC. {\u5386\u53f2\uff0c\u5730\u7406\uff0c\u4f53\u80b2} \nD. {\u8bed\u2f42\uff0c\u6570\u5b66}\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3191152350487737, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.44667014224702756, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.2963332999770349, "meta-llama/Meta-Llama-3-8B": 0.3376787962540697, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.34445452180439035}}, {"question": "\u201c\u7184\u706b\u8fde\u4e09\u6708\uff0c\u5bb6\u4e66\u62b5\u4e07\u91d1\u201d\u53e4\u4ee3\u4e66\u4fe1\u901a\u8fc7\u90ae\u9a7f\u4f20\u9012\u3002\u5510\u4ee3\u7ba1\u7406\u8fd9\u7c7b\u5de5\u4f5c\u7684\u4e2d\u592e\u7ba1\u7406\u673a\u6784\u662f\nA. \u4e2d\u4e66\u7701\nB. \u5c1a\u4e66\u7701\nC. \u4e0b\u4e66\u7701\nD. \u95e8\u4e0b\u7701\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4097932893567426, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.47773126326913173, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5510\u4ee3\u67d0\u8bcf\u4ee4\u6279\u8bc4\u5f53\u65f6\u5b58\u5728\u201c\u6063\u884c\u541e\u5e76\uff0c\u83ab\u60e7\u7ae0\u7a0b\u201d\u548c\u201c\u53e3\u5206\u6c38\u4e1a\uff08\u56fd\u5bb6\u6388\u4e88\u7684\u7530\u5730\uff09\uff0c\u8fdd\u6cd5\u5356\u4e70\u201d\u7684\u73b0\u8c61\u3002\u8fd9\u8868\u660e\u5f53\u65f6\nA. \u201c\u5e02\u201d\u7a81\u7834\u7a7a\u95f4\u9650\u5236\nB. \u4e95\u7530\u5236\u74e6\u89e3\nC. \u5747\u7530\u5236\u53d7\u5230\u7834\u574f\nD. \u5206\u5c01\u5236\u6062\u590d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.30601362565976303, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6783902008265088, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6995574362149363}}, {"question": "\u79e6\u59cb\u7687\u7edf\u4e00\u4e2d\u56fd\u540e\uff0c\u5c06\u5168\u56fd\u6587\u5b57\u7edf\u4e00\u6210\u4e86\u4e0b\u9762\u54ea\u4e00\u79cd\nA. \u5c0f\u7bc6\nB. \u9e1f\u866b\u6587\nC. \u884c\u4e66\nD. \u6977\u4e66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6821880750840191, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8830282394917478, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8965342802409904}}, {"question": "\u4e0b\u5217\u6025\u6027\u5fc3\u808c\u6897\u6b7b\u5371\u5bb3\u6027\u6700\u5927\u7684\u662f\nA. \u4e0b\u58c1\u5408\u5e76\u540e\u58c1\u5fc3\u808c\u6897\u6b7b\nB. \u524d\u58c1\u5408\u5e76\u9ad8\u4fa7\u58c1\u5fc3\u808c\u6897\u6b7b\nC. \u4e0b\u58c1\u5fc3\u808c\u6897\u6b7b\u5e76\u53d1\u4e09\u5ea6\u623f\u5ba4\u4f20\u5bfc\u963b\u6ede\nD. \u524d\u58c1\u5fc3\u808c\u6897\u6b7b\u5e76\u53d1\u4e09\u5ea6\u623f\u5ba4\u4f20\u5bfc\u963b\u6ede\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4436867527613698, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.46057751535673197, "HuggingFaceH4/zephyr-7b-beta": 0.9040484731995295, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e73\u7cdc\u5fae\u7c92\u4e2d\u542b\u91cf\u6700\u591a\u7684\u6210\u5206\u662f\nA. \u86cb\u767d\u8d28\nB. \u78f7\u8102\nC. \u80c6\u56fa\u9187\nD. \u4e09\u9170\u7518\u6cb9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5501667453606544, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9762\u8bbf\u8c03\u67e5\u7684\u4e00\u4e2a\u4e3b\u8981\u7f3a\u70b9\u662f\nA. \u65f6\u95f4\u77ed\uff0c\u96be\u4ee5\u6df1\u5165\nB. \u7b54\u6848\u7b80\u5355\uff0c\u96be\u4ee5\u6df1\u5165\nC. \u8c03\u67e5\u5bf9\u8c61\u6613\u53d7\u8bbf\u95ee\u8005\u7684\u4e3b\u89c2\u5f71\u54cd\nD. \u8c03\u67e5\u5bf9\u8c61\u4e0d\u7406\u89e3\u95ee\u5377\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.82017560913128, "meta-math/MetaMath-Mistral-7B": 0.8853565536684433, "itpossible/Chinese-Mistral-7B-v0.1": 0.7101653997988835, "HuggingFaceH4/zephyr-7b-beta": 0.9999406549310592, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.755794989553765, "meta-llama/Meta-Llama-3-8B": 0.6526395411473102, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.718210197816761}}, {"question": "\u6709\u4e9b\u540c\u5b66\u4e00\u65e6\u8003\u4e0a\u5927\u5b66\uff0c\u538c\u5b66\u60c5\u7eea\u5c31\u5f00\u59cb\u6076\u6027\u91ca\u653e\uff0c\u62d2\u7edd\u52aa\u529b\u5b66\u4e60\uff0c\u8fd9\u4e0d\u5f97\u4e0d\u8ba9\u6211\u4eec\u91cd\u65b0\u601d\u8003\u6559\u80b2\u7684\u610f\u4e49\u3002\u4e0b\u5217\u5173\u4e8e\u6559\u80b2\u7684\u5185\u6db5\uff0c\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f()\nA. \u6559\u80b2\u7684\u6700\u7ec8\u76ee\u6807\u662f\u4f20\u6388\u77e5\u8bc6\nB. \u6559\u80b2\u4ee5\u5f71\u54cd\u4eba\u7684\u8eab\u5fc3\u53d1\u5c55\u4e3a\u76f4\u63a5\u6216\u9996\u8981\u76ee\u6807\nC. \u6559\u80b2\u6700\u76f4\u63a5\u7684\u76ee\u6807\u662f\u4fc3\u8fdb\u4eba\u7684\u8eab\u5fc3\u53d1\u5c55\nD. \u6559\u80b2\u7684\u57fa\u672c\u7740\u773c\u70b9\u662f\u4eba\uff0c\u662f\u4eba\u7684\u53d1\u5c55\uff0c\u4eba\u662f\u6559\u80b2\u7684\u51fa\u53d1\u70b9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8816078491811202, "meta-math/MetaMath-Mistral-7B": 0.9813206300973522, "itpossible/Chinese-Mistral-7B-v0.1": 0.7213094764901727, "HuggingFaceH4/zephyr-7b-beta": 0.9719646416496318, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.917812162333221, "meta-llama/Meta-Llama-3-8B": 0.861890602209643, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6293667083601774}}, {"question": "\u5e02\u573a\u8425\u9500\u7ba1\u7406\u8005\u6b32\u5bf9\u6d88\u8d39\u8005\u7684\u5e74\u9f84\u52a0\u4ee5\u89c2\u6d4b\uff0c\u8fd9\u79cd\u8fdb\u884c\u6d4b\u5b9a\u3001\u52a0\u5de5\u7684\u5c3a\u5ea6\u662f\nA. \u6bd4\u4f8b\u5c3a\u5ea6\nB. \u987a\u5e8f\u5c3a\u5ea6\nC. \u95f4\u8ddd\u5c3a\u5ea6\nD. \u540d\u4e49\u5c3a\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3331189220919959, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.442238156315394, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u540d\u53e5\u4e2d\uff0c\u4e0d\u662f\u51fa\u81ea\u5c48\u539f\u300a\u79bb\u9a9a\u300b\u7684\u662f\nA. \u60df\u8349\u6728\u4e4b\u96f6\u843d\u516e\uff0c\u6050\u7f8e\u4eba\u4e4b\u8fdf\u66ae\nB. \u4ea6\u4f59\u5fc3\u4e4b\u6240\u5584\u516e\uff0c\u867d\u4e5d\u6b7b\u5176\u72b9\u672a\u6094\nC. \u60bc\u826f\u4f1a\u4e4b\u6c38\u8bc0\u516e\uff0c\u54c0\u4e00\u901d\u800c\u5f02\u4e61\nD. \u8def\u6f2b\u6f2b\u5176\u4fee\u8fdc\u516e\uff0c\u543e\u5c06\u4e0a\u4e0b\u800c\u6c42\u7d22\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.2966017332563094, "itpossible/Chinese-Mistral-7B-v0.1": 0.3632122790984035, "HuggingFaceH4/zephyr-7b-beta": 0.616966566689929, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38893657498403433, "meta-llama/Meta-Llama-3-8B": 0.5150773314693601, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7750098602178355}}, {"question": "\u98ce\u9669\u8865\u507f\u7387\u662f\nA. \u52a0\u5728\u65e0\u98ce\u9669\u7684\u8d34\u73b0\u7387\u4e0a\uff0c\u4ee5\u4fbf\u5728\u51b3\u7b56\u4e2d\u628a\u98ce\u9669\u8003\u8651\u8fdb\u53bb\nB. \u53ea\u4f9b\u98ce\u9669\u51b3\u7b56\u8005\u51b3\u7b56\u65f6\u7528\nC. B\u548cC\u90fd\u5bf9\nD. \u8981\u4ece\u65e0\u98ce\u9669\u8d34\u73b0\u7387\u51cf\u53bb\uff0c\u4ee5\u4fbf\u5728\u51b3\u7b56\u4e2d\u8003\u8651\u98ce\u9669\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5425125249165893, "meta-math/MetaMath-Mistral-7B": 0.6941791061712363, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9766166127548292, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5758813524344352, "meta-llama/Meta-Llama-3-8B": 0.42545358471254624, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8260477065114731}}, {"question": "\u5728\u53e4\u4ee3\u897f\u65b9\uff0c\u5bf9\u5e74\u8f7b\u4e00\u4ee3\u4e0d\u4ec5\u5f3a\u8c03\u4f53\u80b2\u548c\u9053\u5fb7\u6559\u80b2\uff0c\u4e5f\u5341\u5206\u91cd\u89c6\u667a\u80b2\u548c\u7f8e\u80b2\uff0c\u5bf9\u513f\u7ae5\u5b9e\u65bd\u5fb7\u3001\u667a\u3001\u4f53\u3001\u7f8e\u548c\u8c10\u53d1\u5c55\u7684\u6559\u80b2\u7684\u662f(\nA. \u65af\u5df4\u8fbe\u4eba\nB. \u5370\u5ea6\u4eba\nC. \u96c5\u5178\u4eba\nD. \u5e0c\u4f2f\u6765\u4eba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8202316883391967, "meta-math/MetaMath-Mistral-7B": 0.9579402161410647, "itpossible/Chinese-Mistral-7B-v0.1": 0.6456758681284144, "HuggingFaceH4/zephyr-7b-beta": 0.9983459474256048, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.932382556602733, "meta-llama/Meta-Llama-3-8B": 0.35823796793174434, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9465787694778951}}, {"question": "\u5728\u6211\u56fd\u7684\u6cd5\u5f8b\u76d1\u7763\u4f53\u7cfb\u4e2d\uff0c\u4ece\u56fd\u52a1\u9662\u5230\u5730\u65b9\u5404\u7ea7\u4eba\u6c11\u653f\u5e9c\u7684\u6cd5\u5f8b\u76d1\u7763\u662f\nA. \u56fd\u5bb6\u53f8\u6cd5\u673a\u5173\u7684\u76d1\u7763\nB. \u56fd\u5bb6\u6743\u529b\u673a\u5173\u7684\u76d1\u7763\nC. \u56fd\u5bb6\u884c\u653f\u673a\u5173\u7684\u76d1\u7763\nD. \u793e\u4f1a\u6027\u7684\u76d1\u7763\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6731061492550338, "meta-math/MetaMath-Mistral-7B": 0.7866115704616484, "itpossible/Chinese-Mistral-7B-v0.1": 0.6801518794435262, "HuggingFaceH4/zephyr-7b-beta": 0.9818629779209986, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6091667768972837, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u76ee\u524d\u8bca\u65ad\u80be\u764c\u6700\u53ef\u9760\u7684\u5f71\u50cf\u5b66\u65b9\u6cd5\u662f\nA. IVU\nB. CT\nC. \u80be\u52a8\u8109\u9020\u5f71\nD. B\u8d85\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5160880565709193, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7558958265876465, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9969947172184248}}, {"question": "\u67d0\u4f01\u4e1a10\u6708\u672b\u8d1f\u503a\u603b\u989d1500\u4e07\u5143\uff6111\u6708\u4efd\u6536\u56de\u5e94\u6536\u8d26\u6b3e150\u4e07\u5143\u5b58\u5165\u94f6\u884c\uff0c\u7528\u94f6\u884c\u5b58\u6b3e\u507f\u8fd8\u5e94\u4ed8\u8d26\u6b3e200\u4e07\u5143\uff0c\u9884\u4ed8\u8d2d\u8d27\u6b3e100\u4e07\u5143\uff61\u8be5\u4f01\u4e1a11\u6708\u672b\u8d1f\u503a\u603b\u989d\u4e3a\nA. 1850\u4e07\u5143\nB. 1350\u4e07\u5143\nC. 1600\u4e07\u5143\nD. 1300\u4e07\u5143\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4e8e\u4e00\u4e2a\u53cc\u51cf\u56e0\u6a21\u578b\uff0c\u5df2\u77e5:(1)$\\mu_{x+t}^{(1)}=\\frac{k}{50-t}\uff0c0 \\leqslant t<50$;(2)$\\mu_{x+t}^{(2)}=\\frac{1}{50-t}\uff0c0 \\leqslant t<50$;(3)$h(2 \\mid T=t)=0.5\uff0c0 \\leqslant t<50$\u3002\u5219$g(20)=()$\u3002\nA. 0.048\nB. 0.036\nC. 0.024\nD. 0.012\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.295278727801358, "meta-math/MetaMath-Mistral-7B": 0.29863342676099575, "itpossible/Chinese-Mistral-7B-v0.1": 0.2885095257630687, "HuggingFaceH4/zephyr-7b-beta": 0.43878958933010315, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.28850952576306876, "meta-llama/Meta-Llama-3-8B": 0.2986334267609957, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.31911525521630035}}, {"question": "\u5173\u4e8e\u9057\u4f20\u4e0e\u53d8\u5f02\u7684\u5173\u7cfb\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u9057\u4f20\u548c\u53d8\u5f02\u90fd\u662f\u7edd\u5bf9\u7684\nB. \u9057\u4f20\u662f\u76f8\u5bf9\u7684\uff0c\u800c\u53d8\u5f02\u662f\u7edd\u5bf9\u7684\nC. \u9057\u4f20\u662f\u7edd\u5bf9\u7684\uff0c\u800c\u53d8\u5f02\u662f\u76f8\u5bf9\u7684\nD. \u9057\u4f20\u548c\u53d8\u5f02\u90fd\u662f\u76f8\u5bf9\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4519326834903132, "meta-math/MetaMath-Mistral-7B": 0.6054595112506579, "itpossible/Chinese-Mistral-7B-v0.1": 0.34600580950320253, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7108084528799007, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u68c0\u9a8c\u7528FeCl3\u6eb6\u6db2\u8150\u8680\u94dc\u5236\u5370\u5237\u7ebf\u8def\u677f\u540e\u6240\u5f97\u7684\u5e9f\u6db2\u6210\u5206\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4e0b\u5217\u6839\u636e\u5b9e\u9a8c\u73b0\u8c61\u5f97\u51fa\u7684\u7ed3\u8bba\u4e0d\u6b63\u786e\u7684\u662f\nA. \u5411\u5e9f\u6db2\u4e2d\u52a0\u5165\u5c11\u91cf\u7684\u8461\u8404\u7cd6\u6eb6\u6db2\uff0c\u65e0\u7ea2\u8272\u6c89\u6dc0\u4ea7\u751f\uff0c\u8bf4\u660e\u5e9f\u6db2\u4e2d\u4e0d\u542bCu2\uff0b\nB. \u5411\u5e9f\u6db2\u4e2d\u6ef4\u52a0\u785d\u9178\u9178\u5316\u7684AgNO3\u6eb6\u6db2\uff0c\u4ea7\u751f\u767d\u8272\u6c89\u6dc0\uff0c\u8bf4\u660e\u5e9f\u6db2\u4e2d\u542b\u6709Cl\uff0d\nC. \u5411\u5e9f\u6db2\u4e2d\u52a0\u5c11\u91cf\u94c1\u7c89\uff0c\u5145\u5206\u53cd\u5e94\u540e\u65e0\u56fa\u4f53\u5269\u4f59\uff0c\u8bf4\u660e\u5e9f\u6db2\u4e2d\u542b\u6709Fe3\uff0b\nD. \u5411\u5e9f\u6db2\u4e2d\u6ef4\u52a0KSCN\u6eb6\u6db2\uff0c\u65e0\u73b0\u8c61\uff0c\u518d\u52a0\u5165\u6c2f\u6c34\u540e\u6eb6\u6db2\u5448\u7ea2\u8272\uff0c\u8bf4\u660e\u5e9f\u6db2\u4e2d\u542b\u6709Fe2\uff0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.46997668284660044}}, {"question": "\u4e0b\u5217\u5404\u7ec4\u7269\u7406\u91cf\u5355\u4f4d\u4e2d\uff0c\u5168\u5c5e\u4e8e\u56fd\u9645\u5355\u4f4d\u5236\u4e2d\u57fa\u672c\u5355\u4f4d\u7684\u662f\nA. kg\u3001T\u3001N\nB. kg\u3001A\u3001s\nC. N\u3001A\u3001s\nD. kg\u3001V\u3001m\uff0fs\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5404\u7ec4\u5b57\uff0c\u5c5e\u4e8e\u5f02\u4f53\u5b57\u5173\u7cfb\u7684\u4e00\u7ec4\u662f\nA. \u4e26\u4e00\u7add\nB. \u66b4\u4e00\u66dd\nC. \u7f6e\u2014\u5bd8\nD. \u932b\u4e00\u8cdc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2986334267609957, "meta-math/MetaMath-Mistral-7B": 0.402028656425726, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3710688922946265, "meta-llama/Meta-Llama-3-8B": 0.29863342676099575, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5111935173554286}}, {"question": "\u4ee5\u4e0b\u72af\u7f6a\u5c5e\u4e8e\u7eaf\u7cb9\u4e0d\u4f5c\u4e3a\u72af\u7684\u662f\nA. \u63a9\u9970\u9690\u7792\u72af\u7f6a\u6240\u5f97\u7f6a\nB. \u7ed1\u67b6\u7f6a\nC. \u91cd\u5a5a\u7f6a\nD. \u9057\u5f03\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4045257140067991, "meta-math/MetaMath-Mistral-7B": 0.6048696235419591, "itpossible/Chinese-Mistral-7B-v0.1": 0.7988457744977407, "HuggingFaceH4/zephyr-7b-beta": 0.9910835437880998, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6551218822971304, "meta-llama/Meta-Llama-3-8B": 0.5008652262720363, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u540c\u6e90\u67d3\u8272\u4f53\u7684\u914d\u5bf9\uff0c\u53d1\u5728\u51cf\u6570\u5206\u88c2\u2160\u7684\nA. \u7ec6\u7ebf\u671f\nB. \u5076\u7ebf\u671f\nC. \u53cc\u7ebf\u671f\nD. \u7c97\u7ebf\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3297196075153115, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4798427743244047, "meta-llama/Meta-Llama-3-8B": 0.37106892011530573, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5646917260264805}}, {"question": "\u5317\u7ea6\u4e8e\u54ea\u4e00\u5e74\u5b9e\u73b0\u4e86\u7b2c\u4e00\u6b21\u4e1c\u6269\nA. 2000\u5e74\nB. 2005\u5e74\nC. 1999\u5e74\nD. 2001\u5e74\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7000143247657844, "meta-math/MetaMath-Mistral-7B": 0.9120170803029348, "itpossible/Chinese-Mistral-7B-v0.1": 0.7213857111350538, "HuggingFaceH4/zephyr-7b-beta": 0.9517226523014585, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6614542131158184, "meta-llama/Meta-Llama-3-8B": 0.4041551194358978, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6d88\u8d39\u8005\u5728\u8d2d\u4e70\u4ef7\u683c\u6602\u8d35\u3001\u8d2d\u4e70\u9891\u7387\u4f4e\u3001\u54c1\u724c\u5dee\u5f02\u5927\u3001\u4e0d\u719f\u6089\u7684\u4ea7\u54c1\u65f6\uff0c\u4f1a\u6295\u5165\u5f88\u5927 \u7684\u7cbe\u529b\u548c\u65f6\u95f4\uff0c\u8fd9\u7c7b\u8d2d\u4e70\u884c\u4e3a\u5c5e\u4e8e\nA. \u53d8\u6362\u578b\u8d2d\u4e70\nB. \u534f\u8c03\u578b\u8d2d\u4e70\nC. \u4e60\u60ef\u578b\u8d2d\u4e70\nD. \u590d\u6742\u578b\u8d2d\u4e70\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.715087817004526, "HuggingFaceH4/zephyr-7b-beta": 0.9785960927477669, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6315996911529025, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8104846857451822}}, {"question": "\u5982\u679c\u80fd\u591f\u505a\u597d\u5357\u6781\u751f\u6001\u73af\u5883\u7684\u627f\u53d7\u529b\u8bc4\u4f30\uff0c\u5e76\u4e25\u683c\u6267\u884c\u6709\u5173\u89c4\u5b9a\uff0c\u5357\u6781\u65c5\u6e38\u9020\u6210\u7684\u751f\u6001\u5f71\u54cd\u5c31\u53ef\u4ee5\u5f97\u5230\u6709\u6548\u63a7\u5236\u3002\u56e0\u4e3a\u4e0e\u5357\u6781\u5927\u9646\u6781\u4e3a\u5e7f\u88a4\u7684\u5730\u57df\u76f8\u6bd4\uff0c\u6e38\u5ba2\u6d3b\u52a8\u7684\u5730\u57df\u4ec5\u4e3a\u603b\u9762\u79ef\u76843%\u5de6\u53f3\u3002\u800c\u5982\u4f55\u7ba1\u7406\u597d\u957f\u671f\u8bbe\u5728\u5357\u6781\u7684\u79d1\u8003\u7ad9\uff0c\u624d\u662f\u66f4\u4e3a\u68d8\u624b\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u4e00\u4e9b\u88ab\u5e9f\u5f03\u7684\u79d1\u8003\u7ad9\uff0c\u5df2\u5bf9\u5357\u6781\u751f\u6001\u73af\u5883\u9020\u6210\u4e86\u65b0\u7684\u5a01\u80c1\u3002\u8fd9\u6bb5\u6587\u5b57\u610f\u5728\u5f3a\u8c03\nA. \u5357\u6781\u751f\u6001\u73af\u5883\u7ba1\u7406\u673a\u5236\u7684\u5fc5\u8981\u6027\nB. \u5357\u6781\u79d1\u8003\u7ad9\u5bf9\u5f53\u5730\u751f\u6001\u73af\u5883\u7684\u5f71\u54cd\nC. \u5357\u6781\u751f\u6001\u65c5\u6e38\u7684\u53d1\u5c55\u524d\u666f\nD. \u5357\u6781\u751f\u6001\u73af\u5883\u8bc4\u4f30\u7684\u610f\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.9455032929198472, "itpossible/Chinese-Mistral-7B-v0.1": 0.35531236441535985, "HuggingFaceH4/zephyr-7b-beta": 0.7452749540711219, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.638200102948751, "meta-llama/Meta-Llama-3-8B": 0.7854941857232706, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9451359070992028}}, {"question": "1941\u5e742\u6708\uff0c\u4ee5\u7f8e\u56fd\u603b\u7edf\u7f57\u65af\u798f\u4ee3\u8868\u8eab\u4efd\u6765\u534e\u7684\u5c45\u91cc\u6b63\u5f0f\u5411\u848b\u4ecb\u77f3\u58f0\u660e\uff1a\u201c\u7f8e\u56fd\u5728\u56fd\u5171\u7ea0\u7eb7\u672a\u89e3\u51b3\u524d\uff0c\u65e0\u6cd5\u5927\u91cf\u63f4\u534e\u3002\u4e2d\u7f8e\u95f4\u7684\u7ecf\u6d4e\u3001\u8d22\u653f\u7b49\u95ee\u9898\u4e0d\u53ef\u80fd\u6709\u4efb\u4f55\u8fdb\u5c55\u3002\u201d\u8fd9\u8868\u660e\u5f53\u65f6\u7f8e\u56fd\nA. \u53cd\u5bf9\u56fd\u6c11\u515a\u91c7\u53d6\u7684\u53cd\u5171\u653f\u7b56\nB. \u4ee5\u4e2d\u56fd\u5185\u90e8\u7ea0\u7eb7\u4e3a\u7531\u62d2\u7edd\u5bf9\u534e\u63f4\u52a9\nC. \u501f\u52a9\u63f4\u52a9\u4fc3\u6210\u56fd\u5171\u4e24\u515a\u5168\u529b\u6297\u65e5\nD. \u6566\u4fc3\u56fd\u5171\u4e24\u515a\u653e\u5f03\u5404\u81ea\u7684\u653f\u6cbb\u4e3b\u5f20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u9885\u5e95\u5916\u4f24\u7684\u75c5\u4eba\uff0c\u8111\u810a\u6db2\u548c\u8840\u6db2\u4ece\u9f3b\u8154\u6d41\u51fa\uff0c\u8bd5\u95ee\u662f\u4f24\u53ca\u4ee5\u4e0b\u54ea\u4e2a\u7ed3\u6784\uff1a\nA. \u7b5b\u9aa8\u7b5b\u677f\nB. \u816d\u9aa8\nC. \u8776\u9aa8\u4f53\nD. \u9f3b\u9aa8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.503251268061006, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.41172527592979796}}, {"question": "\u7f16\u5236\u516c\u5171\u5173\u7cfb\u9884\u7b97\u7684\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\nA. \u9886\u5bfc\u51b3\u7b56\u6cd5\nB. \u4f1a\u8bae\u8ba8\u8bba\u6cd5\nC. \u9500\u552e\u989d\u63d0\u6210\u6cd5\nD. \u5b9e\u62a5\u5b9e\u9500\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6211\u56fd\uff0c\u53ea\u6709\u8fdb\u4e00\u6b65\u589e\u5f3a\u516c\u6709\u5236\u7ecf\u6d4e\u7684\u6d3b\u529b\uff0c\u624d\u80fd\nA. \u66f4\u5145\u5206\u5730\u53d1\u6325\u5e02\u573a\u5728\u8d44\u6e90\u914d\u7f6e\u4e2d\u7684\u57fa\u7840\u6027\u4f5c\u7528\nB. \u9632\u6b62\u4e24\u6781\u5206\u5316\uff0c\u5b9e\u73b0\u540c\u6b65\u5bcc\u88d5\nC. \u4fdd\u8bc1\u6211\u56fd\u793e\u4f1a\u4e3b\u4e49\u7ecf\u6d4e\u7684\u53d1\u5c55\u65b9\u5411\nD. \u5de9\u56fa\u516c\u6709\u5236\u7ecf\u6d4e\u7684\u4e3b\u5bfc\u5730\u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7531\u4e8e\u7cbe\u795e\u538b\u529b\u5f15\u8d77\u7684\u547c\u5438\u7cfb\u7edf\u75be\u75c5\u6709\nA. \u80c3\u6e83\u75a1\nB. \u795e\u7ecf\u6027\u5455\u5410\nC. \u652f\u6c14\u7ba1\u54ee\u5598\nD. \u795e\u7ecf\u6027\u76ae\u708e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43838726297790565, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.833560930421444, "HuggingFaceH4/zephyr-7b-beta": 0.9843621363293742, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7130843816602459, "meta-llama/Meta-Llama-3-8B": 0.903442810791047, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9464135639614477}}, {"question": "\u8fd1\u4ee3\u65b0\u95fb\u4e8b\u4e1a\u6b63\u5f0f\u5f62\u6210\u4e8e\nA. \u8d44\u672c\u4e3b\u4e49\u793e\u4f1a\u521d\u671f\nB. \u5c01\u5efa\u793e\u4f1a\u672b\u671f\nC. \u5974\u96b6\u793e\u4f1a\u672b\u671f\nD. \u5c01\u5efa\u793e\u4f1a\u4e2d\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8d23\u4efb\u5185\u5bb9\u4e2d\uff0c\u5e94\u7531\u65bd\u5de5\u9879\u76ee\u7ecf\u7406\u627f\u62c5\u7684\u662f\nA. \u65bd\u5de5\u5b89\u5168\uff0c\u8d28\u91cf\u8d23\u4efb\nB. \u9879\u76ee\u6295\u6807\u8d23\u4efb\nC. \u4f01\u4e1a\u5e02\u573a\u7ecf\u8425\u8d23\u4efb\nD. \u4f01\u4e1a\u603b\u90e8\u7ba1\u7406\u8d23\u4efb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6411274860397314, "meta-math/MetaMath-Mistral-7B": 0.98686264284086, "itpossible/Chinese-Mistral-7B-v0.1": 0.8936991628728852, "HuggingFaceH4/zephyr-7b-beta": 0.999084656887608, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9503676910642769, "meta-llama/Meta-Llama-3-8B": 0.8136303972190728, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9970747862935522}}, {"question": "\u519c\u4e1a\u751f\u6001\u7cfb\u7edf\u517b\u5206\u5faa\u73af\u7684\u4e09\u4e2a\u4e3b\u8981\u517b\u5206\u5e93\u662f\nA. \u6c34\u4f53\u5e93\u3001\u571f\u58e4\u5e93\u3001\u751f\u7269\u5e93\nB. \u571f\u58e4\u6709\u6548\u517b\u5206\u5e93\u3001\u571f\u58e4\u77ff\u7269\u5e93\u3001\u571f\u58e4\u6709\u673a\u7269\u5e93\nC. \u690d\u7269\u5e93\u3001\u52a8\u7269\u5e93\u3001\u571f\u58e4\u5e93\nD. \u571f\u58e4\u5e93\u3001\u5ca9\u77f3\u5e93\u3001\u751f\u7269\u5e93\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6434940207401038, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5858957832727165, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7d2b\u8272\u6d0b\u8471\u662f\u751f\u7269\u5b66\u5b9e\u9a8c\u7684\u5e38\u7528\u6750\u6599\uff0c\u4ee5\u4e0b\u53d9\u8ff0\u9519\u8bef\u7684\u662f\nA. \u89c2\u5bdf\u6709\u4e1d\u5206\u88c2\uff0c\u5b9c\u9009\u53d6\u6d0b\u8471\u6839\u5c16\u5206\u751f\u533a\u7ec6\u80de\nB. \u89c2\u5bdf\u8d28\u58c1\u5206\u79bb\u4e0e\u590d\u539f\uff0c\u5b9c\u9009\u53d6\u7d2b\u8272\u9cde\u7247\u53f6\u5916\u8868\u76ae\u7ec6\u80de\nC. \u63d0\u53d6\u6db2\u6ce1\u4e2d\u7684\u7d2b\u8272\u8272\u7d20\uff0c\u53ef\u7528\u6e05\u6c34\u4f5c\u6eb6\u5242\nD. \u9009\u53d6\u7ecf\u4f4e\u6e29\u8bf1\u5bfc\u7684\u6d0b\u8471\u6839\u5c16\u5236\u6210\u7684\u4e34\u65f6\u88c5\u7247\uff0c\u5728\u663e\u5fae\u955c\u4e0b\u53ef\u89c2\u5bdf\u5230\u8054\u4f1a\u73b0\u8c61\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e00\u4e2a\u51cf\u6cd5\u7b97\u5f0f\u91cc\uff0c\u88ab\u51cf\u6570\u3001\u51cf\u6570\u4e0e\u5dee\u7684\u548c\u662f120\uff0c\u4e14\u5dee\u662f\u51cf\u6570\u76843\u500d\uff0c\u5dee\u662f\nA. 45\nB. 40\nC. 20\nD. 60\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.2926661889467771, "itpossible/Chinese-Mistral-7B-v0.1": 0.2731272040287072, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.28108825044289903, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u60a3\u8005\u5bb6\u5c5e\u6765\u533b\u9662\u627e\u9aa8\u79d1\u533b\u751f\uff0c\u8bc9\u5176\u4eb2\u5c5e\u56e0\u53f3\u811a\u8e1d\u90e8\u626d\u4f24\u4e0d\u80fd\u4e0a\u73ed\uff0c\u8bf7\u6c42\u5f00\u5177\u5047\u6761\u3002\u533b\u751f\u8981\u6c42\u60a3\u8005\u524d\u6765\u5c31\u8bca\uff0c\u5bb6\u5c5e\u8ff0\u60a3\u8005\u884c\u52a8\u56f0\u96be\uff0c\u6765\u533b\u9662\u4e0d\u4fbf\u3002\u6839\u636e\u300a\u533b\u5e08\u6cd5\u300b\u76f8\u5173\u89c4\u5b9a\uff0c\u533b\u751f\u7684\u6b63\u786e\u505a\u6cd5\u662f\nA. \u8003\u8651\u60a3\u8005\u884c\u52a8\u4e0d\u4fbf\uff0c\u4e3a\u5176\u5f00\u5177\u5047\u6761\nB. \u8003\u8651\u5bb6\u5c5e\u5e26\u60a3\u8005\u6765\u8bca\u56f0\u96be\uff0c\u4e3a\u5176\u5f00\u5177\u5047\u6761\nC. \u843d\u5b9e\u7b80\u5316\u5c31\u533b\u6d41\u7a0b\uff0c\u4e3a\u5176\u5f00\u5177\u5047\u6761\nD. \u672a\u7ecf\u4eb2\u81ea\u8bca\u67e5\u60a3\u8005\uff0c\u4e0d\u5f00\u5177\u5047\u6761\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4496067912916617, "meta-math/MetaMath-Mistral-7B": 0.4208921300529744, "itpossible/Chinese-Mistral-7B-v0.1": 0.6908289906680901, "HuggingFaceH4/zephyr-7b-beta": 0.8921541088871353, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8574569994873176, "meta-llama/Meta-Llama-3-8B": 0.926193889487822, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9059451544460151}}, {"question": "\u4e0b\u5217\u4e0d\u7b26\u5408\u80be\u5c0f\u7403\u6e90\u6027\u8840\u5c3f\u7684\u662f\nA. \u7ea2\u7ec6\u80de\u7ba1\u578b\nB. \u5c3f\u7ea2\u7ec6\u80de\u5bf9\u79f0\u5bb9\u79ef\u5206\u5e03\u66f2\u7ebf\nC. \u53ef\u4f34\u5927\u91cf\u86cb\u767d\u5c3f\nD. \u76f8\u5dee\u663e\u5fae\u955c\u89c1\u5927\u91cf\u53d8\u5f62\u7ea2\u7ec6\u80de\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f51\u7edc\u653b\u51fb\u4e0e\u9632\u5fa1\u5904\u4e8e\u4e0d\u5bf9\u79f0\u72b6\u6001\u662f\u56e0\u4e3a\nA. \u5e94\u7528\u7684\u8106\u5f31\u6027\nB. \u7ba1\u7406\u7684\u8106\u5f31\u6027\nC. \u8f6f\u4ef6\u7684\u8106\u5f31\u6027\nD. \u7f51\u7edc\u8f6f\uff0c\u786c\u4ef6\u7684\u590d\u6742\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8617238164346535, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6067991166928565, "meta-llama/Meta-Llama-3-8B": 0.5014154602161829, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8092655543667763}}, {"question": "\u7532\u4ee55\u4e07\u5143\u8d2d\u5f97\u4e00\u5757\u624b\u8868\uff0c\u7532\u7684\u670b\u53cb\u4e59\u53d1\u73b0\u8be5\u8868\u7cfb\u9ad8\u4eff\u54c1\uff0c\u4f46\u672a\u544a\u8bc9\u7532\u3002\u4e19\u89c1\u8be5\u8868\u6709\u610f\u8d2d\u4e70\uff0c\u4e59\u4e3a\u8ba9\u4e19\u4e70\u4e0b\u8be5\u8868\uff0c\u5bf9\u4e19\u58f0\u79f0\u8be5\u8868\u7cfb\u7edd\u7248\u6b63\u54c1\uff0c\u4e19\u4fe1\u4ee5\u4e3a\u771f\uff0c\u9042\u4ee55.5\u4e07\u5143\u4e70\u4e0b\u8be5\u8868\u3002\u7532\u4e19\u4e4b\u95f4\u4e70\u5356\u5408\u540c\u7684\u6548\u529b\nA. \u6548\u529b\u5f85\u5b9a\nB. \u53ef\u64a4\u9500\nC. \u65e0\u6548\nD. \u6709\u6548\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3905323461588276, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6133324863884445, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.44741285930580843}}, {"question": "\u5c0f\u4f24\u53e3\u51fa\u8840\u65f6\uff0c\u6211\u4eec\u8981\u7528___\u51b2\u6d17\u4f24\u53e3\nA. \u751f\u7406\u76d0\u6c34\nB. \u5168\u4e0d\u5bf9\nC. \u51b7\u6c34\nD. \u70ed\u6c34\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3823323838725932, "meta-math/MetaMath-Mistral-7B": 0.5101503317493314, "itpossible/Chinese-Mistral-7B-v0.1": 0.6416297919148657, "HuggingFaceH4/zephyr-7b-beta": 0.9781406783778481, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45403639984712674, "meta-llama/Meta-Llama-3-8B": 0.7561008082295442, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8092181340704315}}, {"question": "\u4e0b\u5217\u4e0d\u5c5e\u4e8e\u67f1\u72b6\u4e0a\u76ae\u5316\u751f\u7684\u6709\nA. \u6162\u6027\u5bab\u9888\u708e\u5f62\u6210\u7684\u5b50\u5bab\u9888\u7cdc\u70c2\nB. Barrett \u98df\u7ba1\nC. \u80c3\u9ecf\u819c\u4e0a\u76ae\u8f6c\u53d8\u4e3a\u80a0\u9ecf\u819c\u4e0a\u76ae\nD. \u809d\u80c6\u7ed3\u77f3\u65f6\u53d1\u751f\u7684\u5316\u751f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5383105747024727, "meta-math/MetaMath-Mistral-7B": 0.9286638214948308, "itpossible/Chinese-Mistral-7B-v0.1": 0.5326190845563334, "HuggingFaceH4/zephyr-7b-beta": 0.9999189794335098, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.965350335009473, "meta-llama/Meta-Llama-3-8B": 0.504540483486473, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679c\u7b49\u5dee\u6570\u5217$\\left\\{ a_{n} \\right\\}$\u4e2d\uff0c$a_{3}+a_{5}+a_{7}=12$\uff0c\u90a3\u4e48$a_{1}+a_{2}+...+a_{9}$\u7684\u503c\u4e3a\nA. 18\nB. 36\nC. 54\nD. 27\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.28661281177772774, "itpossible/Chinese-Mistral-7B-v0.1": 0.2731272040287072, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2801288226217134, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9886\u4e8b\u5a5a\u59fb\uff0c\u662f\u6307\u5728\u9a7b\u5728\u56fd\u4e0d\u53cd\u5bf9\u7684\u524d\u63d0\u4e0b\uff0c\u4e00\u56fd\u6388\u6743\u5176\u9a7b\u5916\u9886\u4e8b\u6216\u5916\u4ea4\u4ee3\u8868\u4f9d\u5176\u672c\u56fd\u6cd5\u5f8b\u89c4\u5b9a\u7684\u65b9\u5f0f\u529e\u7406\u7ed3\u5a5a\u624b\u7eed\uff0c\u6210\u7acb\u5a5a\u59fb\u7684\u5236\u5ea6\uff0c\u5b83\u9002\u7528\u4e8e\nA. \u672c\u56fd\u4fa8\u6c11\u4e0e\u672c\u56fd\u4fa8\u6c11\u7ed3\u5a5a\nB. \u9886\u4e8b\u5b98\u5458\u4e0e\u5916\u4ea4\u5b98\u5458\u7ed3\u5a5a\nC. \u9a7b\u5728\u56fd\u516c\u6c11\u4e0e\u4efb\u4f55\u7b2c\u4e09\u56fd\u516c\u6c11\u7ed3\u5a5a\nD. \u9a7b\u5728\u56fd\u516c\u6c11\u4e0e\u672c\u56fd\u516c\u6c11\u7ed3\u5a5a\u767b\u8bb0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u5211\u6cd5\u89c4\u5b9a\uff1a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u56fd\u5bb6\u5de5\u4f5c\u4eba\u5458\u548c\u519b\u4eba\u5728\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u9886\u57df\u5916\u72af\u672c\u6cd5\u89c4\u5b9a\u4e4b\u7f6a\u7684\uff0c\u9002\u7528\u672c\u6cd5\u3002\u8fd9\u4e00\u89c4\u5b9a\u662f\u6211\u56fd\u5211\u6cd5\u5728\u7a7a\u95f4\u9002\u7528\u6548\u529b\u95ee\u9898\u4e0a\u91c7\u53d6\u7684\nA. \u666e\u904d\u7ba1\u8f96\u539f\u5219\nB. \u4fdd\u62a4\u7ba1\u8f96\u539f\u5219\nC. \u5c5e\u5730\u7ba1\u8f96\u539f\u5219\nD. \u5c5e\u4eba\u7ba1\u8f96\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u53e3\u8001\u9f84\u5316\u5bf9\u6d88\u8d39\u7ed3\u6784\u5e26\u6765\u7684\u5f71\u54cd\u662f\nA. \u52b3\u52a1\u6d88\u8d39\u652f\u51fa\u6bd4\u91cd\u5feb\u901f\u4e0a\u5347\nB. \u8010\u7528\u6d88\u8d39\u54c1\u652f\u51fa\u6bd4\u91cd\u5feb\u901f\u4e0a\u5347\nC. \u98df\u54c1\u652f\u51fa\u6bd4\u91cd\u5feb\u901f\u4e0a\u5347\nD. \u6559\u80b2\u652f\u51fa\u6bd4\u91cd\u5feb\u901f\u4e0a\u5347\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6378647845161536, "meta-math/MetaMath-Mistral-7B": 0.9860748963775648, "itpossible/Chinese-Mistral-7B-v0.1": 0.5423437022720973, "HuggingFaceH4/zephyr-7b-beta": 0.9870892224098416, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7389\u7c73\u5bbd\u53f6\uff08A\uff09\u5bf9\u7a84\u53f6\uff08a\uff09\u4e3a\u663e\u6027\uff0c\u5bbd\u53f6\u6742\u4ea4\u79cd\uff08Aa\uff09\u7389\u7c73\u8868\u73b0\u4e3a\u9ad8\u4ea7\uff0c\u6bd4AA\u548caa\u54c1\u79cd\u7684\u4ea7\u91cf\u5206\u522b\u9ad812%\u548c20%\u3002\u7389\u7c73\u6709\u8338\u6bdb\uff08D\uff09\u5bf9\u65e0\u8338\u6bdb\uff08d\uff09\u4e3a\u663e\u6027\uff0c\u6709\u8338\u6bdb\u7389\u7c73\u690d\u682a\u5177\u6709\u663e\u8457\u7684\u6297\u75c5\u80fd\u529b\uff0c\u8be5\u663e\u6027\u57fa\u56e0\u7eaf\u5408\u65f6\u690d\u682a\u5e7c\u82d7\u671f\u5c31\u4e0d\u80fd\u5b58\u6d3b\u3002\u4e24\u5bf9\u57fa\u56e0\u72ec\u7acb\u9057\u4f20\u3002\u9ad8\u4ea7\u6709\u8338\u6bdb\u7389\u7c73\u81ea\u4ea4\u4ea7\u751fF1\uff0c\u518d\u8ba9F1\u968f\u673a\u4ea4\u914d\u4ea7\u751fF2\uff0c\u5219\u6709\u5173F1\u4e0eF2\u7684\u6210\u719f\u690d\u682a\u4e2d\uff0c\u53d9\u8ff0\u6b63\u786e\u7684\u662f\nA. \u90fd\u67099\u79cd\u57fa\u56e0\u578b\nB. \u5bbd\u53f6\u6709\u8338\u6bdb\u7c7b\u578b\u5206\u522b\u53601/2\u548c3/8\nC. \u6709\u8338\u6bdb\u4e0e\u65e0\u8338\u6bdb\u6bd4\u5206\u522b\u4e3a2\u22361\u548c2\u22363\nD. \u9ad8\u4ea7\u6297\u75c5\u7c7b\u578b\u5206\u522b\u53601/3\u548c1/10\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.34239623393788804, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.30300686740596594, "meta-llama/Meta-Llama-3-8B": 0.2731272040287072, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b9e\u5598\u7684\u4e3b\u8981\u75c5\u53d8\u810f\u8151\u662f\nA. \u5fc3\nB. \u80be\nC. \u80ba\nD. \u809d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5664879764074274, "meta-math/MetaMath-Mistral-7B": 0.6327804576173494, "itpossible/Chinese-Mistral-7B-v0.1": 0.49171753420846787, "HuggingFaceH4/zephyr-7b-beta": 0.997276976037681, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8786866202461053, "meta-llama/Meta-Llama-3-8B": 0.6054069948533094, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9454750553747057}}, {"question": "\u82cf\u683c\u62c9\u5e95\u8ba4\u4e3a\u4ec0\u4e48\u662f\u89e3\u51b3\u65e9\u671f\u5e0c\u814a\u81ea\u7136\u54f2\u5b66\u4f17\u8bf4\u7eb7\u7ead\u3001\u83ab\u8877\u4e00\u662f\u95ee\u9898\u7684\u5173\u952e\uff1f\nA. \u5efa\u7acb\u7edf\u4e00\u800c\u6743\u5a01\u7684\u5b66\u6d3e\nB. \u5efa\u7acb\u96c6\u6743\u653f\u6cbb\nC. \u901a\u8fc7\u4e00\u7cfb\u5217\u6218\u4e89\u4f7f\u56fd\u5bb6\u5f3a\u76db\u540e\u518d\u6574\u7406\u54f2\u5b66\u95ee\u9898\nD. \u5f3a\u8c03\u77e5\u8bc6\u7684\u4f5c\u7528\uff0c\u63d0\u9ad8\u77e5\u8bc6\u7684\u5730\u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9348732419566045, "meta-math/MetaMath-Mistral-7B": 0.9912179035196115, "itpossible/Chinese-Mistral-7B-v0.1": 0.8806321529810097, "HuggingFaceH4/zephyr-7b-beta": 0.9367138047007624, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9494589595145666, "meta-llama/Meta-Llama-3-8B": 0.9774346811409644, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9850705088107079}}, {"question": "\u8c37\u7c7b\uff0c\u85af\u7c7b\u662f\u6211\u56fd\u81b3\u98df\u80fd\u91cf\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u4f46\u5176\u4e3b\u8981\u7684\u7f3a\u9677\u662f\u7f3a\u4e4f\nA. \u78b3\u6c34\u5316\u5408\u7269\nB. \u7ef4\u751f\u7d20\nC. \u8102\u80aa\nD. \u4f18\u8d28\u86cb\u767d\u8d28\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5110393452589354, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6662656198341633, "HuggingFaceH4/zephyr-7b-beta": 0.8351005682570756, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47639336104713786, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u88ab\u79f0\u4e3a\u662f\u201c\u79d1\u5b66\u7ba1\u7406\u4e4b\u7236\u201d\u7684\u662f\nA. \u6cf0\u7f57\nB. \u6cd5\u7ea6\u5c14\nC. \u7f57\u4f2f\u7279\u6b27\u6587\nD. \u4e9a\u5f53\u65af\u5bc6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6325462821397972, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f9d\u82f1\u56fd\u6cd5\u5f8b\u89c4\u5b9a\uff0c\u5bf9\u4ea7\u54c1\u8d23\u4efb\u91c7\u53d6\u7684\u5f52\u8d23\u539f\u5219\u662f\nA. \u516c\u5e73\u8d23\u4efb\u539f\u5219\nB. \u8fc7\u9519\u63a8\u5b9a\u8d23\u4efb\u539f\u5219\nC. \u8fc7\u5931\u8d23\u4efb\u539f\u5219\nD. \u65e0\u8fc7\u5931\u8d23\u4efb\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4175059779434643, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u5e02\u573a\u7ecf\u6d4e\u4e2d\u5904\u4e8e\u6700\u6838\u5fc3\u3001\u6700\u7a81\u51fa\u7684\u5730\u4f4d\u7684\u662f\uff08\uff09\uff0c\u5b83\u662f\u5e02\u573a\u7ecf\u6d4e\u6240\u8981\u786e\u7acb\u7684\u6700\u4e3b\u8981\u3001\u6700\u91cd\u8981\u7684\u9053\u5fb7\nA. \u8bda\u4fe1\u9053\u5fb7\nB. \u4e92\u5229\u4e92\u60e0\nC. \u4ec1\u7231\nD. \u5148\u4e49\u540e\u5229\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4818638137644875, "meta-math/MetaMath-Mistral-7B": 0.9860213047259165, "itpossible/Chinese-Mistral-7B-v0.1": 0.8750104901812287, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5514254149366931, "meta-llama/Meta-Llama-3-8B": 0.7397715639963431, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5215327306053068}}, {"question": "\u4e0b\u5217\u54ea\u4e2a\u9009\u9879\u4e0d\u7b26\u5408\u6211\u56fd\u6cd5\u5f8b\u89c4\u5b9a\u7684\u201c\u53f8\u6cd5\u673a\u5173\u4f9d\u6cd5\u72ec\u7acb\u884c\u4f7f\u804c\u6743\u201d\u539f\u5219\u7684\u542b\u4e49\uff1f\nA. \u53f8\u6cd5\u673a\u5173\u53ca\u5176\u5de5\u4f5c\u4eba\u5458\u5728\u72ec\u7acb\u884c\u4f7f\u804c\u6743\u65f6\u4e0d\u5f97\u8fdd\u53cd\u7a0b\u5e8f\u89c4\u5b9a\nB. \u4efb\u4f55\u673a\u5173\u3001\u56e2\u4f53\u548c\u4e2a\u4eba\u4e0d\u5f97\u4ee5\u4efb\u4f55\u5f62\u5f0f\u5e72\u9884\u53f8\u6cd5\u6d3b\u52a8\nC. \u53f8\u6cd5\u6743\u4e0d\u5f97\u7531\u4e00\u822c\u7684\u884c\u653f\u673a\u5173\u6765\u884c\u4f7f\nD. \u53f8\u6cd5\u673a\u5173\u65e2\u8981\u72ec\u7acb\u884c\u4f7f\u804c\u6743\uff0c\u53c8\u4e0d\u5f97\u65e0\u9650\u5ea6\u5730\u4f7f\u7528\u81ea\u7531\u88c1\u91cf\u6743\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5927\u6c14\u4e2d\u67d0\u4e00\u9ad8\u7a0b\u4e0a\u7684\u6c14\u538b\uff0c\u7b49\u4e8e\u8be5\u5904\u5355\u4f4d\u6c34\u5e73\u9762\u79ef\u4e0a\u627f\u53d7\u7684\u5927\u6c14\u67f1\u7684\u91cd\u91cf\uff0c\u8be5\u6c14\u67f1\u7684\u9ad8\u5ea6\u4e3a[ ]\u3002\nA. \u4ece\u8be5\u9ad8\u7a0b\u5230\u6d77\u62d4 1000m\nB. \u4ece\u6d77\u5e73\u9762\u5230\u5927\u6c14\u9876\u754c\nC. \u4ece\u6d77\u5e73\u9762\u5230\u6d77\u62d4 1000m\nD. \u4ece\u8be5\u9ad8\u7a0b\u5230\u5927\u6c14\u9876\u754c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3984585186882418, "meta-math/MetaMath-Mistral-7B": 0.3789723727850207, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5976\u916a\u98df\u54c1\u5728\u6211\u56fd\u54ea\u4e00\u533a\u57df\u7684\u5c11\u6570\u6c11\u65cf\u996e\u98df\u4e2d\u6700\u4e3a\u5e38\u89c1\nA. \u4e1c\u5317\nB. \u4e1c\u5357\nC. \u897f\u5317\nD. \u897f\u5357\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.42005435516839146, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5602456663271304, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u751f\u4ea7\u4e0a\u9c7c\u7c7b\u751f\u957f\u7684\u8b66\u6212\u6d53\u5ea6\u4e00\u822c\u662f\u6307\u6c34\u4e2d\u6eb6\u89e3\u6c27\u6d53\u5ea6\u4e3a\nA. 2mg/L\nB. 5mg/L\nC. 1mg/L\nD. 3mg/L\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5528630996364458, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7535\u89c6\u548c\u5e7f\u64ad\u5a92\u4ecb\u7684\u5171\u540c\u5f31\u70b9\u662f\nA. \u529f\u80fd\u5355\u4e00\nB. \u4f20\u64ad\u6548\u679c\u8f83\u5f31\nC. \u611f\u67d3\u529b\u8f83\u5dee\nD. \u4f20\u64ad\u6548\u679c\u7a0d\u7eb5\u5373\u901d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3802392203343767, "HuggingFaceH4/zephyr-7b-beta": 0.8827808624750372, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45772067363605984, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8981\u4fdd\u6301\u793e\u4f1a\u4e3b\u4e49\u65b0\u95fb\u4e8b\u4e1a\u6c38\u8fdc\u5c5e\u4e8e\u4eba\u6c11\u7684\u6839\u672c\u5c5e\u6027\uff0c\u5c31\u5fc5\u987b\u575a\u6301\nA. \u5bf9\u515a\u8d1f\u8d23\u548c\u5bf9\u4eba\u6c11\u8d1f\u8d23\u76f8\u4e00\u81f4\u7684\u539f\u5219\nB. \u65b0\u95fb\u7684\u6307\u5bfc\u6027\u539f\u5219\nC. \u6b63\u9762\u5ba3\u4f20\u4e3a\u4e3b\u7684\u65b9\u9488\nD. \u65b0\u95fb\u7684\u771f\u5b9e\u6027\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8053041612427522, "meta-math/MetaMath-Mistral-7B": 0.9035542143403746, "itpossible/Chinese-Mistral-7B-v0.1": 0.6732242705541973, "HuggingFaceH4/zephyr-7b-beta": 0.9999337500303089, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6784311090378166, "meta-llama/Meta-Llama-3-8B": 0.9056350804473772, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.860361140002149}}, {"question": "\u751f\u7269\u4f53\u53ef\u4ee5\u76f4\u63a5\u5229\u7528\u7684\u80fd\u91cf\u7269\u8d28\u662f\nA. ATP\nB. \u78f7\u9178\u808c\u9178\nC. FAD\nD. ADP\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9227185055647793, "meta-math/MetaMath-Mistral-7B": 0.9987676940354485, "itpossible/Chinese-Mistral-7B-v0.1": 0.8411065070449911, "HuggingFaceH4/zephyr-7b-beta": 0.9996167382427057, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9613419882851831, "meta-llama/Meta-Llama-3-8B": 0.730383886422115, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9480814919841757}}, {"question": "\u5728\u5546\u4ee3\u91d1\u6587\u4e2d\uff0c\u5b57\u4f53\u7b14\u753b\u7684\u660e\u663e\u7279\u5f81\u662f\nA. \u5757\u9762\u7b14\u753b\nB. \u5e73\u76f4\u7b14\u753b\nC. \u65b9\u6298\u7b14\u753b\nD. \u7ebf\u6761\u7b14\u753b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3583275689754825, "meta-llama/Meta-Llama-3-8B": 0.3017444864199175, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.48473039805807633}}, {"question": "\u6839\u636e1980\u5e74\u300a\u8054\u5408\u56fd\u56fd\u9645\u8d27\u7269\u4e70\u5356\u5408\u540c\u516c\u7ea6\u300b\u7684\u89c4\u5b9a\uff0c\u5982\u679c\u5356\u65b9\u4e0d\u6309\u7167\u5408\u540c\u89c4\u5b9a\u7684\u65f6\u95f4\u4ea4\u8d27\u7684\u672c\u8eab\u5df2\u7ecf\u6784\u6210\u6839\u672c\u8fdd\u53cd\u5408\u540c\uff0c\u5219\u4e70\u65b9\nA. \u53ea\u80fd\u8981\u6c42\u5b9e\u9645\u5c65\u884c\nB. \u53ef\u4ee5\u4e0d\u7ed9\u5356\u65b9\u4e00\u6bb5\u5408\u7406\u7684\u671f\u9650\u5373\u53ef\u64a4\u9500\u5408\u540c\nC. \u5fc5\u987b\u7ed9\u5356\u65b9\u4e00\u6bb5\u5408\u7406\u7684\u671f\u9650\u5c65\u884c\u5408\u540c\nD. \u53ea\u80fd\u91c7\u7528\u8bf7\u6c42\u635f\u5bb3\u8d54\u507f\u7684\u65b9\u6cd5\u6765\u8865\u6551\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8002927587661567, "meta-math/MetaMath-Mistral-7B": 0.9510546556001804, "itpossible/Chinese-Mistral-7B-v0.1": 0.7990098424342089, "HuggingFaceH4/zephyr-7b-beta": 0.999813165868355, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9430950666575433, "meta-llama/Meta-Llama-3-8B": 0.3886499409051946, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4e8e\u513f\u7ae5\uff0c\u9996\u9009\u7684\u996e\u54c1\u662f\nA. \u767d\u5f00\u6c34\nB. \u8fd0\u52a8\u996e\u6599\nC. \u679c\u6c41\nD. \u4e73\u996e\u6599\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6305594345625847, "itpossible/Chinese-Mistral-7B-v0.1": 0.9300922885818178, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6802154539539095, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6d41\u884c\u75c5\u5b66\u4e0e\u4e34\u5e8a\u533b\u5b66\u7684\u533a\u522b\u5728\u4e8e\nA. \u4e0d\u6d89\u53ca\u836f\u7269\u6cbb\u7597\nB. \u7814\u7a76\u75be\u75c5\u7684\u75c5\u56e0\u5b66\nC. \u63d0\u4f9b\u8bca\u65ad\u4f9d\u636e\nD. \u5728\u7fa4\u4f53\u6c34\u5e73\u4e0a\u7814\u7a76\u75be\u75c5\u73b0\u8c61\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8542812685838658, "meta-math/MetaMath-Mistral-7B": 0.9669475805391503, "itpossible/Chinese-Mistral-7B-v0.1": 0.5926726519945745, "HuggingFaceH4/zephyr-7b-beta": 0.9959184601127774, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8944355621705995, "meta-llama/Meta-Llama-3-8B": 0.9050353850235905, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9754258060858092}}, {"question": "\u671f\u521d\u548c\u671f\u672b\u4f59\u989d\u5747\u5728\u501f\u65b9\u7684\u8d26\u6237\uff0c\u4e00\u822c\u5c5e\u4e8e\nA. \u6536\u5165\u7c7b\u8d26\u6237\nB. \u6240\u6709\u8005\u6743\u76ca\u7c7b\u8d26\u6237\nC. \u8d44\u4ea7\u7c7b\u8d26\u6237\nD. \u8d1f\u503a\u7c7b\u8d26\u6237\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5704005695125708, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0a\u6d88\u5316\u9053\u662f\u6307\nA. \u4ece\u53e3\u8154\u5230\u80c3\nB. \u53e3\u8154\u548c\u54bd\nC. \u4ece\u53e3\u8154\u5230\u98df\u7ba1\nD. \u4ece\u53e3\u8154\u5230\u5341\u4e8c\u6307\u80a0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8503705030963863}}, {"question": "\u5728\u4e0b\u5217\u83dc\u80b4\u9999\u5473\u642d\u914d\u4e2d\uff0c\u4e0d\u5408\u9002\u7684\u642d\u914d\u662f\nA. \u767d\u82b7\u4e4b\u9999\uff0c\u5b9c\u914d\u8c5a\u87f9\nB. \u849c\u85a4\u4e4b\u9999\uff0c\u5b9c\u914d\u91cd\u8165\u7b49\nC. \u8335\u9999\u3001\u4e01\u9999\uff0c\u591a\u4e0e\u51b7\u51bb\u3001\u6c34\u6676\u7c7b\u83b1\u5e0c\u914d\u4f0d\nD. \u6cb9\u8102\u4e73\u9165\u4e4b\u9999\uff0c\u591a\u7528\u4e8e\u70ed\u70f9\u4e4b\u98df\u7269\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u54ea\u4e2a\u5b57\u5e38\u7528\u4f5c\u8868\u793a\u987a\u5e8f\u7684\u7b2c\u4e94\u4f4d\nA. \u620a\nB. \u6210\nC. \u620c\nD. \u620d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3306562312783846, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u7ba1\u7406\u601d\u60f3\u53f2\u4e0a\uff0c\u9996\u6b21\u63d0\u51fa\u201c\u975e\u6b63\u5f0f\u7ec4\u7ec7\u201d\u6982\u5ff5\u7684\u7f8e\u56fd\u5b66\u8005\u662f\nA. \u5df4\u7eb3\u5fb7\nB. \u6234\u7ef4\u65af\nC. \u5361\u65af\u7279\nD. \u5fb7\u9c81\u514b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.41135426427907135}}, {"question": "\u7ecf\u6d4e\u5168\u7403\u5316\u7684\u5b9e\u8d28\u51b3\u5b9a\u4e86\u5b83\u7684\u53d1\u5c55\u5fc5\u7136\u662f\nA. \u6709\u5229\u4e8e\u793e\u4f1a\u4e3b\u4e49\u56fd\u5bb6\nB. \u6709\u5229\u4e8e\u6240\u6709\u56fd\u5bb6\nC. \u6709\u5229\u4e8e\u53d1\u8fbe\u8d44\u672c\u4e3b\u4e49\u56fd\u5bb6\nD. \u6709\u5229\u4e8e\u53d1\u5c55\u4e2d\u56fd\u5bb6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4231968770937444, "meta-math/MetaMath-Mistral-7B": 0.7154893690946084, "itpossible/Chinese-Mistral-7B-v0.1": 0.3737337702717088, "HuggingFaceH4/zephyr-7b-beta": 0.902805214185063, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7508272618530809, "meta-llama/Meta-Llama-3-8B": 0.7716044857690064, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c0f\u8bf4\u300a\u6843\u56ed\u300b\u4e2d\uff0c\u79cd\u6843\u4eba\u738b\u8001\u5927\u7ed9\u751f\u75c5\u7684\u5973\u513f\u4e70\u7684\u7269\u54c1\u662f\nA. \u7ed2\u7ebf\u978b\nB. \u7ee3\u6795\nC. \u73bb\u7483\u6843\u5b50\nD. \u7ea2\u5934\u7ef3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.343344623487626, "meta-math/MetaMath-Mistral-7B": 0.325455072595945, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2997240426459713, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.46275241405821343}}, {"question": "\u6587\u4ef6\u578b\u75c5\u6bd2\u4e0d\u80fd\u611f\u67d3\u7684\u6587\u4ef6\u7c7b\u578b\u662f\nA. EXE\u7c7b\u578b\nB. COM\u7c7b\u578b\nC. HTML\u7c7b\u578b\nD. SYS\u7c7b\u578b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.697996340432057, "meta-math/MetaMath-Mistral-7B": 0.9080680401996104, "itpossible/Chinese-Mistral-7B-v0.1": 0.46969081036676064, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8597414653227442, "meta-llama/Meta-Llama-3-8B": 0.456515957484564, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9602783801158787}}, {"question": "1\u5206\u5b50\u8f6f\u8102\u9178\u5f7b\u5e95\u6c27\u5316\u5206\u89e3\uff0c\u51c0\u4ea7\u751f\u591a\u5c11\u5206\u5b50ATP\nA. 127\nB. 128\nC. 106\nD. 131\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31604271402418355, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9136997898910401, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4415437440966292, "meta-llama/Meta-Llama-3-8B": 0.3191152350487737, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u79cd\u9057\u4f20\u75c5\u53d7\u4e24\u5bf9\u7b49\u4f4d\u57fa\u56e0A\u3001a\u548cB\u3001b\u63a7\u5236\uff0cA\u3001a\u4f4d\u4e8e\u5e38\u67d3\u8272\u4f53\u4e0a\uff0cB\u3001b\u4f4d\u4e8eX\u67d3\u8272\u4f53\u4e0a\uff0c\u5176\u4e2da\u548cb\u5747\u4e3a\u81f4\u75c5\u57fa\u56e0\uff0c\u4e14\u53ea\u6709\u540c\u65f6\u542b\u6709A\u548cB\u65f6\u4e2a\u4f53\u8868\u73b0\u6b63\u5e38\u3002\u4e0b\u5217\u5173\u4e8e\u8be5\u9057\u4f20\u75c5\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u81ea\u7136\u4eba\u7fa4\u4e2d\uff0c\u8be5\u75c5\u7684\u7537\u6027\u60a3\u8005\u591a\u4e8e\u5973\u6027\u60a3\u8005\nB. \u7236\u6bcd\u5747\u6b63\u5e38\uff0c\u5973\u513f\u60a3\u75c5\u6982\u7387\u6700\u5927\u4e3a25%\nC. \u7236\u6bcd\u5747\u60a3\u75c5\uff0c\u5b50\u5973\u53ef\u80fd\u5747\u4e0d\u60a3\u75c5\nD. \u67d0\u4e00\u7537\u6027\u60a3\u8005\u7684\u81f4\u75c5\u57fa\u56e0\u4e0d\u53ef\u80fd\u5168\u6765\u81ea\u4e8e\u5176\u6bcd\u4eb2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.35347162922091135, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u8425\u517b\u5b66\u4f1a\u4e13\u5bb6\u59d4\u5458\u4f1a\u4e8e1997\u5e74\u5bf9\u300a\u4e2d\u56fd\u5c45\u6c11\u81b3\u98df\u6307\u5357\u300b\u8fdb\u884c\u4e86\u4fee\u6539\uff0c\u516c\u5e03\u4e86\u65b0\u81b3\u98df\u6307\u5357\uff0c\u5f3a\u8c03\"\u5e38\u5403\u5976\u7c7b\u3001\u8c46\u7c7b\"\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u6211\u56fd\u5c45\u6c11\u81b3\u98df\u4e2d\u54ea\u4e00\u79cd\u7269\u8d28\u7684\u6444\u5165\u91cf\u4e25\u91cd\u4e0d\u8db3?\nA. \u7ef4\u751f\u7d20A\nB. \u9499\nC. \u94c1\nD. \u86cb\u767d\u8d28\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41430195076560766, "meta-math/MetaMath-Mistral-7B": 0.7420864745335447, "itpossible/Chinese-Mistral-7B-v0.1": 0.9212944858020456, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u5316\u8113\u6027\u5173\u8282\u708e\uff0c\u6b63\u786e\u7684\u662f\nA. \u5173\u8282\u6db2\u57f9\u517b\u591a\u4e3a\u767d\u8272\u8461\u7403\u83cc\nB. \u591a\u89c1\u4e8e\u8001\u5e74\u5973\u6027\uff0c\u53ef\u65e9\u671f\u5173\u8282\u8154\u5185\u6ce8\u5c04\u6297\u751f\u7d20\nC. \u5173\u8282\u6db2\u5916\u89c2\u53ef\u5448\u900f\u660e\u6216\u6d51\u6d4a\u9ec4\u767d\u8272\nD. \u5173\u8282\u955c\u53ef\u89c1\u5927\u91cf\u9769\u5170\u9634\u6027\u83cc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8993197477836041, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.39325343687947123, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.615280585458539}}, {"question": "\u65e5\u672c\u662f\u5728\u54ea\u4e00\u5e74\u6210\u4e3a\u4e16\u754c\u7b2c\u4e8c\u5927\u7ecf\u6d4e\u5f3a\u56fd\u7684\nA. 1966\u5e74\nB. 1967\u5e74\nC. 1968\u5e74\nD. 1970\u5e74\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2821833983601388, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7a0b\u98a2\u8bd7\u4e91\uff1a\u201c\u95f2\u6765\u65e0\u4e8b\u4e0d\u4ece\u5bb9\uff0c\u7761\u89c9\u4e1c\u7a97\u65e5\u5df2\u7ea2\u3002\u4e07\u7269\u9759\u89c2\u7686\u81ea\u5f97\uff0c\u56db\u65f6\u4f73\u5174\u4e0e\u4eba\u540c\u3002\u9053\u901a\u5929\u5730\u6709\u5f62\u5916\uff0c\u601d\u5165\u98ce\u4e91\u53d8\u6001\u4e2d\u3002\u5bcc\u8d35\u4e0d\u6deb\u8d2b\u8d31\u4e50\uff0c\u7537\u513f\u5230\u6b64\u662f\u8c6a\u96c4\u3002\u201d\u5176\u4f53\u73b0\u7684\u4e3b\u65e8\u662f\nA. \u5f20\u626c\u81ea\u6211\u7684\u4eba\u751f\u6001\u5ea6\nB. \u4eba\u7c7b\u4e0e\u81ea\u7136\u548c\u8c10\u5171\u5904\nC. \u4eba\u4e0e\u4e07\u4e8b\u4e07\u7269\u7686\u540c\u7406\nD. \u65e0\u4e3a\u800c\u6cbb\u7684\u601d\u60f3\u7406\u5ff5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4887813400920945, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e0d\u662f\u8102\u6eb6\u6027\u7ef4\u751f\u7d20\u7684\u662f\nA. \u7ef4\u751f\u7d20D\nB. \u7ef4\u751f\u7d20A\nC. \u7ef4\u751f\u7d20C\nD. \u7ef4\u751f\u7d20K\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5698869329277777, "meta-math/MetaMath-Mistral-7B": 0.6067167203651356, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6109588524261915, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6684858034998465, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u62e9\u5076\u7684()\u7406\u8bba\u7684\u57fa\u672c\u5047\u8bf4\u662f\uff1a\u5728\u914d\u5076\u9009\u62e9\u4e2d\uff0c\u6bcf\u4e2a\u4eba\u90fd\u5728\u4ed6\u6216\u5979\u7684\u5408\u9002\u4eba\u9009\u7684\u8303\u56f4\u5185\u5bfb\u627e\u4f34\u4fa3\uff0c\u540e\u8005\u7ed9\u4ed6\u6216\u5979\u7684\u6700\u5927\u9650\u5ea6\u7684\u9700\u8981\u6ee1\u8db3\u63d0\u4f9b\u4e86\u6700\u53ef\u9760\u7684\u4fdd\u8bc1\u3002\nA. \u4e92\u8865\u9700\u8981\nB. \u4ef7\u503c\nC. \u4ea4\u6362\nD. \u89d2\u8272\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.457549790295691, "meta-math/MetaMath-Mistral-7B": 0.7198531900166777, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9918809922268902, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.900695067074299, "meta-llama/Meta-Llama-3-8B": 0.6569477825217686, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9558369100376017}}, {"question": "\u4eba\u4f53\u4e2d\u6c1f\u7f3a\u4e4f\u65f6\uff0c\u4f1a\u51fa\u73b0\u7684\u76f8\u5173\u7684\u75c7\u72b6\u4e3a\nA. \u635f\u4f24\u514d\u75ab\u529f\u80fd\nB. \u6c1f\u9aa8\u75c5\nC. \u6591\u91c9\u75c7\nD. \u9aa8\u8d28\u758f\u677e\u75c7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6cd5\u5f8b\u8d23\u4efb\u4e0e\u6cd5\u5f8b\u5236\u88c1\u7684\u5173\u7cfb\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u6cd5\u5f8b\u5236\u88c1\u548c\u6cd5\u5f8b\u8d23\u4efb\u4e92\u4e3a\u6761\u4ef6\nB. \u6709\u6cd5\u5f8b\u5236\u88c1\u5fc5\u6709\u6cd5\u5f8b\u8d23\u4efb\nC. \u6cd5\u5f8b\u8d23\u4efb\u662f\u6cd5\u5f8b\u5236\u88c1\u7684\u4f53\u73b0\nD. \u6709\u6cd5\u5f8b\u8d23\u4efb\u5c31\u6709\u6cd5\u5f8b\u5236\u88c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6709\u5173\u4e8b\u5b9e\u53ca\u539f\u7406\u5206\u6790\u90fd\u6b63\u786e\u7684\u662f\nA. \u4e8b\u5b9e\uff1a\u78b3\u9178\u6c22\u94a0\u5e38\u4f5c\u53d1\u9175\u7c89\u3002\u539f\u7406\uff1a\u78b3\u9178\u6c22\u94a0\u4e0e\u9762\u56e2\u4e2d\u7684\u6709\u673a\u9178\u53cd\u5e94\u4ea7\u751fCO2\u3002\nB. \u4e8b\u5b9e\uff1a\u94f5\u6001\u5316\u80a5\u548c\u8349\u6728\u7070\u4e0d\u80fd\u6df7\u7528\u3002\u539f\u7406\uff1aK2CO3\u4e0e\u94f5\u76d0\u53d1\u751f\u590d\u5206\u89e3\u53cd\u5e94\u3002\nC. \u4e8b\u5b9e\uff1a\u8fbd\u5b81\u8230\u5e95\u90e8\u53ef\u4ee5\u9576\u5d4c\u4e00\u4e9b\u94dc\u5757\u3002\u539f\u7406\uff1a\u8fd9\u79cd\u4fdd\u62a4\u6cd5\u53eb\u727a\u7272\u9633\u6781\u7684\u9634\u6781\u4fdd\u62a4\u6cd5\u3002\nD. \u4e8b\u5b9e\uff1a\u6db2\u5316\u6c14\u7076\u5177\u6539\u5929\u7136\u6c14\u7076\u5177\uff0c\u53ef\u4ee5\u51cf\u5c11\u7a7a\u6c14\u8fdb\u5165\u5b54\u7684\u5b54\u5f84\u3002\u539f\u7406\uff1a\u4f7f\u5929\u7136\u6c14\u5145\u5206\u71c3\u70e7\uff0c\u907f\u514d\u751f\u6210CO\u548c\u635f\u5931\u70ed\u91cf\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3527719778012357, "HuggingFaceH4/zephyr-7b-beta": 0.9958604977178573, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u4e3apH\u5bf9\u9176\u4fc3\u53cd\u5e94\u901f\u5ea6\u5f71\u54cd\u7684\u8bba\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u80c3\u86cb\u767d\u9176\u7684\u6700\u9002pH\u7ea6\u4e3a1.8\nB. \u80f0\u86cb\u767d\u9176\u7684\u6700\u9002pH\u63a5\u8fd18.0\nC. \u4eba\u4f53\u5185\u6781\u5927\u591a\u6570\u9176\u7684\u6700\u9002pH\u7ea6\u4e3a1.8\nD. \u9176\u7684\u6700\u9002pH\u4e0d\u53d7\u9176\u7eaf\u5ea6\u4e0e\u5e95\u7269\u79cd\u7c7b\u7b49\u7684\u5f71\u54cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.45028144704430756, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6709\u5173\u751f\u957f\u7d20\u7684\u8bf4\u6cd5\u9519\u8bef\u7684\u662f\nA. \u5411\u65e5\u8475\u7684\u7ec6\u5ae9\u82b1\u76d8\u8ddf\u7740\u592a\u9633\u8f6c\u662f\u751f\u957f\u7d20\u8c03\u8282\u7684\u7ed3\u679c\nB. \u5355\u4fa7\u5149\u7167\u5c04\u71d5\u9ea6\u80da\u82bd\u9798\u53ef\u4f7f\u5176\u751f\u957f\u7d20\u5206\u5e03\u53d1\u751f\u53d8\u5316\nC. \u751f\u957f\u7d20\u7684\u53d1\u73b0\u6e90\u4e8e\u4eba\u4eec\u5bf9\u690d\u7269\u5411\u5149\u6027\u7684\u7814\u7a76\nD. \u6e29\u7279\u53d1\u73b0\u4e86\u751f\u957f\u7d20\u7684\u5316\u5b66\u672c\u8d28\u662f\u5432\u54da\u4e59\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5202594847405422, "meta-llama/Meta-Llama-3-8B": 0.6197193160375025, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4014884562720673}}, {"question": "\u6570\u636e\u7ed3\u6784\u4e0d\u5305\u542b\u7684\u5185\u5bb9\u662f\nA. \u6570\u636e\u7684\u5b58\u50a8\u7ed3\u6784\nB. \u5bf9\u6570\u636e\u65bd\u52a0\u7684\u64cd\u4f5c\nC. \u6570\u636e\u7684\u5143\u7d20\u6765\u6e90\nD. \u6570\u636e\u7684\u903b\u8f91\u7ed3\u6784\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8874335688078062, "meta-math/MetaMath-Mistral-7B": 0.9851565970606668, "itpossible/Chinese-Mistral-7B-v0.1": 0.7748808006277155, "HuggingFaceH4/zephyr-7b-beta": 0.9876089302250776, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6760391215491238, "meta-llama/Meta-Llama-3-8B": 0.9551494446863495, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.912554907388819}}, {"question": "\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u4e19\u8ba4\u4e3a\u5176\u59bb\u5b50\u88ab\u9b54\u9b3c\u9644\u4f53\uff0c\u53ea\u6709\u7528\u706b\u70e7\uff0c\u9b54\u9b3c\u624d\u80fd\u9003\u79bb\uff0c\u4e3a\u4e86\u62ef\u6551\u5176\u59bb\u5b50\uff0c\u4fbf\u5728\u5176\u59bb\u5b50\u8eab\u4e0a\u6d47\u4e0a\u6c7d\u6cb9\uff0c\u7ed3\u679c\u5c06\u5176\u59bb\u5b50\u70e7\u6b7b\u3002\u7531\u4e8e\u4e19\u5b58\u5728\u8ff7\u4fe1\u601d\u60f3\uff0c\u5c5e\u4e8e\u8ff7\u4fe1\u72af\uff0c\u4e0d\u6784\u6210\u72af\u7f6a\nB. \u4e01\u8bef\u628a\u4e00\u4e2a\u7537\u5b50\u5f53\u4f5c\u5973\u5b50\u5b9e\u65bd\u5f3a\u5978\uff0c\u4e01\u6784\u6210\u5f3a\u5978\u7f6a\u672a\u9042\nC. \u7532\u4e3a\u4e86\u8ba9\u5176\u4ec7\u4eba\u5362\u67d0\u6b7b\u4ea1\uff0c\u4fbf\u5230\u5e99\u91cc\u7948\u7977\u8ba9\u795e\u6740\u6b7b\u5362\u67d0\uff0c\u56e0\u88ab\u4eba\u53d1\u73b0\u800c\u88ab\u6293\u83b7\uff0c\u7532\u6784\u6210\u6545\u610f\u6740\u4eba\u7f6a\u672a\u9042\nD. \u4e59\u4e3a\u4e86\u6740\u6b7b\u5468\u67d0\uff0c\u4e70\u4e86\u7812\u971c\u653e\u5230\u5468\u67d0\u7684\u6c34\u676f\u91cc\uff0c\u56e0\u7812\u971c\u5931\u6548\uff0c\u5468\u67d0\u6ca1\u6709\u4e2d\u6bd2\u3002\u7531\u4e8e\u7812\u971c\u5931\u6548\u4e0d\u53ef\u80fd\u81f4\u4f7f\u5468\u67d0\u6b7b\u4ea1\uff0c\u6545\u4e59\u4e0d\u6784\u6210\u72af\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.29863342676099575, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.42876490286251856, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5161176258261744}}, {"question": "\u4ee5\u4e0b\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f\nA. \u7531\u4e2d\u56fd\u8fd0\u8f7d\u706b\u7bad\u6280\u672f\u7814\u7a76\u9662\u7814\u5236\u7684\u957f\u5f81\u4e94\u53f7\u7cfb\u5217\u8fd0\u8f7d\u706b\u7bad\u5c5e\u4e8e\u65e0\u6bd2\u3001\u65e0\u6c61\u67d3\u3001\u9ad8\u6027\u80fd\u3001\u5927\u63a8\u529b\u7684\u5927\u578b\u6db2\u4f53\u8fd0\u8f7d\u706b\u7bad\nB. \u9009\u62e9\u5728\u4e2d\u5348\u53d1\u5c04\u201c\u80d6\u4e94\u201d\u7684\u539f\u56e0\u4e4b\u4e00\u662f\u8003\u8651\u5230\u536b\u661f\u592a\u9633\u5e06\u677f\u4e0e\u592a\u9633\u5149\u7ebf\u7684\u76f8\u5bf9\u5173\u7cfb\uff0c\u5373\u6709\u5229\u4e8e\u536b\u661f\u5145\u7535\nC. 2019\u5e7412\u670827\u65e5\uff0c\u6635\u79f0\u201c\u80d6\u4e94\u201d\u7684\u957f\u5f81\u4e94\u53f7\u9065\u4e09\u8fd0\u8f7d\u706b\u7bad\u5347\u7a7a\uff0c\u5728\u661f\u7bad\u6210\u529f\u5206\u79bb\u540e\u987a\u5229\u5c06\u5b9e\u8df5\u4e8c\u5341\u53f7\u536b\u661f\u9001\u5165\u9884\u5b9a\u8f68\u9053\nD. \u53d1\u5c04\u201c\u80d6\u4e94\u201d\u7684\u6587\u660c\u662f\u6211\u56fd\u7eac\u5ea6\u6700\u4f4e\u3001\u552f\u4e00\u9760\u6d77\u7684\u53d1\u5c04\u573a\uff0c\u4e5f\u662f\u6211\u56fd\u9996\u4e2a\u4f4e\u7eac\u5ea6\u53d1\u5c04\u573a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2696976915887272, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.33863640659411287, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.344785902992216, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7272377864732276}}, {"question": "\u7528\u6d41\u901f\u4eea\u65bd\u6d4b\u70b9\u6d41\u901f\u65f6\uff0c\u6bcf\u6b21\u8981\u6c42\u65bd\u6d4b\u7684\u65f6\u95f4___\nA. \u8d8a\u77ed\u8d8a\u597d\nB. \u4e0d\u53d7\u9650\u5236\nC. \u8d8a\u957f\u8d8a\u597d\nD. \u5927\u7ea6 100s\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5363925018392305, "HuggingFaceH4/zephyr-7b-beta": 0.9794856494835029, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.378807395318333, "meta-llama/Meta-Llama-3-8B": 0.4481817164548853, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u4e24\u4e2a\u5171\u70b9\u2f12\uff0c\u2f24\u2f29\u5206\u522b\u662f30 N\u548c40 N. \u5982\u679c\u5b83\u4eec\u4e4b\u95f4\u7684\u5939\u2ec6\u662f90\u00b0\uff0c\u90a3\u4e48\u8fd9\u4e24\u4e2a\u2f12\u5408\u2f12\u7684\u2f24\u2f29\u662f\nA. 0\nB. 80N\nC. 110N\nD. 50N\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5de5\u7a0b\u9879\u76ee\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\uff0c\u65bd\u5de5\u5355\u4f4d\u4e3a\u786e\u4fdd\u5b89\u5168\uff0c\u5728\u5904\u7406\u5b89\u5168\u9690\u60a3\u65f6\uff0c\u8bbe\u7f6e\u4e86\u591a\u9053\u9632\u7ebf\uff0c\u4f53\u73b0\u4e86\u5bf9\u5b89\u5168\u9690\u60a3\u5904\u7406\u7684\nA. \u91cd\u70b9\u5904\u7406\u539f\u5219\nB. \u5197\u4f59\u5b89\u5168\u5904\u7406\u539f\u5219\nC. \u9632\u707e\u4e0e\u51cf\u707e\u5e76\u91cd\u5904\u7406\u539f\u5219\nD. \u5355\u9879\u9690\u60a3\u7efc\u5408\u5904\u7406\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7315688524389806, "meta-math/MetaMath-Mistral-7B": 0.7867535940298982, "itpossible/Chinese-Mistral-7B-v0.1": 0.8849822025196553, "HuggingFaceH4/zephyr-7b-beta": 0.999081726328269, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9209716976883051, "meta-llama/Meta-Llama-3-8B": 0.6274251020882582, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9307826652043574}}, {"question": "\u67d0\u4eba\u5728\u4e00\u5e74\u5185\u611f\u5192\u7684\u6982\u7387\u670d\u4ece\u6df7\u5408\u6cca\u677e\u5206\u5e03\uff0c\u53c2\u6570$\\lambda$\u670d\u4ece$(0\uff0c5)$\u4e0a\u7684\u5747\u5300\u5206\u5e03\uff0c\u5219\u4ed6\u5728\u4e00\u5e74\u5185\u611f\u5192\u7684\u6b21\u6570\u4e0d\u5c11\u4e8e 2 \u6b21\u7684\u6982\u7387\u662f( )\u3002\nA. 0.61\nB. 0.41\nC. 0.81\nD. 0.21\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4134293489930365, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5369445989974944}}, {"question": "\u5efa\u7acb\u56fd\u9645\u653f\u6cbb\u7ecf\u6d4e\u65b0\u79e9\u5e8f\u7684\u957f\u671f\u6597\u4e89\u4e2d\uff0c\u5fc5\u987b\u59cb\u7ec8\nA. \u4ee5\u548c\u5e73\u5171\u5904\u4e94\u9879\u539f\u5219\u4e3a\u57fa\u7840\nB. \u4ee5\u81ea\u7531\u3001\u5e73\u7b49\u539f\u5219\u4e3a\u57fa\u7840\nC. \u4ee5\u4eba\u6743\u9ad8\u4e8e\u4e3b\u6743\u7684\u539f\u5219\u4e3a\u57fa\u7840\nD. \u4ee5\u4e0d\u7ed3\u76df\u539f\u5219\u4e3a\u57fa\u7840\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.800292734950485, "meta-math/MetaMath-Mistral-7B": 0.817261242296336, "itpossible/Chinese-Mistral-7B-v0.1": 0.6765037998047615, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8600056025310251, "meta-llama/Meta-Llama-3-8B": 0.47255281949930045, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.42183413379009355}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u5e94\u5f53\u6570\u7f6a\u5e76\u7f5a\u7684\u662f\nA. \u4e19\u662f\u53f8\u6cd5\u5de5\u4f5c\u4eba\u5458\uff0c\u5728\u5211\u4e8b\u8bc9\u8bbc\u8fc7\u7a0b\u4e2d\u5c06\u5224\u59044\u5e74\u6709\u671f\u5f92\u5211\u7684\u72af\u7f6a\u5206\u5b50\u9002\u7528\u7f13\u5211\u540e\u6536\u53d7\u72af\u7f6a\u5206\u5b50\u5bb6\u5c5e\u7ed9\u76841\u4e07\u5143\nB. \u75322004\u5e741\u6708\u76d7\u7a83\u4ed6\u4eba\u8d22\u72692000\u5143\uff0c2005\u5e741\u6708\u76d7\u7a83\u4ed6\u4eba\u8d22\u72693000\u5143\nC. \u4e59\u4f2a\u9020\u5546\u52a1\u90e8\u7684\u6279\u6587\u540e\uff0c\u5229\u7528\u8be5\u4f2a\u9020\u7684\u6279\u6587\u9a97\u53d6\u67d0\u4f01\u4e1a5\u4e07\u5143\nD. \u4e01\u56e0\u4e3a\u62a2\u52ab\u4ed6\u4eba\u8d22\u7269\u88ab\u5224\u5904\u6709\u671f\u5f92\u52113\u5e74\uff0c\u5728\u5211\u7f5a\u6267\u884c\u671f\u95f4\u53d1\u73b0\u5176\u8fd8\u6709\u53e6\u5916\u5e94\u5f53\u5224\u59044\u5e74\u6709\u671f\u5f92\u5211\u7684\u6f0f\u7f6a\u6ca1\u6709\u5224\u5904\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2777229469001534, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.49574050441458145, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4576668516639495}}, {"question": "\u4e00\u4e2a\u65c5\u884c\u8005\u8981\u53bb\u706b\u8f66\u7ad9\uff0c\u65e9\u4e0a\u4ece\u65c5\u9986\u51fa\u53d1\uff0c\u5230\u8fbe\u4e00\u4e2a\u5341\u5b57\u8def\u53e3\u3002\u5341\u5b57\u8def\u53e3\u5206\u522b\u901a\u5411\u4e1c\u5357\u897f\u5317\u56db\u4e2a\u65b9\u5411\uff0c\u56db\u4e2a\u65b9\u5411\u4e0a\u5206\u522b\u6709\u996d\u5e97\u3001\u65c5\u9986\u3001\u4e66\u5e97\u548c\u706b\u8f66\u7ad9\u3002\u4e66\u5e97\u5728\u996d\u5e97\u7684\u4e1c\u5317\u65b9\uff0c\u996d\u5e97\u5728\u706b\u8f66\u7ad9\u7684\u897f\u5317\u65b9\u3002\u8be5\u65c5\u884c\u8005\u8981\u53bb\u706b\u8f66\u7ad9\uff0c\u5e94\u5f53\u5f80\u54ea\u4e2a\u65b9\u5411\u8d70\nA. \u897f\nB. \u5317\nC. \u5357\nD. \u4e1c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c60\u5c81\u3002\u81ea\u8ff01\u5e74\u534a\u524d\u56e0\u76f4\u80a0\u80bf\u7624\u5728\u5916\u9662\u624b\u672f\u6cbb\u7597\uff0c\u5177\u4f53\u672f\u5f0f\u4e0d\u6e05\u3002\u68c0\u67e5\u89c1\u5de6\u4e0b\u8179\u90e8\u65c1\u6b63\u4e2d12cm\u8179\u76f4\u808c\u5207\u53e3\u7622\u75d5\uff0c\u5176\u5916\u4fa7\u6709\u4e00\u80a0\u9020\u53e3\uff0c\u809b\u95e8\u5df2\u4e0d\u5b58\u5728\u3002\u60a3\u8005\u4e00\u822c\u72b6\u51b5\u5c1a\u597d\uff0c\u80a0\u9020\u53e3\u6392\u4fbf\u6b63\u5e38\u3002\u809d\u810fCT\u89c1\u809d\u53f3\u540e\u53f6\u4e0b\u6bb5\u5185\u6709\u4e00\u76f4\u5f842.5cm\u7ed3\u8282\uff0c\u73af\u5468\u5f3a\u5316\u660e\u663e\uff0c\u8840CEA\u5347\u9ad8\uff0cAFP\u3001CA19-9\u6b63\u5e38\u3002\u76ee\u524d\u6700\u4f73\u7684\u6cbb\u7597\u65b9\u6848\u662f\nA. \u53f3\u534a\u809d\u5207\u9664\u672f\nB. \u809d\u90e8\u5206\u5207\u9664\u672f\nC. \u80bf\u7624\u5c04\u9891\u6d88\u878d\nD. \u5168\u8eab\u5316\u7597\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37030806958984686, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.40194587418310046, "HuggingFaceH4/zephyr-7b-beta": 0.9030761085614019, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6444289172367038}}, {"question": "\u67d3\u8272\u4f53\u7684\u67d0\u4e00\u90e8\u4f4d\u589e\u52a0\u4e86\u81ea\u5df1\u7684\u67d0\u4e00\u533a\u6bb5\u7684\u67d3\u8272\u4f53\u7ed3\u6784\u53d8\u5f02\u79f0\u4e3a\nA. \u6613\u4f4d\nB. \u7f3a\u5931\nC. \u5012\u4f4d\nD. \u91cd\u590d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4343107853917528, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.49339050768839293, "meta-llama/Meta-Llama-3-8B": 0.6895412903926346, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53e4\u5e0c\u814a\u552f\u7269\u4e3b\u4e49\u7684\u4ee3\u8868\u662f\nA. \u82cf\u683c\u62c9\u5e95\nB. \u5fb7\u8c1f\u514b\u5229\u7279\nC. \u4e9a\u91cc\u58eb\u591a\u5fb7\nD. \u67cf\u62c9\u56fe\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6075634376204768, "itpossible/Chinese-Mistral-7B-v0.1": 0.8717457232452149, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9394244333230618, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9551494434096485}}, {"question": "\u5728\u51b3\u7b56\u6811\u4e2d\uff0c\u7528\u4f5c\u5206\u88c2\u8282\u70b9\u7684information gain\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f\nA. \u4fe1\u606f\u589e\u76ca\u66f4\u52a0\u503e\u5411\u4e8e\u9009\u62e9\u6709\u8f83\u591a\u53d6\u503c\u7684\u5c5e\u6027\nB. \u4fe1\u606f\u589e\u76ca\u53ef\u4ee5\u4f7f\u7528\u71b5\u5f97\u5230\nC. \u8f83\u5c0f\u4e0d\u7eaf\u5ea6\u7684\u8282\u70b9\u9700\u8981\u66f4\u591a\u7684\u4fe1\u606f\u6765\u533a\u5206\u603b\u4f53\nD. \u4ee5\u4e0a\u5747\u4e0d\u662f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.322677563517414, "meta-math/MetaMath-Mistral-7B": 0.40646067737101904, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9081429767175992, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.35752348940568696, "meta-llama/Meta-Llama-3-8B": 0.38603628713220584, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u516c\u6c11\u9053\u5fb7\u5efa\u8bbe\u5b9e\u65bd\u7eb2\u8981\u300b\u4e2d\u63d0\u51fa\u7684\u201c\u7231\u56fd\u5b88\u6cd5\u3001\u660e\u793c\u8bda\u4fe1\u3001\u56e2\u7ed3\u53cb\u5584\u3001\u52e4\u4fed\u81ea\u5f3a\u3001\u656c\u4e1a\u5949\u732e\u201d\uff0c\u662f\u6211\u56fd\u516c\u6c11\u5e94\u5f53\u9075\u5b88\u7684\nA. \u57fa\u672c\u9053\u5fb7\u89c4\u8303\nB. \u5bb6\u5ead\u9053\u5fb7\u89c4\u8303\nC. \u804c\u4e1a\u9053\u5fb7\u89c4\u8303\nD. \u516c\u5171\u9053\u5fb7\u89c4\u8303\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5584961465412299, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6559\u80b2\u7684\u4e24\u4e2a\u6700\u57fa\u672c\u529f\u80fd\u662f()\u3002\nA. \u653f\u6cbb\u4e0e\u7ecf\u6d4e\nB. \u4fc3\u8fdb\u4e2a\u4f53\u793e\u4f1a\u5316\u4e0e\u4e2a\u6027\u5316\nC. \u4fc3\u8fdb\u793e\u4f1a\u53d1\u5c55\u4e0e\u4e2a\u4f53\u53d1\u5c55\nD. \u6587\u5316\u4e0e\u79d1\u6280\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.470327239156247, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6cd5\u6cbb\u601d\u7ef4\u7684\u7406\u89e3\uff0c\u6b63\u786e\u7684\u662f\nA. \u6cd5\u6cbb\u601d\u7ef4\u662f\u4e00\u79cd\u8ba4\u8bc6\u601d\u7ef4\u800c\u4e0d\u662f\u5b9e\u8df5\u601d\u7ef4\nB. \u6cd5\u6cbb\u601d\u7ef4\u987b\u4ee5\u5408\u6cd5\u6027\u5224\u65ad\u4f5c\u4e3a\u5176\u6838\u5fc3\u5185\u5bb9\nC. \u6cd5\u6cbb\u601d\u7ef4\u4e3b\u8981\u662f\u7acb\u6cd5\u673a\u5173\u91c7\u7528\u7684\u601d\u7ef4\u65b9\u5f0f\nD. \u6cd5\u6cbb\u601d\u7ef4\u662f\u5b9e\u4f53\u601d\u7ef4\u800c\u4e0d\u662f\u7a0b\u5e8f\u601d\u7ef4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6328777679856608, "meta-math/MetaMath-Mistral-7B": 0.9553261078984391, "itpossible/Chinese-Mistral-7B-v0.1": 0.5303553882948552, "HuggingFaceH4/zephyr-7b-beta": 0.9948724061561142, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9035074449458845, "meta-llama/Meta-Llama-3-8B": 0.9261938822189474, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9813579252514291}}, {"question": "\u4f7f\u7528\u706d\u706b\u5668\u6251\u6551\u706b\u707e\u65f6\u8981\u5bf9\u51c6\u706b\u707e\uff08\uff09\u55b7\u5c04\u3002\nA. \u6839\u90e8\nB. \u4e0a\u90e8\nC. \u4e2d\u90e8\nD. \u5747\u53ef\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.371068906204966, "itpossible/Chinese-Mistral-7B-v0.1": 0.31292412927280244, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7492890589856313, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7958709576788267}}, {"question": "\u5973\u6027\uff0c43\u5c81\u3002\u809d\u5916\u80c6\u7ba1\u7ed3\u77f3\u75c7\u53f23\u5e74\uff0c10\u5c0f\u65f6\u524d\u7a81\u7136\u53f3\u4e0a\u8179\u7ede\u75db\uff0c\u6076\u5fc3\u3001\u5455\u5410\uff0c\u7ee7\u800c\u51fa\u73b0\uff0c\u5bd2\u6218\u3001\u9ad8\u70ed\u3001\u795e\u5fd7\u6de1\u6f20\u3001\u55dc\u7761\uff0c\u67e5\u4f53\uff1aT\uff1a40\u2103\uff0cP120\u6b21\u6bcf\u5206\uff0cBP\uff1a85/60mmHg\uff0c\u4e0a\u8179\u8f7b\u538b\u75db\u3002\u6700\u53ef\u80fd\u8bca\u65ad\u662f\nA. \u6025\u6027\u80f0\u817a\u708e\nB. \u6025\u6027\u6897\u963b\u5316\u8113\u6027\u80c6\u7ba1\u708e\nC. \u6025\u6027\u80c6\u56ca\u708e\nD. \u809d\u8113\u80bf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3581240209483902, "meta-math/MetaMath-Mistral-7B": 0.46601037218839503, "itpossible/Chinese-Mistral-7B-v0.1": 0.8588992425505196, "HuggingFaceH4/zephyr-7b-beta": 0.48125657075901024, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5528959826883278, "meta-llama/Meta-Llama-3-8B": 0.5328076801839399, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.47984278270189873}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u7684\u8bf4\u6cd5\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u7269\u4f53\u7684\u901f\u5ea6\u53d8\u5316\u8d8a\u5feb\uff0c\u52a0\u901f\u5ea6\u8d8a\u2f24\nB. \u7269\u4f53\u7684\u901f\u5ea6\u4e3a\u96f6\u65f6\uff0c\u52a0\u901f\u5ea6\u4e5f\u4e3a\u96f6\nC. \u7269\u4f53\u7684\u901f\u5ea6\u53d8\u5316\u91cf\u8d8a\u2f24\uff0c\u52a0\u901f\u5ea6\u8d8a\u2f24\nD. \u7269\u4f53\u7684\u901f\u5ea6\u8d8a\u2f24\uff0c\u52a0\u901f\u5ea6\u4e5f\u8d8a\u2f24\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u91c7\u53d6\u66b4\u529b\u91cd\u4f24\u7684\u65b9\u6cd5\u62d2\u4e0d\u6267\u884c\u4eba\u6c11\u6cd5\u9662\u5df2\u7ecf\u751f\u6548\u7684\u6c11\u4e8b\u5224\u51b3\uff0c\u9020\u6210\u6cd5\u9662\u6267\u884c\u4eba\u5458\u4e59\u91cd\u4f24\u3002\u7532\u7684\u884c\u4e3a\u6784\u6210\nA. \u59a8\u5bb3\u516c\u52a1\u7f6a\nB. \u6545\u610f\u4f24\u5bb3\u7f6a\nC. \u62d2\u4e0d\u6267\u884c\u5224\u51b3\u3001\u88c1\u5b9a\u7f6a\nD. \u8fc7\u5931\u81f4\u4eba\u91cd\u4f24\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e16\u7eaa\u4f01\u4e1a\u4e0e\u7ade\u4e89\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u672c\u8d28\u4e0a\u662f\nA. \u6d88\u706d\u5bf9\u624b\nB. \u4e92\u60e0\u4e92\u5229\nC. \u7ade\u4e89\u5408\u4f5c\nD. \u4e92\u76f8\u4fc3\u8fdb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7042314923656142, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u89c4\u8303\u793e\u4f1a\u79e9\u5e8f\u548c\u7ba1\u7406\u516c\u5171\u4e8b\u52a1\u8fd9\u4e24\u4e2a\u65b9\u9762\u7684\u529f\u80fd\u96c6\u4e2d\u4f53\u73b0\u4e8e\nA. \u7ecf\u6d4e\u5236\u5ea6\nB. \u6559\u80b2\u5236\u5ea6\nC. \u653f\u6cbb\u5236\u5ea6\nD. \u5b97\u6559\u5236\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7942762620351463, "meta-math/MetaMath-Mistral-7B": 0.8960518343658307, "itpossible/Chinese-Mistral-7B-v0.1": 0.7213854747329387, "HuggingFaceH4/zephyr-7b-beta": 0.9708484729248968, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9173989549468679, "meta-llama/Meta-Llama-3-8B": 0.7909436934270098, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9724328570822116}}, {"question": "\u2f00\u4e2a\u7269\u4f53\u505a\u2f83\u7531\u843d\u4f53\u8fd0\u52a8\uff0c\u53d6g = 10 m/s^2\uff0c\u52192s\u672b\u7269\u4f53\u7684\u901f\u5ea6\u4e3a\nA. 70m/s\nB. 30m/s\nC. 50m/s\nD. 20m/s\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5247637129945718, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e16\u754c\u4e0a\u4e00\u5929\u4e2d\u6700\u65e9\u8fce\u6765\u65ed\u65e5\u4e1c\u5347\u7684\u56fd\u5bb6\u662f\nA. \u4e2d\u56fd\nB. \u7f8e\u56fd\nC. \u6590\u6d4e\nD. \u51b0\u5c9b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44438777539771496, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.48881713858025694, "meta-llama/Meta-Llama-3-8B": 0.44128422604316603, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b54\u5b50\u66f0\uff1a\u201c\u4e0a\u597d\u5219\u6c11\u83ab\u6562\u4e0d\u656c\u3002\u4e0a\u597d\u4e49\u5219\u6c11\u83ab\u6562\u4e0d\u670d\u3002\u4e0a\u597d\u4fe1\u5219\u6c11\u83ab\u6562\u4e0d\u7528\u60c5\uff0c\u592b\u5982\u662f\uff0c\u662f\u56db\u65b9\u4e4b\u6c11\u8941\u8d1f\u5176\u5b50\u800c\u81f3\u77e3\uff0c\u7109\u7528\u7a3c\uff01\u201d\u8fd9\u6bb5\u8bdd\u8bf4\u660e\u5b54\u5b50\u65bd\u6559\u5185\u5bb9\nA. \u5177\u6709\u5168\u9762\u6027\nB. \u7ed3\u5408\u793e\u4f1a\u751f\u4ea7\nC. \u8131\u79bb\u793e\u4f1a\u751f\u4ea7\nD. \u8d23\u4efb\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6309\u7167\u9009\u62e9\u6027\u5206\u7c7b\u53ef\u4ee5\u628a\u8bfe\u7a0b\u5206\u4e3a\nA. \u5730\u65b9\u8bfe\u7a0b\u548c\u6821\u672c\u8bfe\u7a0b\nB. \u5fb7\u80b2\u8bfe\u7a0b\u3001\u667a\u80b2\u8bfe\u7a0b\u3001\u4f53\u80b2\u8bfe\u7a0b\u3001\u7f8e\u80b2\u8bfe\u7a0b\u548c\u52b3\u52a8\u6280\u672f\u8bfe\u7a0b\u7b49\nC. \u5fc5\u4fee\u8bfe\u7a0b\u548c\u9009\u4fee\u8bfe\u7a0b\nD. \u5206\u79d1\u8bfe\u7a0b\u4e0e\u7efc\u5408\u8bfe\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44667014868464233, "meta-math/MetaMath-Mistral-7B": 0.6783902111170863, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5214427937963255, "meta-llama/Meta-Llama-3-8B": 0.6949905177755696, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u6709\u53e5\u4fd7\u8bdd\u201c\u4e00\u4e2a\u597d\u5144\u5f1f\u4e0d\u5982\u4e00\u4e2a\u597d\u90bb\u5c45\u201d\u3002\u8054\u60f3\u6539\u9769\u5f00\u653e\u4ee5\u6765\u4e2d\u56fd\u7684\u5916\u4ea4\uff0c\u7a81\u51fa\u4f53\u73b0\u8fd9\u79cd\u601d\u60f3\u7684\u662f\nA. \u79ef\u6781\u53c2\u52a0\u56fd\u9645\u7ef4\u548c\u884c\u52a8\nB. \u79ef\u6781\u53c2\u52a0\u4e16\u754c\u8d38\u6613\u7ec4\u7ec7\nC. \u53c2\u4e0e\u53d1\u8d77\u521b\u7acb\u4e9a\u592a\u7ecf\u5408\u7ec4\u7ec7\nD. \u53d1\u8d77\u6210\u7acb\u4e0a\u6d77\u5408\u4f5c\u7ec4\u7ec7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6469488542572547, "meta-math/MetaMath-Mistral-7B": 0.7981534981918792, "itpossible/Chinese-Mistral-7B-v0.1": 0.5940327246278464, "HuggingFaceH4/zephyr-7b-beta": 0.9994577319461535, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8120075557292578, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.582158713053937}}, {"question": "\u67d0\u6c34\u6587\u7ad9\u63a7\u5236\u9762\u79ef\u4e3a 680km^2\uff0c\u591a\u5e74\u5e73\u5747\u5e74\u5f84\u6d41\u6a21\u6570\u4e3a 10 L/(s\u00b7km^2)\uff0c\u5219\u6362\u7b97\u6210\u5e74\u5f84\u6d41\u6df1\u4e3a\nA. 315.4mm\nB. 463.8mm\nC. 408.5mm\nD. 587.5mm\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2712019384407753, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3982518743172179}}, {"question": "\u4e0b\u5217\u4e0e\u795e\u7ecf\u7ec6\u80de\u6709\u5173\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u795e\u7ecf\u9012\u8d28\u5728\u7a81\u89e6\u95f4\u9699\u4e2d\u7684\u79fb\u52a8\u6d88\u8017ATP\nB. \u7a81\u89e6\u540e\u819c\u4e0a\u53d7\u4f53\u86cb\u767d\u7684\u5408\u6210\u9700\u8981\u6d88\u8017ATP\nC. \u795e\u7ecf\u7ec6\u80de\u5174\u594b\u540e\u6062\u590d\u4e3a\u9759\u606f\u72b6\u6001\u6d88\u8017ATP\nD. ATP\u80fd\u5728\u795e\u7ecf\u5143\u7ebf\u7c92\u4f53\u7684\u5185\u819c\u4e0a\u4ea7\u751f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.38495463066147895}}, {"question": "\u7532\u5c06\u501f\u7ed9\u4e59\u7684\u7b14\u8bb0\u672c\u7535\u8111\u5356\u7ed9\u4e19\uff0c\u7532\u3001\u4e19\u7ea6\u5b9a\u7531\u4e19\u76f4\u63a5\u5411\u4e59\u8bf7\u6c42\u8fd4\u8fd8\u7535\u8111\u3002\u8be5\u7535\u8111\u7684\u4ea4\u4ed8\u65b9\u5f0f\u5c5e\u4e8e\nA. \u6307\u793a\u4ea4\u4ed8\nB. \u7b80\u6613\u4ea4\u4ed8\nC. \u73b0\u5b9e\u4ea4\u4ed8\nD. \u5360\u6709\u6539\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2821833983601388, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.431033945941338, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.857415796222165}}, {"question": "\u5728\u521b\u9020\u6027\u601d\u7ef4\u4e2d\uff08\uff09\u8d77\u7740\u6838\u5fc3\u4f5c\u7528\u3002\nA. \u96c6\u4e2d\u601d\u7ef4\nB. \u5e38\u89c4\u601d\u7ef4\nC. \u53d1\u6563\u601d\u7ef4\nD. \u62bd\u8c61\u601d\u7ef4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8346126410926668, "meta-math/MetaMath-Mistral-7B": 0.9846907585010882, "itpossible/Chinese-Mistral-7B-v0.1": 0.8277412902609944, "HuggingFaceH4/zephyr-7b-beta": 0.9816058921816604, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9676125219346055, "meta-llama/Meta-Llama-3-8B": 0.9375145304935099, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9640199248560878}}, {"question": "\u767d\u5589\u6bd2\u7d20\u5bf9\u86cb\u767d\u8d28\u751f\u7269\u5408\u6210\u7684\u6291\u5236\u4f5c\u7528\u662f\nA. \u5bf9eIF2\u8fdb\u884c\u5171\u4ef7\u4fee\u9970\nB. \u5bf9eIF1 \u8fdb\u884c\u5171\u4ef7\u4fee\u9970\nC. \u5bf9eEF1\u8fdb\u884c\u5171\u4ef7\u4fee\u9970\nD. \u5bf9eEF2\u8fdb\u884c\u5171\u4ef7\u4fee\u9970\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "SNMP\u5b9e\u4f53\u53ef\u4ee5\u5bf9MIB-2\u4e2d\u7684\u5bf9\u8c61\u6267\u2f8f\u7684\u64cd\u4f5c\u6709\nA. Get,Set,GetNext \nB. Get,Set,Trap \nC. Get,GetNext,Trap \nD. Set,Trap,GetNext\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3376204968816013, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5504396545954163, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.46946100949437053}}, {"question": "ABO\u8840\u578b\u7cfb\u7edf\u7684\u4e3b\u8981\u6297\u4f53\u662f\nA. I\u607cE\nB. IgM\nC. IgA\nD. IgG\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ea7\u751f\u611f\u751f\u7535\u573a\u7684\u6839\u6e90\u662f\uff1a\nA. \u5747\u5300\u78c1\u573a\nB. \u53d8\u5316\u78c1\u573a\nC. \u975e\u5747\u5300\u78c1\u573a\nD. \u7a33\u6052\u7535\u573a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8741434531710592, "meta-math/MetaMath-Mistral-7B": 0.8823734646520647, "itpossible/Chinese-Mistral-7B-v0.1": 0.9033333681042766, "HuggingFaceH4/zephyr-7b-beta": 0.9973346195999537, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9412611653407155, "meta-llama/Meta-Llama-3-8B": 0.8763486503586153, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9908571695032384}}, {"question": "\u4e0b\u5217\u6587\u7ae0\u4e2d\uff0c\u5728\u987a\u53d9\u4e4b\u4e2d\u53c8\u65f6\u65f6\u8fd0\u7528\u4e86\u63d2\u53d9\u548c\u5012\u53d9\u624b\u6cd5\u7684\u662f\nA. \u300a\u5c11\u5973\u5c0f\u6e14\u300b\nB. \u300a\u7ec4\u7ec7\u90e8\u6765\u4e86\u4e2a\u5e74\u8f7b\u4eba\u300b\nC. \u300a\u978b\u300b\nD. \u300a\u767e\u5408\u82b1\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3127242143872847, "meta-math/MetaMath-Mistral-7B": 0.38376160636627166, "itpossible/Chinese-Mistral-7B-v0.1": 0.2731272040287072, "HuggingFaceH4/zephyr-7b-beta": 0.6487062340098906, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.36617639865290497, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b0\u65f6\u671f\u6211\u56fd\u7231\u56fd\u4e3b\u4e49\u7684\u4e3b\u9898\u662f\nA. \u56e2\u7ed3\u4e00\u5207\u7231\u56fd\u540c\u80de\nB. \u5efa\u8bbe\u548c\u53d1\u5c55\u4e2d\u56fd\u7279\u8272\u793e\u4f1a\u4e3b\u4e49\nC. \u52a8\u5458\u548c\u9f13\u821e\u5168\u56fd\u5404\u65cf\u4eba\u6c11\u56e2\u7ed3\u594b\u6597\nD. \u5efa\u7acb\u5171\u4ea7\u4e3b\u4e49\u793e\u4f1a\u5236\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6022627845877748, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5129523785761476, "meta-llama/Meta-Llama-3-8B": 0.5752404553160102, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe$\\mu_s^{(1)}=\\frac{1}{a-x}(0 \\leqslant x \\leqslant a)$\uff0c\u4e14$\\mu^{(2)}(x)=1\uff0cl_0^{(\\tau)}=a$\uff0c\u5219\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u6709( )\u3002(1)$l_x^{(\\tau)}=(a-x) e^{-x}$;(2)$d_x^{(1)}=e^{-x-1}-e^{-x}$;(3)$d_x^{(2)}=(a-x-1) e^{-x}-(a-x-2) e^{-x-1}$\u3002\nA. (1) (2) (3)\nB. (1) (2)\nC. (1) (3)\nD. (2) (3)\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.28218339836013884, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6d3b\u8840\u836f\u4e2d\uff0c\u54ea\u4e00\u5473\u4e0d\u517c\u6709\u884c\u6c14\u4f5c\u7528\nA. \u90c1\u91d1\nB. \u4e94\u7075\u8102\nC. \u7384\u80e1\nD. \u4e09\u68f1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3976839276686676, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2801288226217134, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u529f\u7387\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u529f\u7387\u662f\u63cf\u8ff0\u505a\u529f\u5feb\u6162\u7684\u7269\u7406\u91cf\uff0c\u5728\u56fd\u9645\u5355\u4f4d\u5236\u4e2d\uff0c\u5176\u5355\u4f4d\u662f\u7126\u8033\uff08J\uff09\nB. \u529f\u7387\u662f\u63cf\u8ff0\u505a\u529f\u591a\u5c11\u7684\u7269\u7406\u91cf\uff0c\u5728\u56fd\u9645\u5355\u4f4d\u5236\u4e2d\uff0c\u5176\u5355\u4f4d\u662f\u74e6\u7279\uff08W\uff09\nC. \u529f\u7387\u662f\u63cf\u8ff0\u505a\u529f\u5feb\u6162\u7684\u7269\u7406\u91cf\uff0c\u5728\u56fd\u9645\u5355\u4f4d\u5236\u4e2d\uff0c\u5176\u5355\u4f4d\u662f\u74e6\u7279\uff08W\uff09\nD. \u529f\u7387\u662f\u63cf\u8ff0\u505a\u529f\u591a\u5c11\u7684\u7269\u7406\u91cf\uff0c\u5728\u56fd\u9645\u5355\u4f4d\u5236\u4e2d\uff0c\u5176\u5355\u4f4d\u662f\u7126\u8033\uff08J\uff09\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.8043757468307048, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8354348912074645, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8594875794910544}}, {"question": "\u5c0f\u9ec4\u5728\u5b66\u4e60\u65f6\u5173\u6ce8\u7684\u662f\u77e5\u8bc6\u7684\u5185\u5bb9\u548c\u4ef7\u503c\uff0c\u800c\u4e0d\u662f\u4e3a\u4e86\u83b7\u5f97\u5206\u6570\u548c\u5956\u52b1\u3002\u6839\u636e\u6210\u5c31\u76ee\u6807\u7406\u8bba\uff0c\u5c0f\u9ec4\u7684\u76ee\u6807\u5bfc\u5411\u5c5e\u4e8e\nA. \u6210\u7ee9\u8d8b\u8fd1\nB. \u6210\u7ee9\u56de\u907f\nC. \u638c\u63e1\u56de\u907f\nD. \u638c\u63e1\u8d8b\u8fd1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.42693268611230684, "HuggingFaceH4/zephyr-7b-beta": 0.6490943227562499, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.36463638986032226, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c34\u6df9\u8def\u9762\u5f71\u54cd\u884c\u8f66\u5b89\u5168\uff0c\u4e0d\u6613\u901a\u884c\u7684\u539f\u56e0\u662f\u4ec0\u4e48\nA. \u8def\u9762\u9644\u7740\u529b\u589e\u5927\nB. \u80fd\u89c1\u5ea6\u4f4e\uff0c\u89c6\u91ce\u6a21\u7cca\nC. \u65e5\u5149\u53cd\u5c04\u963b\u6321\u89c6\u7ebf\nD. \u65e0\u6cd5\u89c2\u5bdf\u5230\u6697\u5751\u548c\u51f8\u8d77\u7684\u8def\u9762\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9974744895850703, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u80be\u810f\u91cd\u5438\u6536\u548c\u5206\u6ccc $\\mathrm{K}^{+}$\u7684\u53d9\u8ff0\uff0c \u6b63\u786e\u7684\u662f\nA. \u8fdc\u7aef\u80be\u5c0f\u7ba1\u5206\u6ccc $K^{+}$\uff0c \u4f46\u4e0d\u91cd\u5438\u6536 $K^{+}$\nB. \u8fd1\u7aef\u80be\u5c0f\u7ba1\u91cd\u5438\u6536 $25 \\% \\sim 30 \\%$ \u7684 $\\mathrm{K}^{+}$\nC. \u8fdc\u66f2\u5c0f\u7ba1\u5206\u6ccc $\\mathrm{K}^{+}$\u53d7\u919b\u56fa\u916e\u8c03\u8282\nD. \u9ad3\u88a2\u91cd\u5438\u6536 $65 \\% \\sim 70 \\%$ \u7684 $\\mathrm{K}^{+}$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u793e\u4f1a\u4e3b\u4e49\u521d\u7ea7\u9636\u6bb5\uff0c\u6211\u56fd\u7684\u6c11\u65cf\u95ee\u9898\u4e3b\u8981\u96c6\u4e2d\u8868\u73b0\u4e3a\nA. \u5c11\u6570\u6c11\u65cf\u548c\u6c11\u65cf\u5730\u533a\u8981\u6c42\u52a0\u5feb\u53d1\u5c55\u7ecf\u6d4e\u6587\u5316\u4e8b\u4e1a\nB. \u5404\u6c11\u65cf\u95f4\u7531\u4e8e\u67d0\u4e9b\u601d\u60f3\u89c2\u5ff5\u5f15\u8d77\u7684\u73b0\u5b9e\u7684\u77db\u76fe\u548c\u7ea0\u7eb7\nC. \u56fd\u5bb6\u600e\u6837\u5411\u6c11\u65cf\u5730\u533a\u7ed9\u4e88\u66f4\u591a\u516c\u5171\u8d22\u653f\u652f\u6301\u7684\u95ee\u9898\nD. \u89e3\u51b3\u4e1c\u90e8\u5730\u533a\u4e0e\u897f\u90e8\u5c11\u6570\u6c11\u65cf\u5730\u533a\u95f4\u7684\u7ecf\u6d4e\u5dee\u8ddd\u95ee\u9898\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.32267756351741395, "meta-math/MetaMath-Mistral-7B": 0.6670876962125541, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3788073923457662, "meta-llama/Meta-Llama-3-8B": 0.5723103363737647, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7528\u4e8c\u9879\u5206\u5e03\u76f4\u63a5\u8ba1\u7b97\u6982\u7387\u6cd5\u68c0\u9a8c Ho: $\\pi=0.4 \\mathrm{Hi}$ : $\\pi>0.4$ \u3002\u5f53\u968f\u673a\u6837\u672c\u542b\u91cf $\\mathrm{n}=10$\uff0c\u9633\u6027\u6570 X=6 \u65f6\uff0c\u4e3a\u4f5c\u7edf\u8ba1\u63a8\u65ad\u5e94\u5c06\u6982\u7387 p=()\u4e0e\u68c0\u9a8c\u6c34\u51c6$\\alpha$\u6bd4\u8f83\u3002\nA. $p(X=6)$\nB. $p(X=6)+p(X=7)+\\cdots+p(X=10)$\nC. $\\mathrm{p}(\\mathrm{X}=7)+\\mathrm{p}(\\mathrm{X}=8)+\\cdots+\\mathrm{p}(\\mathrm{X}=10)$\nD. $\\mathrm{p}(\\mathrm{X}=6)+\\mathrm{p}(\\mathrm{X}=5)+\\cdots+\\mathrm{p}(\\mathrm{X}=0)$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2801288226217134, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3616053738546807, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2810882504428991, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u5b50\uff0c\u5c5e\u4e8e\u4e3b\u8c13\u5012\u88c5\u53e5\u7684\u4e00\u53e5\u662f\nA. \u751a\u77e3\u53e4\u66f8\u4e4b\u96e3\u8b80\u4e5f\u3002\nB. \u543e\u4ee5\u5b50\u7232\u7570\u4e4b\u554f\uff0c\u66fe\u7531\u8206\u6c42\u4e4b\u554f\u3002\nC. \u6b64\u53ef\u8b02\u77e5\u7fa9\u8206\u4e0d\u7fa9\u4e4b\u522b\u4e4e?\nD. \u53e4\u4eca\u6240\u50b3\uff0c\u4e0d\u53ef\u8aa3\u4e5f\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3128363857141096, "meta-math/MetaMath-Mistral-7B": 0.4562863714654386, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.42096270677603886, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u60c5\u7eea\u3002\u53ea\u662f\u4e00\u79cd\u8eab\u4f53\u72b6\u6001\u7684\u611f\u89c9\u201d\u662f() \u7684\u89c2\u70b9\u3002\nA. \u963f\u8bfa\u5fb7\nB. \u5170\u683c\nC. \u8a79\u59c6\u65af\nD. \u62c9\u624e\u52d2\u65af\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.29290375848132455, "meta-math/MetaMath-Mistral-7B": 0.32770783487659344, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7269\u4f53\u4ee560J\u7684\u521d\u52a8\u80fd\uff0c\u4eceA\u70b9\u51fa\u53d1\u4f5c\u7ad6\u76f4\u4e0a\u629b\u8fd0\u52a8\uff0c\u5728\u5b83\u4e0a\u5347\u5230\u67d0\u2f00\u2fbc\u5ea6\u65f6\uff0c\u52a8\u80fd\u635f\u5931\u4e8630J\uff0c\u2f7d\u673a\u68b0\u80fd\u635f\u5931\u4e8610J\uff0c\u5219\u8be5\u7269\u4f53\u518d\u843d\u56de\u5230A\u5904\u7684\u52a8\u80fd\u4e3a\uff08\u5047\u8bbe\u7a7a\u2f53\u963b\u2f12\u2f24\u2f29\u6052\u5b9a\uff09\nA. 50J\nB. 40J\nC. 20J\nD. 30J\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.29319990276779784, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6807\u51c6\u5316\u6d4b\u9a8c\u4e2d\u4e0d\u5c5e\u4e8e\u667a\u529b\u6d4b\u9a8c\u7684\u662f\nA. \u65af\u5766\u798f-\u6bd4\u7eb3\u91cf\u8868\nB. \u660e\u5c3c\u82cf\u8fbe\u6d4b\u9a8c\nC. \u97e6\u514b\u65af\u52d2\u91cf\u8868\nD. \u745e\u6587\u6d4b\u9a8c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4358945674046615, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.410931557689587}}, {"question": "\u5973\uff0c55\u5c81\u3002\u5de6\u4fa7\u4e73\u623f\u5185\u80bf\u57574\u00d73cm\uff0c\u57fa\u5e95\u4e0d\u56fa\u5b9a\uff0c\u5de6\u814b\u4e0b\u53ef\u89e6\u53ca\u591a\u4e2a\u8d28\u786c\u6dcb\u5df4\u7ed3\u76f8\u4e92\u878d\u5408\uff0c\u6dcb\u5df4\u6d3b\u68c0\u75c5\u7406\u62a5\u544a\u4e73\u817a\u764c\u8f6c\u79fb\uff0c\u672a\u53d1\u73b0\u8fdc\u5904\u8f6c\u79fb\u3002\u6309\u7167\u56fd\u9645\u6807\u51c6\uff0c\u5e94\u5c5e\u4e8e\u7684\u5206\u671f\u662f\nA. T\u2082N\u2082M\u2080\nB. T\u2083N\u2081M\u2080\nC. T\u2082N\u2081M\u2080\nD. T\u2081N\u2081M\u2080\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u5173\u4e73\u9178\u8131\u6c22\u9176\u540c\u5de5\u9176\u7684\u8bba\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u5b83\u4eec\u7684\u7535\u6cf3\u884c\u4e3a\u76f8\u540c\nB. \u5b83\u4eec\u5bf9\u540c\u4e00\u5e95\u7269\u6709\u4e0d\u540c\u7684Km\u503c\nC. \u5b83\u4eec\u5728\u4eba\u4f53\u5404\u7ec4\u7ec7\u5668\u5b98\u7684\u5206\u5e03\u65e0\u663e\u8457\u5dee\u522b\nD. M\u4e9a\u57fa\u548cH\u4e9a\u57fa\u5728\u4e73\u9178\u8131\u6c22\u9176\u540c\u5de5\u9176\u4e2d\u5206\u5e03\u4e00\u6837\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5935940427833691, "meta-math/MetaMath-Mistral-7B": 0.7783518934313175, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.995922072583813, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8018738411453854, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1/3\u7684\u76f8\u53cd\u6570\u662f\nA. 1/3\nB. 3\nC. -1/3\nD. -3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8586256855992377, "meta-math/MetaMath-Mistral-7B": 0.9946071025070804, "itpossible/Chinese-Mistral-7B-v0.1": 0.6341612489180077, "HuggingFaceH4/zephyr-7b-beta": 0.999487941237837, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9649512126859849, "meta-llama/Meta-Llama-3-8B": 0.8848182333855594, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.909156483622242}}, {"question": "Goodpasture \u7efc\u5408\u5f81\u9996\u9009\u7684\u6cbb\u7597\u65b9\u6cd5\u662f\nA. \u7cd6\u76ae\u8d28\u6fc0\u7d20+\u73af\u78f7\u9170\u80fa+\u8840\u6db2\u900f\u6790\nB. \u7cd6\u76ae\u8d28\u6fc0\u7d20\nC. \u7cd6\u76ae\u8d28\u6fc0\u7d20+\u73af\u78f7\u9170\u80fa+\u8840\u6d46\u7f6e\u6362\nD. \u7cd6\u76ae\u8d28\u6fc0\u7d20+\u73af\u78f7\u9170\u80fa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37047588396851644, "meta-math/MetaMath-Mistral-7B": 0.5967953719857468, "itpossible/Chinese-Mistral-7B-v0.1": 0.34182412994643, "HuggingFaceH4/zephyr-7b-beta": 0.7963962932804042, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5965102467760363, "meta-llama/Meta-Llama-3-8B": 0.4318598503855155, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u5b88\u6cd5\u7684\u7406\u89e3\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u884c\u4f7f\u6cd5\u5b9a\u6743\u5229\u662f\u5b88\u6cd5\u884c\u4e3a\nB. \u7531\u4e8e\u5408\u540c\u4e0d\u662f\u6cd5\u5f8b\uff0c\u56e0\u800c\u9075\u5b88\u5408\u540c\u5e76\u975e\u5b88\u6cd5\nC. \u67d0\u4f01\u4e1a\u53ca\u65f6\u3001\u8db3\u989d\u5411\u56fd\u5bb6\u7eb3\u7a0e\uff0c\u5c5e\u4e8e\u79ef\u6781\u7684\u5b88\u6cd5\nD. \u5b88\u6cd5\u901a\u5e38\u662f\u6cd5\u5f8b\u548c\u9053\u5fb7\u7684\u5171\u540c\u8981\u6c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9246193248804245, "meta-math/MetaMath-Mistral-7B": 0.9921434180478922, "itpossible/Chinese-Mistral-7B-v0.1": 0.9224052886306335, "HuggingFaceH4/zephyr-7b-beta": 0.9992455891737065, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9916601365992291, "meta-llama/Meta-Llama-3-8B": 0.9102215586471509, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9779866306647866}}, {"question": "\u4e0b\u5217\u53e4\u6ce8\u7c7b\u578b\uff0c\u5c5e\u4e8e\u5bf9\u53e4\u4e66\u6587\u5b57\u8fdb\u884c\u8fa8\u97f3\u91ca\u4e49\u7684\u4e00\u9879\u662f\nA. \u97f3\u4e49\nB. \u4f20\u6ce8\nC. \u8865\u6ce8\nD. \u96c6\u89e3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9436090805717301, "meta-math/MetaMath-Mistral-7B": 0.9560711551908643, "itpossible/Chinese-Mistral-7B-v0.1": 0.875593489257438, "HuggingFaceH4/zephyr-7b-beta": 0.999917261723029, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.943935516240722, "meta-llama/Meta-Llama-3-8B": 0.61164079792877, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u4ea4\u6d41\u65b9\u5f0f\u4e2d\u4e0d\u5c5e\u4e8e\u5b9e\u65f6\u7684\u4fe1\u606f\u4ea4\u6d41\u65b9\u5f0f\u7684\u662f\nA. ICQ\nB. MSN\nC. E-mail\nD. QQ\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6127027859842377, "meta-math/MetaMath-Mistral-7B": 0.9051141613813241, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9945559846787327, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9045367290845635, "meta-llama/Meta-Llama-3-8B": 0.9051378039021795, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9933175540697828}}, {"question": "\u5173\u4e8e ARMA (auto regressive moving average model)\uff08\u81ea\u56de\u5f52\u6ed1\u52a8\u5e73\u5747\u6a21\u578b\uff09\u3001 AR (auto regressive model)\uff08\u81ea\u56de\u5f52\u6a21\u578b\uff09\u3001 MA\uff08\u6ed1\u52a8\u5e73\u5747\u6a21\u578b\uff09 \u6a21\u578b\u7684\u529f\u7387\u8c31\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. AR\u6a21\u578b\u5728\u96f6\u70b9\u63a5\u8fd1\u5355\u4f4d\u5706\u65f6\uff0cAR\u8c31\u662f\u4e00\u4e2a\u5c16\u5cf0\nB. MA\u6a21\u578b\u662f\u540c\u4e00\u4e2a\u5168\u901a\u6ee4\u6ce2\u5668\u4ea7\u751f\u7684\nC. MA\u6a21\u578b\u5728\u6781\u70b9\u63a5\u8fd1\u5355\u4f4d\u5706\u65f6\uff0cMA\u8c31\u662f\u4e00\u4e2a\u6df1\u8c37\nD. RMA\u8c31\u65e2\u6709\u5c16\u5cf0\u53c8\u6709\u6df1\u8c37\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ece\u653f\u6cbb\u7ecf\u6d4e\u7ed3\u6784\u770b\uff0c\u5fb7\u610f\u5fd7\u5e1d\u56fd(1871\uff5e1919)\u5448\u73b0\u7684\u662f\u201c\u7ecf\u6d4e\u5de8\u4eba\u3001\u653f\u6cbb\u8ddb\u5b50\u7684\u534a\u8d44\u672c\u4e3b\u4e49\u6216\u8005\u8bf4\u662f\u534a\u4e13\u5236\u4e3b\u4e49\u7684\u793e\u4f1a\u5f62\u6001\u201d\u3002\u4ee5\u4e0b\u80fd\u652f\u6301\u8fd9\u4e00\u89c2\u70b9\u7684\u53f2\u5b9e\u4e0d\u5305\u62ec\nA. \u5fb7\u610f\u5fd7\u7687\u5e1d\u662f\u56fd\u5bb6\u5143\u9996\nB. \u7687\u5e1d\u62e5\u6709\u4e3b\u5bb0\u8bae\u4f1a\u3001\u521b\u5236\u6cd5\u5f8b\u7b49\u6743\u529b\nC. \u5bb0\u76f8\u5bf9\u7687\u5e1d\u8d1f\u8d23\uff0c\u800c\u4e0d\u662f\u5bf9\u8bae\u4f1a\u8d1f\u8d23\nD. \u5fb7\u610f\u5fd7\u5de5\u4e1a\u5316\u8fdb\u7a0b\u52a0\u5feb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5096453663047156}}, {"question": "\u5982\u56fe\u6240\u793a\u6e56\u4e2d\u6709\u4e00\u5c0f\u8239\uff0c\u6709\u4eba\u7528\u7ef3\u7ed5\u8fc7\u5cb8\u4e0a\u4e00\u5b9a\u9ad8\u5ea6\u5904\u7684\u5b9a\u6ed1\u8f6e\u62c9\u6e56\u4e0a\u7684\u8239\u5411\u5cb8\u8fb9\u8fd0\u52a8\uff0c\u8bbe\u8be5\u4eba\u4ee5\u5300\u901f\u7387v0\u6536\u7ef3\uff0c\u7ef3\u957f\u4e0d\u53d8\uff0c\u6e56\u6c34\u9759\u6b62\uff0c\u5219\u5c0f\u8239\u7684\u8fd0\u52a8\u662f\nA. \u5300\u52a0\u901f\u8fd0\u52a8\nB. \u5300\u51cf\u901f\u8fd0\u52a8\nC. \u53d8\u52a0\u901f\u8fd0\u52a8\nD. \u53d8\u51cf\u901f\u8fd0\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u653f\u5e9c\u5e94\u8be5\u5b9e\u65bd\u4e00\u6761\u6cd5\u6848\u6765\u7981\u6b62\u5728\u901a\u52e4\u706b\u8f66\u4e0a\u9500\u552e\u548c\u996e\u7528\u9152\u7cbe\u996e\u6599\u3002\u6700\u8fd1\uff0c\u653f\u5e9c\u8fd0\u7528\u5176\u6cd5\u5f8b\u6743\u529b\uff0c\u901a\u8fc7\u4e86\u4e00\u6761\u7981\u6b62\u5728\u901a\u52e4\u706b\u8f66\u4e0a\u62bd\u70df\uff0c\u6765\u4fdd\u62a4\u4e0a\u4e0b\u73ed\u4eba\u7684\u5065\u5eb7\u7684\u6cd5\u5f8b\u3002\u5f53\u559d\u9189\u4e86\u9152\u7684\u4e58\u5ba2\u4e0b\u4e86\u706b\u8f66\uff0c\u94bb\u8fdb\u4ed6\u4eec\u7684\u6c7d\u8f66\u540e\u5f00\u8f66\uff0c\u516c\u4f17\u9762\u4e34\u7684\u5371\u9669\u4e0e\u706b\u8f66\u4e0a\u4e0d\u62bd\u70df\u7684\u4e58\u5ba2\u88ab\u8feb\u547c\u5438\u9999\u70df\u7684\u70df\u5c18\u6240\u9762\u4e34\u7684\u5371\u9669\u81f3\u5c11\u662f\u4e00\u6837\u5927\u3002\u5728\u8bc1\u660e\u5728\u901a\u52e4\u706b\u8f66\u4e0a\u559d\u542b\u6709\u9152\u7cbe\u7684\u996e\u6599\u5e94\u8be5\u88ab\u7981\u6b62\u65f6\u4f5c\u8005\u4f9d\u8d56\u4e8e\nA. \u559d\u542b\u6709\u9152\u7cbe\u7684\u996e\u6599\u6709\u5bb3\u4e2a\u4eba\u5065\u5eb7\u7684\u4e8b\u5b9e\nB. \u5728\u62bd\u70df\u7684\u5f71\u54cd\u4e0e\u559d\u542b\u9152\u7cbe\u7684\u996e\u6599\u7684\u5f71\u54cd\u4e24\u8005\u4e4b\u95ee\u505a\u4e86\u4e00\u4e2a\u6bd4\u8f83\nC. \u5bf9\u62bd\u70df\u548c\u559d\u9152\u7cbe\u996e\u6599\u505a\u4e86\u4e00\u4e2a\u5145\u6ee1\u611f\u60c5\u7684\u6307\u8d23\u6027\u63cf\u8ff0\nD. \u4eba\u4eec\u9700\u8981\u4fdd\u62a4\u4ee5\u514d\u53d7\u4ed6\u4eba\u7684\u884c\u4e3a\u5bf9\u81ea\u5df1\u9020\u6210\u4f24\u5bb3\u7684\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u81ea\u7136\u8d44\u6e90\u7684\u5206\u7c7b\u4e2d\uff0c\u5929\u7136\u6c14\u5c5e\u4e8e\nA. \u66ff\u4ee3\u8d44\u6e90\nB. \u73af\u5883\u8d44\u6e90\nC. \u77ff\u4ea7\u8d44\u6e90\nD. \u751f\u7269\u8d44\u6e90\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.310313193127302, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5581267872721702}}, {"question": "\u5728\u73bb\u7483\u4e0a\u53ef\u6807\u5ea6\u65f6\u7528\u5230\u4e00\u79cd\u9178\uff0c\u8be5\u9178\u662f\nA. \u6c30\u6c22\u9178\nB. \u786b\u9178\nC. \u7a00\u76d0\u9178\nD. \u6c22\u6c1f\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35378712543517676, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7916774493002867, "HuggingFaceH4/zephyr-7b-beta": 0.9372549570547044, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.42284688246515895, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u4e24\u4e2a\u540c\u6837\u7684\u6728\u5757\uff0c\u4ece\u540c\u9ad8\u5ea6\u81ea\u7531\u4e0b\u843d\uff0c\u5728\u4e0b\u843d\u4e2d\uff0c\u5176\u4e2d\u4e00\u6728\u5757\u88ab\u6c34\u5e73\u98de\u6765\u7684\u5b50\u5f39\u51fb\u4e2d\uff0c\u5e76\u4f7f\u5b50\u5f39\u9677\u4e8e\u5176\u4e2d\uff0c\u5b50\u5f39\u7684\u8d28\u91cf\u4e0d\u80fd\u5ffd\u7565\uff0c\u4e0d\u8ba1\u7a7a\u6c14\u963b\u529b\uff0c\u5219\nA. \u6761\u4ef6\u4e0d\u8db3\uff0c\u65e0\u6cd5\u786e\u5b9a\nB. \u88ab\u51fb\u6728\u5757\u540e\u5230\u8fbe\u5730\u9762\nC. \u88ab\u51fb\u6728\u5757\u5148\u5230\u8fbe\u5730\u9762\nD. \u4e24\u6728\u5757\u540c\u65f6\u5230\u8fbe\u5730\u9762\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9057\u4f20\u56fe\u8ddd\u7684\u5355\u4f4d\u2015\u2015\u5398\u6469\uff08centi-Morgan\uff09\u8868\u793a\u7684\u610f\u4e49\u662f\nA. \u4e24\u4e2a\u57fa\u56e0\u95f4\u7684\u6838\u82f7\u9178\u6570\u76ee\nB. \u76f8\u5f53\u4e8e100\u4e2a\u6838\u82f7\u9178\nC. \u8fde\u9501\u57fa\u56e0\u95f4\u91cd\u7ec4\u9891\u7387\u7684\u5ea6\u91cf\u5355\u4f4d\nD. \u76f8\u5f53\u4e8e\u5398\u7c73\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9221818328605915, "meta-math/MetaMath-Mistral-7B": 0.8763486541587834, "itpossible/Chinese-Mistral-7B-v0.1": 0.8384074999966842, "HuggingFaceH4/zephyr-7b-beta": 0.992931573721883, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9587982072134059, "meta-llama/Meta-Llama-3-8B": 0.9641661223801624, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9989209809546848}}, {"question": "\u8de8\u56fd\u516c\u53f8\u4f26\u7406\u5173\u7cfb\u6574\u5408\u6218\u7565\u7684\u7406\u8bba\u4f9d\u636e\u662f\nA. \u8de8\u6587\u5316\u7ba1\u7406\u7406\u8bba\nB. \u6574\u5408\u540c\u5316\u7406\u8bba\nC. \u540c\u5316\u7406\u8bba\nD. \u6574\u5408\u7406\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3395893495876965, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53f3\u5fc3\u8870\u7aed\u65f6\u7ec4\u7ec7\u6db2\u751f\u6210\u589e\u52a0\u800c\u81f4\u6c34\u80bf\u7684\u4e3b\u8981\u673a\u5236\u662f\nA. \u8840\u7ba1\u901a\u900f\u6027\u589e\u52a0\nB. \u6bdb\u7ec6\u8840\u7ba1\u8840\u538b\u589e\u9ad8\nC. \u7ec4\u7ec7\u6db2\u80f6\u4f53\u6e17\u900f\u538b\u589e\u9ad8\nD. \u7ec4\u7ec7\u6db2\u9759\u6c34\u538b\u964d\u4f4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c83\u5c14\u592b\u58f0\u79f0\uff0c\u89e3\u91ca\u5eb7\u5fb7\u7684\u4f26\u7406\u5b66\u4f7f\u5176\u5305\u542b\u4e00\u7ec4\u6709\u9650\u7684\u7ea6\u675f\uff1a\nA. \u4f1a\u4ea7\u751f\u4e00\u5e45\u6ca1\u6709\u5438\u5f15\u529b\u7684\u9053\u5fb7\u5723\u5f92\u56fe\u666f\u3002\nB. \u4ea7\u751f\u4e00\u4e2a\u8981\u6c42\u592a\u9ad8\u7684\u7406\u8bba\u3002\nC. \u96be\u4ee5\u7f6e\u4fe1\u5730\u4e3a\u9053\u5fb7\u4ef7\u503c\u8bbe\u5b9a\u4e86\u4e00\u4e2a\u201c\u4e0a\u9650\u201d\u3002\nD. \u5c06\u7406\u8bba\u8f6c\u5316\u4e3a\u5951\u7ea6\u4e3b\u4e49\u7684\u4e00\u79cd\u5f62\u5f0f\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.46139144500295565, "meta-math/MetaMath-Mistral-7B": 0.5159701992688639, "itpossible/Chinese-Mistral-7B-v0.1": 0.3371472741266552, "HuggingFaceH4/zephyr-7b-beta": 0.6794197482069261, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5014154826951555, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9352466122482698}}, {"question": "\u4ee5\u4e0b\u8f6f\u4ef6\u4e2d\uff0c___\u4e0d\u662f\u64cd\u4f5c\u7cfb\u7edf\u8f6f\u4ef6\nA. Windows xp\nB. unix\nC. linux\nD. microsoft office\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6199644221990128, "meta-math/MetaMath-Mistral-7B": 0.9801913111183919, "itpossible/Chinese-Mistral-7B-v0.1": 0.6644733780173256, "HuggingFaceH4/zephyr-7b-beta": 0.9992096378564738, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9894947707958576, "meta-llama/Meta-Llama-3-8B": 0.9696196465596876, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9995784649469925}}, {"question": "\u80a1\u4efd\u6709\u9650\u516c\u53f8\u521b\u7acb\u5927\u4f1a\u5fc5\u987b\u6709\uff08\uff09\uff0c\u65b9\u53ef\u4e3e\u884c\u3002\nA. \u5168\u4f53\u8ba4\u80a1\u4eba\u51fa\u5e2d\nB. \u4ee3\u8868\u80a1\u4efd\u603b\u6570\u8fc7\u534a\u6570\u7684\u53d1\u8d77\u4eba\u3001\u8ba4\u80a1\u4eba\u51fa\u5e2d\nC. \u53d1\u8d77\u4eba\u3001\u8ba4\u80a1\u4eba\u51fa\u5e2d\u4eba\u6570\u5360\u603b\u4eba\u6570\u4e09\u5206\u4e4b\u4e8c\u4ee5\u4e0a\nD. \u5168\u4f53\u53d1\u8d77\u4eba\u51fa\u5e2d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4726299345239996, "meta-math/MetaMath-Mistral-7B": 0.6732372857654035, "itpossible/Chinese-Mistral-7B-v0.1": 0.6832143695815559, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7837235329997351, "meta-llama/Meta-Llama-3-8B": 0.5818614272854437, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9397194549234168}}, {"question": "\u4ee5\u4e0b\u5bf9k-means\u805a\u7c7b\u7b97\u6cd5\u89e3\u91ca\u6b63\u786e\u7684\u662f\nA. \u80fd\u81ea\u52a8\u8bc6\u522b\u7c7b\u7684\u4e2a\u6570,\u4e0d\u662f\u968f\u5373\u6311\u9009\u521d\u59cb\u70b9\u4e3a\u4e2d\u5fc3\u70b9\u8ba1\u7b97\nB. \u4e0d\u80fd\u81ea\u52a8\u8bc6\u522b\u7c7b\u7684\u4e2a\u6570,\u4e0d\u662f\u968f\u5373\u6311\u9009\u521d\u59cb\u70b9\u4e3a\u4e2d\u5fc3\u70b9\u8ba1\u7b97\nC. \u4e0d\u80fd\u81ea\u52a8\u8bc6\u522b\u7c7b\u7684\u4e2a\u6570,\u968f\u5373\u6311\u9009\u521d\u59cb\u70b9\u4e3a\u4e2d\u5fc3\u70b9\u8ba1\u7b97\nD. \u80fd\u81ea\u52a8\u8bc6\u522b\u7c7b\u7684\u4e2a\u6570,\u968f\u5373\u6311\u9009\u521d\u59cb\u70b9\u4e3a\u4e2d\u5fc3\u70b9\u8ba1\u7b97\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u57ce\u5e02\u5316\u7684\u57fa\u672c\u52a8\u529b\u662f\nA. \u5de5\u4e1a\u5316\nB. \u7ecf\u6d4e\u53d1\u5c55\nC. \u670d\u52a1\u4e1a\u53d1\u5c55\nD. \u519c\u4e1a\u53d1\u5c55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4004432460396654, "meta-math/MetaMath-Mistral-7B": 0.5149178127008898, "itpossible/Chinese-Mistral-7B-v0.1": 0.5600823228651131, "HuggingFaceH4/zephyr-7b-beta": 0.44908055755489823, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5303062061966396, "meta-llama/Meta-Llama-3-8B": 0.6764727030788122, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6210\u8bed\u201c\u8bf7\u541b\u5165\u74ee\u201d\uff0c\u201c\u8bf7\u201d\u7684\u662f\u8c01\nA. \u5468\u6587\u738b\nB. \u5b8b\u8944\u516c\nC. \u5468\u5174\nD. \u6765\u4fca\u81e3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6cd5\u7684\u7279\u5f81\u4e4b\u4e00\uff1a\u6cd5\u662f\u5177\u6709\u666e\u904d\u6027\u7684\u793e\u4f1a\u89c4\u8303\u3002\u8fd9\u91cc\u5173\u4e8e\u201c\u6cd5\u7684\u666e\u904d\u6027\u201d\u7684\u7406\u89e3\uff0c\u6700\u6070\u5f53\u7684\u4e00\u9879\u662f\nA. \u6cd5\u5f8b\u9762\u524d\u4eba\u4eba\u5e73\u7b49\uff0c\u4efb\u4f55\u4eba\u90fd\u4e0d\u5f97\u6709\u8d85\u8d8a\u6cd5\u5f8b\u7684\u7279\u6743\nB. \u6cd5\u5f8b\u5728\u56fd\u5bb6\u6743\u529b\u7ba1\u8f96\u8303\u56f4\u5185\u5177\u6709\u666e\u904d\u6548\u529b\nC. \u6cd5\u5f8b\u7684\u5185\u5bb9\u4e0e\u4eba\u7c7b\u7684\u666e\u904d\u8981\u6c42\u76f8\u4e00\u81f4\nD. \u6cd5\u5f8b\u9762\u4e34\u7740\u5168\u7403\u5316\u3001\u4e00\u4f53\u5316\u7684\u8d8b\u52bf\uff0c\u6211\u56fd\u7684\u6cd5\u5f8b\u8981\u4e0e\u56fd\u9645\u63a5\u8f68\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5074407251876918, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5764709767824646}}, {"question": "\u5173\u4e8e\u77ac\u65f6\u901f\u5ea6\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u662f\u7269\u4f53\u5728\u67d0\u6bb5\u65f6\u95f4\u5185\u7684\u901f\u5ea6\nB. \u662f\u7269\u4f53\u5728\u901a\u8fc7\u67d0\u2f00\u6bb5\u4f4d\u79fb\u8fc7\u7a0b\u4e2d\u7684\u901f\u5ea6\nC. \u4ee5\u4e0a\u8bf4\u6cd5\u5747\u6b63\u786e\nD. \u662f\u7269\u4f53\u901a\u8fc7\u67d0\u2f00\u4f4d\u7f6e\u65f6\u7684\u901f\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u56db\u79cd\u5b58\u50a8\u5668\u4e2d\uff0c\u5b58\u53d6\u901f\u5ea6\u6700\u5feb\u7684\u662f\nA. \u78c1\u5e26\nB. \u5185\u5b58\u50a8\u5668\nC. \u786c\u76d8\nD. \u8f6f\u76d8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8749689658650159, "meta-math/MetaMath-Mistral-7B": 0.9931176513351166, "itpossible/Chinese-Mistral-7B-v0.1": 0.7266389304676149, "HuggingFaceH4/zephyr-7b-beta": 0.9997013089099246, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8955040475200602, "meta-llama/Meta-Llama-3-8B": 0.8856639942013216, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9967496389655736}}, {"question": "\u793e\u4f1a\u8c03\u67e5\u7814\u7a76\u4e2d\u5305\u542b\u4e00\u4e2a\u4ee5\u4e0a\u4e9a\u6982\u5ff5\u6216\u53d6\u503c\u7684\u6982\u5ff5\u662f\nA. \u6307\u6807\nB. \u547d\u9898\nC. \u5047\u8bbe\nD. \u53d8\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6811474011239076, "meta-math/MetaMath-Mistral-7B": 0.8119830481888505, "itpossible/Chinese-Mistral-7B-v0.1": 0.585978727918457, "HuggingFaceH4/zephyr-7b-beta": 0.9987781737924801, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5159701992688639, "meta-llama/Meta-Llama-3-8B": 0.9605071920840004, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9794951575105386}}, {"question": "\u9a7e\u9a76\u4eba\u6709\u4e0b\u5217\u54ea\u79cd\u8fdd\u6cd5\u884c\u4e3a\u4e00\u6b21\u8bb06\u5206\nA. \u4f7f\u7528\u5176\u4ed6\u8f66\u8f86\u884c\u9a76\u8bc1\nB. \u8fdd\u6cd5\u5360\u7528\u5e94\u6025\u8f66\u9053\u884c\u9a76\nC. \u996e\u9152\u540e\u9a7e\u9a76\u673a\u52a8\u8f66\nD. \u8f66\u901f\u8d85\u8fc7\u89c4\u5b9a\u65f6\u901f50%\u4ee5\u4e0a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6cd5\u5f8b\u5173\u7cfb\u7684\u5185\u5bb9\u662f\u6cd5\u5f8b\u5173\u7cfb\u4e3b\u4f53\u4e4b\u95f4\u7684\u6cd5\u5f8b\u6743\u5229\u548c\u6cd5\u5f8b\u4e49\u52a1\uff0c\u4e8c\u8005\u4e4b\u95f4\u5177\u6709\u7d27\u5bc6\u7684\u8054\u7cfb\u3002\u4e0b\u5217\u6709\u5173\u6cd5\u5f8b\u6743\u5229\u548c\u4e49\u52a1\u76f8\u4e92\u5173\u7cfb\u7684\u8868\u8ff0\u4e2d\uff0c\u54ea\u79cd\u8bf4\u6cd5\u6ca1\u6709\u6b63\u786e\u63ed\u793a\u8fd9\u4e00\u5173\u7cfb\uff1f\nA. \u6743\u5229\u548c\u4e49\u52a1\u7684\u5b58\u5728\u3001\u53d1\u5c55\u90fd\u5fc5\u987b\u4ee5\u53e6\u4e00\u65b9\u7684\u5b58\u5728\u548c\u53d1\u5c55\u4e3a\u6761\u4ef6\nB. \u4eab\u6709\u6743\u5229\u662f\u4e3a\u4e86\u66f4\u597d\u5730\u5c65\u884c\u4e49\u52a1\nC. \u4e49\u52a1\u7684\u8bbe\u5b9a\u76ee\u7684\u662f\u4fdd\u969c\u6743\u5229\u7684\u5b9e\u73b0\nD. \u6743\u5229\u548c\u4e49\u52a1\u5728\u6cd5\u5f8b\u5173\u7cfb\u4e2d\u7684\u5730\u4f4d\u6709\u4e3b\u6b21\u4e4b\u5206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9931532094168299, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.44273857470433675, "meta-llama/Meta-Llama-3-8B": 0.48006664386995895, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u540c\u4e00\u4e2a\u4f53\u4e2d\u7ec6\u80de\u6709\u4e1d\u5206\u88c2\u548c\u51cf\u6570\u7b2c\u4e8c\u6b21\u5206\u88c2\u7684\u53d9\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u4e24\u8005\u540e\u671f\u67d3\u8272\u4f53\u884c\u4e3a\u76f8\u540c\uff0c\u4e14\u7ec6\u80de\u4e2d\u67d3\u8272\u4f53\u6570\u2236DNA\u5206\u5b50\u6570<1\nB. \u524d\u8005\u524d\u671f\u6709\u67d3\u8272\u4f53\u3001\u7eba\u9524\u4f53\u7684\u51fa\u73b0\uff0c\u540e\u8005\u524d\u671f\u53ea\u6709\u67d3\u8272\u4f53\u7684\u51fa\u73b0\nC. \u4e24\u8005\u95f4\u671f\u76f8\u5173\u590d\u5236\u7684\u539f\u6599\u5747\u4e3a\u8131\u6c27\u6838\u82f7\u9178\u3001\u6c28\u57fa\u9178\nD. \u4e24\u8005\u4e2d\u671f\u67d3\u8272\u884c\u4e3a\u4e0d\u540c\uff0c\u67d3\u8272\u4f53\u6570\u76ee\u4e5f\u4e0d\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6d88\u8d39\u8005\u5173\u7cfb\u6307\u7684\u662f\u4f01\u4e1a\u4e0e\u5176\u4ea7\u54c1\u6216\u670d\u52a1\u7684\u8d2d\u4e70\u8005\u6216\u76f4\u63a5\u6d88\u8d39\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53c8\u79f0\u4e3a\nA. \u751f\u4ea7\u5173\u7cfb\nB. \u516c\u5171\u5173\u7cfb\nC. \u987e\u5ba2\u5173\u7cfb\nD. \u4e70\u5356\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6585391663129283, "meta-math/MetaMath-Mistral-7B": 0.979195852415163, "itpossible/Chinese-Mistral-7B-v0.1": 0.7375911105249436, "HuggingFaceH4/zephyr-7b-beta": 0.998888681221097, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9153305634195668, "meta-llama/Meta-Llama-3-8B": 0.5884320916525562, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9936399849450305}}, {"question": "\u53d8\u91cf\u53d8\u6362\u7684\u76ee\u7684\u662f\nA. \u53d8\u91cf\u6b63\u6001\u5316\nB. \u65b9\u5dee\u9f50\u6027\u5316\nC. \u66f2\u7ebf\u76f4\u7ebf\u5316\nD. $A\uff0cB\uff0cC$\u5747\u5bf9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.526929215782995, "meta-math/MetaMath-Mistral-7B": 0.657323098652916, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.64572338235239}}, {"question": "\u94f6\u884c\u4e0e\u81ea\u7136\u4eba\u4e4b\u95f4\u7684\u501f\u6b3e\u5408\u540c\u5c5e\u4e8e\nA. \u5355\u52a1\u5408\u540c\nB. \u5b9e\u8df5\u6027\u5408\u540c\nC. \u6709\u507f\u5408\u540c\nD. \u4ece\u5408\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6525160647148533, "meta-math/MetaMath-Mistral-7B": 0.6268009716760231, "itpossible/Chinese-Mistral-7B-v0.1": 0.4032266951760906, "HuggingFaceH4/zephyr-7b-beta": 0.9985226013727273, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7588136277099656, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u81ea\u7531\u738b\u56fd\u662f\u6307\u4eba\u4eec\nA. \u5141\u8bb8\u81ea\u7531\u7ade\u4e89\u7684\u8d44\u672c\u4e3b\u4e49\u72b6\u6001\nB. \u4e0d\u518d\u53d7\u81ea\u7136\u89c4\u5f8b\u548c\u793e\u4f1a\u89c4\u5f8b\u652f\u914d\u7684\u72b6\u6001\nC. \u5904\u4e8e\u7edd\u5bf9\u81ea\u7531\u7684\u539f\u59cb\u793e\u4f1a\u72b6\nD. \u6446\u8131\u4e86\u81ea\u7136\u548c\u793e\u4f1a\u5173\u7cfb\u7684\u5974\u5f79\uff0c\u6210\u4e3a\u81ea\u5df1\u793e\u4f1a\u5173\u7cfb\u4e3b\u4eba\u7684\u72b6\u6001\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7803595941315082, "meta-math/MetaMath-Mistral-7B": 0.904349473547213, "itpossible/Chinese-Mistral-7B-v0.1": 0.8037448121544349, "HuggingFaceH4/zephyr-7b-beta": 0.9499293713779814, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5593087263539448, "meta-llama/Meta-Llama-3-8B": 0.970848433033863, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9575680862697972}}, {"question": "\u7532\u3001\u4e59\u7b7e\u8ba2\u8015\u725b\u79df\u8d41\u5408\u540c\uff0c\u7532\u5c06\u8015\u725b\u79df\u7ed9\u4e59\u3002\u5728\u79df\u8d41\u671f\u9650\u5185\uff0c\u7532\u3001\u4e59\u53c8\u8fbe\u6210\u8015\u725b\u4e70\u5356\u5408\u540c\uff0c\u7ea6\u5b9a\u7532\u5c06\u8015\u725b\u5356\u7ed9\u4e59\uff0c\u5219\u8015\u725b\u7684\u4ea4\u4ed8\u65b9\u5f0f\u4e3a\nA. \u5360\u6709\u6539\u5b9a\nB. \u73b0\u5b9e\u4ea4\u4ed8\nC. \u7b80\u6613\u4ea4\u4ed8\nD. \u6307\u793a\u4ea4\u4ed8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3393227209307047, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u5546\u54c1\u7ecf\u6d4e\u4e2d\uff0c\u5f62\u6210\u4ef7\u503c\u7684\u62bd\u8c61\u52b3\u52a8\u7684\u652f\u51fa\u5fc5\u987b\u501f\u52a9\u4e8e\nA. \u5269\u4f59\u52b3\u52a8\nB. \u8d44\u672c\u4e3b\u4e49\u751f\u4ea7\u65b9\u5f0f\nC. \u5546\u54c1\u7684\u751f\u4ea7\u5f62\u5f0f\nD. \u5177\u4f53\u52b3\u52a8 \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u6218\u4e2d\u66fe\u6709\u4e00\u8258\u5fb7\u519b\u6f5c\u8247\u5728\u4e00\u5c0f\u65f6\u5185\u51fb\u6c89\u4e09\u8258\u82f1\u56fd\u5de1\u6d0b\u8230\uff0c\u4ee4\u4e16\u754c\u6d77\u519b\u4e3a\u4e4b\u9707\u60ca\uff0c\u5b83\u662f\uff1a\nA. U-15\nB. U-17\nC. U-5\nD. U-9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8164021464807761, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5821193484575958}}, {"question": "\u6839\u636e\u6211\u56fd\u300a\u4e13\u5229\u6cd5\u300b\u89c4\u5b9a\uff0c\u5916\u89c2\u8bbe\u8ba1\u4e13\u5229\u6743\u7684\u4fdd\u62a4\u8303\u56f4\nA. \u4ee5\u8868\u793a\u5728\u56fe\u7247\u6216\u8005\u7167\u7247\u4e2d\u7684\u8be5\u4ea7\u54c1\u7684\u5916\u89c2\u8bbe\u8ba1\u4e3a\u51c6\nB. \u4ee5\u7533\u8bf7\u6587\u4ef6\u7684\u5185\u5bb9\u4e3a\u51c6\nC. \u4ee5\u6743\u5229\u8981\u6c42\u7684\u5185\u5bb9\u4e3a\u51c6\nD. \u4ee5\u8bf4\u660e\u4e66\u8bb0\u8f7d\u7684\u5185\u5bb9\u4e3a\u51c6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6620232078463633, "meta-math/MetaMath-Mistral-7B": 0.8838743523264863, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7242294847731438, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6237889861704374, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8109965392294228}}, {"question": "\u9499\u7684\u751f\u7406\u529f\u80fd\u6709\nA. \u6297\u6c27\u5316\nB. \u7ef4\u6301\u89c6\u89c9\nC. \u4e0e\u9aa8\u9abc\u7259\u9f7f\u53d1\u80b2\u6709\u5173\nD. \u4fc3\u8fdb\u98df\u6b32\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9404764175758253, "meta-math/MetaMath-Mistral-7B": 0.9965855932578367, "itpossible/Chinese-Mistral-7B-v0.1": 0.8267352427303031, "HuggingFaceH4/zephyr-7b-beta": 0.9999002118193526, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9399727863410787, "meta-llama/Meta-Llama-3-8B": 0.8415466722356318, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9854675378907026}}, {"question": "\u82f1\u56fd\u7ba1\u7406\u5b66\u5bb6\u970d\u91d1\u68ee\u8bf4\uff1a\u201c\u54f2\u5b66\u7684\u4efb\u52a1\u5728\u4e8e\u5b83\u5fc5\u987b\u5148\u4e8e\u884c\u52a8\u3002\u5018\u82e5\u54f2\u5b66\u5bb6\u4e0d\u80fd\u6210\u4e3a\u7ba1\u7406\u8005\uff0c\u90a3\u4e48\u7ba1\u7406\u8005\u5c31\u5e94\u6210\u4e3a\u54f2\u5b66\u5bb6\u3002\u201d\u8fd9\u4e00\u89c2\u70b9\u5f3a\u8c03\u4e86\nA. \u771f\u7406\u662f\u68c0\u9a8c\u8ba4\u8bc6\u6b63\u786e\u4e0e\u5426\u7684\u6807\u51c6\nB. \u54f2\u5b66\u5bf9\u7ba1\u7406\u5b9e\u8df5\u6709\u6307\u5bfc\u4f5c\u7528\nC. \u54f2\u5b66\u662f\u79d1\u5b66\u7684\u4e16\u754c\u89c2\nD. \u7ba1\u7406\u79d1\u5b66\u662f\u54f2\u5b66\u7684\u57fa\u7840\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8676246191368597, "meta-math/MetaMath-Mistral-7B": 0.9704502595816545, "itpossible/Chinese-Mistral-7B-v0.1": 0.8415466821706733, "HuggingFaceH4/zephyr-7b-beta": 0.9929404558782391, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9675013125720048, "meta-llama/Meta-Llama-3-8B": 0.864719068026787, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.972244601591652}}, {"question": "\u201c\u56fd\u4f53\u521d\u5efa\uff0c\u6c11\u6743\u672a\u5f20\uff0c\u662f\u4ee5\u91ce\u5fc3\u5bb6\u7ade\u6b32\u8986\u6c11\u653f\u800c\u590d\u5e1d\u5236\u2026\u2026\u6240\u5e78\u9769\u547d\u4e4b\u5143\u6c14\u672a\u6d88\uff0c\u65b0\u65e7\u4e24\u6d3e\u7686\u4e89\u76f8\u53cd\u5bf9\u5e1d\u5236\u81ea\u4e3a\u8005\uff0c\u800c\u6c11\u56fd\u4e43\u5f97\u4e2d\u5174\u3002\u201d\u8fd9\u6bb5\u6587\u5b57\u5199\u4e8e\nA. 20\u4e16\u7eaa\u521d\u671f\nB. 19\u4e16\u7eaa\u4e2d\u671f\nC. 20\u4e16\u7eaa\u4e2d\u671f\nD. 19\u4e16\u7eaa\u672b\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.37106890620496596, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u60a3\u8005\uff0c\u4e8e\u590f\u65e5\u611f\u53d7\u6691\u6e7f\uff0c\u8eab\u70ed\u70e6\u6e34\uff0c\u5c0f\u4fbf\u4e0d\u5229\uff0c\u5927\u4fbf\u6cc4\u6cfb\u3002\u6cbb\u7597\u5b9c\u9996\u9009\u7684\u65b9\u5242\u662f\nA. \u9999\u85b7\u6563\nB. \u53c2\u82d3\u767d\u672f\u6563\nC. \u516d\u4e00\u6563\nD. \u85ff\u9999\u6b63\u6c14\u6563\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7edf\u8ba1\u5b66\u4e0a\u7684\u7cfb\u7edf\u8bef\u5dee\u3001\u6d4b\u91cf\u8bef\u5dee\u3001\u62bd\u6837\u8bef\u5dee\u5728\u5b9e\u9645\u5de5\u4f5c\u4e2d:\nA. \u7cfb\u7edf\u8bef\u5dee\u548c\u62bd\u6837\u8bef\u5dee\u4e0d\u53ef\u907f\u514d\nB. \u5747\u4e0d\u53ef\u907f\u514d\nC. \u6d4b\u91cf\u8bef\u5dee\u548c\u62bd\u6837\u8bef\u5dee\u4e0d\u53ef\u907f\u514d\nD. \u7cfb\u7edf\u8bef\u5dee\u548c\u6d4b\u91cf\u8bef\u5dee\u4e0d\u53ef\u907f\u514d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.26560468668687814, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4fc3\u9500\u4ece\u672c\u8d28\u4e0a\u8bf4\u662f\u4e00\u79cd\u5356\u65b9\u4e0e\u4e70\u65b9\u7684\u4fe1\u606f\u4f20\u64ad\u6c9f\u901a\uff0c\u8fd9\u79cd\u4fe1\u606f\u6c9f\u901a\u7684\u7279\u5f81\u662f\nA. \u4ece\u5356\u65b9\u5411\u4e70\u65b9\u4f20\u64ad\u6c9f\u901a\nB. \u53cd\u590d\u5faa\u73af\u7684\u3001\u53cc\u5411\u5f0f\u7684\u4f20\u64ad\u6c9f\u901a\nC. \u4ece\u4e70\u65b9\u5411\u5356\u65b9\u4f20\u64ad\u6c9f\u901a\nD. \u4e00\u6b21\u6027\u7684\u53cc\u5411\u4f20\u64ad\u6c9f\u901a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6999642147735926, "meta-math/MetaMath-Mistral-7B": 0.7550361177449166, "itpossible/Chinese-Mistral-7B-v0.1": 0.7195841359262055, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6442052021955199, "meta-llama/Meta-Llama-3-8B": 0.6468110805314542, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9667823179811662}}, {"question": "\u53e4\u5e0c\u814a\u8bd7\u4eba\u897f\u6469\u5c3c\u5fb7\u65af\u5199\u9053\uff1a\u201c\u5fb7\u884c\uff0f\u4f4f\u5728\u96be\u6500\u767b\u7684\u9ad8\u5c71\uff0f\u2026\u2026\u6c42\u77e5\uff0c\u667a\u6167\uff0c\u5411\u5584\uff0f\u624d\u80fd\u767b\u4e0a\u8fd9\u4eba\u6027\u4e4b\u5dc5\u3002\u201d\u4e0b\u5217\u89c2\u70b9\u4e0e\u8be5\u53e5\u6700\u4e3a\u5951\u5408\u7684\u662f\nA. \u201c\u4eba\u662f\u4e07\u7269\u7684\u5c3a\u5ea6\u201d\nB. \u4fe1\u5949\u5723\u7ecf\uff0c\u732e\u8eab\u4e0a\u5e1d\nC. \u201c\u7f8e\u5fb7\u5373\u77e5\u8bc6\u201d\nD. \u793e\u4f1a\u79e9\u5e8f\u5efa\u7acb\u5728\u7ea6\u5b9a\u4e0a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.782918849279159, "meta-math/MetaMath-Mistral-7B": 0.963174648559506, "itpossible/Chinese-Mistral-7B-v0.1": 0.7936986897589579, "HuggingFaceH4/zephyr-7b-beta": 0.9990754140047856, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9301188220003014, "meta-llama/Meta-Llama-3-8B": 0.3956423519781027, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9775344021762593}}, {"question": "\u9a7e\u9a76\u4eba\u8fde\u7eed\u9a7e\u9a76\u4e0d\u5f97\u8d85\u8fc7\u591a\u957f\u65f6\u95f4\nA. 6\u5c0f\u65f6\nB. 10\u5c0f\u65f6\nC. 8\u5c0f\u65f6\nD. 4\u5c0f\u65f6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65e0\u8272\u900f\u660e\u819c\u7684\u7279\u70b9\u4e3a\nA. \u900f\u5149\u6027\u5dee\uff0c\u589e\u6e29\u6548\u679c\u4e0d\u663e\u8457\nB. \u900f\u5149\u6027\u597d\uff0c\u589e\u6e29\u6548\u679c\u663e\u8457\nC. \u900f\u5149\u6027\u5dee\uff0c\u589e\u6e29\u6548\u679c\u663e\u8457\nD. \u900f\u5149\u6027\u597d\u3001\u589e\u6e29\u6548\u679c\u4e0d\u663e\u8457\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41340328853062375, "meta-math/MetaMath-Mistral-7B": 0.7068861609387631, "itpossible/Chinese-Mistral-7B-v0.1": 0.5416482522291075, "HuggingFaceH4/zephyr-7b-beta": 0.3499320087587726, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8170625298125906, "meta-llama/Meta-Llama-3-8B": 0.3735450164678785, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6864308825387873}}, {"question": "\u4e2d\u56fd\u63d0\u51fa\u548c\u5e73\u5171\u5904\u4e94\u9879\u539f\u5219\uff0c\u6700\u521d\u662f\u7528\u4e8e\u5904\u7406\nA. \u540c\u793e\u4f1a\u4e3b\u4e49\u56fd\u5bb6\u7684\u5173\u7cfb\nB. \u540c\u897f\u65b9\u8d44\u672c\u4e3b\u4e49\u56fd\u5bb6\u7684\u5173\u7cfb\nC. \u540c\u4e00\u5207\u56fd\u5bb6\u7684\u5173\u7cfb\nD. \u540c\u6c11\u65cf\u4e3b\u4e49\u56fd\u5bb6\u7684\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u8c03\u67e5\u7814\u7a76\u4e2d\uff0c\u5173\u4e8e\u4e00\u4e2a\u6216\u66f4\u591a\u6982\u5ff5\u4e0e\u53d8\u91cf\u7684\u9648\u8ff0\u88ab\u79f0\u4e3a\nA. \u547d\u9898\nB. \u95ee\u9898\u6027\u6307\u6807\nC. \u5047\u8bf4\nD. \u8bca\u65ad\u6027\u6307\u6807\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6438747550649531, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u7269\u4f53\u505a\u76f4\u7ebf\u8fd0\u52a8\uff0c\u4f4d\u79fb\u9075\u5faa\u7684\u2f45\u7a0b\u4e3a$x=6t-t^2$\uff08\u5176\u4e2d\uff0cx\u7684\u5355\u4f4d\u4e3am\uff0ct\u7684\u5355\u4f4d\u4e3as\uff09\u3002\u5219\u8be5\u7269\u4f53 \u57280\uff5e4s\u65f6\u95f4\u5185\u901a\u8fc7\u7684\u8def\u7a0b\u4e3a\nA. 10m\nB. 9m\nC. 11m\nD. 8m\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.398556353954811, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c06\u4e00\u679a\u786c\u5e01\u72ec\u7acb\u7684\u63b7\u4e24\u6b21\uff0c \u5f15\u8fdb\u4e8b\u4ef6: $A_1={\u7b2c\u4e00\u6b21\u51fa\u73b0\u6b63\u9762}\uff0c A_2={\u7b2c\u4e8c\u6b21\u51fa\u73b0\u6b63\u9762}\uff0c A_3={\u6b63\u3001\u53cd\u9762\u5404\u51fa\u73b0\u4e00\u6b21}\uff0c A_4={\u51fa\u73b0\u4e24\u6b21\u6b63\u9762}\uff0c \u5219\u4e8b\u4ef6\nA. $A_1\uff0c A_2\uff0c A_3$ \u76f8\u4e92\u72ec\u7acb\nB. $A_1\uff0c A_2\uff0c A_3$ \u4e24\u4e24\u72ec\u7acb\nC. $A_2\uff0c A_3\uff0c A_4$ \u4e24\u4e24\u72ec\u7acb\nD. $A_2\uff0c A_3\uff0c A_4$ \u76f8\u4e92\u72ec\u7acb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8336931293223682, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3164901664353555, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u51b3\u5b9a\u5e7f\u544a\u7b56\u5212\u6210\u8d25\u7684\u5173\u952e\u662f\nA. \u5e7f\u544a\u521b\u610f\nB. \u5e7f\u544a\u5b9a\u4f4d\nC. \u5e7f\u544a\u8c03\u67e5\nD. \u5e7f\u544a\u8bc9\u6c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5226734326562088, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7105484503096262}}, {"question": "\u6211\u56fd\u53e4\u4ee3\u8457\u540d\u7ed8\u753b\u4f5c\u54c1\u300a\u6d1b\u795e\u8d4b\u56fe\u300b\u7684\u4f5c\u8005\u662f\nA. \u5f20\u62e9\u7aef\nB. \u5c55\u5b50\u8654\nC. \u987e\u607a\u4e4b\nD. \u960e\u7acb\u672c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3118866287896564, "meta-math/MetaMath-Mistral-7B": 0.3468666740605408, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3392357547465047, "meta-llama/Meta-Llama-3-8B": 0.5220127299408656, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8736398629956991}}, {"question": "\u8fd1\u5e74\u6765\u6d4b\u91cd\u2f12\u52a0\u901f\u5ea6g\u503c\u7684\u2f00\u79cd\u2f45\u6cd5\u53eb\u201c\u5bf9\u79f0\u2f83\u7531\u4e0b\u843d\u6cd5\u201d\u3002\u5177\u4f53\u505a\u6cd5\u662f\uff1a\u5c06\u771f\u7a7a\u2ed3\u76f4\u7ba1\u6cbf\u7ad6\u76f4\u2f45\u5411\u653e\u7f6e\uff0c\u2f83\u5176\u4e2dO\u70b9\u5411\u4e0a\u629b\u2f29\u7403\u2f1c\u843d\u2f84\u539f\u5904\u6240\u2f64\u65f6\u95f4\u4e3at2\uff0c\u5728\u2f29\u7403\u8fd0\u52a8\u8fc7\u7a0b\u4e2d\u7ecf\u8fc7\u2f50O\u70b9\u2fbch\u5904\uff0c\u2f29\u7403\u79bb\u5f00h\u5904\u2f84\u2f1c\u56de\u5230h\u5904\u6240\u2f64\u65f6\u95f4\u4e3at1\uff0c\u6d4b\u5f97t1\u3001t2\u3001h\uff0c\u5219\u91cd\u2f12\u52a0\u901f\u5ea6\u7684\u8868\u8fbe\u5f0f\u4e3a\nA. $8h/((t_2)^2 -(t_1)^2)$\nB. $16h/((t_2)^2 -(t_1)^2)$\nC. $2h/((t_2)^2 -(t_1)^2)$\nD. $4h/((t_2)^2 -(t_1)^2)$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u63a7\u5236\u4e86\u82f1\u56fd\u62a5\u7eb8\u4e09\u5206\u4e4b\u4e00\u53d1\u884c\u91cf\u7684\u65b0\u95fb\u56fd\u9645\u516c\u53f8\uff0c\u5b83\u7684\u4e1a\u4e3b\u662f\nA. \u6bd4\u7ef4\u5e03\u9c81\u514b\nB. \u6c64\u59c6\u68ee\nC. \u9ed8\u591a\u514b\nD. \u9a6c\u514b\u65af\u97e6\u5c14\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3445043900133436, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5686603870525466, "HuggingFaceH4/zephyr-7b-beta": 0.3908304748611699, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7849554091386804, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8923238118633303}}, {"question": "\u201c\u5c3d\u7ba1\u7f57\u9a6c\u6cd5\u5f8b\u5728\u5987\u5973\u7ee7\u627f\u8d22\u4ea7\u4e0a\u505a\u4e86\u4e25\u683c\u9650\u5236\uff0c\u4f46\u4e00\u4e9b\u806a\u660e\u4eba\u5f80\u5f80\u80fd\u627e\u5230\u6cd5\u5f8b\u7684\u6f0f\u6d1e\u2026\u2026\u968f\u7740\u7f57\u9a6c\u5728\u5730\u4e2d\u6d77\u5730\u533a\u7684\u6269\u5f20\uff0c\u5987\u5973\u4eec\u4e5f\u5f00\u59cb\u5360\u6709\u5927\u5b97\u8d22\u4ea7\u3002\u2026\u2026\u8bb8\u591a\u5987\u5973\u7ba1\u7406\u7740\u5bb6\u5ead\u7684\u8d22\u653f\u4e8b\u52a1\u548c\u5927\u5730\u4ea7\u3002\u201d\u8fd9\u6bb5\u8bdd\u8868\u660e\nA. \u7f57\u9a6c\u6cd5\u5173\u4e8e\u8d22\u4ea7\u5173\u7cfb\u7684\u89c4\u8303\u4e0d\u591f\u4e25\u5bc6\nB. \u5987\u5973\u7684\u8d22\u4ea7\u7ee7\u627f\u6743\u5bcc\u6709\u81ea\u7136\u6cd5\u7684\u7cbe\u795e\nC. \u5987\u5973\u7684\u8d22\u4ea7\u5360\u6709\u4f53\u73b0\u516c\u6c11\u6cd5\u7684\u7075\u6d3b\u6027\nD. \u5987\u5973\u5728\u5bb6\u5ead\u4e2d\u7684\u5730\u4f4d\u9010\u6e10\u4e0e\u7537\u5b50\u5e73\u7b49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5190117834740092, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4954089550447509}}, {"question": "\u6700\u65e9\u6839\u636e\u6742\u4ea4\u5b9e\u9a8c\u7684\u7ed3\u679c\u5efa\u7acb\u8d77\u9057\u4f20\u5b66\u57fa\u672c\u539f\u7406\u7684\u79d1\u5b66\u5bb6\u662f\nA. Barbara McClintock\nB. Aristotle\nC. James D.Watson\nD. Gregor Mendel\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9735059280020766, "meta-math/MetaMath-Mistral-7B": 0.9995386486818815, "itpossible/Chinese-Mistral-7B-v0.1": 0.9560414015016803, "HuggingFaceH4/zephyr-7b-beta": 0.9999891997606867, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9979179612861976, "meta-llama/Meta-Llama-3-8B": 0.9232505997406619, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7237049113084068}}, {"question": "\u6768\u594e\u677e\u5728\u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u5efa\u56fd\u53f2\u7814\u7a76\uff082\uff09\u300b\u4e2d\u66fe\u8fd9\u6837\u8bc4\u4ef7\u65b0\u4e2d\u56fd\u7684\u5916\u4ea4\u653f\u7b56\uff1a\u201c\u8fd9\u662f\u65b0\u4e2d\u56fd\u5916\u4ea4\u653f\u7b56\u4ece\u7a81\u51fa\u5f3a\u8c03\u610f\u8bc6\u5f62\u6001\u7684\u201c\u4e00\u8fb9\u5012\u201d\uff0c\u8f6c\u5411\u8f83\u591a\u5730\u8003\u8651\u56fd\u5bb6\u73b0\u5b9e\u5229\u76ca\u800c\u5f00\u59cb\u8d70\u5411\u52a1\u5b9e\u7684\u76f8\u5f53\u91cd\u8981\u7684\u6807\u5fd7\u3002\u201d\u8be5\u89c2\u70b9\u4f9d\u636e\u7684\u53f2\u5b9e\u5e94\u8be5\u662f\nA. \u201c\u6253\u626b\u5e72\u51c0\u5c4b\u5b50\u518d\u8bf7\u5ba2\u201d\u653f\u7b56\u7684\u63d0\u51fa\nB. \u53c2\u52a0\u65e5\u5185\u74e6\u4f1a\u8bae\nC. \u548c\u5e73\u5171\u5904\u4e94\u9879\u539f\u5219\u7684\u63d0\u51fa\nD. \u201c\u6c42\u540c\u5b58\u5f02\u201d\u65b9\u9488\u7684\u63d0\u51fa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38789855968433284, "meta-llama/Meta-Llama-3-8B": 0.3534716292209113, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6736675614045927}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u5c5e\u4e8e\u533b\u751f\u4e49\u52a1\u7684\u662f\nA. \u4fdd\u969c\u81ea\u8eab\u5b89\u5168\nB. \u4f7f\u7528\u533b\u7597\u8bbe\u5907\nC. \u59a5\u5584\u4fdd\u7ba1\u75c5\u5386\nD. \u5f00\u5c55\u79d1\u5b66\u7814\u7a76\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.42096269224722155, "meta-math/MetaMath-Mistral-7B": 0.7200487103837448, "itpossible/Chinese-Mistral-7B-v0.1": 0.6524355131198802, "HuggingFaceH4/zephyr-7b-beta": 0.6627906003609975, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5262386711124637, "meta-llama/Meta-Llama-3-8B": 0.6813927990647504, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9774214285578756}}, {"question": "\u67d0\u6d41\u57df\u591a\u5e74\u5e73\u5747\u964d\u6c34\u91cf\u4e3a 800mm\uff0c\u591a\u5e74\u5e73\u5747\u5f84\u6d41\u6df1\u4e3a 400mm\uff0c\u5219\u8be5\u6d41\u57df\u591a\u5e74\u5e73\u5747\u5f84\u6d41\u7cfb\u6570\u4e3a[]\u3002\nA. 0.50\nB. 0.35\nC. 0.47\nD. 0.65\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38745561900026004, "meta-llama/Meta-Llama-3-8B": 0.38181829050992233, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.43185999152121485}}, {"question": "\u6b63\u5f0f\u542f\u52a8\u6b27\u6d32\u4e00\u4f53\u5316\u7684\u8ba1\u5212\u662f\nA. \u767d\u91cc\u5b89\u8ba1\u5212\nB. \u827e\u68ee\u8c6a\u5a01\u5c14\u8ba1\u5212\nC. \u8212\u66fc\u8ba1\u5212\nD. \u9a6c\u6b47\u5c14\u8ba1\u5212\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.42667049058421413, "meta-math/MetaMath-Mistral-7B": 0.5878419405266443, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8341034169010201, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6126103801993703, "meta-llama/Meta-Llama-3-8B": 0.6035369486284418, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9244180875405333}}, {"question": "\u4fde\u67d0\u5411\u5b59\u67d0\u58f0\u79f0\u8981\u8d2d\u4e7080\u514b\u6d77\u6d1b\u56e0\uff0c\u5b59\u67d0\u4fbf\u4ece\u5916\u5730\u8d2d\u4e70\u4e8680\u514b\u6d77\u6d1b\u56e0\u3002\u5230\u8fbe\u7ea6\u5b9a\u4ea4\u8d27\u5730\u70b9\u540e\uff0c\u4fde\u67d0\u638f\u51fa\u4eff\u771f\u624b\u67aa\u5a01\u80c1\u5b59\u67d0\uff0c\u4ece\u5b59\u67d0\u624b\u4e2d\u593a\u53d6\u4e8680\u514b\u6d77\u6d1b\u56e0\u3002\u6b64\u540e\u534a\u5e74\u5185\uff0c\u56e0\u6ca1\u6709\u627e\u5230\u4e70\u4e3b\uff0c\u4fde\u67d0\u4e00\u76f4\u6301\u670980\u514b\u6d77\u6d1b\u56e0\u3002\u534a\u5e74\u540e\uff0c\u4fde\u67d0\u5c0680\u514b\u6d77\u6d1b\u56e0\u9001\u7ed9\u5176\u6bd2\u763e\u5f88\u5927\u7684\u670b\u53cb\u94b1\u67d0\uff0c\u94b1\u67d0\u56e0\u8fc7\u91cf\u5438\u98df\u6d77\u6d1b\u56e0\u800c\u6b7b\u4ea1\u3002\u5173\u4e8e\u672c\u6848\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u5bf9\u4fde\u67d0\u5e94\u5f53\u6309\u7167\u8d29\u5356\u6bd2\u54c1\u7f6a\u5b9a\u7f6a\u5904\u7f5a\nB. \u5bf9\u4fde\u67d0\u5e94\u5f53\u6309\u7167\u975e\u6cd5\u6301\u6709\u6bd2\u54c1\u7f6a\u5b9a\u7f6a\u5904\u7f5a\nC. \u5bf9\u4fde\u67d0\u5e94\u5f53\u6309\u7167\u62a2\u52ab\u7f6a\u52a0\u91cd\u6784\u6210\u5b9a\u7f6a\u5904\u7f5a\nD. \u5bf9\u4fde\u67d0\u5e94\u5f53\u6309\u7167\u62a2\u52ab\u7f6a\u5b9a\u7f6a\u5904\u7f5a\uff0c\u4f46\u662f\u4e0d\u5c5e\u4e8e\u6301\u67aa\u62a2\u52ab\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "$f(x)=x^3-3x^2+2$\u5728\u533a\u95f4[-1,1]\u4e0a\u7684\u6700\u5927\u503c\u662f\nA. 4\nB. 1\nC. 0\nD. 2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6559\u5b66\u6539\u9769\u7684\u6838\u5fc3\u662f\nA. \u5b9e\u65bd\u4e2a\u522b\u6559\u5b66\nB. \u66f4\u65b0\u6559\u5b66\u89c2\u5ff5\nC. \u6539\u9769\u6559\u5b66\u5185\u5bb9\nD. \u91c7\u7528\u73b0\u4ee3\u6559\u5b66\u6280\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5101505551964066, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8594742475358164, "HuggingFaceH4/zephyr-7b-beta": 0.991233934804928, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5391667395616144, "meta-llama/Meta-Llama-3-8B": 0.8997753036874644, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8301776177796877}}, {"question": "\u80bf\u7624\u5b9e\u8d28\u7684\u4f5c\u7528\u4e0d\u5305\u62ec\nA. \u5f71\u54cd\u80bf\u7624\u7684\u751f\u7269\u5b66\u884c\u4e3a\nB. \u5bf9\u80bf\u7624\u8fdb\u884c\u7ec4\u7ec7\u5b66\u5206\u7c7b\nC. \u53c2\u4e0e\u80bf\u7624\u7684\u514d\u75ab\u53cd\u5e94\nD. \u5224\u65ad\u80bf\u7624\u7684\u5206\u5316\u65b9\u5411\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4188060303437025, "meta-math/MetaMath-Mistral-7B": 0.5740062292160226, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6469483588194206, "meta-llama/Meta-Llama-3-8B": 0.35347162922091135, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5784\u65ad\u8d44\u672c\u4e3b\u4e49\u56fd\u5bb6\u7684\u91d1\u878d\u5be1\u5934\u5728\u7ecf\u6d4e\u4e0a\u7684\u7edf\u6cbb\u4e3b\u8981\u501f\u52a9\u4e8e\nA. \u201c\u53c2\u4e0e\u5236\u201d\nB. \u201c\u4f01\u4e1a\u8054\u5408\u201d\nC. \u201c\u4e2a\u4eba\u8054\u5408\u201d\nD. \u201c\u4ee3\u7406\u5236\u201d \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5401621751871098, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.47713736625841036}}, {"question": "\u5728\u6709\u673a\u78f7\u6740\u866b\u836f\u4e2d\u6bd2\u7684\u89e3\u6bd2\u836f\u4e2d\uff0c\u4e3b\u8981\u7528\u4e8e\u5bf9\u6297\u5916\u5468 M \u80c6\u78b1\u53d7\u4f53\u6d3b\u6027\u7684\u662f\nA. \u6c2f\u89e3\u78f7\u5b9a\nB. \u7898\u89e3\u78f7\u5b9a\nC. \u5c71\u83a8\u83ea\u78b1\nD. \u4e1c\u83a8\u83ea\u78b1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.27717492990815085, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.36476188275188565, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.308176776288574, "meta-llama/Meta-Llama-3-8B": 0.325455072595945, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c88\u4ece\u6587\u7684\u5c0f\u8bf4\u5177\u6709\nA. \u6563\u6587\u5316\u7279\u5f81\nB. \u54f2\u7406\u5316\u7279\u5f81\nC. \u79d1\u5b66\u5316\u7279\u5f81\nD. \u8bd7\u5316\u7279\u5f81\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2885095257630687, "meta-math/MetaMath-Mistral-7B": 0.4305970803346233, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.965670438460913, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "IEEE802.1Q VLAN\u80fd\u2f40\u6301\u7684\u6700\u2f24\u4e2a\u6570\u4e3a\nA. 4094\nB. 256\nC. 2048\nD. 1024\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6235091757770106, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7989058161012477, "meta-llama/Meta-Llama-3-8B": 0.9199260792971095, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9984141430950841}}, {"question": "\u65b0\u95fb\u62a5\u9053\u7684\u8d77\u70b9\u662f\nA. \u5199\u4f5c\nB. \u7f16\u8f91\nC. \u8bc4\u8bba\nD. \u91c7\u8bbf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6686769070577451, "meta-math/MetaMath-Mistral-7B": 0.9755250960307679, "itpossible/Chinese-Mistral-7B-v0.1": 0.8702920321111542, "HuggingFaceH4/zephyr-7b-beta": 0.9999040453383633, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9901373074550867, "meta-llama/Meta-Llama-3-8B": 0.5613323580077196, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9927589854782063}}, {"question": "\u6559\u80b2\u5236\u5ea6\u7684\u6f5c\u529f\u80fd\u4f53\u73b0\u5728\nA. \u5b9e\u73b0\u793e\u4f1a\u5316\u7684\u529f\u80fd\nB. \u7f29\u5c0f\u6536\u5165\u5dee\u8ddd\u7684\u529f\u80fd\nC. \u6587\u51ed\u529f\u80fd\nD. \u4f20\u6388\u77e5\u8bc6\u4e0e\u79d1\u5b66\u6280\u672f\u7684\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b8c\u6210\u201c\u8db3\u5c16\u5728\u5730\u9762\u4e0a\u5212\u4e00\u4e2a\u5708\u7684\u52a8\u4f5c\u201d\uff0c\u9ad3\u5173\u8282\u9700\u8981\u505a\u54ea\u4e9b\u8fd0\u52a8\nA. \u5185\u65cb\u548c\u5916\u65cb\nB. \u4ee5\u4e0a\u90fd\u6709\nC. \u524d\u5c48\u548c\u540e\u4f38\nD. \u5185\u6536\u548c\u5916\u5c55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5279784410721762, "HuggingFaceH4/zephyr-7b-beta": 0.5506780040527623, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6078244645526235, "meta-llama/Meta-Llama-3-8B": 0.5357296193387678, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.49720786924396193}}, {"question": "\u2f00\u8d28\u91cf\u4e3am\u7684\u8fd0\u52a8\u5458\u4ece\u4e0b\u8e72\u72b6\u6001\u5411\u4e0a\u8d77\u8df3\uff0c\u7ecf\u25b3t\u65f6\u95f4\uff0c\u8eab\u4f53\u4f38\u76f4\u5e76\u521a\u597d\u79bb\u5f00\u5730\u2faf\uff0c\u901f\u5ea6\u4e3av\uff0e\u5728\u6b64\u8fc7\u7a0b\u4e2d\nA. \u5730\u2faf\u5bf9\u4ed6\u7684\u51b2\u91cf\u4e3amv\uff0dmg\u25b3t\uff0c\u5730\u2faf\u5bf9\u4ed6\u505a\u7684\u529f\u4e3a\u96f6\nB. \u5730\u2faf\u5bf9\u4ed6\u7684\u51b2\u91cf\u4e3amv+mg\u25b3t\uff0c\u5730\u2faf\u5bf9\u4ed6\u505a\u7684\u529f\u4e3amv2/2\nC. \u5730\u2faf\u5bf9\u4ed6\u7684\u51b2\u91cf\u4e3amv+mg\u25b3t\uff0c\u5730\u2faf\u5bf9\u4ed6\u505a\u7684\u529f\u4e3a\u96f6\nD. \u5730\u2faf\u5bf9\u4ed6\u7684\u51b2\u91cf\u4e3amv\uff0c\u5730\u2faf\u5bf9\u4ed6\u505a\u7684\u529f\u4e3amv2/2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5132583971336683, "meta-math/MetaMath-Mistral-7B": 0.855102334316774, "itpossible/Chinese-Mistral-7B-v0.1": 0.35347162922091135, "HuggingFaceH4/zephyr-7b-beta": 0.917756499520511, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6137569264764302, "meta-llama/Meta-Llama-3-8B": 0.38253225505393756, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5635696118029061}}, {"question": "\u5728FCA\u672f\u8bed\u4e2d\uff0c\u8d27\u7269\u7684\u98ce\u9669\u4ece\u5356\u65b9\u8f6c\u79fb\u7ed9\u4e70\u65b9\u7684\u65f6\u95f4\u662f\nA. \u8d27\u7269\u786e\u5b9a\u5728\u5408\u540c\u9879\u4e0b\u65f6\nB. \u8d27\u7269\u88c5\u5230\u8fd0\u8f93\u5de5\u5177\u4e0a\u65f6\nC. \u5356\u65b9\u5c06\u8d27\u7269\u4ea4\u7ed9\u4e70\u65b9\u652f\u914d\u65f6\nD. \u5356\u65b9\u5c06\u8d27\u7269\u4ea4\u7ed9\u6307\u5b9a\u627f\u8fd0\u4eba\u65f6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3751817604149114, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.43051191006558315, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "20\u4e16\u7eaa30\u5e74\u4ee3\u524d\u671f\uff0c\u201c\u5de6\u201d\u503e\u9519\u8bef\u9886\u5bfc\u4eba\u628a\u53cd\u5bf9\u8d44\u4ea7\u9636\u7ea7\u540c\u53cd\u5e1d\u53cd\u5c01\u5efa\u5e76\u5217\uff0c\u4f01\u56fe\u6bd5\u5176\u529f\u4e8e\u4e00\u5f79\uff0c\u4f7f\u4e2d\u56fd\u9769\u547d\u906d\u53d7\u5de8\u5927\u635f\u5931\u3002\u201c\u5de6\u201d\u503e\u9519\u8bef\u7684\u8868\u73b0\u6709\nA. \u575a\u6301\u56fd\u6c11\u9769\u547d\u8def\u7ebf\nB. \u653e\u5f03\u65e0\u4ea7\u9636\u7ea7\u5bf9\u9769\u547d\u7684\u9886\u5bfc\u6743\nC. \u628a\u6c11\u65cf\u8d44\u4ea7\u9636\u7ea7\u4f5c\u4e3a\u9769\u547d\u5bf9\u8c61\nD. \u5b9e\u884c\u5de5\u519c\u6b66\u88c5\u5272\u636e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7361684452849444, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5663882133088587, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\uff08\uff09\u6307\u7684\u662f\u5bf9\u81ea\u5df1\u7684\u751f\u7406\u6027\u522b\u5f3a\u70c8\u4e0d\u6ee1\u610f\u3002\nA. \u5f02\u88c5\u7656\nB. \u6027\u53d8\u6001\nC. \u6027\u522b\u8ba4\u540c\u969c\u788d\nD. \u604b\u7269\u7656\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9337515411692863, "meta-math/MetaMath-Mistral-7B": 0.9926974121498339, "itpossible/Chinese-Mistral-7B-v0.1": 0.9222153331177376, "HuggingFaceH4/zephyr-7b-beta": 0.9949851758092818, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9863238806183815, "meta-llama/Meta-Llama-3-8B": 0.7443707588414165, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9969211721605479}}, {"question": "\u4e0b\u5217\u54ea\u79cd\u7ef4\u751f\u7d20\uff0c\u53c2\u4e0e\u6784\u6210\u8f6c\u6c28\u9176\u7684\u8f85\u9176\nA. VitB2\nB. VitB1\nC. VitB12\nD. VitB6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.689490933073518, "meta-math/MetaMath-Mistral-7B": 0.9544519521136386, "itpossible/Chinese-Mistral-7B-v0.1": 0.3893870401988077, "HuggingFaceH4/zephyr-7b-beta": 0.999319914202879, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9036414958517532, "meta-llama/Meta-Llama-3-8B": 0.43059708033462335, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53f2\u8f7d\uff0c\u5b8b\u592a\u7956\u67d0\u65e5\u95f7\u95f7\u4e0d\u4e50\uff0c\u6709\u4eba\u95ee\u4ed6\u539f\u56e0\uff0c\u4ed6\u8bf4\uff1a\u201c\u5c14\u8c13\u5e1d\u738b\u53ef\u5bb9\u6613\u884c\u4e8b\u8036\u2026\u2026\u5076\u6709\u8bef\u5931\uff0c\u53f2\u5b98\u5fc5\u4e66\u4e4b\uff0c\u6211\u6240\u4ee5\u4e0d\u4e50\u4e5f\u3002\u201d\u6b64\u4e8b\u53cd\u6620\u4e86\nA. \u5b8b\u592a\u7956\u4e0d\u613f\u53f2\u4e66\u8bb0\u5f55\u5176\u771f\u5b9e\u8a00\u884c\nB. \u53f2\u5b98\u4e0e\u541b\u4e3b\u95f4\u5b58\u5728\u5c16\u9510\u77db\u76fe\nC. \u5b8b\u4ee3\u53f2\u5b98\u6240\u64b0\u53f2\u4e66\u5168\u90fd\u771f\u5b9e\u53ef\u4fe1\nD. \u91cd\u53f2\u4f20\u7edf\u5f71\u54cd\u541b\u4e3b\u4e2a\u4eba\u884c\u4e3a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.41902333070136855, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u673a\u80fd\u4e3b\u4e49\u5fc3\u7406\u5b66\u6d3e\u7684\u4ee3\u8868\u4eba\u7269\u662f\nA. \u534e\u751f\u4e0e\u65af\u91d1\u7eb3\nB. \u51af\u7279\u4e0e\u94c1\u94a6\u7eb3\nC. \u8a79\u59c6\u65af\u4e0e\u675c\u5a01\nD. \u9b4f\u7279\u66fc\u4e0e\u82db\u52d2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.415264922844711, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2013\u5e749\u6708\uff0c\u5de5\u4e1a\u4e0e\u4fe1\u606f\u5316\u90e8\u4f1a\u540c\u56fd\u52a1\u9662\u6709\u5173\u90e8\u95e8\u7f16\u5236\u4e86\u300a\u4fe1\u606f\u5316\u53d1\u5c55\u89c4\u5212\u300b\uff0c\u4f5c\u4e3a\u6307\u5bfc\u4eca\u540e\u4e00\u4e2a\u65f6\u671f\u52a0\u5feb\u63a8\u52a8\u6211\u56fd\u4fe1\u606f\u5316\u53d1\u5c55\u7684\u884c\u52a8\u7eb2\u9886\u6ca1\u5728\u300a\u4fe1\u606f\u5316\u53d1\u5c55\u89c4\u5212\u300b\u4e2d\uff0c\u63d0\u51fa\u4e86\u6211\u56fd\u672a\u6765\u53d1\u5c55\u7684\u6307\u5bfc\u601d\u60f3\u548c\u57fa\u672c\u539f\u5219\u3002\u4ee5\u4e0b\u5173\u4e8e\u4fe1\u606f\u5316\u53d1\u5c55\u7684\u53d9\u8ff0\u4e2d\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u4fe1\u606f\u5316\u53d1\u5c55\u7684\u57fa\u672c\u539f\u5219\u662f\uff1a\u7edf\u7b79\u53d1\u5c55\u3001\u6709\u5e8f\u63a8\u8fdb\u3001\u9700\u6c42\u7275\u5f15\u3001\u5e02\u573a\u5bfc\u5411\u3001\u5b8c\u5584\u673a\u5236\u3001\u521b\u65b0\u9a71\u52a8\u3001\u52a0\u5f3a\u7ba1\u7406\u3001\u4fdd\u969c\u5b89\u5168\nB. \u76ee\u524d\uff0c\u6211\u56fd\u7684\u4fe1\u606f\u5316\u5efa\u8bbe\u5904\u4e8e\u5f00\u5c55\u9636\u6bb5\nC. \u4fe1\u606f\u5316\u53d1\u5c55\u7684\u4e3b\u8981\u4efb\u52a1\u5305\u62ec\u4fc3\u8fdb\u5de5\u4e1a\u9886\u57df\u4fe1\u606f\u5316\u6df1\u5ea6\u5e94\u7528\uff0c\u5305\u62ec\u63a8\u8fdb\u4fe1\u606f\u6280\u672f\u5728\u5de5\u4e1a\u9886\u57df\u5168\u9762\u666e\u53ca\uff0c\u63a8\u52a8\u7efc\u5408\u96c6\u6210\u5e94\u7528\u548c\u4e1a\u52a1\u534f\u8c03\u521b\u65b0\u7b49\nD. \u4fe1\u606f\u5316\u53d1\u5c55\u7684\u4e3b\u8981\u4efb\u52a1\u5305\u62ec\u63a8\u8fdb\u519c\u4e1a\u519c\u6751\u4fe1\u606f\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39258744443754756, "meta-math/MetaMath-Mistral-7B": 0.37491960840523747, "itpossible/Chinese-Mistral-7B-v0.1": 0.5003916213329689, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47478799303487373, "meta-llama/Meta-Llama-3-8B": 0.3528619536675027, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u51b7\u6c34\u51b7\u5374\u7684\u6548\u679c\u6700\u5173\u952e\u7684\u662f\nA. \u7528\u55b7\u6dcb\u6c34\nB. \u4fdd\u6301\u6c34\u6e290\u00b0C\nC. \u52a0\u5feb\u6c34\u7684\u6d41\u52a8\nD. \u4ee5\u4e0a\u5168\u90e8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9734239263202024, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6493194998821182}}, {"question": "\u8ba1\u7b97\u673a\u8f6f\u4ef6\u5305\u62ec\u7a0b\u5e8f\u3001\u6570\u636e\u548c\nA. \u64cd\u4f5c\u7cfb\u7edf\nB. \u7b97\u6cd5\nC. \u6e90\u7a0b\u5e8f\nD. \u6709\u5173\u6587\u6863\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9360591393035127, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8526655222130095, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3755444596898393}}, {"question": "\u636e\u7edf\u8ba1\uff0c\u52301600\u5e74\uff0c\u5168\u897f\u6b27\u7684\u603b\u4ef7\u683c\u6c34\u5e73\u6bd41500\u5e74\u9ad8\u51fa200-300%\uff0c\u5176\u4e2d\u897f\u73ed\u7259\u4e0a\u6da8\u5e45\u5ea6\u6700\u9ad8\uff0c\u5e73\u5747\u8fbe\u52304\u500d\u3002\u6cd5\u56fd\u3001\u82f1\u56fd\u3001\u5fb7\u56fd\u3001\u8377\u5170\u5219\u5e73\u5747\u4e0a\u6da8\u4e24\u500d\u5230\u4e24\u500d\u534a\u3002\u8fd9\u4e00\u73b0\u8c61\u5bf9\u6b27\u6d32\u793e\u4f1a\u7684\u5f71\u54cd\u662f\nA. \u793e\u4f1a\u9636\u5c42\u56fa\u5316\u4e25\u91cd\nB. \u8d44\u672c\u4e3b\u4e49\u840c\u82bd\u51fa\u73b0\nC. \u5c01\u5efa\u7ecf\u6d4e\u5173\u7cfb\u7834\u574f\nD. \u8d35\u91cd\u91d1\u5c5e\u5927\u91cf\u6d41\u5165\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.45544583153193324, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.38745561900026004, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b57\u7b26\u4e32\u201c\u4e2d\u5b66\u8ba1\u7b97\u673a\u7b49\u7ea7\u8003\u8bd5\u201d\u5728\u8ba1\u7b97\u673a\u4e2d\u5360\u7528\u7684\u5b58\u50a8\u5b57\u8282\u6570\u662f\nA. 72B\nB. 9B\nC. 36B\nD. 18B\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35287679503824954, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6038891900305002, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6de1\u6f20\u578b\u7532\u72b6\u817a\u529f\u80fd\u4ea2\u8fdb\u75c7\u7684\u7279\u70b9\u662f\nA. \u4e0d\u6613\u5e76\u53d1\u5fc3\u623f\u98a4\u52a8\nB. \u7532\u72b6\u817a\u5f25\u6f2b\u80bf\u5927\nC. \u773c\u88c2\u589e\u5bbd\uff0c\u7a81\u773c\nD. \u660e\u663e\u6d88\u7626\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.682899759735178, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u7b49\u6bd4\u6570\u5217$\\left\\{ a_{n} \\right\\}$\u4e2d\uff0c$a_{n}\\gt 0(n\\in N^{*})$\u4e14$a_{4}=4,a_{6}=16$\uff0c\u5219\u6570\u5217$\\left\\{ a_{n} \\right\\}$\u7684\u516c\u6bd4q\u662f\nA. 2\nB. 1\nC. 4\nD. 3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3208267503383532, "meta-math/MetaMath-Mistral-7B": 0.3964696419236105, "itpossible/Chinese-Mistral-7B-v0.1": 0.31049698480066085, "HuggingFaceH4/zephyr-7b-beta": 0.6801538187366282, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4117253102680102, "meta-llama/Meta-Llama-3-8B": 0.37354501646787847, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.36717002281588484}}, {"question": "\u5728\u516c\u5171\u4ea4\u5f80\u4e2d\uff0c\u5c0a\u91cd\u4eba\u3001\u7406\u89e3\u4eba\uff0c\u5bf9\u4e8e\u8001\u4eba\u3001\u513f\u7ae5\u548c\u6b8b\u75be\u4eba\u5458\uff0c\u4e88\u4ee5\u7279\u522b\u7684\u5173\u7231\u548c\u7167\u987e\uff1b\u5728\u4e2a\u4eba\u793c\u4eea\u4e0a\uff0c\u7a7f\u7740\u6574\u6d01\uff0c\u4e3e\u6b62\u5f97\u4f53\uff0c\u4e0d\u6cb9\u8154\u6ed1\u8c03\uff0c\u4e0d\u6c61\u8a00\u79fd\u8bed\uff1b\u5728\u516c\u5171\u573a\u6240\uff0c\u9075\u5b88\u516c\u5171\u89c4\u5219\uff0c\u7ef4\u62a4\u516c\u5171\u79e9\u5e8f\u3002\u8fd9\u4f53\u73b0\u7684\u662f\u793e\u4f1a\u516c\u5fb7\u4e2d\nA. \u9075\u7eaa\u5b88\u6cd5\u7684\u8981\u6c42\nB. \u4fdd\u62a4\u73af\u5883\u7684\u8981\u6c42\nC. \u7231\u62a4\u516c\u7269\u7684\u8981\u6c42\nD. \u6587\u660e\u793c\u8c8c\u7684\u8981\u6c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.966018524220202, "meta-math/MetaMath-Mistral-7B": 0.9981595113434372, "itpossible/Chinese-Mistral-7B-v0.1": 0.950550214664178, "HuggingFaceH4/zephyr-7b-beta": 0.9998148777363326, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9963644651231743, "meta-llama/Meta-Llama-3-8B": 0.8733796721970247, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7438844022751974}}, {"question": "\u51b7\u51bb\u4fdd\u85cf\u7684\u6e29\u5ea6\u4e00\u822c\u8981\u6c42\u5728\u591a\u5c11\u6444\u6c0f\u5ea6\nA. 1\nB. -5\nC. -20\nD. -10\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6889012311620374, "meta-math/MetaMath-Mistral-7B": 0.7802007198702272, "itpossible/Chinese-Mistral-7B-v0.1": 0.6133650562661819, "HuggingFaceH4/zephyr-7b-beta": 0.8659061536191984, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8765169736893624, "meta-llama/Meta-Llama-3-8B": 0.6730526120241602, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.743226911486616}}, {"question": "\u5584\u610f\u5c65\u884c\u56fd\u9645\u4e49\u52a1\u7684\u4f9d\u636e\u662f\nA. \u6761\u7ea6\u5fc5\u987b\u9075\u5b88\nB. \u4e3b\u6743\u5e73\u7b49\nC. \u5584\u610f\u539f\u5219\nD. \u56fd\u9645\u9053\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8fdb\u5165\u96c6\u5408\u7ba1\u7684\u5c0f\u7ba1\u6db2\u5448\nA. \u7b49\u6e17\nB. \u4f4e\u6e17\nC. \u4f4e\u6e17\u6216\u7b49\u6e17\nD. \u9ad8\u6e17\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.35888231301687173, "HuggingFaceH4/zephyr-7b-beta": 0.5055657939838022, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3586670570633395, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5408022586914334}}, {"question": "\u4e0b\u5217\u54ea\u4e00\u9879\u662f\u8bad\u7ec3\u8fc7\u7a0b\u7684\u7279\u6027\u3002\nA. \u9636\u6bb5\u6027\u548c\u53d8\u5316\u6027\nB. \u8fde\u7eed\u6027\u548c\u9636\u6bb5\u6027\nC. \u6709\u6548\u6027\u548c\u5ba2\u89c2\u6027\nD. \u53ca\u65f6\u6027\u548c\u53ef\u9760\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5982\u679c\u4e0d\u662f\u6709\u4eba\u53d1\u660e\u4e86\u706b\u8f66\uff0c\u5982\u679c\u4e0d\u662f\u6709\u4eba\u628a\u94c1\u8f68\u94fa\u8fdb\u6df1\u5c71\uff0c\u4f60\u600e\u4e48\u4e5f\u4e0d\u4f1a\u53d1\u73b0\u53f0\u513f\u6c9f\u8fd9\u4e2a\u5c0f\u6751\u3002\u201d\u4ee5\u8fd9\u6837\u7684\u53e5\u5b50\u5f00\u5934\u7684\u4f5c\u54c1\u662f\nA. \u300a\u5317\u65b9\u7684\u6cb3\u300b\nB. \u300a\u5728\u5c71\u533a\u6536\u8d2d\u7ad9\u300b\nC. \u300a\u5927\u6dd6\u8bb0\u4e8b\u300b\nD. \u300a\u54e6\uff0c\u9999\u96ea\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u80ba\u4e0b\u754c\u7684\u4f53\u8868\u6295\u5f71\uff0c\u5728\u814b\u4e2d\u7ebf\u4e0a\u4e0e\nA. \u7b2c6\u808b\u76f8\u4ea4\nB. \u7b2c7\u808b\u76f8\u4ea4\nC. \u7b2c9\u808b\u76f8\u4ea4\nD. \u7b2c8\u808b\u76f8\u4ea4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.31483005318115603, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.44296664074106246, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3220562534414596, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8fdb\u5165\u521d\u4e2d\u540e\uff0c\u5c0f\u78ca\u4e3a\u4e86\u8d62\u5f97\u5728\u73ed\u7ea7\u7684\u5730\u4f4d\u548c\u6ee1\u8db3\u81ea\u5c0a\u9700\u8981\u800c\u523b\u82e6\u5b66\u4e60\uff0c\u6839\u636e\u5965\u82cf\u4f2f\u5c14\u7684\u7406\u8bba\uff0c\u5c0f\u78ca\u7684\u5b66\u4e60\u52a8\u673a\u5c5e\u4e8e\nA. \u81ea\u6211\u63d0\u9ad8\u5185\u9a71\u529b\nB. \u9644\u5c5e\u5185\u9a71\u529b\nC. \u8ba4\u8bc6\u5185\u9a71\u529b\nD. \u751f\u7406\u5185\u9a71\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6631692675714648, "meta-math/MetaMath-Mistral-7B": 0.9745067408679157, "itpossible/Chinese-Mistral-7B-v0.1": 0.6534244807144866, "HuggingFaceH4/zephyr-7b-beta": 0.9999503291505147, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7748347951566797, "meta-llama/Meta-Llama-3-8B": 0.42886487419360814, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9192317199127892}}, {"question": "\u201c\u6211\u8981\u7528\u624b\u6307\u90a3\u5929\u8fb9\u7684\u6392\u6d6a\uff0c\u6211\u8981\u7528\u624b\u6491\u90a3\u6258\u4f4f\u592a\u9633\u7684\u5927\u6d77\u201d\u4e00\u53e5\u51fa\u81ea\nA. \u90ed\u8def\u751f\u7684\u300a\u76f8\u4fe1\u672a\u6765\u300b\nB. \u8212\u5a77\u7684\u300a\u53cc\u6845\u8239\u300b\nC. \u5180\u6d9d\u7684\u300a\u6211\u300b\nD. \u7a46\u65e6\u7684\u300a\u51ac\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3075818303735863, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u4f4d\u653f\u6cbb\u5bb6\u5728\u8bc4\u4ef7\u7f8e\u56fd1787\u5e74\u5baa\u6cd5\u65f6\uff0c\u66fe\u6307\u51fa\uff1a\u201c\u6beb\u65e0\u7591\u95ee\uff0c\u6211\u4eec\u7684\u5baa\u6cd5\u4e4b\u6240\u4ee5\u6052\u4e45\uff0c\u5c31\u5728\u4e8e\u5b83\u7b80\u6d01\u3002\u5b83\u662f\u4e00\u5757\u5960\u57fa\u77f3\uff0c\u800c\u4e0d\u662f\u4e00\u5ea7\u5b8c\u7f8e\u7684\u5927\u53a6\u3002\u201d\u5176\u610f\u5728\u80af\u5b9a\u5baa\u6cd5\nA. \u5b9e\u8df5\u4e0d\u53d7\u65f6\u4ee3\u800c\u8c03\u6574\nB. \u6761\u6587\u4e0e\u4f53\u7cfb\u7684\u5b8c\u5907\u6027\nC. \u5404\u9879\u5185\u5bb9\u5177\u6709\u524d\u77bb\u6027\nD. \u6240\u8574\u542b\u7684\u539f\u5219\u4e0e\u7cbe\u795e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5121788714655077, "meta-math/MetaMath-Mistral-7B": 0.4088096610972863, "itpossible/Chinese-Mistral-7B-v0.1": 0.7213854867127849, "HuggingFaceH4/zephyr-7b-beta": 0.9992304956813833, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9468221707422138, "meta-llama/Meta-Llama-3-8B": 0.8056532214059611, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.961667327110062}}, {"question": "\u4e0b\u5217\u620f\u5267\u4f5c\u54c1\u4e2d\uff0c\u521b\u4f5c\u4e8e\u53e4\u5e0c\u814a\u65f6\u671f\u7684\u662f\nA. \u300a\u7f57\u5bc6\u6b27\u4e0e\u6731\u4e3d\u53f6\u300b\nB. \u300a\u7b49\u5f85\u6208\u591a\u300b\nC. \u300a\u94a6\u5dee\u5927\u81e3\u300b\nD. \u300a\u66fe\u7f57\u7c73\u4fee\u65af\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.9468821709977223, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8661292186540241, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9639249377623618}}, {"question": "\u4e0b\u5217\u4e0d\u5c5e\u4e8e\u4e2d\u56fd\u996e\u98df\u517b\u751f\u7684\u539f\u5219\u4e3a\nA. \u98df\u9971\u4e3a\u6b62\nB. \u56e0\u4eba\u800c\u5b9c\nC. \u4e94\u5473\u8c03\u548c\nD. \u5408\u7406\u8c03\u914d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.39267125216098137, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4145085576820955}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u5730\u7406\u4f4d\u7f6e\u7684\u63cf\u8ff0\u4e0e\u57ce\u5e02\u4e4b\u95f4\u5bf9\u5e94\u5173\u7cfb\u9519\u8bef\u7684\u662f\nA. \u4e1c\u538b\u6c5f\u6dee\uff0c\u897f\u631f\u5173\u9647\uff0c\u5317\u901a\u5e7d\u71d5\uff0c\u5357\u7cfb\u8346\u8944\u2014\u2014\u5357\u4eac\nB. \u5357\u629a\u767e\u8d8a\uff0c\u5317\u671b\u4e2d\u5dde\uff0c\u636e\u4e94\u5cad\u4e4b\u8981\u4f1a\uff0c\u627c\u8d63\u95fd\u7ca4\u6e58\u4e4b\u8981\u51b2\u2014\u2014\u8d63\u5dde\nC. \u5c45\u6d59\u53f3\u4e4b\u4e0a\u6e38\uff0c\u63a7\u9131\u9633\u4e4b\u8098\u814b\uff0c\u5236\u95fd\u8d8a\u4e4b\u5589\u542d\uff0c\u901a\u5ba3\u6b59\u4e4b\u58f0\u52bf\u2014\u2014\u8862\u5dde\nD. \u5317\u901a\u6c5d\u6d1b\uff0c\u897f\u5e26\u79e6\u8700\uff0c\u5357\u906e\u6e56\u5e7f\uff0c\u4e1c\u77b0\u5434\u8d8a\u2014\u2014\u8944\u9633\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38369877502722677, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8f93\u5c3f\u7ba1\nA. \u5f00\u53e3\u4e8e\u8180\u80f1\u9888\nB. \u53ef\u5206\u4e3a\u8179\u3001\u76c6\u4e24\u6bb5\nC. \u4e3a\u8179\u819c\u5185\u4f4d\u5668\u5b98\nD. \u8d77\u59cb\u4e8e\u80be\u76c2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.32205625344145955, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8007222851970698, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38277148274919337, "meta-llama/Meta-Llama-3-8B": 0.5052963528989117, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0a\u6d77\u5b9d\u94a2\u4e3a\u5b9e\u65bd\u201c\u2f9b\u51fa\u53bb\u201d\u6218\u7565\uff0c\u5c06\u5728\u97e9\u56fd\u4eac\u757f\u9053\u6295\u8d44\u65b0\u5efa\u94a2\u6750\u52a0\u2f2f\u914d\u9001\u4e2d\u2f3c\uff0c\u63d0\u4f9b\u6c7d\u2ecb\u677f\u6750\u4ed3\u50a8\u3001\u526a\u5207\u3001\u914d\u9001\u7b49\u670d\u52a1\u3002\u5b9d\u94a2\u5b9e\u65bd\u201c\u2f9b\u51fa\u53bb\u201d\u6218\u7565\u7684\u4e3b\u8981\u2f6c\u7684\u662f\nA. \u964d\u4f4e\u8fd0\u8d39\nB. \u4fdd\u62a4\u73af\u5883\nC. \u8f93\u51fa\u6280\u672f\nD. \u6269\u2f24\u5e02\u573a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6529950239618388, "meta-math/MetaMath-Mistral-7B": 0.5152599369133779, "itpossible/Chinese-Mistral-7B-v0.1": 0.9512310825442295, "HuggingFaceH4/zephyr-7b-beta": 0.9998257718837508, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9935399125845532, "meta-llama/Meta-Llama-3-8B": 0.84886589334717, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.877631690626533}}, {"question": "\u7537\u6027\uff0c35\u5c81\uff0c3\u4e2a\u6708\u6765\u95f4\u65ad\u4e0a\u8179\u75db\uff0c\u6709\u65f6\u591c\u95f4\u75db\u9192\uff0c\u53cd\u9178\u30021\u5929\u524d\u9ed1\u4fbf1\u6b21\uff0c\u65e0\u5455\u8840\uff0c\u4f46\u8179\u75db\u51cf\u8f7b\uff0c\u5316\u9a8c\u5927\u4fbf\u9690\u8840\u5448\u9633\u6027\u3002\u8be5\u60a3\u8005\u6700\u53ef\u80fd\u7684\u8bca\u65ad\u662f\nA. \u80c3\u6e83\u75a1\nB. \u6162\u6027\u80c3\u708e\nC. \u5341\u4e8c\u6307\u80a0\u6e83\u75a1\nD. \u80c3\u764c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.516527464840411, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5395986045913069, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u84b8\u6c7d\u3001\u7535\u529b\u548c\u81ea\u52a8\u7eba\u7ec7\u673a\u751a\u81f3\u662f\u6bd4\u5df4\u5c14\u8d1d\u65af\u3001\u62c9\u65af\u62dc\u5c14\u548c\u5e03\u6717\u57fa\u8bf8\u4f4d\u516c\u6c11\u66f4\u5371\u9669\u4e07\u5206\u7684\u9769\u547d\u5bb6\u3002\u201d\u8fd9\u4e00\u8bba\u65ad\u7684\u542b\u4e49\u662f\nA. \u79d1\u6280\u9769\u547d\u5bfc\u81f4\u793e\u4f1a\u653f\u6cbb\u9769\u547d\nB. \u79d1\u6280\u9769\u547d\u662f\u5bf9\u7edf\u6cbb\u9636\u7ea7\u7684\u6781\u5927\u5a01\u80c1\nC. \u79d1\u6280\u9769\u547d\u5bf9\u53d8\u9769\u793e\u4f1a\u5236\u5ea6\u5177\u6709\u76f4\u63a5\u7684\u51b3\u5b9a\u4f5c\u7528\nD. \u6ee5\u7528\u79d1\u6280\u9769\u547d\u7684\u6210\u679c\u4f1a\u5bf9\u4eba\u7c7b\u9020\u6210\u201c\u5371\u9669\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5e7f\u4e49\u9057\u4f20\u7387\u662f\u4e00\u4e2a\u6bd4\u7387\uff0c\u5176\u5206\u5b50\u4e3a\u9057\u4f20\u65b9\u5dee\uff0c\u5206\u6bcd\u4e3a\nA. \u8868\u578b\u65b9\u5dee\nB. \u4e0a\u4f4d\u6027\u65b9\u5dee\nC. \u663e\u6027\u65b9\u5dee\nD. \u52a0\u6027\u65b9\u5dee\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4588123566081842, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5099803739829025, "meta-llama/Meta-Llama-3-8B": 0.3723801373943884, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6768\u67f3\u662f\u6211\u56fd\u53e4\u4ee3\u8bd7\u8bcd\u91cc\u8f83\u4e3a\u5e38\u89c1\u7684\u610f\u8c61\uff0c\u5f80\u5f80\u8574\u6db5\u79bb\u522b\u4e4b\u60c5\u3002\u4e0b\u5217\u540d\u53e5\u4e2d\u4e0d\u662f\u8868\u8fbe\u9001\u522b\u610f\u8c61\u7684\u662f\nA. \u5ead\u9662\u6df1\u6df1\u6df1\u51e0\u8bb8?\u6768\u67f3\u5806\u70df\uff0c\u5e55\u5e18\u65e0\u91cd\u6570\nB. \u4eca\u5bb5\u9152\u9192\u4f55\u5904?\u6768\u67f3\u5cb8\u3001\u6653\u98ce\u6b8b\u6708\nC. \u7f8c\u7b1b\u4f55\u987b\u6028\u6768\u67f3\uff0e\u6625\u98ce\u4e0d\u5ea6\u7389\u95e8\u5173\nD. \u6e2d\u57ce\u671d\u96e8\u6e11\u8f7b\u5c18\u3002\u5ba2\u820d\u9752\u9752\u67f3\u8272\u65b0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "20\u4e16\u7eaa\u521d\uff0c\u5973\u5b66\u5728\u4e2d\u56fd\u5f00\u59cb\u5174\u8d77\uff0c\u51e0\u5341\u5e74\u95f4\u4e2d\u56fd\u5973\u6027\u8d70\u51fa\u95fa\u9601\uff0c\u8d70\u5411\u5b66\u6821\u548c\u804c\u573a\uff0c\u83b7\u5f97\u4e86\u72ec\u7acb\u548c\u89e3\u653e\u7684\u53ef\u80fd\u3002\u800c\u8fd9\u4e00\u53d8\u5316\u5728\u4e0a\u6d77\u8fd9\u4e9b\u8fd1\u4ee3\u5927\u90fd\u5e02\u5c24\u4e3a\u5267\u70c8\u548c\u591a\u6837\u3002\u5bf9\u8fd9\u4e9b\u73b0\u8c61\u7684\u89e3\u91ca\u6700\u5408\u7406\u7684\u662f\nA. \u5973\u6027\u89e3\u653e\u6700\u65e9\u5174\u8d77\u4e8e\u4e0a\u6d77\nB. \u7537\u5973\u95f4\u5b9e\u73b0\u4e86\u6743\u5229\u4e0a\u7684\u5e73\u7b49\nC. \u5973\u5b66\u5174\u8d77\u5bfc\u81f4\u4f20\u7edf\u793c\u6559\u5d29\u6e83\nD. \u653f\u6cbb\u8fd0\u52a8\u63a8\u52a8\u4e86\u5987\u5973\u89e3\u653e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35888231301687173, "meta-math/MetaMath-Mistral-7B": 0.6784311117543628, "itpossible/Chinese-Mistral-7B-v0.1": 0.33424000363035195, "HuggingFaceH4/zephyr-7b-beta": 0.9927110147855203, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6379189098846619, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u4eba\u4e3a\u81ea\u7136\u7acb\u6cd5\u201d\u7684\u89c2\u70b9\u5c5e\u4e8e\nA. \u4e3b\u89c2\u552f\u5fc3\u4e3b\u4e49\nB. \u5bbf\u547d\u8bba\nC. \u673a\u68b0\u552f\u7269\u4e3b\u4e49\nD. \u5ba2\u89c2\u552f\u5fc3\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4996075024014804, "meta-math/MetaMath-Mistral-7B": 0.7210672217926787, "itpossible/Chinese-Mistral-7B-v0.1": 0.4645499962284442, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6109588524261916, "meta-llama/Meta-Llama-3-8B": 0.4088039123969215, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6888378110131681}}, {"question": "\u7532\u5411\u4e59\u501f\u6b3e\uff0c\u5c06\u81ea\u5df1\u7684\u6c7d\u8f66\u62b5\u62bc\u7ed9\u4e59\uff0c\u529e\u7406\u4e86\u62b5\u62bc\u767b\u8bb0\u3002\u540e\u7532\u53c8\u5411\u4e19\u501f\u6b3e\uff0c\u5c06\u8be5\u8f66\u8d28\u62bc\u7ed9\u4e19\u3002\u4e19\u5728\u5360\u6709\u8be5\u8f66\u671f\u95f4\uff0c\u53d1\u73b0\u6c7d\u8f66\u6709\u6545\u969c\uff0c\u9001\u5230\u4e01\u5382\u4fee\u7406\u3002\u4e01\u56e0\u672a\u6536\u5230\u4fee\u7406\u8d39\u5c06\u8be5\u8f66\u7559\u7f6e\u3002\u672c\u6848\u7684\u62c5\u4fdd\u7269\u6743\u53d7\u507f\u987a\u5e8f\u662f\nA. \u7559\u7f6e\u6743\uff1b\u62b5\u62bc\u6743\uff1b\u8d28\u6743\nB. \u8d28\u6743\uff1b\u7559\u7f6e\u6743\uff1b\u62b5\u62bc\u6743\nC. \u7559\u7f6e\u6743\uff1b\u8d28\u6743\uff1b\u62b5\u62bc\u6743\nD. \u62b5\u62bc\u6743\uff1b\u8d28\u6743\uff1b\u7559\u7f6e\u6743\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3444545270076965, "meta-llama/Meta-Llama-3-8B": 0.29863342676099575, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3222217017095491}}, {"question": "\u4e0b\u9762\u4e0d\u5c5e\u4e8e\u5c40\u57df\u7f51\u7edc\u786c\u4ef6\u7ec4\u6210\u7684\u662f\nA. \u4e2a\u4eba\u8ba1\u7b97\u673a\u5de5\u4f5c\u7ad9\nB. \u6d4f\u89c8\u5668\nC. \u7f51\u7edc\u670d\u52a1\u5668\nD. \u7f51\u7edc\u4ea4\u6362\u673a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9435076195814827, "meta-math/MetaMath-Mistral-7B": 0.9977839781312059, "itpossible/Chinese-Mistral-7B-v0.1": 0.9120170791072371, "HuggingFaceH4/zephyr-7b-beta": 0.9994080355496121, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9887319128692961, "meta-llama/Meta-Llama-3-8B": 0.8049257262345744, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9992500123433313}}, {"question": "\u5982\u679c\u53d1\u5c55\u4e2d\u56fd\u5bb6\u653f\u5e9c\u91c7\u7eb3\u73b0\u4ee3\u5fae\u89c2\u7ecf\u6d4e\u5b66\u5bb6\u5ead\u7406\u8bba\u5206\u6790\u6240\u5f97\u51fa\u7684\u653f\u7b56\u5efa\u8bae\uff0c\u5c31\u53ef\u80fd\nA. \u964d\u4f4e\u751f\u80b2\u7387\nB. \u51cf\u5c11\u6b7b\u4ea1\u7387\nC. \u63d0\u9ad8\u751f\u80b2\u7684\u8d28\u91cf\nD. \u5b9e\u73b0\u4eba\u53e3\u4f18\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.702092345483037, "meta-math/MetaMath-Mistral-7B": 0.9715195087960187, "itpossible/Chinese-Mistral-7B-v0.1": 0.4221216124438063, "HuggingFaceH4/zephyr-7b-beta": 0.9998772682805123, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.524446388211715, "meta-llama/Meta-Llama-3-8B": 0.5918385622803117, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8919752029560247}}, {"question": "\u8d44\u672c\u4e3b\u4e49\u7ecf\u6d4e\u5371\u673a\u5448\u73b0\u51fa\u5468\u671f\u6027\u7684\u539f\u56e0\u5728\u4e8e\nA. \u8d44\u672c\u4e3b\u4e49\u518d\u751f\u4ea7\u7684\u5468\u671f\u6027\nB. \u8d44\u672c\u4e3b\u4e49\u57fa\u672c\u77db\u76fe\nC. \u8d44\u672c\u4e3b\u4e49\u7684\u57fa\u672c\u77db\u76fe\u5468\u671f\u6027\nD. \u8d44\u672c\u4e3b\u4e49\u57fa\u672c\u77db\u76fe\u8fd0\u52a8\u7684\u7279\u70b9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3922225509067182, "meta-math/MetaMath-Mistral-7B": 0.3900852433273489, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5269471109232332, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u4e0d\u4f1a\u6d89\u53ca\u5230\u6ca5\u9752\u53d8\u5f62\u6027\u7684\u6307\u6807\u662f\nA. \u9488\u5165\u5ea6\nB. \u8f6f\u5316\u70b9\nC. \u542b\u8721\u91cf\nD. \u7c98\u9644\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3967757634198264, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5414438117600966}}, {"question": "\u7269\u4f53\u7531\u4e8e\u53d7\u5916\u754c\u5f71\u54cd\uff08\u5982\u5347\u5546\u3001\u53d8\u5f62\u7b49\uff09\u800c\u5176\u6709\u7684\u80fdf\u79f0\u4e3a\nA. \u673a\u68b0\u80fd\nB. \u60ef\u80fd\nC. \u52bf\u80fd\nD. \u52a8\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30702421322472195, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u745e\u58eb\u5b97\u6559\u6539\u9769\u5bb6\u9886\u8896\u662f\nA. \u9a6c\u4e01\u00b7\u8def\u5fb7\nB. \u4e4c\u5229\u65af\u00b7\u8328\u6e29\u5229\nC. \u4ea8\u5229\u516b\u4e16\nD. \u52a0\u5c14\u6587\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u610f\u5927\u5229\u6700\u5927\u7684\u901a\u8baf\u793e\u662f\nA. \u5171\u540c\u793e\nB. \u65f6\u4e8b\u793e\nC. \u7f57\u65af\u5854\u793e\nD. \u5b89\u838e\u793e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.46589234207604846, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.517418758253961, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9593127605593171}}, {"question": "\u5b5f\u5b50\u8bf4\uff1a\u201c\u4e0d\u4ee5\u89c4\u77e9\uff0c\u4e0d\u6210\u65b9\u5706\u201d\uff0c\u8fd9\u91cc\u201c\u89c4\u77e9\u201d\u7684\u610f\u601d\u662f\nA. \u6f5c\u89c4\u5219\nB. \u7f8e\u5fb7\u5584\u884c\nC. \u5706\u89c4\u66f2\u5c3a\nD. \u6cd5\u5f8b\u6761\u6587\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u9879\u76ee\u7b56\u5212\u5de5\u4f5c\u5185\u5bb9\u4e2d\uff0c\u5c5e\u4e8e\u5b9e\u65bd\u9636\u6bb5\u7ba1\u7406\u7b56\u5212\u7684\u662f\nA. \u9879\u76ee\u98ce\u9669\u7ba1\u7406\u4e0e\u5de5\u7a0b\u4fdd\u9669\u65b9\u6848\nB. \u4e1a\u4e3b\u65b9\u9879\u76ee\u7ba1\u7406\u7ec4\u7ec7\u673a\u6784\nC. \u751f\u4ea7\u8fd0\u8425\u671f\u8bbe\u65bd\u7ba1\u7406\u603b\u4f53\u65b9\u6848\nD. \u9879\u76ee\u5b9e\u65bd\u671f\u7ba1\u7406\u603b\u4f53\u65b9\u6848\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u516c\u5143500-800\u5e74\u95f4\uff0c\u6559\u4f1a\u5bf9\u86ee\u65cf\u8fdb\u884c\u6559\u5316\u7684\u5de5\u4f5c\u4e3b\u8981\u662f\nA. \u704c\u8f93\u57fa\u7763\u6559\u4fe1\u4ef0\nB. \u6069\u5178\u6559\u533a\u7684\u6559\u6c11\nC. \u653f\u6559\u5408\u4e00\nD. \u5174\u529e\u6559\u80b2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.49222307659271974, "meta-math/MetaMath-Mistral-7B": 0.9082106463777587, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8065966996892343, "meta-llama/Meta-Llama-3-8B": 0.49914276644260075, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9383131563144603}}, {"question": "\u572870\u514b\u7684\u6c34\u4e2d\u653e\u516510\u514b\u7cd6\uff0c\u914d\u6210\u7cd6\u6c34\uff0c\u542b\u7cd6\u7387\u4e3a\nA. 12.50%\nB. 1.25%\nC. 87.55\nD. 14.30%\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.34671647917417475, "meta-llama/Meta-Llama-3-8B": 0.3528619536675028, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6261913963713603}}, {"question": "\u6c34\u80bf\u578b\u8425\u517b\u4e0d\u826f\u7684\u4e3b\u8981\u7279\u5f81\u662f\nA. \u4ee5\u86cb\u767d\u8d28\u7f3a\u4e4f\u4e3a\u4e3b\nB. \u4ee5\u77ff\u7269\u8d28\u7f3a\u4e4f\u4e3a\u4e3b\nC. \u4ee5\u80fd\u91cf\u7f3a\u4e4f\u4e3a\u4e3b\nD. \u86cb\u767d\u8d28\u548c\u80fd\u91cf\u540c\u65f6\u7f3a\u4e4f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4167500531591177, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1909\u5e742\u6708\uff0c\u5728\u6bd4\u5c14\u65af\u7b49\u4eba\u7684\u79ef\u6781\u52aa\u529b\u4e0b\uff0c\u201c\u7f8e\u56fd\u5168\u56fd\u5fc3\u7406\u536b\u751f\u59d4\u5458\u4f1a\u201d\u5728\u54ea\u4e2a\u57ce\u5e02\u6210\u7acb\nA. \u7ebd\u7ea6\nB. \u65e7\u91d1\u5c71\nC. \u8d39\u57ce\nD. \u534e\u76db\u987f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.35002888908720886, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5838437235454041, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5404\u9879\uff0c\u5c5e\u4e8e\u56fd\u9645\u53f8\u6cd5\u57fa\u672c\u539f\u5219\u7684\u662f\nA. \u4fdd\u62a4\u5f31\u65b9\u5f53\u4e8b\u4eba\u5408\u6cd5\u6743\u76ca\u7684\u539f\u5219\nB. \u610f\u601d\u81ea\u6cbb\u539f\u5219\nC. \u6700\u60e0\u56fd\u5f85\u9047\nD. \u975e\u6b67\u89c6\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6539943916150066, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6399455077911805, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u5728\u6b63\u5e38\u60c5\u51b5\u4e0b\u7ec4\u7ec7\u6db2\u751f\u6210\u4e0e\u56de\u6d41\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u7ec4\u7ec7\u6db2\u4e2d\u7684\u6709\u4e9b\u7269\u8d28\u7ecf\u5fae\u9759\u8109\u7aef\u8fdb\u5165\u8840\u6db2\nB. \u7ec4\u7ec7\u6db2\u4e0d\u65ad\u751f\u6210\u4e0e\u56de\u6d41\uff0c\u5e76\u4fdd\u6301\u52a8\u6001\u5e73\u8861\nC. \u751f\u6210\u4e0e\u56de\u6d41\u7684\u7ec4\u7ec7\u6db2\u4e2d\u6c27\u6c14\u7684\u542b\u91cf\u76f8\u7b49\nD. \u8840\u6d46\u4e2d\u7684\u6709\u4e9b\u7269\u8d28\u7ecf\u6bdb\u7ec6\u8840\u7ba1\u52a8\u8109\u7aef\u8fdb\u5165\u7ec4\u7ec7\u6db2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4339643156962409, "meta-math/MetaMath-Mistral-7B": 0.5794769804852786, "itpossible/Chinese-Mistral-7B-v0.1": 0.4970892922304963, "HuggingFaceH4/zephyr-7b-beta": 0.9580523872996491, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7292951342702955, "meta-llama/Meta-Llama-3-8B": 0.8182836589012976, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9323045976866703}}, {"question": "\u4e9a\u91cc\u58eb\u591a\u5fb7\u8ba4\u4e3a\u5728\u54f2\u5b66\u5c5e\u4e8e\u4ee5\u4e0b\u54ea\u79cd\u4eba\uff1f\nA. \u5168\u4f53\u57ce\u90a6\u516c\u6c11\nB. \u541b\u738b\nC. \u81ea\u7531\u4eba\nD. \u667a\u8005\u4e0e\u54f2\u5b66\u5bb6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5802373109213156, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u4f20\u7edf\u6587\u5316\u5bf9\u4eba\u7c7b\u6587\u660e\u8d21\u732e\u5de8\u5927\uff0c\u6211\u56fd\u53e4\u4ee3\u7684\u4eba\u6c11\u5728\u90a3\u65f6\u5019\u5c31\u5df2\u7ecf\u5e7f\u6cdb\u5e94\u7528\u4e86\uff0c\u4e66\u4e2d\u5145\u5206\u8bb0\u8f7d\u4e86\u53e4\u4ee3\u5316\u5b66\u7814\u7a76\u6210\u679c\u3002\u4e0b\u5217\u5173\u4e8e\u53e4\u4ee3\u5316\u5b66\u7684\u5e94\u7528\u548c\u8bb0\u8f7d\uff0c\u5bf9\u5176\u8bf4\u660e\u4e0d\u5408\u7406\u7684\u662f\nA. \u300a\u672c\u8349\u7ecf\u96c6\u6ce8\u300b\u4e2d\u8bb0\u8f7d\u4e86\u533a\u5206\u785d\u77f3(KNO3)\u548c\u6734\u6d88(Na2SO4)\u7684\u65b9\u6cd5\uff1a\u201c\u4ee5\u706b\u70e7\u4e4b\uff0c\u7d2b\u9752\u70df\u8d77\uff0c\u4e43\u771f\u785d\u77f3\u4e5f\u201d\u8fd9\u662f\u5229\u7528\u4e86\u201c\u7130\u8272\u53cd\u5e94\u201d\nB. \u6211\u56fd\u53e4\u4ee3\u4eba\u6c11\u5e38\u7528\u660e\u77fe\u9664\u53bb\u94dc\u5668\u4e0a\u7684\u94dc\u9508[Cu2(OH)2CO3]\nC. \u674e\u767d\u6709\u8bd7\u4e91\u201c\u65e5\u7167\u9999\u7089\u751f\u7d2b\u70df\u201d\u8fd9\u662f\u63cf\u5199\u201c\u7898\u7684\u5347\u534e\u201d\nD. \u300a\u672c\u8349\u7eb2\u76ee\u300b\u4e2d\u8bb0\u8f7d\u201c(\u706b\u836f)\u4e43\u7130\u6d88(KNO3)\u3001\u786b\u9ec4\u3001\u6749\u6728\u70ad\u6240\u5408\uff0c\u4ee5\u70fd\u71e7\u94f3\u6781\u201d\u8fd9\u662f\u5229\u7528\u4e86\u201cKNO3\u7684\u6c27\u5316\u6027\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5288544104913047, "meta-math/MetaMath-Mistral-7B": 0.7103735288605441, "itpossible/Chinese-Mistral-7B-v0.1": 0.35832757394471465, "HuggingFaceH4/zephyr-7b-beta": 0.9779196938238639, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.798905852010506, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.35909979018274696}}, {"question": "M51\u4f4d\u4e8e\nA. \u5927\u718a\u5ea7\nB. \u5c0f\u718a\u5ea7\nC. \u5929\u7434\u5ea7\nD. \u730e\u72ac\u5ea7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4e00\u5f20\u5904\u7406\u5b8c\u6210\u7684\u56fe\u4e66\u9986\u7247\uff0c\u5206\u522b\u7528BMP\uff0824\u4f4d\u4f4d\u56fe\uff09\u548cJPG\u4e24\u79cd\u683c\u5f0f\u4fdd\u5b58\u540c\u4e00\u5f20\u56fe\u7247\u65f6\uff0c\u751f\u6210\u7684\u4e24\u4e2a\u6587\u4ef6\u6240\u5360\u7528\u7684\u78c1\u76d8\u7a7a\u95f4\nA. \u4e0d\u80fd\u786e\u5b9a\nB. \u4e24\u8005\u4e00\u6837\u5927\nC. BMP\u683c\u5f0f\u7684\u5927\nD. JPG\u683c\u5f0f\u7684\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4697679424145983, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c55 \u5c81\u3002\u53cd\u590d\u53d1\u4f5c\u4e0a\u8179\u75db\u5e76\u53d1\u70ed 8 \u5e74\uff0c\u8fd1\u65e5\u518d\u6b21\u53d1\u4f5c\u3002\u67e5\u4f53\uff1aT39\u2103\uff0cP112 \u6b21/\u5206\uff0cBP132/86mmHg\uff0c\u5de9\u819c\u4e0d\u9ec4\uff0c\u4e0a\u8179\u538b\u75db\uff0c\u8f7b\u5ea6\u808c\u7d27\u5f20\uff0c\u809d\u533a\u53e9\u75db\u3002\u5316\u9a8c\uff1aHb132g/L\uff0cWBC 13.8\u00d7109/L\uff0cN86%\uff0cALT86U/L\uff0cT-Bil 28.5\u00b5mol/L\u3002MRCP \u63d0\u793a\uff1a\u5de6\u4fa7\u809d\u5185\u80c6\u7ba1\u5c40\u9650\u6027\u6269\u5f20\uff0c\u5176\u5185\u53ef\u89c1\u591a\u53d1\u4f4e\u4fe1\u53f7\u5f71\uff0c\u53f3\u4fa7\u809d\u5185\u80c6\u7ba1\u53ca\u809d\u5916\u80c6\u7ba1\u672a\u89c1\u6269\u5f20\uff0c\u80c6\u56ca\u4e0d\u5927\uff0c\u809d\u5de6\u53f6\u4f53\u79ef\u7f29\u5c0f\u3002\u8be5\u60a3\u8005\u7684\u8bca\u65ad\u662f\nA. \u80c6\u56ca\u708e\nB. \u5de6\u809d\u5185\u80c6\u7ba1\u7ed3\u77f3\u3001\u80c6\u7ba1\u708e\nC. \u5de6\u809d\u809d\u764c\u8113\u80bf\nD. \u5de6\u809d\u5185\u80c6\u7ba1\u7ed3\u77f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39249816259976017, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7588572486646615, "HuggingFaceH4/zephyr-7b-beta": 0.9639221327320444, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7131293116794816, "meta-llama/Meta-Llama-3-8B": 0.6837089248023804, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7626692539032833}}, {"question": "\u8bbe\u968f\u673a\u53d8\u91cf $X$ \u670d\u4ece\u6b63\u6001\u5206\u5e03 $N\\left(\\mu\uff0c \\sigma^2\\right)\uff0c \\sigma>0$\uff0c \u5219\u968f\u7740 $\\sigma$ \u7684\u589e\u5927\uff0c \u6982\u7387 $P\\{|X-\\mu|<1\\}$\nA. \u51cf\u5c0f\nB. \u589e\u5927\nC. \u589e\u51cf\u4e0d\u5b9a\nD. \u4fdd\u6301\u4e0d\u53d8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3647618879011802, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u76d7\u7a83\u6b63\u5728\u4f7f\u7528\u7684\u5e7f\u64ad\u7535\u89c6\u8bbe\u65bd\uff0c\u6570\u989d\u7279\u522b\u5de8\u5927\uff0c\u9020\u6210\u4e25\u91cd\u540e\u679c\u7684\uff0c\u5e94\u5f53\nA. \u6309\u7167\u76d7\u7a83\u7f6a\u548c\u7834\u574f\u5e7f\u64ad\u7535\u89c6\u8bbe\u65bd\u7f6a\u5b9e\u884c\u6570\u7f6a\u5e76\u7f5a\nB. \u6309\u7167\u76d7\u7a83\u7f6a\u5b9a\u7f6a\uff0c\u6309\u7167\u7834\u574f\u5e7f\u64ad\u7535\u89c6\u8bbe\u65bd\u7f6a\u7684\u6cd5\u5b9a\u5211\u5904\u7f5a\nC. \u6309\u7167\u7834\u574f\u5e7f\u64ad\u7535\u89c6\u8bbe\u65bd\u7f6a\u5b9a\u7f6a\u5904\u7f5a\nD. \u6309\u7167\u76d7\u7a83\u7f6a\u5b9a\u7f6a\u5904\u7f5a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e0b\u5217\u5173\u4e8e\u673a\u68b0\u6ce2\u7684\u8868\u8ff0\u4e2d\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u6ce2\u7684\u632f\u5e45\u3001\u9891\u7387\u3001\u76f8\u4f4d\u4e0e\u6ce2\u6e90\u76f8\u540c\nB. \u5728\u6ce2\u7684\u4f20\u64ad\u65b9\u5411\u4e0a\uff0c\u76f8\u4f4d\u5dee\u4e3a2\u7684\u4e24\u8d28\u5143\u4e4b\u95f4\u7684\u8ddd\u79bb\u79f0\u4e3a\u6ce2\u957f\nC. \u632f\u52a8\u72b6\u6001\u5728\u4ecb\u8d28\u4e2d\u4f20\u64ad\u65f6\uff0c\u6ce2\u7ebf\u4e0a\u5404\u8d28\u5143\u5747\u53ef\u89c6\u4e3a\u65b0\u7684\u5b50\u6ce2\u6ce2\u6e90\nD. \u673a\u68b0\u6ce2\u5b9e\u9645\u4e0a\u5c31\u662f\u5728\u6ce2\u7684\u4f20\u64ad\u65b9\u5411\u4e0a\uff0c\u4ecb\u8d28\u4e2d\u5404\u8d28\u5143\u7684\u96c6\u4f53\u53d7\u8feb\u632f\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5759519186285699, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4606796658671747, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7433685862153327, "meta-llama/Meta-Llama-3-8B": 0.3681585634507254, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u673a\u4f53\u5728\u5e94\u6fc0\u72b6\u6001\u4e0b\u4ee3\u8c22\u53d8\u5316\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u8102\u80aa\u52a8\u5458\u52a0\u901f\nB. \u4ee3\u8c22\u7387\u589e\u52a0\nC. \u8461\u8404\u7cd6\u7684\u50a8\u5b58\u52a0\u901f\nD. \u86cb\u767d\u8d28\u5206\u89e3\u52a0\u901f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4361689548297615, "meta-math/MetaMath-Mistral-7B": 0.5369443549076318, "itpossible/Chinese-Mistral-7B-v0.1": 0.4693609738413433, "HuggingFaceH4/zephyr-7b-beta": 0.992058430918459, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8989788905757721, "meta-llama/Meta-Llama-3-8B": 0.9181422743044957, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.984303781535537}}, {"question": "\u4e0b\u5217\u5404\u9879\u4e2d\uff0c\u4f53\u73b0\u8c28\u614e\u6027\u539f\u5219\u8981\u6c42\u7684\u662f\nA. \u56fa\u5b9a\u8d44\u4ea7\u91c7\u7528\u52a0\u901f\u6298\u65e7\u6cd5\nB. \u5b58\u8d27\u91c7\u7528\u5386\u53f2\u6210\u672c\u8ba1\u4ef7\nC. \u8d39\u7528\u5e94\u4e0e\u5f53\u671f\u6536\u5165\u76f8\u914d\u6bd4\nD. \u6536\u5165\u786e\u8ba4\u91c7\u7528\u6743\u8d23\u53d1\u751f\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3723801373943884, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8ba9\u53d7\u4f17\u900f\u8fc7\u5a92\u4ecb\u7ecf\u5e38\u770b\u5230\u4f60\uff0c\u53ef\u4ee5\u589e\u5f3a\nA. \u719f\u77e5\u6027\nB. \u6743\u5a01\u6027\nC. \u53ef\u4fe1\u6027\nD. \u63a5\u8fd1\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3444545270076965, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7881654533029633, "meta-llama/Meta-Llama-3-8B": 0.3386364065941128, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u6210\u7acb\u6700\u65e9\u7684\u81ea\u6cbb\u533a\u662f\nA. \u897f\u85cf\u81ea\u6cbb\u533a\nB. \u5b81\u590f\u56de\u65cf\u81ea\u6cbb\u533a\nC. \u5e7f\u897f\u58ee\u65cf\u81ea\u6cbb\u533a\nD. \u5185\u8499\u53e4\u81ea\u6cbb\u533a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3160424181481997, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7346836722907443, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c30\u5c81\u3002\u591a\u98df\u3001\u6613\u9965\u3001\u5fc3\u60b8\u3001\u591a\u6c57\u4f34\u5927\u4fbf\u6b21\u6570\u589e\u591a3\u4e2a\u6708\uff0c\u4f53\u91cd\u4e0b\u964d7kg\u3002\u67e5\u4f53\uff1a\u76ae\u80a4\u6e7f\u6da6\uff0c\u5f25\u6f2b\u6027\u7532\u72b6\u817a\u2161\u5ea6\u80bf\u5927\u3002\u5fc3\u7387120\u6b21\uff0f\u5206\uff0c\u5f8b\u4e0d\u9f50\uff0c\u65e9\u640f3\uff5e4\u6b21\uff0f\u5206\u3002\u5b9e\u9a8c\u5ba4\u68c0\u67e5\uff1aTgAb(-)\uff0cTPOAb(-)\uff0cTRAb(+)\uff1b\u809d\u529f\u80fd\u548c\u8840\u5e38\u89c4\u5747\u6b63\u5e38\u3002\u8be5\u60a3\u8005\u6700\u53ef\u80fd\u7684\u8bca\u65ad\u662f\nA. \u7ed3\u8282\u6027\u6bd2\u6027\u7532\u72b6\u817a\u80bf\nB. \u6865\u672c\u7532\u72b6\u817a\u708e\nC. \u4e9a\u6025\u6027\u7532\u72b6\u817a\u708e\nD. Graves\u75c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3422622145739089, "meta-math/MetaMath-Mistral-7B": 0.6322728754214296, "itpossible/Chinese-Mistral-7B-v0.1": 0.36385828438381157, "HuggingFaceH4/zephyr-7b-beta": 0.7910197664090477, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7497662788238901, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5935373334782078}}, {"question": "\u738b\u5148\u2f63\u5230\u94f6\u2f8f\u5b58\u4e86\u2f00\u7b14\u4e09\u5e74\u671f\u7684\u5b9a\u671f\u5b58\u6b3e\uff0c\u5e74\u5229\u7387\u662f4.25%\uff0e\u82e5\u5230\u671f\u540e\u53d6\u51fa\u5f97\u5230\u672c\u606f\uff08\u672c\u2fa6+\u5229\u606f\uff0933825\u5143\uff0e\u8bbe\u738b\u5148\u2f63\u5b58\u2f0a\u7684\u672c\u2fa6\u4e3ax\u5143\uff0c\u5219\u4e0b\u2faf\u6240\u5217\u2f45\u7a0b\u6b63\u786e\u7684\u662f\nA. 3\u00d74.25%x=33825 \nB. 3(x+4.25x)=33825 \nC. x+3\u00d74.25%x=33825 \nD. x+4.25%x=33825 \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.38253225505393756, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.40322669517609055, "HuggingFaceH4/zephyr-7b-beta": 0.7550361314792503, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.44817184827332673, "meta-llama/Meta-Llama-3-8B": 0.3546612443924434, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3790504665396701}}, {"question": "\u83dc\u7bee\u5b50\u5de5\u7a0b\u89e3\u51b3\u4ec0\u4e48\u95ee\u9898\nA. \u89e3\u51b3\u5065\u5eb7\u5b89\u5168\u95ee\u9898\nB. \u4e3b\u98df\u4f9b\u5e94\u548c\u6e29\u9971\u95ee\u9898\nC. \u526f\u98df\u4f9b\u5e94\u548c\u98df\u7269\u4e30\u5bcc\u95ee\u9898\nD. \u89e3\u51b3\u98df\u54c1\u65b9\u4fbf\u8425\u517b\u95ee\u9898\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.40053673846705107, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.303006867405966, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9a7e\u9a76\u673a\u52a8\u8f66\u901a\u8fc7\u7a84\u8def\u3001\u7a84\u6865\u65f6\uff0c\u6700\u9ad8\u901f\u5ea6\u4e0d\u80fd\u8d85\u8fc7\u591a\u5c11\nA. 50\u516c\u91cc/\u5c0f\u65f6\nB. 30\u516c\u91cc/\u5c0f\u65f6\nC. 60\u516c\u91cc/\u5c0f\u65f6\nD. 40\u516c\u91cc/\u5c0f\u65f6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39745672808145865, "meta-math/MetaMath-Mistral-7B": 0.8059312725746574, "itpossible/Chinese-Mistral-7B-v0.1": 0.5647479453352917, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4863146515454336, "meta-llama/Meta-Llama-3-8B": 0.4220412305175926, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8154147257723565}}, {"question": "\u5728\u4e0b\u5217\u5404\u79cd\u57fa\u56e0\u578b\u4e2d\uff0c\u54ea\u4e00\u79cd\u80fd\u4ea7\u751f8\u79cd\u7c7b\u578b\u914d\u5b50\nA. AABbddEe\nB. aaBbEe\nC. AaBBDdEe\nD. DdHhee\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6978797859467447, "meta-math/MetaMath-Mistral-7B": 0.9062574582437016, "itpossible/Chinese-Mistral-7B-v0.1": 0.45221964317516966, "HuggingFaceH4/zephyr-7b-beta": 0.9238558697339161, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5858156456713525, "meta-llama/Meta-Llama-3-8B": 0.5951218669747018, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.722190997811324}}, {"question": "\u8840\u4e2d\u54ea\u4e00\u79cd\u80c6\u7ea2\u7d20\u589e\u52a0\u4f1a\u4f7f\u5c3f\u7ea2\u7d20\u5b9a\u6027\u8bd5\u9a8c\u5448\u9633\u6027\u53cd\u5e94\nA. \u7ed3\u5408\u80c6\u7ea2\u7d20\nB. \u6e38\u79bb\u80c6\u7ea2\u7d20\nC. \u672a\u7ed3\u5408\u80c6\u7ea2\u7d20\nD. \u95f4\u63a5\u53cd\u6620\u80c6\u7ea2\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.48416225442688915, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.43899596948895486, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u6d89\u53ca\u4eba\u4eec\u6700\u57fa\u672c\u7684\u793e\u4f1a\u751f\u6d3b\u9700\u6c42\u7684\u5404\u4e2a\u5177\u4f53\u9886\u57df\u5b9e\u73b0\u63a7\u5236\u7684\u5f62\u5f0f\u662f\nA. \u5fae\u89c2\u63a7\u5236\nB. \u4e60\u4fd7\u63a7\u5236\nC. \u5916\u5728\u63a7\u5236\nD. \u5236\u5ea6\u5316\u63a7\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53d1\u7535\u673a\u6b63\u5e38\u8fd0\u884c\u65f6\u6c14\u9699\u78c1\u573a\u7531\nA. \u5269\u78c1\u573a\u4ea7\u751f\nB. \u5b9a\u5b50\u7535\u6d41\u4ea7\u751f\nC. \u8f6c\u5b50\u7535\u6e90\u4ea7\u751f\nD. \u8f6c\u5b50\u7535\u6d41\u4e0e\u5b9a\u5b50\u7535\u6d41\u5171\u540c\u4ea7\u751f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4414456842471216, "meta-math/MetaMath-Mistral-7B": 0.6662656264608808, "itpossible/Chinese-Mistral-7B-v0.1": 0.423442608492013, "HuggingFaceH4/zephyr-7b-beta": 0.9532654742207808, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7452724391122035, "meta-llama/Meta-Llama-3-8B": 0.5898528643433336, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7413594131389544}}, {"question": "\u4e2d\u56fd\u53e4\u4ee3\u66fe\u53d1\u5c55\u51fa\u81ea\u5df1\u72ec\u7279\u7684\u5929\u6587\u4f53\u7cfb\uff0c\u6211\u4eec\u73b0\u5728\u6240\u8bf4\u7684\u201c\u519c\u5386\u201d\u5c31\u662f\u6307\u8fc7\u53bb\u6570\u5343\u5e74\u6765\u6211\u56fd\u4e00\u76f4\u6cbf\u7528\u7684\u5386\u6cd5\u7cfb\u7edf\uff0c\u201c\u519c\u5386\u201d\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u5386\u6cd5\nA. \u9633\u5386\nB. \u9634\u5386\nC. \u9634\u9633\u5408\u5386\nD. \u548c\u73b0\u5728\u7684\u516c\u5386\u76f8\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8733476565001292}}, {"question": "\u4ee5\u4e0b\u4e0d\u5c5e\u4e8e\u504f\u5dee\u884c\u4e3a\u8d1f\u529f\u80fd\u7684\u662f\nA. \u635f\u5bb3\u4e2a\u4eba\u548c\u793e\u4f1a\u7684\u5229\u76ca\nB. \u5e72\u6270\u6b63\u5e38\u7684\u793e\u4f1a\u751f\u6d3b\u79e9\u5e8f\nC. \u51cf\u5f31\u4ed6\u4eba\u9075\u4ece\u89c4\u8303\u7684\u610f\u613f\nD. \u52a0\u5f3a\u793e\u4f1a\u56e2\u7ed3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9707729660767175, "meta-math/MetaMath-Mistral-7B": 0.9952217332087941, "itpossible/Chinese-Mistral-7B-v0.1": 0.909778221850748, "HuggingFaceH4/zephyr-7b-beta": 0.9848670805360993, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.98733875589903, "meta-llama/Meta-Llama-3-8B": 0.8883061959376457, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6555480940870467}}, {"question": "\u4e16\u754c\u8d38\u6613\u7ec4\u7ec7\u5728\u5904\u7406\u56fd\u9645\u8d38\u6613\u5173\u7cfb\u4e2d\uff0c\u5b9e\u65bd\u7684\u6700\u91cd\u8981\u539f\u5219\u662f\nA. \u81ea\u7531\u7ade\u4e89\u539f\u5219\nB. \u975e\u6b67\u89c6\u539f\u5219\nC. \u900f\u660e\u5ea6\u539f\u5219\nD. \u516c\u5e73\u8d38\u6613\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.651099852653219, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9086290548043667, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9185626144500086}}, {"question": "\u79d1\u5b66\u7684\u5b66\u4e1a\u751f\u6daf\u89c4\u5212\u7684\u7279\u70b9\u4e0d\u5305\u62ec\nA. \u72ec\u7279\u6027\nB. \u9636\u6bb5\u6027\nC. \u7efc\u5408\u6027\nD. \u53d1\u5c55\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4489842344514253, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u4eec\u5728\u8f6c\u6362\u5de5\u4f5c\u8fc7\u7a0b\u4e2d\u5bfc\u81f4\u7684\u5931\u4e1a\u79f0\u4e3a\nA. \u7ed3\u6784\u6027\u5931\u4e1a\nB. \u5468\u671f\u6027\u5931\u4e1a\nC. \u6469\u64e6\u6027\u5931\u4e1a\nD. \u5b63\u8282\u6027\u5931\u4e1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4349703601226525, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5028667229653594, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4653251075846108, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.58817751337563}}, {"question": "\u6885\u7279\u6797\u514b\u7684\u300a\u9752\u9e1f\u300b\u3001\u300a\u4e0d\u901f\u4e4b\u5ba2\u300b\u7b49\u4f5c\u54c1\u662f\u54ea\u4e2a\u6d41\u6d3e\u620f\u5267\u7684\u5178\u578b\u4e4b\u4f5c\u3002\nA. \u8352\u8bde\u6d3e\nB. \u8c61\u5f81\u6d3e\nC. \u5199\u5b9e\u6d3e\nD. \u5370\u8c61\u6d3e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5785693188803073, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.44800948719330547, "meta-llama/Meta-Llama-3-8B": 0.44864321384958544, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bca\u65ad\u9ad8\u6e17\u9ad8\u8840\u7cd6\u7efc\u5408\u5f81\u6700\u91cd\u8981\u7684\u68c0\u67e5\u662f\nA. \u8840\u7cd6\nB. \u8840\u548c\u5c3f\u916e\u4f53\nC. \u8840\u7535\u89e3\u8d28\nD. \u8840\u6d46\u6e17\u900f\u538b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7631055035746408, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.495012932548756, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53RIP\u5411\u76f8\u90bb\u7684\u8def\u7531\u5668\u53d1\u9001\u66f4\u65b0\u65f6\uff0c\u5b83\u4f7f\u2f64\u591a\u5c11\u79d2\u4e3a\u66f4\u65b0\u8ba1\u65f6\u7684\u65f6\u95f4\u503c\nA. 20\nB. 15\nC. 25\nD. 30\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.987081418041202, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u53e4\u4ee3\u4e66\u6cd5\u5bb6\u4e2d\u6709\u201c\u4e66\u5723\u201d\u79f0\u53f7\u7684\u662f\nA. \u5f20\u65ed\nB. \u6000\u7d20\nC. \u738b\u7fb2\u4e4b\nD. \u989c\u771f\u537f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.38273863901740396, "meta-math/MetaMath-Mistral-7B": 0.6505308760871008, "itpossible/Chinese-Mistral-7B-v0.1": 0.7675148323257839, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7831802361673156, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9908170109879718}}, {"question": "\u4e0b\u5217\u5404\u7ec4\u5b57\uff0c\u5173\u7cfb\u4e3a\u5f02\u4f53\u5b57\u7684\u662f\nA. \u7fe6\u2014\u526a\nB. \u4f57\u2014\u4ed6\nC. \u53cd\u2014\u8fd4\nD. \u8bf4\u2014\u60a6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u539f\u764c\u57fa\u56e0\u4e2d\uff0c\u4ee5\u70b9\u7a81\u53d8\u4e3a\u4e3b\u8981\u6fc0\u6d3b\u65b9\u5f0f\u6210\u4e3a\u764c\u57fa\u56e0\u7684\u662f\nA. ras\nB. myc\nC. cyclinD1l\nD. PDGF\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.744420491991374, "meta-math/MetaMath-Mistral-7B": 0.994715142156818, "itpossible/Chinese-Mistral-7B-v0.1": 0.5150773592035978, "HuggingFaceH4/zephyr-7b-beta": 0.9977919696233654, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.968790551002953, "meta-llama/Meta-Llama-3-8B": 0.7223367138105992, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8768582709653676}}, {"question": "\u5728\u4e09\u89d2\u5f62ABC\u4e2d\uff0c\u82e5a=2, b+c=7, cosB=-1/4\uff0c\u5219sinA=\nA. \\sqrt{5}/4\nB. \\sqrt{15}/8\nC. 5/8\nD. 3/4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31712010892822357, "meta-math/MetaMath-Mistral-7B": 0.5140406332527598, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9232890128129215, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.29863342676099575, "meta-llama/Meta-Llama-3-8B": 0.28966338381871215, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u52a8\u8109\u7ca5\u6837\u786c\u5316\u590d\u5408\u75c5\u53d8\u4e2d\u6700\u5371\u9669\u7684\u5e76\u53d1\u75c7\u662f\nA. \u8840\u6813\u7684\u5f62\u6210\nB. \u9499\u5316\nC. \u6591\u5757\u5185\u51fa\u8840\nD. \u52a8\u8109\u7624\u7684\u5f62\u6210\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65e5\u5e38\u751f\u6d3b\u4e2d\u5403\u5b8c\u7cd6\u679c\u518d\u5403\u82f9\u679c\uff0c\u4f1a\u89c9\u5f97\u82f9\u679c\u662f\u9178\u7684\uff0c\u8fd9\u79cd\u73b0\u8c61\u5c5e\u4e8e\nA. \u611f\u89c9\u4ee3\u507f\nB. \u8054\u89c9\nC. \u611f\u89c9\u9002\u5e94\nD. \u611f\u89c9\u5bf9\u6bd4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7687638808792966, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.619130592705011}}, {"question": "\u8fdd\u613f\u662f\u6307\u8fdd\u80cc\u81ea\u5df1\u7684\u610f\u613f\u800c\u53d1\u751f\u6027\u5173\u7cfb\uff0c\u4e0b\u5217\u8868\u8ff0\u4e0d\u5c5e\u4e8e\u8fdd\u613f\u7684\u662f\nA. \u56e0\u4e3a\u611f\u6fc0\u800c\u53d1\u751f\u60c5\u7231\nB. \u4e24\u4e2a\u76f8\u7231\u7684\u4eba\u4e3a\u53d1\u5c55\u5a5a\u59fb\u5173\u7cfb\u800c\u53d1\u751f\u60c5\u7231\nC. \u6027\u77e5\u8bc6\u532e\u4e4f\u60c5\u51b5\u4e0b\u800c\u53d1\u751f\u60c5\u7231\nD. \u4e3a\u4e86\u83b7\u5f97\u5229\u76ca\u800c\u53d1\u751f\u60c5\u7231\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4682032120116989, "itpossible/Chinese-Mistral-7B-v0.1": 0.6034309418958488, "HuggingFaceH4/zephyr-7b-beta": 0.998711945477643, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.900159437372063, "meta-llama/Meta-Llama-3-8B": 0.7078945050856348, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.908951531241309}}, {"question": "\u5f02\u6b65\u7535\u52a8\u673a\u91c7\u7528\u5b9a\u5b50\u7535\u538b\u8c03\u901f\u65f6\uff0c\u5e38\u91c7\u7528\u6676\u95f8\u7ba1\u4ea4\u6d41\u8c03\u538b\u7535\u8def\uff0c\u4ee5\u4e0b\u7535\u8def\u4e2d\u6bd4\u8f83\u597d\uff0c\u4e14\u7ecf\u5e38\u4f7f\u7528\u7684\u65b9\u6848\u662f\nA. \u5f00\u4e09\u89d2\u5f62\u8fde\u63a5\u2014\u6676\u95f8\u7ba1\u4e0e\u8d1f\u8f7d\u63a5\u6210\u5185\u4e09\u89d2\u5f62\u7684\u4e09\u76f8\u8c03\u538b\u7535\u8def\nB. \u65e0\u4e2d\u7ebf\u534a\u63a7\u661f\u5f62\u8fde\u63a5\u2014\u534a\u63a7\u8c03\u538b\u7535\u8def\uff0c\u6bcf\u76f8\u53ea\u6709\u4e00\u4e2a\u6676\u95f8\u7ba1\nC. \u65e0\u4e2d\u7ebf\u661f\u5f62\u8fde\u63a5\u2014\u4e0d\u5e26\u4e2d\u7ebf\u7684\u4e09\u76f8\u8c03\u538b\u7535\u8def\uff0c\u6700\u5927\u79fb\u76f8\u8303\u56f4150\u00b0\uff0c\u65e0\u4e09\u6b21\u8c10\u6ce2\nD. \u5e26\u4e2d\u7ebf\u661f\u5f62\u8fde\u63a5\u2014\u5e26\u4e2d\u7ebf\u7684\u4e09\u76f8\u8c03\u538b\u7535\u8def\uff0c\u8981\u6c42\u89e6\u53d1\u79fb\u76f8180\u00b0\uff0c\u4e09\u76f8\u5bf9\u79f0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3771721635077975, "meta-math/MetaMath-Mistral-7B": 0.7961422026391591, "itpossible/Chinese-Mistral-7B-v0.1": 0.33075425723442, "HuggingFaceH4/zephyr-7b-beta": 0.8838463339579545, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7557950115560945, "meta-llama/Meta-Llama-3-8B": 0.2821833983601388, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6387009339729277}}, {"question": "\u8bbe$\\bigtriangleup ABC$\u7684\u5185\u89d2A\uff0cB\uff0cC\u6240\u5bf9\u8fb9\u7684\u957f\u5206\u522b\u662fa\uff0cb\uff0cc\uff0c\u4e14$f(A)=\\sqrt{3}sinA-2cos^{2}\\frac{A}{2}+3$\uff0c\u5f53\u51fd\u6570f(A)\u53d6\u5230\u6700\u5927\u503c\u65f6$\\bigtriangleup ABC$\u7684\u5f62\u72b6\u662f\nA. \u4e0d\u786e\u5b9a\nB. \u9510\u89d2\u4e09\u89d2\u5f62\nC. \u949d\u89d2\u4e09\u89d2\u5f62\nD. \u76f4\u89d2\u4e09\u89d2\u5f62\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.29167779446893877, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6fc0\u70c8\u7684\u5e02\u573a\u7ade\u4e89\u4e2d\uff0c\u6709\u4e9b\u4f4d\u5c45\u6b21\u5e2d\u7684\u4f01\u4e1a\u91c7\u53d6\u4e86\u4ee5\u8fdb\u653b\u4e3a\u4e3b\u7684\u7ade\u4e89\u7b56\u7565\uff0c\u4ee5\u83b7\u53d6\u66f4\u591a\u7684\u5e02\u573a\u4efd\u989d\uff0c\u8fd9\u79cd\u7ade\u4e89\u8005\u901a\u5e38\u88ab\u79f0\u4e3a\nA. \u5e02\u573a\u6311\u6218\u8005\nB. \u5e02\u573a\u9886\u5bfc\u8005\nC. \u5e02\u573a\u8865\u7f3a\u8005\nD. \u5e02\u573a\u8ffd\u968f\u8005\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8931598318548253, "meta-math/MetaMath-Mistral-7B": 0.9936704559604254, "itpossible/Chinese-Mistral-7B-v0.1": 0.8457102561974205, "HuggingFaceH4/zephyr-7b-beta": 0.999920023898839, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9524189832096066, "meta-llama/Meta-Llama-3-8B": 0.9162252088092112, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9945847585815693}}, {"question": "\u6839\u636e\u6240\u4f53\u73b0\u7684\u5229\u76ca\u7684\u6027\u8d28\u4e0d\u540c\uff0c\u6c11\u4e8b\u6743\u5229\u53ef\u5206\u4e3a\u8d22\u4ea7\u6743\u548c\u4eba\u8eab\u6743\u3002\u4e0b\u5217\u9009\u9879\u4e2d\u5c5e\u4e8e\u8d22\u4ea7\u6743\u7684\u662f\nA. \u7ee7\u627f\u6743\nB. \u540d\u8a89\u6743\nC. \u8096\u50cf\u6743\nD. \u59d3\u540d\u6743\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6837089183575975, "meta-math/MetaMath-Mistral-7B": 0.9797442372170414, "itpossible/Chinese-Mistral-7B-v0.1": 0.8899638216663562, "HuggingFaceH4/zephyr-7b-beta": 0.9548453842179608, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9519750590047024, "meta-llama/Meta-Llama-3-8B": 0.9674901564561643, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9873285579014074}}, {"question": "\u590f\u5b63\u56e0\u6c14\u6e29\u9ad8\uff0c\u96e8\u91cf\u591a\uff0c\u868a\u5a92\u5bc6\u5ea6\u5347\u9ad8\u800c\u5f15\u8d77\u7684\u868a\u5a92\u4f20\u67d3\u75c5\u7684\u6d41\u884c\u5c5e\u4e8e\nA. \u5bbf\u4e3b\u56e0\u7d20\u53d1\u751f\u53d8\u5316\uff0c\u5bbf\u4e3b\u6bd4\u91cd\u589e\u52a0\nB. \u5728\u6d41\u884c\u75c5\u5b66\u4e09\u89d2\u6a21\u578b\u4e2d\u73af\u5883\u56e0\u7d20\u4e0d\u53d8\uff0c\u75c5\u56e0\u6bd4\u91cd\u589e\u52a0\nC. \u75c5\u56e0\u3001\u5bbf\u4e3b\u73af\u5883\u4e09\u8981\u7d20\u4fdd\u6301\u52a8\u6001\u5e73\u8861\nD. \u73af\u5883\u56e0\u7d20\u53d1\u751f\u53d8\u5316\uff0c\u5bfc\u81f4\u75c5\u56e0\u6bd4\u91cd\u589e\u52a0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5222669698640303, "meta-math/MetaMath-Mistral-7B": 0.7368945890782275, "itpossible/Chinese-Mistral-7B-v0.1": 0.5627888988520748, "HuggingFaceH4/zephyr-7b-beta": 0.9944198008667147, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4666517791026952, "meta-llama/Meta-Llama-3-8B": 0.5732746718691297, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5194829405909805}}, {"question": "\u7532\u59d4\u6258\u4e59\u516c\u53f8\u5c06\u4e00\u6279\u8d27\u7269\u8fd0\u5f80A\u5730\u3002\u540e\u7532\u5c06\u8fd0\u8f93\u9014\u4e2d\u7684\u8d27\u7269\u5356\u7ed9\u4e19\uff0c\u53cc\u65b9\u5bf9\u98ce\u9669\u7684\u627f\u62c5\u6ca1\u6709\u7ea6\u5b9a\u3002\u7532\u3001\u4e19\u7b7e\u8ba2\u4e70\u5356\u5408\u540c\u540e\uff0c\u8be5\u6279\u8d27\u7269\u6bc1\u635f\u3001\u706d\u5931\u7684\u98ce\u9669\nA. \u81ea\u8d27\u7269\u8fd0\u62b5A\u5730\u65f6\u8d77\u7531\u4e19\u627f\u62c5\nB. \u81ea\u8d27\u7269\u4ea4\u4ed8\u7ed9\u4e59\u516c\u53f8\u65f6\u8d77\u7531\u4e19\u627f\u62c5\nC. \u81ea\u8d27\u6b3e\u4ed8\u6e05\u65f6\u8d77\u7531\u4e19\u627f\u62c5\nD. \u81ea\u4e70\u5356\u5408\u540c\u6210\u7acb\u65f6\u8d77\u7531\u4e19\u627f\u62c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.38191739129543567, "meta-math/MetaMath-Mistral-7B": 0.685487381898378, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5302172434345954, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ece\u751f\u7269\u5b66\u89d2\u5ea6\u770b\uff0c\u8fd0\u52a8\u6280\u672f\u7684\u5f62\u6210\u5f52\u529f\u4e8e\nA. \u6761\u4ef6\u53cd\u5c04\u7684\u5efa\u7acb\u4e0e\u5de9\u56fa\nB. \u8fd0\u52a8\u6280\u80fd\u7684\u8d2e\u5b58\u6570\u91cf\nC. \u7075\u654f\u7d20\u8d28\u597d\nD. \u5fc3\u7406\u7d20\u8d28\u7684\u53d1\u5c55\u6c34\u5e73\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4303372198138565, "meta-math/MetaMath-Mistral-7B": 0.4619154283166679, "itpossible/Chinese-Mistral-7B-v0.1": 0.5246735631058513, "HuggingFaceH4/zephyr-7b-beta": 0.57247099860202, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9039720467299385, "meta-llama/Meta-Llama-3-8B": 0.6869708378394235, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9312783593474087}}, {"question": "\u6211\u56fd\u53d1\u5c04\u7684\u201c\u795e\u5dde\u516d\u53f7\u201d\u8f7d\u2f08\u2edc\u8239\uff0c\u4e0e\u201c\u795e\u5dde\u4e94\u53f7\u201d\u2edc\u8239\u76f8\u2f50\uff0c\u5b83\u5728\u66f4\u2fbc\u7684\u8f68\u9053\u4e0a\u7ed5\u5730\u7403\u505a\u5300\u901f\u5706\u5468\u8fd0\u52a8\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u201c\u795e\u5dde\u516d\u53f7\u201d\u7684\u5468\u671f\u66f4\u77ed\nB. \u2edc\u8239\u6240\u53d7\u5408\u2f12\u4e3a\u96f6\nC. \u201c\u795e\u5dde\u516d\u53f7\u201d\u7684\u901f\u5ea6\u8f83\u2f29\nD. \u2edc\u8239\u9759\u2f4c\u5728\u2f9a\u9053\u7684\u4e0a\u7a7a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5168\u7403\u7b2c\u4e00\u4e2a\u53ef\u7528\u4e8e\u7b2c\u4e09\u65b9\u8ba4\u8bc1\u7684\u793e\u4f1a\u8d23\u4efb\u56fd\u9645\u6807\u51c6\u5373\nA. ISO9000\nB. ISO14000\nC. SA8000\nD. SA9000\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.792578622733708, "meta-math/MetaMath-Mistral-7B": 0.9238933067727735, "itpossible/Chinese-Mistral-7B-v0.1": 0.7114191500743302, "HuggingFaceH4/zephyr-7b-beta": 0.8160606232273911, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9810659929742219, "meta-llama/Meta-Llama-3-8B": 0.8461415138523103, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9894937617811901}}, {"question": "\u5316\u5b66\u4e0e\u751f\u4ea7\u3001\u751f\u6d3b\u5bc6\u5207\u76f8\u5173\u3002\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u798f\u5c14\u9a6c\u6797\u53ef\u7528\u4e8e\u4fdd\u5b58\u6d77\u9c9c\u4ea7\u54c1\nB. \u5de5\u4e1a\u4e0a\u5229\u7528Cl2\u4e0e\u6f84\u6e05\u77f3\u7070\u6c34\u53cd\u5e94\u5236\u53d6\u6f02\u767d\u7c89\nC. \u5929\u7136\u7ea4\u7ef4\u548c\u5408\u6210\u7ea4\u7ef4\u7684\u4e3b\u8981\u6210\u5206\u662f\u7ea4\u7ef4\u7d20\nD. \u7845\u80f6\u5438\u9644\u80fd\u529b\u5f3a\uff0c\u5e38\u7528\u4f5c\u50ac\u5316\u5242\u8f7d\u4f53\u548c\u98df\u54c1\u5e72\u71e5\u5242\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39093036885410615, "meta-math/MetaMath-Mistral-7B": 0.5114296304240291, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3500288944602904, "meta-llama/Meta-Llama-3-8B": 0.4181874540337694, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.415883018132939}}, {"question": "\u6839\u636e\u8fd0\u52a8\u65f6\u5fc3\u7387\u548c\u5f3a\u5ea6\u76f8\u5173\u5173\u7cfb\u6807\u51c6\uff0c\u5fc3\u7387110\u6b21/\u5206\u949f\u7684\u953b\u70bc\u5f3a\u5ea6\u5927\u7ea6\u662f\nA. 60%\nB. 50%\nC. 80%\nD. 70%\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3709299943457361, "meta-math/MetaMath-Mistral-7B": 0.5443943211945877, "itpossible/Chinese-Mistral-7B-v0.1": 0.36150852560561064, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4219746784721703}}, {"question": "\u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u5211\u4e8b\u8bc9\u8bbc\u6cd5\u300b\u89c4\u5b9a\uff0c\u51e1\u9700\u8981\u63d0\u8d77\u516c\u8bc9\u7684\u6848\u4ef6\uff0c\u4e00\u5f8b\u7531\nA. \u53f8\u6cd5\u884c\u653f\u673a\u5173\u5ba1\u67e5\u51b3\u5b9a\nB. \u4eba\u6c11\u68c0\u5bdf\u9662\u5ba1\u67e5\u51b3\u5b9a\nC. \u4eba\u6c11\u6cd5\u9662\u5ba1\u67e5\u51b3\u5b9a\nD. \u516c\u5b89\u673a\u5173\u5ba1\u67e5\u51b3\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.9739977104756417, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5766694965298798, "meta-llama/Meta-Llama-3-8B": 0.9710122862108103, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8591566463757129}}, {"question": "\u4e16\u754c\u4e0a\u7b2c\u4e00\u4e2a\u5efa\u7acb\u5171\u548c\u5236\u7684\u8d44\u672c\u4e3b\u4e49\u56fd\u5bb6\u662f\u54ea\u4e00\u4e2a\nA. \u8377\u5170\nB. \u82f1\u56fd\nC. \u6cd5\u56fd\nD. \u7f8e\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6793098109586048, "meta-math/MetaMath-Mistral-7B": 0.8966798224369577, "itpossible/Chinese-Mistral-7B-v0.1": 0.34686667406054084, "HuggingFaceH4/zephyr-7b-beta": 0.6439140813714136, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6095080784052589, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53\u53d1\u73b0\u7406\u60f3\u4e0e\u73b0\u5b9e\u7684\u77db\u76fe\u65f6\uff0c\u4e0d\u52a0\u5206\u6790\u5730\u5168\u76d8\u8ba4\u540c\u5f53\u4e0b\u7684\u73b0\u5b9e\uff0c\u800c\u5bf9\u7406\u60f3\u5931\u53bb\u4fe1\u5fc3\u548c\u70ed\u60c5\uff0c\u8bf4\u4ec0\u4e48\u201c\u544a\u522b\u7406\u60f3\u201d\u3001\u201c\u8eb2\u907f\u5d07\u9ad8\u201d\u3002\u8fd9\u79cd\u8ba4\u8bc6\u4e0a\u7684\u8bef\u533a\u5c5e\u4e8e\nA. \u4ee5\u4e3b\u89c2\u6765\u5426\u5b9a\u5ba2\u89c2\nB. \u4ee5\u73b0\u5b9e\u6765\u5426\u5b9a\u7406\u60f3\nC. \u4ee5\u8fc7\u53bb\u6765\u5426\u5b9a\u73b0\u5728\nD. \u4ee5\u7406\u60f3\u6765\u5426\u5b9a\u73b0\u5b9e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44381669056084816, "meta-math/MetaMath-Mistral-7B": 0.7379249973326278, "itpossible/Chinese-Mistral-7B-v0.1": 0.955806040888208, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7465434030453498, "meta-llama/Meta-Llama-3-8B": 0.8069740314334903, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6739581714063461}}, {"question": "\u73b0\u4ee3\u4f20\u64ad\u6548\u679c\u7814\u7a76\u5174\u8d77\u7684\u5a92\u4ecb\u80cc\u666f\u65f6\u671f\u662f\nA. \u7535\u5b50\u5a92\u4ecb\nB. \u5927\u4f17\u62a5\u520a\nC. \u653f\u515a\u62a5\u520a\nD. \u9ec4\u8272\u62a5\u520a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.47495142270765867, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u79cd\u690d\u7269\u75c5\u6bd2V\u662f\u901a\u8fc7\u7a3b\u98de\u8671\u5438\u98df\u6c34\u7a3b\u6c41\u6db2\u5728\u6c34\u7a3b\u95f4\u4f20\u64ad\u7684\u3002\u7a3b\u7530\u4e2d\u9752\u86d9\u6570\u91cf\u7684\u589e\u52a0\u53ef\u51cf\u5c11\u8be5\u75c5\u6bd2\u5728\u6c34\u7a3b\u95f4\u7684\u4f20\u64ad\u3002\u4e0b\u5217\u53d9\u8ff0\u6b63\u786e\u7684\u662f\nA. \u9752\u86d9\u4e0e\u7a3b\u98de\u8671\u662f\u6355\u98df\u5173\u7cfb\nB. \u6c34\u7a3b\u548c\u75c5\u6bd2V\u662f\u4e92\u5229\u5171\u751f\u5173\u7cfb\nC. \u75c5\u6bd2V\u4e0e\u9752\u86d9\u662f\u5bc4\u751f\u5173\u7cfb\nD. \u6c34\u7a3b\u4e0e\u9752\u86d9\u662f\u7ade\u4e89\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5584375541185632, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6846435304899116, "meta-llama/Meta-Llama-3-8B": 0.6036570663342438, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6683259337203482}}, {"question": "\u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u5baa\u6cd5\u300b\u89c4\u5b9a\uff0c\u56fd\u5bb6\u7684\u4e00\u5207\u6743\u529b\u5c5e\u4e8e\u4eba\u6c11\uff0c\u4eba\u6c11\u884c\u4f7f\u6743\u529b\u7684\u673a\u5173\u662f\nA. \u6700\u9ad8\u4eba\u6c11\u6cd5\u9662\u548c\u6700\u9ad8\u4eba\u6c11\u68c0\u5bdf\u9662\nB. \u56fd\u52a1\u9662\u548c\u5730\u65b9\u5404\u7ea7\u4eba\u6c11\u653f\u5e9c\nC. \u4e2d\u592e\u519b\u4e8b\u59d4\u5458\u4f1a\nD. \u5168\u56fd\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u548c\u5730\u65b9\u5404\u7ea7\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9745729626986215, "meta-math/MetaMath-Mistral-7B": 0.9975796618928353, "itpossible/Chinese-Mistral-7B-v0.1": 0.9778431145243758, "HuggingFaceH4/zephyr-7b-beta": 0.9998981805036811, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9868778517403635, "meta-llama/Meta-Llama-3-8B": 0.9861926561851397, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9897935276056113}}, {"question": "\u793e\u4f1a\u5b66\u7814\u7a76\u5e94\u8be5\u5728\u56de\u7b54\u201c\u5c06\u4f1a\u600e\u6837\u201d\u7684\u57fa\u7840\u4e0a\uff0c\u8fdb\u4e00\u6b65\u56de\u7b54\u201c\u5e94\u8be5\u600e\u6837\u201d\u7684\u95ee\u9898\uff0c\u8fd9\u6307\u7684\u662f\u793e\u4f1a\u5b66\u7684\nA. \u9884\u6d4b\u6027\u529f\u80fd\nB. \u89c4\u8303\u6027\u529f\u80fd\nC. \u89e3\u91ca\u6027\u529f\u80fd\nD. \u63cf\u8ff0\u6027\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8129851245470645, "meta-math/MetaMath-Mistral-7B": 0.9375145354225177, "itpossible/Chinese-Mistral-7B-v0.1": 0.8564356002480656, "HuggingFaceH4/zephyr-7b-beta": 0.9695124880862922, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3431547327443468, "meta-llama/Meta-Llama-3-8B": 0.522019016872571, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9300448609055838}}, {"question": "\u82e5\u60f3\u89e3\u51b3\u4e00\u4e2a\u4e2d\u7b49\u96be\u5ea6\u7684\u95ee\u9898\uff0c\u6240\u9700\u8981\u7684\u52a8\u673a\u5f3a\u5ea6\u6700\u597d\u662f\nA. \u96f6\nB. \u4e2d\u7b49\nC. \u5f88\u4f4e\nD. \u5f88\u9ad8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6436392141967794, "meta-math/MetaMath-Mistral-7B": 0.9783514770124453, "itpossible/Chinese-Mistral-7B-v0.1": 0.4258861893623922, "HuggingFaceH4/zephyr-7b-beta": 0.9891340487213052, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8129427042093389, "meta-llama/Meta-Llama-3-8B": 0.5859786989973701, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9561118426128652}}, {"question": "\u5df2\u77e5\u9664\u6570\u662f132\uff0c\u5546\u662f0.001\uff0c\u4f59\u6570\u662f0.1\uff0c\u88ab\u9664\u6570\u662f\nA. 0.133\nB. 1.33\nC. 0.232\nD. 1.3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34828870176887816, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2821833983601387, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1979\u5e74\u4ee5\u6765\uff0c\u56fd\u5bb6\u5df2\u7ec4\u7ec7\u5317\u4eac\u652f\u63f4\u5185\u8499\u53e4\u3001\u6cb3\u5317\u652f\u63f4\u8d35\u5dde\u3001\u6c5f\u82cf\u652f\u63f4\u5e7f\u897f\u548c\u65b0\u7586\uff0c\u5c71\u4e1c\u652f\u63f4\u9752\u6d77\uff0c\u5929\u6d25\u652f\u63f4\u7518\u8083\uff0c\u4e0a\u6d77\u652f\u63f4\u4e91\u5357\u548c\u5b81\u590f\uff0c\u5168\u56fd\u652f\u63f4\u897f\u85cf\u3002\u540e\u53c8\u786e\u5b9a\u5728\u575a\u6301\u5168\u56fd\u652f\u63f4\u897f\u85cf\u7684\u540c\u65f6\uff0c\u56db\u5ddd\u3001\u6d59\u6c5f\u3001\u4e0a\u6d77\u3001\u5929\u6d25\u56db\u7701\u5e02\u91cd\u70b9\u652f\u63f4\u897f\u85cf\u3002\u8fd9\u4e00\u884c\u52a8\u53eb\u505a\nA. \u897f\u90e8\u5f00\u53d1\nB. \u624b\u62c9\u624b\u884c\u52a8\nC. \u9633\u5149\u5de5\u7a0b\nD. \u5bf9\u53e3\u652f\u63f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.49602114543322473, "meta-math/MetaMath-Mistral-7B": 0.7530044090045963, "itpossible/Chinese-Mistral-7B-v0.1": 0.886570314203201, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6108968869485741, "meta-llama/Meta-Llama-3-8B": 0.917316264180261, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u533a\u95f4 $[0\uff0c8]$ \u5185\uff0c \u5bf9\u51fd\u6570 $f(x)=\\sqrt[3]{8 x-x^2}$\uff0c \u7f57\u5c14\u5b9a\u7406\uff08\uff09.\nA. \u6210\u7acb\uff0c \u5e76\u4e14 $f^{\\prime}(2)=0$\nB. \u6210\u7acb\uff0c \u5e76\u4e14 $f^{\\prime}(8)=0$\nC. \u4e0d\u6210\u7acb\nD. \u6210\u7acb\uff0c \u5e76\u4e14 $f^{\\prime}(4)=0$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7636824496764194, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u9634\u9633\u5931\u8c03\u75c5\u673a\u4e2d\uff0c\u9633\u504f\u8870\u4ee5\u54ea\u810f\u4e3a\u6839\u672c\nA. \u813e\nB. \u5fc3\nC. \u80be\nD. \u809d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4313109487035697, "meta-math/MetaMath-Mistral-7B": 0.4548907353686772, "itpossible/Chinese-Mistral-7B-v0.1": 0.5028667080646879, "HuggingFaceH4/zephyr-7b-beta": 0.7552046997098988, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.471326501728242, "meta-llama/Meta-Llama-3-8B": 0.35125369870111733, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5777517929053557}}, {"question": "\u738b\u56fd\u7ef4\u5728\u300a\u4eba\u95f4\u8bcd\u8bdd\u300b\u4e2d\u63d0\u51fa\u4e86\nA. \u5883\u754c\u8bf4\nB. \u98ce\u683c\u8bf4\nC. \u683c\u8c03\u8bf4\nD. \u795e\u97f5\u8bf4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3726147475087035, "meta-math/MetaMath-Mistral-7B": 0.3632122790984035, "itpossible/Chinese-Mistral-7B-v0.1": 0.5124666513977002, "HuggingFaceH4/zephyr-7b-beta": 0.9948099435861415, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7368464887263697, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f20\u8861\u53d1\u660e\u7684\u5730\u52a8\u4eea\u4e0a\u6709\u51e0\u6761\u9f99\nA. \u516b\u6761\nB. \u4e8c\u5341\u4e8c\u6761\nC. \u5341\u4e8c\u6761\nD. \u56db\u6761\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6240\u6709\u6743\u7684\u4f26\u7406\u610f\u4e49\u53cd\u6620\u5728\u5b83\u4e0e\uff08B\uff09\u7684\u5173\u7cfb\u4e4b\u4e2d\nA. \u5e73\u7b49\u6743\nB. \u8d22\u4ea7\u6743\nC. \u6240\u6709\u5236\nD. \u4ea4\u6362\u6743\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.743471653571866, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65e0\u4ea7\u9636\u7ea7\u593a\u53d6\u653f\u6743\u7684\u6839\u672c\u76ee\u7684\u662f\nA. \u5f7b\u5e95\u6253\u788e\u65e7\u7684\u8d44\u4ea7\u9636\u7ea7\u56fd\u5bb6\u673a\u5668 \nB. \u89e3\u653e\u548c\u4fc3\u8fdb\u793e\u4f1a\u751f\u4ea7\u529b\u7684\u53d1\u5c55\nC. \u6539\u53d8\u65e0\u4ea7\u9636\u7ea7\u53ca\u5176\u4ed6\u52b3\u52a8\u7fa4\u4f17\u53d7\u5265\u524a\u3001\u53d7\u538b\u8feb\u7684\u5730\u4f4d \nD. \u5b9e\u73b0\u5171\u4ea7\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5759518962845397, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5P(w)\u8868\u793a\u8bcd\u6761w\u7684\u6982\u7387\uff0c\u5047\u8bbe\u5df2\u77e5P\uff08\u5357\u4eac\uff09=0.8\uff0cP\uff08\u5e02\u957f\uff09=0.6\uff0cP\uff08\u6c5f\u5927\u6865\uff09=0.4\uff1aP\uff08\u5357\u4eac\u5e02\uff09=0.3\uff0cP\uff08\u957f\u6c5f\u5927\u6865\uff09=0.5\uff1a\u5982\u679c\u5047\u8bbe\u524d\u540e\u4e24\u4e2a\u8bcd\u7684\u51fa\u73b0\u662f\u72ec\u7acb\u7684\uff0c\u90a3\u4e48\u5206\u8bcd\u7ed3\u679c\u5c31\u662f\nA. \u5357\u4eac_\u5e02\u957f_\u6c5f\u5927\u6865\nB. \u5357\u4eac\u5e02_\u957f\u6c5f_\u5927\u6865\nC. \u5357\u4eac\u5e02\u957f_\u6c5f\u5927\u6865\nD. \u5357\u4eac\u5e02_\u957f\u6c5f\u5927\u6865\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3460058095032025}}, {"question": "\u515a\u7684\u5341\u516b\u5927\u4ee5\u6765\uff0c\u6211\u56fd\u5728\u91cd\u8981\u5386\u53f2\u8282\u70b9\u5df2\u4f9d\u6cd5\u5b9e\u65bd\u4e24\u6b21\u7279\u8d66\uff0c\u5177\u6709\u91cd\u5927\u610f\u4e49\u3002\u4e0b\u5217\u4e0e\u4e4b\u76f8\u5173\u7684\u8bf4\u6cd5\u9519\u8bef\u7684\u662f\nA. \u7279\u8d66\u987b\u7ecf\u4eba\u6c11\u6cd5\u9662\u88c1\u5b9a\nB. \u7279\u8d66\u5bf9\u8c61\u5305\u62ec\u88ab\u5224\u5904\u76d1\u7981\u5211\u548c\u975e\u76d1\u7981\u5211\u7684\u7f6a\u72af\nC. \u4e24\u6b21\u7279\u8d66\u65f6\u95f4\u5206\u522b\u4e3a2015\u5e74\u548c2019\u5e74\nD. \u7279\u8d66\u4ee4\u7531\u5168\u56fd\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u5e38\u52a1\u59d4\u5458\u4f1a\u7b7e\u53d1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30559070232016367, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u4e2a\u56fd\u5bb6\u7684\u4eba\u5747\u56fd\u6c11\u6536\u5165\u8d85\u8fc7\uff08\uff09\u7f8e\u5143\uff0c\u4fe1\u7528\u4ea4\u6613\u5c06\u4e0d\u53ef\u907f\u514d\u5730\u6210\u4e3a\u5e02\u573a\u7ecf\u6d4e\u7684\u4e3b\u5bfc\u7ecf\u6d4e\u5f62\u5f0f\uff0c\u8fd9\u4e2a\u56fd\u5bb6\u4e5f\u5c06\u4ece\u5e02\u573a\u7ecf\u6d4e\u4f53\u5236\u8f6c\u800c\u5f00\u59cb\u8fdb\u5165\u4fe1\u7528\u7ecf\u6d4e\u4f53\u5236\nA. 2000\nB. 3000\nC. 4000\nD. 1000\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f7f\u7528\u7684\u7535\u6c14\u8bbe\u5907\u6309\u6709\u5173\u5b89\u5168\u89c4\u7a0b\uff0c\u5176\u5916\u58f3\u5e94\u6709\u4ec0\u4e48\u9632\u62a4\u63aa\u65bd\uff1f\nA. \u4fdd\u62a4\u6027\u63a5\u96f6\u6216\u63a5\u5730\nB. \u5305\u80f6\u76ae\nC. \u65e0\nD. \u9632\u9508\u6f06\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7011198380368199, "meta-math/MetaMath-Mistral-7B": 0.9387997889072478, "itpossible/Chinese-Mistral-7B-v0.1": 0.8844096317231098, "HuggingFaceH4/zephyr-7b-beta": 0.9943330388757224, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8654225571015806, "meta-llama/Meta-Llama-3-8B": 0.8184173996102773, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.90862911631248}}, {"question": "\u53d8\u538b\u5668\u4e00\u3001\u4e8c\u6b21\u7ed5\u7ec4\u7684\u531d\u6570\u4e4b\u6bd4\u4e3a25\uff0c\u4e8c\u6b21\u4fa7\u7535\u538b\u4e3a400V\uff0c\u4e00\u6b21\u4fa7\u7535\u538b\u4e3a\nA. 12500V\nB. 35000V\nC. 10000V\nD. 15000V\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34300881932417676, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6329404125024619, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c0f\u65cf\u4e0e\u6c11\u65cf\u7684\u6839\u672c\u533a\u522b\u662f\nA. \u5229\u76ca\u5173\u7cfb\nB. \u7ecf\u6d4e\u5173\u7cfb\nC. \u5730\u57df\u5173\u7cfb\nD. \u8840\u7f18\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6350450559694266, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5055494658709165, "HuggingFaceH4/zephyr-7b-beta": 0.9978733450408392, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6952068294623777, "meta-llama/Meta-Llama-3-8B": 0.7790571021186238, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9822501651194965}}, {"question": "\u5c55\u5f00\u5f0f\u4e2d\uff0cx\u7684\u7cfb\u6570\u4e3a$\\left( x-\\frac{2}{x} \\right)^{5}$\nA. 20\nB. 5\nC. 40\nD. 10\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u827a\u672f\u4f5c\u54c1\u7684\u5b58\u5728\u65b9\u5f0f\u4e3a\u5206\u7c7b\u4f9d\u636e\uff0c\u4e0d\u5c5e\u4e8e\u7a7a\u95f4\u827a\u672f\u7684\u662f\nA. \u96d5\u5851\nB. \u7ed8\u753b\nC. \u5efa\u7b51\nD. \u6587\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8223023342686581, "meta-math/MetaMath-Mistral-7B": 0.9698911854535641, "itpossible/Chinese-Mistral-7B-v0.1": 0.5636743584840147, "HuggingFaceH4/zephyr-7b-beta": 0.9983292098498268, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9539541470580282, "meta-llama/Meta-Llama-3-8B": 0.95221864751208, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8976526424141194}}, {"question": "\u4f53\u80b2\u8bfe\u4e0a\uff0c\u82cf\u8001\u5e08\u53d1\u73b0\u5f20\u521a\u5750\u5728\u64cd\u573a\u8fb9\u53d1\u6123\uff0c\u4fbf\u8be2\u95ee\u60c5\u51b5\uff0c\u5f20\u521a\u8bf4\uff1a\u201c\u6211\u6700\u597d\u7684\u670b\u53cb\u8d70\u4e86\uff0c\u6211\u5f88\u96be\u8fc7!\u201d\u82cf\u8001\u5e08\u4ece\u6b64\u6ce8\u610f\u89c2\u5bdf\u5f20\u521a\uff0c\u8ddf\u4ed6\u804a\u5929\u3002\u6709\u4e00\u5929\uff0c\u5f20\u521a\u54ed\u7740\u544a\u8bc9\u82cf\u8001\u5e08\uff1a\u201c\u6211 \u6700\u597d\u7684\u670b\u53cb\u5c31\u662f\u6211\u7238\u7238\uff0c\u4ed6\u51fa\u8f66\u7978\u53bb\u4e16\u4e86!\u201d\u4e8e\u662f\uff0c\u82cf\u8001\u5e08\u7ecf\u5e38\u5f00\u5bfc\u4ed6\uff0c\u5e2e\u4ed6\u4ece\u60b2\u4f24\u4e2d\u8d70\u4e86 \u51fa\u6765\u3002\u5bf9\u4e8e\u82cf\u8001\u5e08\u7684\u7684\u884c\u4e3a\uff0c\u4ee5\u4e0b\u8bf4\u6cd5\u6b63\u786e\u7684\u9009\u9879\u662f\nA. \u4e13\u5fc3\u6559\u5b66\uff0c\u4e0d\u5fc5\u627f\u62c5\u5176\u4ed6\u8d23\u4efb\nB. \u7ec6\u5fc3\u89c2\u5bdf\uff0c\u9002\u65f6\u6355\u6349\u6559\u81ea\u5951\u673a\nC. \u4fb5\u72af\u9690\u79c1\uff0c\u5e72\u6270\u5b66\u751f\u79c1\u4eba\u751f\u6d3b\nD. \u504f\u7231\u5f20\u521a\uff0c\u672a\u80fd\u5173\u6ce8\u5176\u4ed6\u5b66\u751f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9607042220390525, "meta-math/MetaMath-Mistral-7B": 0.9950715256382349, "itpossible/Chinese-Mistral-7B-v0.1": 0.9246688877202395, "HuggingFaceH4/zephyr-7b-beta": 0.9999691080785865, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9950636334806613, "meta-llama/Meta-Llama-3-8B": 0.9712845987770516, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9992376778426936}}, {"question": "\u201c\u521d\u51fa\u8305\u5e90\u201d\u4e2d\u7684\u201c\u8305\u5e90\u201d\u672c\u610f\u662f\u6307\u8c01\u7684\u7684\u4f4f\u5904\nA. \u8bf8\u845b\u4eae\nB. \u53f8\u9a6c\u8fc1\nC. \u5218\u5907\nD. \u53f8\u9a6c\u5149\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3395893495876965, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4394042117234178, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.443917475109951}}, {"question": "\u4e0b\u9762\u4e0d\u662f\u4e09\u56fd\u6545\u4e8b\u7684\u662f\nA. \u8349\u8239\u501f\u7bad\nB. \u4e09\u987e\u8305\u5e90\nC. \u8d1f\u8346\u8bf7\u7f6a\nD. \u8d64\u58c1\u5927\u6218\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3667640768175962, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u5207\u8fdd\u53cd\u65e2\u5b9a\u7684\u751f\u6d3b\u65b9\u5f0f\u3001\u98ce\u4fd7\u4e60\u60ef\u7684\u884c\u4e3a\uff0c\u90fd\u88ab\u79f0\u4e3a\nA. \u8fdd\u4f8b\u884c\u4e3a\nB. \u8fdd\u7ae0\u884c\u4e3a\nC. \u8fdd\u6cd5\u884c\u4e3a\nD. \u8fdd\u89c4\u884c\u4e3a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7867535899703708, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6837774182769838}}, {"question": "\u4ee5\u4e0b\u56db\u4e2a\u6d41\u661f\u96e8\u4e2d\uff0c2022\u5e74\u6781\u5927\u5f53\u5929\u53d7\u6708\u5149\u5e72\u6270\u6700\u4e25\u91cd\u7684\u662f\nA. \u82f1\u4ed9\u5ea7\u6d41\u661f\u96e8\nB. \u53cc\u5b50\u5ea7\u6d41\u661f\u96e8\nC. \u5b9d\u74f6\u5ea7\u03b7\u6d41\u661f\u96e8\nD. \u8c61\u9650\u4eea\u5ea7\u6d41\u661f\u96e8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.264634220591857, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.29863342676099575, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.2980747244459593}}, {"question": "\u4e0b\u5217\u54ea\u7ec4\u6210\u8bed\u53cd\u6620\u4e86\u540c\u4e00\u79cd\u4eba\u9645\u5173\u7cfb\nA. \u76f8\u6fe1\u4ee5\u6cab\u8210\u728a\u60c5\u6df1\nB. \u7ed3\u8349\u8854\u73af\u9752\u6885\u7af9\u9a6c\nC. \u8d1f\u8346\u8bf7\u7f6a\u8f7d\u821f\u8986\u821f\nD. \u7434\u745f\u548c\u9e23\u7834\u955c\u91cd\u5706\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u2f64\u6027\u8d28\u63cf\u8ff0\u6cd5\u8868\u793a\u7b2c\u2f00\u8c61\u9650\u7684\u6240\u6709\u70b9\u7684\u96c6\u5408\nA. {(x,y)|x<0.y<0} \nB. {(x,y)|x>0,Y<0}\nC. {(x,y)|x>0,y>0}\nD. {(x,y)|x<0,y>0}\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4774821464452964, "meta-math/MetaMath-Mistral-7B": 0.6177599327739317, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7156968499104317, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6797\u8001\u5b9e\u5185\u5411\uff0c\u8c26\u865a\u52e4\u594b\uff0c\u4e14\u5177\u6709\u4eb2\u548c\u529b\uff0c\u8fd9\u4e9b\u63cf\u8ff0\u8bf4\u660e\nA. \u80fd\u529b\nB. \u6c14\u8d28\nC. \u6027\u683c\u7279\u5f81\nD. \u8ba4\u77e5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8286325165659498, "meta-math/MetaMath-Mistral-7B": 0.766513663432252, "itpossible/Chinese-Mistral-7B-v0.1": 0.6642650864609898, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6400632887173385, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6222507073463187}}, {"question": "\u6cd5\u5f8b\u53d1\u5c55\u7684\u4e00\u822c\u89c4\u5f8b\u662f\nA. \u4e0d\u65ad\u8fdb\u6b65\u7684\u89c4\u5f8b\nB. \u4e0e\u5b97\u6559\u3001\u9053\u5fb7\u4ece\u878d\u5408\u5230\u5206\u5316\u7684\u89c4\u5f8b\nC. \u4e0e\u56fd\u5bb6\u540c\u6b65\u4ea7\u751f\u7684\u89c4\u5f8b\nD. \u4f34\u968f\u751f\u4ea7\u529b\u53d1\u5c55\u8fdb\u7a0b\u7684\u89c4\u5f8b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5316\u5b66\u4e0e\u751f\u4ea7\u3001\u751f\u6d3b\u5bc6\u5207\u76f8\u5173\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u660e\u77fe\u4f5c\u4e3a\u51c0\u6c34\u5242\uff0c\u53ef\u4ee5\u9664\u53bb\u5e9f\u6c34\u4e2d\u7684\u94dc\u79bb\u5b50\nB. \u7130\u706b\u7684\u4e94\u5f69\u7f24\u7eb7\u662f\u67d0\u4e9b\u91d1\u5c5e\u5143\u7d20\u5316\u5b66\u6027\u8d28\u7684\u5c55\u73b0\nC. \u8fc7\u6c27\u5316\u94a0\u53ef\u7528\u4f5c\u547c\u5438\u9762\u5177\u4e2d\u7684\u4f9b\u6c27\u5242\nD. \u7528\u6d3b\u6027\u70ad\u4e3a\u7cd6\u8131\u8272\u548c\u7528\u81ed\u6c27\u6f02\u767d\u7eb8\u6d46\u7684\u539f\u7406\u76f8\u4f3c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28141066483141647, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3148300531811561, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6559\u80b2\u65e2\u6709\u57f9\u517b\u521b\u65b0\u7cbe\u795e\u7684\u80fd\u529b\uff0c\u4e5f\u6709\u627c\u6740\u521b\u65b0\u7cbe\u795e\u7684\u529b\u91cf\u3002\u4ece\u4f5c\u7528\u7684\u65b9\u5411\u770b\uff0c\u6b64\u89c2\u70b9\u53cd\u6620\u4e86\u6559\u80b2\u5177\u6709 ()\nA. \u663e\u6027\u529f\u80fd\u4e0e\u9690\u6027\u529f\u80fd\nB. \u793e\u4f1a\u529f\u80fd\u4e0e\u4e2a\u4f53\u529f\u80fd\nC. \u6b63\u5411\u529f\u80fd\u4e0e\u8d1f\u5411\u529f\u80fd\nD. \u653f\u6cbb\u529f\u80fd\u4e0e\u7ecf\u6d4e\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5793300601772163, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8479857818923178, "HuggingFaceH4/zephyr-7b-beta": 0.9029251289922122, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6120843716955251, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8481593193645116}}, {"question": "\u5e02\u573a\u8425\u9500\u6d3b\u52a8\u7684\u63a7\u5236\u7684\u7b2c\u4e00\u6b65\u662f\nA. \u91c7\u53d6\u884c\u52a8\nB. \u8fdb\u884c\u6bd4\u8f83\nC. \u76d1\u6d4b\u5b9e\u7ee9\nD. \u5efa\u7acb\u76ee\u6807\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9033333596464627, "meta-math/MetaMath-Mistral-7B": 0.9932098168820477, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9996053658711925, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9763940235880355, "meta-llama/Meta-Llama-3-8B": 0.9703546721045219, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9943130091079466}}, {"question": "\u4e0b\u5217\u53ef\u80fd\u4e0d\u7b26\u5408\u5546\u4e1a\u4f26\u7406\u89c4\u8303\u7684\u662f\nA. \u4f01\u4e1a\u5728\u751f\u4ea7\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u5065\u5eb7\u3001\u5b89\u5168\u7684\u5de5\u4f5c\u73af\u5883\nB. \u540c\u80a1\u4e0d\u540c\u6743\nC. \u4f01\u4e1a\u7981\u6b62\u4ece\u4e0d\u5c0a\u91cd\u4eba\u6743\u3001\u65e0\u4eba\u6027\u9053\u4e49\u7684\u4f01\u4e1a\u91c7\u8d2d\u7269\u8d44\nD. \u5458\u5de5\u52aa\u529b\u5de5\u4f5c\u4ee5\u8fbe\u6210\u516c\u53f8\u76ee\u6807\u5e76\u83b7\u5f97\u85aa\u916c\u4e0e\u804c\u4e1a\u5347\u8fc1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3361950722028681, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u8def\u4e0a\u9047\u89c1\u5c0f\u65f6\u5019\u7684\u540c\u4f34\uff0c\u867d\u7136\u53eb\u4e0d\u51fa\u4ed6(\u5979)\u7684\u59d3\u540d\uff0c\u4f46\u786e\u8ba4\u662f\u8ba4\u8bc6\u7684\uff0c\u6b64\u65f6\u7684\u5fc3\u7406\u6d3b\u52a8\u662f\nA. \u4fdd\u6301\nB. \u518d\u8ba4\nC. \u8bc6\u8bb0\nD. \u56de\u5fc6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.48890764282820565, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5509855218536812, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u5a74\u513f\u667a\u529b\u53d1\u80b2\u6709\u4fc3\u8fdb\u4f5c\u7528\u7684\u8102\u80aa\u9178\u4e3a\nA. n-6\u7cfb\u5217\u7684\u82b1\u751f\u56db\u70ef\u9178\u548cDHA\u3001n-3\u7cfb\u5217\u7684\u03b3\u4e00\u4e9a\u9ebb\u9178\u548cDHA\nB. n-6\u7cfb\u5217\u7684\u82b1\u751f\u56db\u70ef\u9178\u3001DHA\u548cEPA\u3001n-3\u7cfb\u5217\u7684\u03b3\u4e00\u4e9a\u9ebb\u9178\nC. n-6\u7cfb\u5217\u7684\u03b3-\u4e9a\u9ebb\u9178\u548c\u82b1\u751f\u56db\u70ef\u9178\u3001n-3\u7cfb\u5217\u7684EPA\u548cDHA\nD. n-6\u7cfb\u5217\u7684\u03b3-\u4e9a\u9ebb\u9178\u548cEPA\u3001n-3\u7cfb\u5217\u7684\u82b1\u751f\u56db\u70ef\u9178\u548cDHA\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u56fd\u5bb6\u5728\u5211\u6cd5\u4e2d\u901a\u8fc7\u7f6a\u5211\u6cd5\u5b9a\u539f\u5219\u7684\u4f53\u73b0\uff0c\u60e9\u7f5a\u4e86\u72af\u7f6a\u4eba\uff0c\u540c\u65f6\u5bf9\u793e\u4f1a\u4e0a\u7684\u5176\u4ed6\u4eba\u4e5f\u4ea7\u751f\u4e00\u5b9a\u7684\u5a01\u6151\u548c\u6559\u80b2\u4f5c\u7528\uff0c\u8fd9\u91cc\u4f53\u73b0\u4e86\u6cd5\u7684\u4e24\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u8fd9\u4e24\u79cd\u4f5c\u7528\u7684\u5173\u7cfb\u662f\nA. \u6cd5\u7684\u7279\u522b\u9884\u9632\u4f5c\u7528\u662f\u76ee\u7684\nB. \u6cd5\u7684\u89c4\u8303\u4f5c\u7528\u548c\u6cd5\u7684\u793e\u4f1a\u4f5c\u7528\u662f\u76ee\u6807\u548c\u76ee\u7684\u7684\u5173\u7cfb\nC. \u6cd5\u7684\u89c4\u8303\u4f5c\u7528\u662f\u76ee\u7684\nD. \u6cd5\u7684\u89c4\u8303\u4f5c\u7528\u548c\u6cd5\u7684\u793e\u4f1a\u4f5c\u7528\u662f\u624b\u6bb5\u548c\u76ee\u7684\u7684\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4016548170048367, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.45342955400923973, "HuggingFaceH4/zephyr-7b-beta": 0.9902369099008022, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.431033945941338, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6693532060489572}}, {"question": "\u4efb\u4f55\u4e2a\u4eba\u548c\u7ec4\u7ec7\u8fdd\u53cd\u56fd\u5bb6\u5b89\u5168\u6cd5\u548c\u6709\u5173\u6cd5\u5f8b\uff0c()\u6216\u8005\u4ece\u4e8b\u5371\u5bb3\u56fd\u5bb6\u5b89\u5168\u6d3b\u52a8\u7684\uff0c\u4f9d\u6cd5\u8ffd\u7a76\u6cd5\u5f8b\u8d23\u4efb\u3002\nA. \u4e0d\u4f5c\u4e3a\nB. \u6ee5\u7528\u804c\u6781\nC. \u73a9\u5ffd\u804c\u5b88\nD. \u4e0d\u5c65\u884c\u7ef4\u62a4\u56fd\u5bb6\u5b89\u5168\u4e49\u52a1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6465667270393667, "meta-math/MetaMath-Mistral-7B": 0.8683521719815113, "itpossible/Chinese-Mistral-7B-v0.1": 0.651919770608008, "HuggingFaceH4/zephyr-7b-beta": 0.9686560346812125, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.672568314358117, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f9d\u636e\u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u6559\u5e08\u6cd5\u300b\uff0c\u4ee5\u4e0b\u60c5\u5f62\u4e2d\uff0c\u5b66\u6821\u4e0d\u80fd\u7ed9\u4e88\u6559\u5e08\u884c\u653f\u5904\u5206\u6216\u8005\u89e3\u8058\u7684\u662f\nA. \u7a7f\u6234\u4e0d\u6574\uff0c\u5f71\u54cd\u4eea\u8868\u7684\nB. \u4fae\u8fb1\u5b66\u751f\uff0c\u5f71\u54cd\u6076\u52a3\u7684\nC. \u6545\u610f\u65f7\u8bfe\uff0c\u635f\u5bb3\u6559\u5b66\u7684\nD. \u4f53\u7f5a\u5b66\u751f\uff0c\u5c61\u72af\u4e0d\u6539\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.565631170079749, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.28108825044289903, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u963f\u53f8\u5339\u6797\u901a\u8fc7\u51cf\u5c11TXA\u2082\u5408\u6210\u800c\u6297\u8840\u5c0f\u677f\u805a\u96c6\u7684\u4f5c\u7528\u73af\u8282\u662f\nA. \u6291\u5236 PGI\u2082\u5408\u6210\u9176\nB. \u6291\u5236 TXA\u2082\u5408\u6210\u9176\nC. \u6291\u5236 PLA\u2082\nD. \u6291\u5236 COX\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6474756698265156, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u5806\u7164\u91cd2500\u5428\uff0c\u70e7\u9505\u7089\u6bcf\u5929\u8981\u7528120\u5428\uff0c\u6bcf\u5929\u9700\u7528\nA. 48%\nB. 12%\nC. 4.80%\nD. 2.40%\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3834538393386079, "meta-math/MetaMath-Mistral-7B": 0.4392175765597299, "itpossible/Chinese-Mistral-7B-v0.1": 0.31649016643535544, "HuggingFaceH4/zephyr-7b-beta": 0.5076252058540246, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u6559\u5b66\u7528\u8bed\u4e2d\uff0c\u4e0d\u5229\u4e8e\u4fc3\u8fdb\u5b66\u751f\u5b66\u4e60\u7684\u662f\nA. \u201c\u521a\u521a\u8fd9\u4f4d\u540c\u5b66\u6982\u62ec\u5f97\u4e0d\u51c6\u786e\uff0c\u8fd8\u662f\u6211\u6765\u5427\u3002\u201d\nB. \u201c\u4f60\u8bfb\u5f97\u5f88\u54cd\u4eae\uff0c\u5047\u8bbe\u518d\u6709\u611f\u60c5\u4e00\u70b9\u5c31\u597d\u4e86\u3002\u4f60\u518d\u8bd5\u8bd5\u3002\u201d\nC. \u201c\u8bf7\u5927\u5bb6\u60f3\u4e00\u60f3\uff0c\u521a\u521a\u8fd9\u4e24\u4f4d\u540c\u5b66\u62a5\u544a\u7684\u7ed3\u8bba\uff0c\u6709\u4f55\u4e0d\u540c?\u201d\nD. \u201c\u8fd9\u4f4d\u540c\u5b66\u7684\u53d1\u8a00\u5e76\u6ca1\u6709\u91cd\u590d\u524d\u9762\u540c\u5b66\u8bf4\u8fc7\u7684\u8bdd\uff0c\u6709\u81ea\u5df1\u7684\u89c2\u70b9\uff0c\u975e\u5e38\u597d!\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.45415666279006855, "meta-math/MetaMath-Mistral-7B": 0.576240475725757, "itpossible/Chinese-Mistral-7B-v0.1": 0.4487220581459842, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6915463021075289, "meta-llama/Meta-Llama-3-8B": 0.5722792310706193, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9328357354537975}}, {"question": "\u8861\u91cf\u4eba\u4f53\u6c27\u7cfb\u7edf\u529f\u80fd\u7684\u5f3a\u5f31\u9664\u4e86\u53ef\u7528\u547c\u5438\u7cfb\u7edf\u6216\u5fc3\u8840\u7ba1\u7cfb\u7edf\u7684\u4e00\u4e9b\u6307\u6807\u5916\uff0c\u5e38\u7528\u7684\u8861\u91cf\u6c27\u8fd0\u8f93\u7cfb\u7edf\u6574\u4f53\u529f\u80fd\u7684\u7efc\u5408\u6027\u6307\u6807\u5c31\u662f\nA. \u80ba\u901a\u6c14\u91cf\nB. \u6700\u5927\u5438\u6c27\u91cf\nC. \u80ba\u6ce1\u901a\u6c14\u91cf\nD. \u80ba\u8840\u6d41\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8038181331304494, "meta-math/MetaMath-Mistral-7B": 0.9842247530014134, "itpossible/Chinese-Mistral-7B-v0.1": 0.5226734177856894, "HuggingFaceH4/zephyr-7b-beta": 0.9587894103425255, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9485564210521881, "meta-llama/Meta-Llama-3-8B": 0.9079511173273378, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5233757923314843}}, {"question": "\u6c47\u603b\u4ed8\u6b3e\u51ed\u8bc1\u7684\u8d37\u65b9\u79d1\u76ee\u662f\nA. \u5e94\u4ed8\u8d26\u6b3e\nB. \u5b9e\u6536\u8d44\u672c\nC. \u7ba1\u7406\u8d39\u7528\nD. \u94f6\u884c\u5b58\u6b3e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4801919940680461, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7535\u89c6\u5267\u300a\u4eac\u534e\u70df\u4e91\u300b\u6839\u636e\u540c\u540d\u5c0f\u8bf4\u6539\u7f16\uff0c\u539f\u4f5c\u4f5c\u8005\u662f\nA. \u6c88\u4ece\u6587\nB. \u6797\u8bed\u5802\nC. \u5468\u4f5c\u4eba\nD. \u8001\u820d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3300364758848944, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u4e0d\u7b26\u5408\u919b\u56fa\u916e\u7624\u68c0\u67e5\u7ed3\u679c\u7684\u662f\nA. \u5c3f\u5448\u9178\u6027\nB. \u7cd6\u8010\u91cf\u51cf\u4f4e\nC. \u8840 HCO3-\u589e\u9ad8\nD. \u8840\u80be\u7d20\u51cf\u4f4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3993599025515597, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4652274139520013}}, {"question": "\u300a\u767e\u5bb6\u59d3\u300b\u4e2d\u6ca1\u6709\u4e0b\u9762\u54ea\u4e2a\u59d3\nA. \u5deb\nB. \u8096\nC. \u4e4c\nD. \u8427\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u56db\u7ec4\u6570\u4e2d\u5c5e\u4e8e\u6b63\u786eIP\u5730\u5740\u7684\u662f\nA. 10.10.10.10.10\nB. 10.1.100.1000\nC. 192.168.0.1\nD. 202.96.104\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8902448771720979, "meta-math/MetaMath-Mistral-7B": 0.9950876355051478, "itpossible/Chinese-Mistral-7B-v0.1": 0.6127031561492603, "HuggingFaceH4/zephyr-7b-beta": 0.9997653143355149, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9720842328721858, "meta-llama/Meta-Llama-3-8B": 0.9084807445845128, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9262267583531961}}, {"question": "\u4e58\u8f66\u3001\u767b\u673a\u3001\u5750\u8239\u65f6\u4e3b\u52a8\u8d2d\u7968\u3001\u81ea\u89c9\u6392\u961f\uff1b\u51fa\u884c\u65f6\u81ea\u89c9\u9075\u5b88\u4ea4\u901a\u89c4\u5219\uff0c\u4e0d\u95ef\u7ea2\u706f\uff1b\u6e38\u89c8\u89c2\u5149\u3001\u8d2d\u7269\u3001\u63d0\u6b3e\u65f6\u6309\u5148\u540e\u987a\u5e8f\uff0c\u4e0d\u63d2\u961f\u7b49\u3002\u8fd9\u662f\u793e\u4f1a\u516c\u5fb7\u4e2d\nA. \u4fdd\u62a4\u73af\u5883\u7684\u8981\u6c42\nB. \u52a9\u4eba\u4e3a\u4e50\u7684\u8981\u6c42\nC. \u6587\u660e\u793c\u8c8c\u7684\u8981\u6c42\nD. \u7231\u62a4\u516c\u7269\u7684\u8981\u6c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9767370357076813, "meta-math/MetaMath-Mistral-7B": 0.9984298050912558, "itpossible/Chinese-Mistral-7B-v0.1": 0.9599155617409973, "HuggingFaceH4/zephyr-7b-beta": 0.9997291716683254, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9962248574705707, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8788636368957629}}, {"question": "\u4fe1\u53f7\u5f39\u767d\u5929\u4e5f\u53ef\u4ee5\u53d1\u5c04\uff0c\u53ea\u662f\u8981\u7528\u4ec0\u4e48\u6837\u7684\u4fe1\u53f7\u989c\u8272\nA. \u7eff\u8272\nB. \u7ea2\u8272\nC. \u9ec4\u8272\nD. \u767d\u8272\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.308176776288574, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f5b\u9640\u5f1f\u5b50\u4e2d\uff0c\u591a\u95fb\u7b2c\u4e00\u7684\u662f\nA. \u7f57\u777a\u7f57\nB. \u963f\u96be\u9640\nC. \u5bcc\u697c\u90a3\nD. \u963f\u96be\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4160513129390438, "HuggingFaceH4/zephyr-7b-beta": 0.424345987471132, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e09\u86c7\u9f99\u864e\u4f1a\u3001\u9c7c\u9999\u8089\u4e1d\u3001\u751f\u7198\u9c7c\u7247\u5206\u522b\u5c5e\u4e8e\nA. \u5e7f\u4e1c\u83dc\u3001\u56db\u5ddd\u83dc\u3001\u6e56\u5357\u83dc\nB. \u6e56\u5357\u83dc\u3001\u5c71\u4e1c\u83dc\u3001\u56db\u5ddd\u83dc\nC. \u5e7f\u4e1c\u83dc\u3001\u6e56\u5357\u83dc\u3001\u56db\u5ddd\u83dc\nD. \u56db\u5ddd\u83dc\u3001\u5e7f\u4e1c\u83dc\u3001\u5c71\u4e1c\u83dc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67e0\u6aac\u6c41\u6709\u54ea\u4e9b\u8425\u517b\u542b\u91cf\nA. \u7ef4\u751f\u7d20B6\nB. \u7ef4\u751f\u7d20A\u548c\u7ef4\u751f\u7d20C\nC. \u7ef4\u751f\u7d20C\nD. \u7ef4\u751f\u7d20B1\u548c\u7ef4\u751f\u7d20C\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.36626654273341464, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.54836603913449}}, {"question": "\u8179\u90e8\u95ed\u5408\u6027\u635f\u4f24\u624b\u672f\u63a2\u67e5\u65f6\u53d1\u73b0\u6a2a\u7ed3\u80a0\u7cfb\u819c\u6839\u90e8\u6709\u5f88\u591a\u6c14\u6ce1\uff0c\u5e94\u9ad8\u5ea6\u6000\u7591\u7684\u521b\u4f24\u662f\nA. \u6a2a\u7ed3\u80a0\u7834\u88c2\nB. \u5341\u4e8c\u6307\u80a0\u7834\u88c2\nC. \u7a7a\u80a0\u7834\u88c2\nD. \u76f4\u80a0\u7834\u88c2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6e29\u548c\u566c\u83cc\u4f53\u7684\u6838\u9178\u7c7b\u578b\u90fd\u662f\nA. ssRNA\nB. ssDNA\nC. dsDNA\nD. dsRNA\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4753669012836654, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4876345176455926}}, {"question": "\u4ee5\u4e0b\u54ea\u4e2a\u4e0d\u662f\u57fa\u7763\u6559\u7684\u4e09\u5927\u4fe1\u8a93\nA. \u81ea\u7531\nB. \u8d1e\u6d01\nC. \u987a\u4ece\nD. \u8d2b\u7a77\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.35286194820709277, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9567799477689378}}, {"question": "\u5ba3\u5e03\u6b27\u76df\u548c\u5730\u4e2d\u6d77\u6cbf\u5cb8\u56fd\u5bb6\u4e4b\u95f4\u5efa\u7acb\u5168\u9762\u4f19\u4f34\u5173\u7cfb\u7684\u6587\u4ef6\u662f\nA. \u300a\u52a0\u5f3a\u6b27\u6d32\u8054\u76df\u7684\u5730\u4e2d\u6d77\u653f\u7b56\u300b\nB. \u300a\u8d70\u5411\u5730\u4e2d\u6d77\u7684\u65b0\u6218\u7565\u300b\nC. \u300a\u5173\u4e8e\u5efa\u7acb\u6b27\u76df\u548c\u5730\u4e2d\u6d77\u5168\u9762\u4f19\u4f34\u5173\u7cfb\u7684\u51b3\u5b9a\u300b\nD. \u300a\u5df4\u585e\u7f57\u90a3\u6761\u7ea6\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u571f\u57fa\u73b0\u573aCBR\u503c\u6d4b\u5b9a\u7ed3\u679c\u4e0e\u5ba4\u5185\u8bd5\u9a8cCBR\u503c\u76f8\u6bd4\nA. \u4e24\u8005\u76f8\u7b49\nB. \u73b0\u573a\u6d4b\u5b9a\u503c\u5927\nC. \u73b0\u573a\u6d4b\u5b9a\u503c\u5c0f\nD. \u9700\u901a\u8fc7\u5bf9\u6bd4\u8bd5\u9a8c\u5efa\u7acb\u6362\u7b97\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7260299751191281, "HuggingFaceH4/zephyr-7b-beta": 0.9988193717465016, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6422406367713517, "meta-llama/Meta-Llama-3-8B": 0.6555481008165538, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9469035730743032}}, {"question": "\u67d0\u996e\u6599\u751f\u4ea7\u4f01\u4e1a\u901a\u8fc7\u5728\u8d85\u5e02\u5929\u82b1\u677f\u4e0a\u5b89\u88c5\u6444\u50cf\u673a\uff0c\u8ffd\u8e2a\u987e\u5ba2\u7684\u8d2d\u7269\u8fc7\u7a0b\uff0c\u636e\u6b64\u91cd\u65b0\u9648\u5217\u4ea7\u54c1\uff0c\u4ee5\u4fbf\u987e\u5ba2\u9009\u8d2d\uff0c\u8fd9\u79cd\u641c\u96c6\u8d44\u6599\u7684\u65b9\u6cd5\u5c5e\u4e8e\nA. \u5b9e\u9a8c\u6cd5\nB. \u8c03\u67e5\u6cd5\nC. \u89c2\u5bdf\u6cd5\nD. \u4e13\u5bb6\u4f30\u8ba1\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9669935410083826, "meta-math/MetaMath-Mistral-7B": 0.9960302947272799, "itpossible/Chinese-Mistral-7B-v0.1": 0.9233950617919076, "HuggingFaceH4/zephyr-7b-beta": 0.9999946271473646, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9796295891600348, "meta-llama/Meta-Llama-3-8B": 0.6986605064917314, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9836835678790754}}, {"question": "\u4e2d\u56fd\u98df\u54c1\u5de5\u4e1a\u4ea7\u503c\u5360\u4e16\u754c\u98df\u54c1\u5de5\u4e1a\u4ea7\u503c\u7684\u6bd4\u4f8b\u662f\nA. 10%\nB. 15%\nC. 30%\nD. 3%\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5521984209865437, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u660e\u77e5\u5356\u6deb\u5973\u8d75\u67d0\u672a\u6ee1 14 \u5468\u5c81\uff0c\u800c\u4e0e\u4e4b\u53d1\u751f\u6027\u4ea4\u6613\uff0c\u7532\u7684\u884c\u4e3a\u6784\u6210\nA. \u6784\u6210\u5f3a\u5978\u7f6a\nB. \u4e0d\u6784\u6210\u72af\u7f6a\nC. \u6784\u6210\u7325\u4eb5\u513f\u7ae5\u7f6a\nD. \u6784\u6210\u5ad6\u5bbf\u5e7c\u5973\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7718681363830373, "HuggingFaceH4/zephyr-7b-beta": 0.6449400888636659, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5132583971902459, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6156401223505028}}, {"question": "\u7cfb\u7edf\u5730\u63d0\u51fa\u4e86\u52a8\u673a\u5f52\u56e0\u7406\u8bba\u7684\u5fc3\u7406\u5b66\u5bb6\u662f\nA. \u6d77\u5fb7\nB. \u534e\u751f\nC. \u8d6b\u5c14\nD. \u97e6\u7eb3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30355678841583494, "meta-math/MetaMath-Mistral-7B": 0.41331497534175965, "itpossible/Chinese-Mistral-7B-v0.1": 0.744102910693248, "HuggingFaceH4/zephyr-7b-beta": 0.44228785022626105, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5174187582539611, "meta-llama/Meta-Llama-3-8B": 0.49077800678506794, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6118893754922848}}, {"question": "\u300a\u56db\u4e16\u540c\u5802\u300b\u91c7\u7528\u7684\u7ed3\u6784\u65b9\u5f0f\u662f\nA. \u51b0\u7cd6\u846b\u82a6\u5f0f\nB. \u957f\u6cb3\u5954\u6d41\u5f0f\nC. \u53cc\u7ebf\u4ea4\u53c9\u5f0f\nD. \u5e76\u5217\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ece\u7cfb\u7edf\u7684\u5f00\u653e\u6027\u4e0a\u770b\uff0c\u519c\u4e1a\u751f\u6001\u7cfb\u7edf\u5c5e\u4e8e\u4ec0\u4e48\u7cfb\u7edf\nA. \u5f00\u653e\u6027\u4e0d\u786e\u5b9a(\u968f\u65f6\u95f4\u6216\u7a7a\u95f4\u4e0d\u65ad\u53d8\u5316)\nB. \u5f00\u653e\u6027\nC. \u5c01\u95ed\u6027\nD. \u534a\u5f00\u653e\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5229\u76ca\u7fa4\u4f53\u53ef\u4ee5\u5206\u4e3a\u673a\u6784\u6027\u5229\u76ca\u7fa4\u4f53\u3001\u793e\u56e2\u6027\u5229\u76ca\u7fa4\u4f53\u548c\u81ea\u7ec4\u6027\u5229\u76ca\u7fa4\u4f53\u4e09\u7c7b\uff0c\u8fd9\u79cd\u5206\u7c7b\u662f\u6839\u636e\nA. \u7fa4\u4f53\u76ee\u6807\u7279\u70b9\nB. \u7fa4\u4f53\u529f\u80fd\nC. \u7fa4\u4f53\u7ed3\u6784\nD. \u7fa4\u4f53\u7ec4\u6210\u5f62\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4911975452041169, "meta-math/MetaMath-Mistral-7B": 0.3301226516819503, "itpossible/Chinese-Mistral-7B-v0.1": 0.4740698349299186, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d6b\u8328\u4f2f\u683c\u7684\u53cc\u56e0\u7d20\u7406\u8bba\u8ba4\u4e3a\uff0c\u63d0\u9ad8\u52b3\u52a8\u6548\u7387\u7684\u5173\u952e\u5728\u4e8e\nA. \u6309\u65f6\u53d1\u653e\u5de5\u8d44\nB. \u4f7f\u5de5\u4f5c\u4e30\u5bcc\u5316\nC. \u52a0\u5f3a\u5de5\u4f5c\u76d1\u7763\nD. \u6539\u53d8\u5de5\u4f5c\u73af\u5883\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.45028143477787935, "meta-math/MetaMath-Mistral-7B": 0.8782255571320811, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8975435955996608, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8674511681155992}}, {"question": "\u4e24\u4e2a\u76f8\u540c\u7684\u5355\u6446\u9759\u2f4c\u4e8e\u5e73\u8861\u4f4d\u7f6e\uff0c\u4f7f\u6446\u7403\u5206\u522b\u4ee5\u2f54\u5e73\u521d\u901fV1\u3001V2 \uff08 V1>V2\uff09\u5728\u7ad6\u76f4\u5e73\u2faf\u5185\u505a\u2f29\u2ec6\u5ea6\u6446\u52a8\uff0c\u5b83\u4eec\u7684\u9891\u7387\u4e0e\u632f\u5e45\u5206\u522b\u4e3af1\uff0cf2\u548cA1\uff0cA2\uff0c\u5219\nA. f1>f2\uff0c A1=A2\nB. f1<f2\uff0c A1=A2\nC. f1=f2\uff0c A1>A2\nD. f1=f2\uff0c A1<A2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33459500942041875, "meta-math/MetaMath-Mistral-7B": 0.333183235354062, "itpossible/Chinese-Mistral-7B-v0.1": 0.28966338381871215, "HuggingFaceH4/zephyr-7b-beta": 0.9299631004275096, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.49501294093062753, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u68c0\u6d4b\u9879\u76ee\u4e2d\u5c5e\u4e8e\u6ca5\u9752\u8def\u9762\u7684\u5173\u952e\u9879\u76ee\u7684\u662f\nA. \u5f2f\u6c89\u503c\nB. \u539a\u5ea6\nC. \u6297\u6ed1\nD. \u5e73\u6574\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4175059567018069, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5469986186804172, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.980539026736385}}, {"question": "\u5728\u72b9\u592a\u6559\u4e2d\uff0c\u628a\u5b83\u4eec\u6240\u671f\u76fc\u7684\u6551\u4e3b\u79f0\u4f5c\nA. \u4e0a\u5e1d\nB. \u201c\u53d7\u818f\u8005\u201d\uff0c\u5373\uff08\u5f25\u8d5b\u4e9a\uff09\nC. \u8036\u548c\u534e\nD. \u6469\u897f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9541587711409686, "meta-math/MetaMath-Mistral-7B": 0.9938901914341766, "itpossible/Chinese-Mistral-7B-v0.1": 0.9558060443501156, "HuggingFaceH4/zephyr-7b-beta": 0.9999363489097051, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9840495826392559, "meta-llama/Meta-Llama-3-8B": 0.9936982143386033, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9999554160119262}}, {"question": "\u4eba\u4eec\u9760\u52b3\u52a8\u4ee5\u7ef4\u6301\u751f\u6d3b\u7684\u4e00\u79cd\u793e\u4f1a\u6027\u7684\u4f4d\u7f6e\uff0c\u79f0\u4e3a\nA. \u4e8b\u4e1a\nB. \u804c\u4e1a\nC. \u5bb6\u5ead\nD. \u884c\u4e1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.623888930086531, "meta-math/MetaMath-Mistral-7B": 0.9729702586403141, "itpossible/Chinese-Mistral-7B-v0.1": 0.9737311345499357, "HuggingFaceH4/zephyr-7b-beta": 0.9593057163438702, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.880436592749867, "meta-llama/Meta-Llama-3-8B": 0.8335064208004455, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9691004516401216}}, {"question": "\u9a7e\u9a76\u673a\u52a8\u8f66\u884c\u7ecf\u4e0b\u5217\u54ea\u79cd\u8def\u6bb5\u4e0d\u5f97\u8d85\u8f66\nA. \u9ad8\u67b6\u8def\nB. \u4eba\u884c\u6a2a\u9053\nC. \u73af\u57ce\u9ad8\u901f\nD. \u4e3b\u8981\u8857\u9053\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39008524332734895, "meta-math/MetaMath-Mistral-7B": 0.45151947352219435, "itpossible/Chinese-Mistral-7B-v0.1": 0.8416057349398495, "HuggingFaceH4/zephyr-7b-beta": 0.5258949995676644, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6238727266960318, "meta-llama/Meta-Llama-3-8B": 0.6539544160099244, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9258118333358697}}, {"question": "\u5df2\u77e5\u67d0\u79cd\u8fd0\u8f93\u4fdd\u9669 2010 \u5e74\u7684\u635f\u5931\u989d$X$(\u5355\u4f4d: \u4e07\u5143) \u670d\u4ece\u4f3d\u739b\u5206\u5e03\uff0c\u53c2\u6570$\\alpha=4\uff0c\\theta=0.4$\uff0c\u4ece 2010 \u5e74\u5230 2011 \u5e74\u7684\u7269\u4ef7\u901a\u6da8\u7387\u4e3a8%\uff0c\u52192010\u5e74\uff0c2011\u5e74\u7684\u5e73\u5747\u635f\u5931\u989d\u5206\u522b\u4e3a( )\u3002\nA. 1.728\uff0c1.6\nB. 1.8\uff0c1.6\nC. 1.6\uff0c1.8\nD. 1.6\uff0c1.728\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2986334267609957, "HuggingFaceH4/zephyr-7b-beta": 0.5586499548921052, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.30817677628857404, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u82e5(x+t)(x+1)\u7684\u79ef\u4e2d\u4e0d\u542b\u6709x\u7684\u4e00\u6b21\u9879\uff0c\u5219t\u4e3a\nA. 1\nB. -1\nC. 0\nD. $\\pm 1$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2821833983601388, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679c\u5728\u6821\u5916\u6709\u4eba\u5411\u4f60\u52d2\u7d22\u91d1\u94b1\uff0c\u4e8b\u540e\u4f60\u6700\u5e94\u8be5\u505a\u4ec0\u4e48\nA. \u4ee5\u4e0a\u90fd\u4e0d\u662f\nB. \u4ee5\u540e\u6bcf\u5929\u5e26\u70b9\u94b1\uff0c\u514d\u5f97\u6ca1\u94b1\u6328\u6253\nC. \u4e0d\u80fd\u8ba9\u4efb\u4f55\u4eba\u77e5\u9053\u8fd9\u4ef6\u4e8b\uff0c\u514d\u5f97\u906d\u62a5\u590d\nD. \u5c3d\u5feb\u544a\u8bc9\u7238\u7238\u5988\u5988\u6216\u8001\u5e08\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9404764234869117, "meta-math/MetaMath-Mistral-7B": 0.9494435314248403, "itpossible/Chinese-Mistral-7B-v0.1": 0.9460580534800157, "HuggingFaceH4/zephyr-7b-beta": 0.9625012675893794, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.797848488128246, "meta-llama/Meta-Llama-3-8B": 0.9475250373414907, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9064624143926011}}, {"question": "\u4e00\u4e2a\u5386\u53f2\u4e8b\u5b9e\u53ef\u7531\u82e5\u5e72\u4e8b\u4ef6\u6784\u6210\uff0c\u4e8b\u4ef6\u53c8\u53ef\u7531\u82e5\u5e72\u4e2a\u5c0f\u4e8b\u4ef6\u6784\u6210\uff0c\u7531\u6b64\u53ef\u4ee5\u6392\u51fa\u4e00\u7cfb\u5217\u4e8b\u4ef6\u7684\u7b49\u7ea7\u6216\u5c42\u6b21\u3002\u4e0b\u5217\u9009\u9879\u7b26\u5408\u4e0a\u8ff0\u8bf4\u6cd5\u7684\u662f\nA. \u300a\u738b\u4f4d\u7ee7\u627f\u6cd5\u300b\u2014\u2014\u300a\u6743\u5229\u6cd5\u6848\u300b\u2014\u2014\u201c\u5149\u8363\u9769\u547d\u201d\nB. \u7ecf\u6d4e\u5168\u7403\u5316\u2014\u2014\u897f\u6b27\u4e00\u4f53\u5316\u2014\u2014\u6b27\u5143\u542f\u7528\nC. \u4e2d\u9014\u5c9b\u6218\u5f79\u2014\u2014\u65af\u5927\u6797\u683c\u52d2\u6218\u5f79\u2014\u2014\u963f\u62c9\u66fc\u6218\u5f79\nD. \u65b0\u4e2d\u56fd\u7684\u5916\u4ea4\u653f\u7b56\u2014\u2014\u7766\u90bb\u5916\u4ea4\u2014\u2014\u4e2d\u7f8e\u5efa\u4ea4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.264634220591857, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e1c\u5357\u4e9a\u91d1\u878d\u5371\u673a\u53d1\u751f\u4e8e\u54ea\u4e00\u5e74\nA. 1997\u5e74\nB. 2007\u5e74\nC. 1982\u5e74\nD. 2003\u5e74\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9298199286063034, "meta-math/MetaMath-Mistral-7B": 0.9897388104322361, "itpossible/Chinese-Mistral-7B-v0.1": 0.9356494570521858, "HuggingFaceH4/zephyr-7b-beta": 0.999276330383572, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9765230838241722, "meta-llama/Meta-Llama-3-8B": 0.9804714542953304, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9965841243131698}}, {"question": "\u53f6\u7ecd\u94a7\u7684\u957f\u7bc7\u5c0f\u8bf4\u662f\nA. \u300a\u6821\u957f\u300b\nB. \u300a\u7a3b\u8349\u4eba\u300b\nC. \u300a\u502a\u7115\u4e4b\u300b\nD. \u300a\u591c\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u79d1\u5b66\u5bb6\u6e29\u7279\u505a\u4e86\u5982\u4e0b\u5b9e\u9a8c\uff1a\u628a\u5207\u4e0b\u7684\u71d5\u9ea6\u5c16\u7aef\u653e\u5728\u743c\u8102\u5757\u4e0a\uff0c\u51e0\u5c0f\u65f6\u540e\uff0c\u79fb\u53bb\u80da\u82bd\u9798\u5c16\u7aef\uff0c\u5c06\u743c\u8102\u5757\u5207\u6210\u5c0f\u5757\u3002\u518d\u5c06\u7ecf\u5904\u7406\u8fc7\u7684\u743c\u8102\u5757\u653e\u5728\u5207\u53bb\u5c16\u7aef\u7684\u71d5\u9ea6\u80da\u82bd\u9798\u4e00\u4fa7\uff0c\u7ed3\u679c\u80da\u82bd\u9798\u4f1a\u671d\u5bf9\u4fa7\u5f2f\u66f2\u751f\u957f\u3002\u4f46\u662f\uff0c\u5982\u679c\u653e\u4e0a\u7684\u662f\u6ca1\u6709\u63a5\u89e6\u8fc7\u80da\u82bd\u9798\u5c16\u7aef\u7684\u743c\u8102\u5757\uff0c\u80da\u82bd\u9798\u5219\u65e2\u4e0d\u751f\u957f\u4e5f\u4e0d\u5f2f\u66f2\u3002\u8be5\u5b9e\u9a8c\u8bc1\u660e\u4e86\nA. \u80da\u82bd\u9798\u4f1a\u5f2f\u5411\u5149\u6e90\u751f\u957f\nB. \u9020\u6210\u80da\u82bd\u9798\u5f2f\u66f2\u7684\u523a\u6fc0\u662f\u67d0\u79cd\u5316\u5b66\u7269\u8d28\nC. \u751f\u957f\u7d20\u53ea\u80fd\u4ece\u5f62\u6001\u5b66\u4e0a\u7aef\u8fd0\u8f93\u5230\u5f62\u6001\u5b66\u4e0b\u7aef\nD. \u751f\u957f\u7d20\u7684\u5316\u5b66\u672c\u8d28\u662f\u5432\u54da\u4e59\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7787338416988743, "meta-math/MetaMath-Mistral-7B": 0.8149480078771499, "itpossible/Chinese-Mistral-7B-v0.1": 0.5455694885272351, "HuggingFaceH4/zephyr-7b-beta": 0.9962625486051476, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9305502560573318, "meta-llama/Meta-Llama-3-8B": 0.34478590299221606, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6559\u80b2\u4e0d\u80fd\u8131\u79bb\u793e\u4f1a\u7684\u7269\u8d28\u6761\u4ef6\u800c\u51ed\u7a7a\u4ea7\u751f\uff0c\u540e\u6765\u7684\u6559\u80b2\u8981\u5728\u4ee5\u524d\u7684\u6559\u80b2\u57fa\u7840\u4e0a\u5411\u524d\u53d1\u5c55\u8fd9\u8bf4\u660e\u6559\u80b2\u5177\u6709 ()\nA. \u5386\u53f2\u7ee7\u627f\u6027\nB. \u751f\u4ea7\u6027\nC. \u76f8\u5bf9\u72ec\u7acb\u6027\nD. \u7edd\u5bf9\u72ec\u7acb\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9014566999431853, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9394324896763285}}, {"question": "\u6559\u5e08\u7684\u8868\u7387\u4f5c\u7528\u4e3b\u8981\u4f53\u73b0\u5728\nA. \u4e3e\u6b62\u7aef\u5e84\nB. \u8c08\u5410\u6587\u96c5\nC. \u8a00\u884c\u4e00\u81f4\nD. \u8863\u7740\u6574\u6d01\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7110813212645148, "meta-math/MetaMath-Mistral-7B": 0.9120170760728997, "itpossible/Chinese-Mistral-7B-v0.1": 0.5974328206150871, "HuggingFaceH4/zephyr-7b-beta": 0.9923373953831026, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5959422331517038, "meta-llama/Meta-Llama-3-8B": 0.8017390004854716, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9955730465559758}}, {"question": "\u67d0\u8de8\u56fd\u516c\u53f8\u5c06\u5176\u76ee\u6807\u5e02\u573a\u5212\u5206\u4e3a\u5317\u7f8e\u5e02\u573a\u3001\u6b27\u6d32\u5e02\u573a\u3001\u4e9a\u6d32\u5e02\u573a\u7b49\uff0c\u5176\u5212\u5206\u7684\u4f9d\u636e\u5c5e\u4e8e\nA. \u4eba\u53e3\u7ec6\u5206\nB. \u5730\u7406\u7ec6\u5206\nC. \u884c\u4e3a\u7ec6\u5206\nD. \u5fc3\u7406\u7ec6\u5206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.9206070135951419, "itpossible/Chinese-Mistral-7B-v0.1": 0.9191741396372327, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.96709467295891, "meta-llama/Meta-Llama-3-8B": 0.909728771430038, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9913983546753709}}, {"question": "\u67d0\u5468\u7684\u65e5\u5747\u6e29\u5206\u522b\u4e3a9\u00b0C\u30019\u00b0C\u300111\u00b0C\u300112\u00b0C\u300113\u00b0C\u300115\u00b0C\u300116\u00b0C\uff0c\u5219\u5bf9\u559c\u6e29\u4f5c\u7269(\u751f\u7269\u5b66\u96f6\u5ea6\u4e3a10\u00b0C)\u6765\u8bf4\uff0c\u8fd9\u5468\u7684\u6d3b\u52a8\u7684\u79ef\u6e29\u4e3a\nA. 67\u00b0C\nB. 18\u00b0C\nC. 85\u00b0C\nD. 17\u00b0C\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.45669488889375404, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7306340849074754}}, {"question": "\u9a7e\u9a76\u673a\u52a8\u8f66\u5728\u9ad8\u901f\u516c\u8def\u9047\u5230\u80fd\u89c1\u5ea6\u4f4e\u4e8e100\u7c73\u7684\u6c14\u8c61\u6761\u4ef6\u65f6\uff0c\u6700\u9ad8\u8f66\u901f\u662f\u591a\u5c11\nA. \u4e0d\u5f97\u8d85\u8fc790\u516c\u91cc/\u5c0f\u65f6\nB. \u4e0d\u5f97\u8d85\u8fc760\u516c\u91cc/\u5c0f\u65f6\nC. \u4e0d\u5f97\u8d85\u8fc740\u516c\u91cc/\u5c0f\u65f6\nD. \u4e0d\u5f97\u8d85\u8fc780\u516c\u91cc/\u5c0f\u65f6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9041994260721438, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5987341406018094, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6700\u5e38\u89c1\u7684\u539f\u53d1\u6027\u6076\u6027\u9aa8\u80bf\u7624\u662f\nA. \u9aa8\u8089\u7624\nB. \u9aa8\u7ea4\u7ef4\u8089\u7624\nC. \u5c24\u6587\u8089\u7624\nD. \u8f6f\u9aa8\u8089\u7624\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43441316748403375, "meta-math/MetaMath-Mistral-7B": 0.9159080616166226, "itpossible/Chinese-Mistral-7B-v0.1": 0.5105908121188107, "HuggingFaceH4/zephyr-7b-beta": 0.6606719155074119, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7716046159438303, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8915150843649883}}, {"question": "\u300a\u8bf4\u6587\u89e3\u5b57\u2022\u53d9\u300b\u4e2d\u6240\u8c13\u201c\u89c6\u800c\u53ef\u8bc6\uff0c\u5bdf\u800c\u89c1\u610f\u201d\uff0c\u6307\u7684\u662f\u201c\u516d\u4e66\u201d\u4e2d\u7684\nA. \u8c61\u5f62\nB. \u6307\u4e8b\nC. \u4f1a\u610f\nD. \u8f6c\u6ce8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe\u51fd\u6570 $f(x)=x \\tan x e^{\\sin x}$\uff0c \u5219 $f(x)$ \u662f\nA. \u5468\u671f\u51fd\u6570\nB. \u65e0\u754c\u51fd\u6570\nC. \u5076\u51fd\u6570\nD. \u5355\u8c03\u51fd\u6570\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.41342935353947124, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7938885586003938, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47453386037351375, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u2f0f\u5185\u4e9a\u94dd\u2f1f\u8d44\u6e90\u4e30\u5bcc\uff0c\u6240\u4ea7\u94dd\u2f1f\u77ff\u2f0f\u4e4e\u5168\u90e8\u4f9b\u51fa\u2f1d\u30022014\u5e74\uff0c\u4e2d\u56fd\u67d0\u4f01\u4e1a\u6295\u8d442\u4ebf\u7f8e\u5143\uff0c\u5f00\u91c7\u2f0f\u5185\u4e9a\u535a\u51ef\u5730\u533a\u7684\u94dd\u2f1f\u77ff\u30022015\u5e749\u2f4929\u2f47\uff0c\u2fb8\u6761\u6ee1\u8f7d\u94dd\u2f1f\u77ff\u768418\u4e07\u5428\u6563\u88c5\u8239\u4ece\u2f0f\u5185\u4e9a\u542f\u7a0b\u5230\u8fbe\u6211\u56fd\u70df\u53f0\u6e2f\u3002\u2f6c\u524d\uff0c\u8be5\u4f01\u4e1a\u6b63\u8ba1\u5212\u52a0\u2f24\u5728\u2f0f\u5185\u4e9a\u6295\u8d44\uff0c\u5efa\u8bbe\u7535\u89e3\u94dd\u57fa\u5730\u3002\u2f0f\u5185\u4e9a\u7684\u94dd\u2f1f\u77ff\u2f0f\u4e4e\u5168\u90e8\u4f9b\u51fa\u2f1d\u7684\u539f\u56e0\u662f\nA. \u2f2f\u4e1a\u5316\u2f54\u5e73\u4f4e\uff0c\u52a0\u2f2f\u80fd\u2f12\u4e0d\u2f9c\nB. \u519c\u77ff\u4ea7\u54c1\u76f4\u63a5\u51fa\u2f1d\u7ecf\u6d4e\u6548\u76ca\u2fbc\nC. \u2f0f\u5185\u4e9a\u5e02\u573a\u5bf9\u94dd\u5236\u54c1\u9700\u6c42\u91cf\u2f29\nD. \u4e2d\u56fd\u5e02\u573a\u5bf9\u94dd\u2f1f\u77ff\u7684\u9700\u6c42\u91cf\u2f24\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u953b\u70bc\u7684\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u6bcf\u5929\u722c\u5c715\u5c0f\u65f6\nB. \u95fb\u9e21\u8d77\u821e\nC. \u91cf\u529b\u800c\u884c\uff0c\u6301\u4e4b\u4ee5\u6052\nD. \u4e09\u5929\u6253\u9c7c\uff0c\u4e24\u5929\u6652\u7f51\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8354348707208353, "meta-math/MetaMath-Mistral-7B": 0.9946125047052898, "itpossible/Chinese-Mistral-7B-v0.1": 0.8450123933406957, "HuggingFaceH4/zephyr-7b-beta": 0.9802228506903471, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9418832305129191, "meta-llama/Meta-Llama-3-8B": 0.8786866234229269, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9909090464808012}}, {"question": "\u6709\u4eba\u559d\u5b8c\u725b\u5976\u540e\u53d1\u751f\u80c0\u6c14\u3001\u8179\u6cfb\uff0c\u5176\u4e3b\u8981\u539f\u56e0\u53ef\u80fd\u662f\u7531\u4e8e\u4f53\u5185\u7f3a\u4e4f\nA. \u6dc0\u7c89\u9176\nB. \u86cb\u767d\u9176\nC. \u4e73\u7cd6\u9176\nD. \u8102\u80aa\u9176\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5911393150474158, "meta-math/MetaMath-Mistral-7B": 0.8383749284802094, "itpossible/Chinese-Mistral-7B-v0.1": 0.7911459883124524, "HuggingFaceH4/zephyr-7b-beta": 0.911971444676088, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5280448026745466, "meta-llama/Meta-Llama-3-8B": 0.9635679263179994, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9991620587159096}}, {"question": "\u5b89\u5168\u8272\u4e2d\u7684()\u8868\u793a\u63d0\u793a\u3001\u5b89\u5168\u72b6\u6001\u53ca\u901a\u884c\u7684\u89c4\u5b9a\nA. \u84dd\u8272\nB. \u9ec4\u8272\nC. \u7eff\u8272\nD. \u7ea2\u8272\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7879353895642816, "meta-math/MetaMath-Mistral-7B": 0.9476577273795361, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9760411592129122, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8796791823906189, "meta-llama/Meta-Llama-3-8B": 0.5220127223167936, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4269884613235058}}, {"question": "\u8bbeNA\u8868\u793a\u963f\u4f0f\u52a0\u5fb7\u7f57\u5e38\u6570\u7684\u503c\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. 4.6 g Na\u5b8c\u5168\u8f6c\u5316\u6210Na2O\u548cNa2O2\u7684\u6df7\u5408\u7269\uff0c\u8f6c\u79fb\u7684\u7535\u5b50\u6570\u4e3a0.2NA\nB. \u5e38\u6e29\u5e38\u538b\u4e0b\uff0c22.4 L\u7684NO2\u548cCO2\u6df7\u5408\u6c14\u4f53\u542b\u67092NA\u4e2aO\u539f\u5b50\nC. \u6807\u51c6\u72b6\u51b5\u4e0b\uff0c1.12 L HCl\u4e0e1.12 L NH3\u6df7\u5408\uff0c\u6c14\u4f53\u5206\u5b50\u603b\u6570\u4e3a0.1NA\nD. 18 g ^18 O2\u4e2d\u542b\u6709\u7684\u4e2d\u5b50\u6570\u4e3a8NA\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5386\u53f2\u4e0a\u7684\u4e8b\u53d8\u662f\u674e\u4e16\u6c11\u53d1\u52a8\u7684\u662f\nA. \u7384\u6b66\u95e8\u4e4b\u53d8\nB. \u9648\u6865\u5175\u53d8\nC. \u9756\u5eb7\u4e4b\u53d8\nD. \u571f\u6728\u4e4b\u53d8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4065058770339685, "itpossible/Chinese-Mistral-7B-v0.1": 0.3556591417532881, "HuggingFaceH4/zephyr-7b-beta": 0.3900720102436832, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6922374479436478, "meta-llama/Meta-Llama-3-8B": 0.5939474992613675, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6593078896964856}}, {"question": "\u674e\u6e05\u7167\u8bcd\u4e2d\u7684\u201c\u7eff\u80a5\u7ea2\u7626\u201d\u63cf\u5199\u7684\u662f\u4ec0\u4e48\u5b63\u8282\u7684\u666f\u8272\nA. \u4ef2\u590f\nB. \u521d\u79cb\nC. \u65e9\u6625\nD. \u665a\u6625\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8fc7\u7a0b\u4e2d\u989c\u8272\u53d8\u5316\u4e0e\u5316\u5b66\u53d8\u5316\u65e0\u5173\u7684\u662f\nA. \u6d45\u9ec4\u7eff\u8272\u7684\u65b0\u5236\u6c2f\u6c34\u4e45\u7f6e\u540e\u53d8\u4e3a\u65e0\u8272\nB. \u94f6\u767d\u8272\u7684\u91d1\u5c5e\u94c1\u7814\u78e8\u6210\u94c1\u7c89\u540e\u53d8\u6210\u9ed1\u8272\nC. \u65b0\u5207\u5272\u5f00\u7684\u91d1\u5c5e\u94a0\u7684\u8868\u9762\u7531\u94f6\u767d\u8272\u8fc5\u901f\u53d8\u6210\u6697\u7070\u8272\nD. \u7ea2\u68d5\u8272\u7684NO2\u901a\u8fc7\u88c5\u6709\u6c34\u7684\u6d17\u6c14\u74f6\u540e\u5f97\u5230\u65e0\u8272\u6c14\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4792568976060358, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3835255432162072}}, {"question": "\u4e0b\u5217\u53e5\u5b50\u4e2d\u6ca1\u6709\u8bed\u75c5\u7684\u4e00\u9879\u662f\uff08 \uff09\nA. \u7236\u4eb2\u5df2\u7ecf\u8d70\u4e86\uff0c\u4f46\u90a3\u756a\u8bed\u91cd\u5fc3\u957f\u7684\u8bdd\u8bed\u65f6\u65f6\u5728\u6211\u7684\u5fc3\u5934\u56de\u60f3\u3002\nB. \u4f1a\u4e0d\u4f1a\u7528\u5fc3\u89c2\u5bdf\uff0c\u80fd\u4e0d\u80fd\u91cd\u89c6\u79ef\u7d2f\uff0c\u662f\u80fd\u5426\u63d0\u9ad8\u5199\u4f5c\u6c34\u5e73\u7684\u5173\u952e\u3002\nC. \u901a\u8fc7\u5f00\u5c55\u201c\u6bcf\u6708\u5c11\u5f00\u4e00\u5929\u8f66\u201d\u7684\u6d3b\u52a8\uff0c\u53ef\u4ee5\u4f7f\u6cf0\u5b89\u7684\u7a7a\u6c14\u66f4\u52a0\u6e05\u65b0\u3002\nD. \u4ed6\u7684\u8bed\u6587\u6210\u7ee9\u4e0d\u4ec5\u5728\u5168\u6821\u5f88\u7a81\u51fa\uff0c\u800c\u4e14\u5728\u6211\u73ed\u4e5f\u540d\u5217\u524d\u8305\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u82e5\u5411\u91cf\u7ec4 $\\boldsymbol{\\alpha}\uff0c \\boldsymbol{\\beta}\uff0c \\boldsymbol{\\gamma}$ \u7ebf\u6027\u65e0\u5173\uff0c $\\boldsymbol{\\alpha}\uff0c \\boldsymbol{\\beta}\uff0c \\boldsymbol{\\delta}$ \u7ebf\u6027\u76f8\u5173\uff0c\u5219( )\nA. $\\boldsymbol{\\delta}$ \u5fc5\u53ef\u7531 $\\boldsymbol{\\alpha}\uff0c \\boldsymbol{\\beta}\uff0c \\boldsymbol{\\gamma}$ \u7ebf\u6027\u8868\u793a;\nB. $\\boldsymbol{\\beta}$ \u5fc5\u4e0d\u53ef\u7531 $\\boldsymbol{\\alpha}\uff0c \\boldsymbol{\\gamma}\uff0c \\boldsymbol{\\delta}$ \u7ebf\u6027\u8868\u793a;\nC. $\\boldsymbol{\\delta}$ \u5fc5\u4e0d\u53ef\u7531 $\\boldsymbol{\\alpha}\uff0c \\boldsymbol{\\beta}\uff0c \\boldsymbol{\\gamma}$ \u7ebf\u6027\u8868\u793a.\nD. $\\boldsymbol{\\alpha}$ \u5fc5\u53ef\u7531 $\\boldsymbol{\\beta}\uff0c \\boldsymbol{\\gamma}\uff0c \\boldsymbol{\\delta}$ \u7ebf\u6027\u8868\u793a;\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3467592701428982, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3357297350676873, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.45513177618661366}}, {"question": "\u6211\u56fd\u7684\u300a\u6559\u5e08\u8d44\u683c\u6761\u4f8b\u300b\u662f\u5728\u54ea\u91cc\u9881\u53d1\u7684\nA. 1950\u5e74\nB. 1993\u5e74\nC. 1985\u5e74\nD. 1995\u5e74\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u808c\u8089\u795e\u7ecf\u5174\u594b\u548c\u6291\u5236\u4f5c\u7528\u76f8\u540c\u53c8\u6709\u62ee\u6297\u4f5c\u7528\u7684\u77ff\u7269\u8d28\u662f\nA. \u9499\u548c\u94be\nB. \u9541\u548c\u78f7\nC. \u78f7\u548c\u9499\nD. \u9499\u548c\u9541\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.2997240426459713, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f20\u64ad\u5b66\u7684\u5b9a\u91cf\u7814\u7a76\u65b9\u6cd5\u4e4b\u4e00\u662f\nA. \u903b\u8f91\u5f52\u7eb3\u6cd5\nB. \u5ba2\u89c2\u63cf\u8ff0\u6cd5\nC. \u5386\u53f2\u6bd4\u8f83\u6cd5\nD. \u5185\u5bb9\u5206\u6790\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7986300334592126, "meta-math/MetaMath-Mistral-7B": 0.8945423362622474, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9997339320926758, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8476765458364549, "meta-llama/Meta-Llama-3-8B": 0.4209627067760388, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4729340346960238}}, {"question": "\u6839\u636e\u6d3b\u52a8\u4efb\u52a1\u7684\u8981\u6c42\uff0c\u6709\u610f\u8bc6\u5730\u4e3b\u52a8\u5730\u628a\u6ce8\u610f\u4ece\u4e00\u4e2a\u5bf9\u8c61\u8f6c\u79fb\u5230\u53e6\u4e00\u4e2a\u5bf9\u8c61\u79f0\u4e3a\nA. \u6ce8\u610f\u7684\u8f6c\u79fb\nB. \u6ce8\u610f\u7684\u7a33\u5b9a\u6027\nC. \u6ce8\u610f\u7684\u5206\u5fc3\nD. \u6ce8\u610f\u7684\u5206\u914d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.81189373178373, "itpossible/Chinese-Mistral-7B-v0.1": 0.7664147998316926, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6466210463039777, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.49822703462191986}}, {"question": "\u67d0\u8bbe\u533a\u7684\u5e02\u653f\u5e9c\u51fa\u53f0\u300a\u89c4\u8303\u64cd\u529e\u9152\u5e2d\u884c\u4e3a\u5b9e\u65bd\u529e\u6cd5\u300b\uff0c\u529e\u6cd5\u89c4\u5b9a\u9664\u5a5a\u5ac1\u9152\u3001\u4e27\u846c\u9152\u5916\u7684\u5176\u4ed6\u4e00\u5f8b\u89c6\u4e3a\u8fdd\u89c4\u9152\u5e2d\uff0c\u4e0b\u5ac1\u4e0d\u51c6\u64cd\u529e\u9152\u5e2d\uff0c\u540c\u65f6\u89c4\u5b9a\uff1a\u201c\u7fa4\u4f17\u64cd\u529e\u5a5a\u5ac1\u9152\u987b\u586b\u5199\u7533\u62a5\u8868\u5e76\u62a5\u533a\u653f\u5e9c\u5907\u6848\u3002\u201d\u5bf9\u6b64\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u8be5\u529e\u6cd5\u5bf9\u4e8e\u201c\u8fdd\u89c4\u9152\u5e2d\u201d\u7684\u89c4\u5b9a\uff0c\u4f53\u73b0\u4e86\u793e\u4f1a\u4e3b\u4e49\u6cd5\u5f8b\u6587\u5316\u7684\u8981\u6c42\nB. \u8be5\u529e\u6cd5\u6709\u5173\u201c\u4e0b\u5ac1\u4e0d\u51c6\u64cd\u529e\u9152\u5e2d\u201d\u7684\u89c4\u5b9a\u7b26\u5408\u516c\u5e8f\u826f\u4fd7\u539f\u5219\nC. \u8be5\u529e\u6cd5\u89c4\u5b9a\u7fa4\u4f17\u64cd\u529e\u5a5a\u5ac1\u9152\u987b\u7533\u62a5\uff0c\u4e0d\u9002\u5f53\u5730\u589e\u52a0\u4e86\u516c\u6c11\u4e49\u52a1\nD. \u8be5\u529e\u6cd5\u4e0d\u5c5e\u4e8e\u89c4\u8303\u6027\u6cd5\u5f8b\u6587\u4ef6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3343095977038262, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5256467671114424, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.57841172661614}}, {"question": "\u672c\u7968\u7684\u7968\u636e\u5173\u7cfb\u57fa\u672c\u5f53\u4e8b\u4eba\u662f\u6307\nA. \u6536\u6b3e\u4eba\u4e0e\u627f\u5151\u4eba\nB. \u51fa\u7968\u4eba\u4e0e\u4ed8\u6b3e\u4eba\nC. \u6536\u6b3e\u4eba\u4e0e\u4fdd\u8bc1\u4eba\nD. \u51fa\u7968\u4eba\u4e0e\u6536\u6b3e\u4eba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5873755839630901, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5893012202001757, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u6211\u56fd\u6700\u9ad8\u4eba\u6c11\u6cd5\u9662\u7684\u53f8\u6cd5\u89e3\u91ca\uff0c\u6d89\u5916\u6c11\u4e8b\u6cd5\u5f8b\u5173\u7cfb\u7684\u8bc9\u8bbc\u65f6\u6548\uff0c\u4f9d\u636e\nA. \u6cd5\u9662\u5730\u6cd5\u786e\u5b9a\nB. \u51b2\u7a81\u89c4\u8303\u786e\u5b9a\u7684\u6c11\u4e8b\u6cd5\u5f8b\u5173\u7cfb\u7684\u51c6\u636e\u6cd5\u786e\u5b9a\nC. \u5f53\u4e8b\u4eba\u81ea\u7531\u7ea6\u5b9a\nD. \u6cd5\u9662\u5730\u6cd5\u6216\u6c11\u4e8b\u6cd5\u5f8b\u5173\u7cfb\u7684\u51c6\u636e\u6cd5\u786e\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35686367501298644, "meta-math/MetaMath-Mistral-7B": 0.4412842204981715, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5530824624955382, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5987341535209013}}, {"question": "\u4e0b\u5217\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f\nA. \u68af\u5ea6\u4e0b\u964d\u6cd5\u662f\u5229\u7528\u5f53\u524d\u4f4d\u7f6e\u7684\u8d1f\u68af\u5ea6\u4f5c\u4e3a\u641c\u7d22\u65b9\u5411\u7684\u65b9\u6cd5\nB. \u5171\u8f6d\u68af\u5ea6\u6cd5\u4ec5\u9700\u5229\u7528\u4e00\u9636\u5bfc\u6570\u7684\u4fe1\u606f\uff0c\u4f46\u662f\u6536\u655b\u901f\u5ea6\u9ad8\u4e8e\u68af\u5ea6\u4e0b\u964d\u6cd5\nC. \u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u76f8\u6bd4\uff0c\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u4f18\u52bf\u662f\u5bf9\u4e8e\u5927\u89c4\u6a21\u6837\u672c\u6548\u7387\u5f88\u9ad8\nD. \u725b\u987f\u6cd5\u548c\u68af\u5ea6\u4e0b\u964d\u6cd5\u76f8\u6bd4\uff0c\u4e00\u4e2a\u52a3\u52bf\u662f\u6c42\u89e3\u590d\u6742\uff0c\u4e00\u4e2a\u4f18\u52bf\u662f\u6536\u655b\u901f\u5ea6\u52a0\u5feb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3742744305879218, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.33065623127838456, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\uff0c\u542b\u6709\u53cc\u5bbe\u8bed\u7684\u4e00\u53e5\u662f\nA. \u9063\u4f7f\u5411\u897f\u57df\u6c42\u4e4b\uff0c\u4e43\u5f97\u7d93\u50cf\u7109\u3002\nB. \u8303\u589e\u6570\u76ee\u9805\u738b\uff0c\u8209\u6240\u4f69\u7389\u73a6\u4ee5\u793a\u4e4b\u8005\u4e09\u3002\nC. \u5ba2\u4eba\u4e0d\u77e5\u5176\u662f\u5546\u541b\u4e5f\u3002\nD. \u6545\u5929\u4e88\u4e4b\u6642\uff0c\u5730\u751f\u4e4b\u8ca1\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b87\u5b99\u751f\u6210\u8bba\u4e5f\u53eb\nA. \u65e9\u671f\u81ea\u7136\u54f2\u5b66\nB. \u4e2d\u671f\u5e0c\u814a\u54f2\u5b66\nC. \u524d\u82cf\u683c\u62c9\u5e95\u54f2\u5b66\nD. \u665a\u671f\u5e0c\u814a\u54f2\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31099993962655337, "meta-math/MetaMath-Mistral-7B": 0.3755160760350868, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.840135839264222, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4861860930976202, "meta-llama/Meta-Llama-3-8B": 0.6820058320004859, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5233759102255567}}, {"question": "\u52a8\u8109\u6536\u7f29\u538b\u4e3b\u8981\u53cd\u6620\nA. \u5faa\u73af\u8840\u91cf\nB. \u5927\u52a8\u8109\u987a\u5e94\u6027\nC. \u5916\u5468\u963b\u529b\nD. \u5fc3\u640f\u51fa\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u53e4\u4ee3\u4e2d\u592e\u96c6\u6743\u7684\u5f3a\u5316\uff0c\u5f80\u5f80\u901a\u8fc7\u91c7\u53d6\u5f31\u5316\u76f8\u6743\u3001\u5b8c\u5584\u76d1\u5bdf\u5236\u3001\u8c03\u6574\u4e2d\u592e\u548c\u5730\u65b9\u5173\u7cfb\u7b49\u624b\u6bb5\u6765\u5b9e\u73b0\uff0c\u5728\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u653f\u6cbb\u67b6\u6784\u7684\u8bbe\u8ba1\u4e0e\u804c\u6743\u7684\u53d8\u66f4\u8f83\u4e3a\u7a81\u51fa\u3002\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u5728\u5730\u4f4d\u548c\u804c\u6743\u4e0a\u5927\u4f53\u76f8\u5f53\u7684\u4e00\u7ec4\u662f\nA. \u897f\u6c49\u5dde\u523a\u53f2\u4e0e\u660e\u76d1\u5bdf\u5fa1\u53f2\nB. \u5b8b\u63d0\u70b9\u5211\u72f1\u53f8\u4e0e\u660e\u901a\u653f\u53f8\nC. \u5510\u653f\u4e8b\u5802\u4e0e\u5b8b\u4e2d\u4e66\u95e8\u4e0b\nD. \u4e1c\u5468\u8bf8\u4faf\u56fd\u4e0e\u5143\u7701\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.303006867405966, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6137131362205761}}, {"question": "\u8bbeA\u4e3an\u9636\u65b9\u9635\uff0c\u4e14|A|=0$\uff0c\u5219 ( ) .\nA. A\u4e2d\u81f3\u5c11\u6709\u4e00\u884c\u5143\u7d20\u5168\u4e3a\u96f6\nB. A\u4e2d\u4efb\u610f\u4e00\u884c\u4e3a\u5176\u5b83\u884c\u7684\u7ebf\u6027\u7ec4\u5408\nC. A\u4e2d\u4e24\u884c(\u5217)\u5bf9\u5e94\u5143\u7d20\u6210\u6bd4\u4f8b\nD. A\u4e2d\u5fc5\u6709\u4e00\u884c\u4e3a\u5176\u5b83\u884c\u7684\u7ebf\u6027\u7ec4\u5408\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4073394377251569, "meta-math/MetaMath-Mistral-7B": 0.38754812506005853, "itpossible/Chinese-Mistral-7B-v0.1": 0.35875203752893203, "HuggingFaceH4/zephyr-7b-beta": 0.9142896157846978, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4287710230229752, "meta-llama/Meta-Llama-3-8B": 0.5140406332527597, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6ca1\u6709\u5473\u857e\u7684\u7ed3\u6784\u662f\nA. \u8f6e\u5ed3\u4e73\u5934\nB. \u4e1d\u72b6\u4e73\u5934\nC. \u53f6\u72b6\u4e73\u5934\nD. \u83cc\u72b6\u4e73\u5934\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u6b63\u786e\u8206\u8bba\u201d\u7684\u542b\u4e49\u901a\u5e38\u5e94\u5f53\u5305\u62ec\nA. \u653f\u6cbb\u65b9\u5411\u6b63\u786e\uff0c\u601d\u60f3\u65b9\u6cd5\u6b63\u786e\uff0c\u8bdd\u9898\u5185\u5bb9\u6b63\u786e\nB. \u653f\u6cbb\u65b9\u5411\u6b63\u786e\uff0c\u4f20\u64ad\u5185\u5bb9\u5065\u5eb7\uff0c\u4f20\u64ad\u901f\u5ea6\u5feb\u6377\nC. \u653f\u6cbb\u65b9\u5411\u6b63\u786e\uff0c\u8206\u8bba\u8bdd\u9898\u9002\u65f6\uff0c\u8206\u8bba\u5f3a\u5ea6\u9002\u4e2d\nD. \u653f\u6cbb\u65b9\u5411\u6b63\u786e\uff0c\u601d\u60f3\u65b9\u6cd5\u79d1\u5b66\uff0c\u4f20\u64ad\u65f6\u95f4\u9002\u5b9c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3618919119940369, "meta-math/MetaMath-Mistral-7B": 0.43306719326038534, "itpossible/Chinese-Mistral-7B-v0.1": 0.35686329776860143, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6122171283884846, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6868218517984545}}, {"question": "\u53d1\u6325\u5236\u5ea6\u89c4\u5219\u5916\u5728\u673a\u5236\u548c()\u7684\u4f5c\u7528\u662f\u5b9e\u73b0\u516c\u6b63\u5408\u7406\u7684\u7ecf\u6d4e\u4f26\u7406\u79e9\u5e8f\u7684\u4e24\u4e2a\u7f3a\u4e00\u4e0d\u53ef\u7684\u9014\u5f84\nA. \u653f\u6cbb\u6743\u5229\u5e73\u7b49\nB. \u9053\u5fb7\u5185\u5728\u673a\u5236\nC. \u5236\u5ea6\u6cd5\u5f8b\nD. \u793e\u4f1a\u516c\u6b63\u79e9\u5e8f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6175596106172433, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4972078981716806, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5590617582087786}}, {"question": "\u674e\u67d0\u539f\u662f\u67d0\u673a\u68b0\u5382\u5382\u957f\uff0c\u540e\u8c03\u5230\u540c\u7ea7\u7684\u516c\u53f8\u5f53\u7ecf\u7406\uff0c\u674e\u67d0\u5b8c\u6210\u7684\u793e\u4f1a\u6d41\u52a8\u662f\nA. \u81ea\u7531\u6d41\u52a8\nB. \u6c34\u5e73\u6d41\u52a8\nC. \u4ee3\u9645\u6d41\u52a8\nD. \u5782\u76f4\u6d41\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0e\u51b3\u7b56\u76ee\u6807\u76f8\u6bd4\uff0c\u8ba1\u5212\u76ee\u6807\u662f\u7ba1\u7406\u6d3b\u52a8\u7684\nA. \u7efc\u5408\u76ee\u6807\nB. \u6218\u7565\u76ee\u6807\nC. \u6574\u4f53\u76ee\u6807\nD. \u5206\u652f\u76ee\u6807\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4612837110734246, "meta-math/MetaMath-Mistral-7B": 0.5217975749304178, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8255118634008737, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5046291963676325, "meta-llama/Meta-Llama-3-8B": 0.2821833983601388, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b0\u95fb\u53d1\u5e03\u4f1a\u662f\u4e00\u79cd\nA. \u76f4\u63a5\u4f20\u64ad\nB. \u591a\u7ea7\u4f20\u64ad\nC. \u4e24\u7ea7\u4f20\u64ad\nD. \u4e09\u7ea7\u4f20\u64ad\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36528783010426397, "meta-math/MetaMath-Mistral-7B": 0.3908304606703761, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.40870097338816025, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9a7e\u9a76\u62fc\u88c5\u673a\u52a8\u8f66\u4e0a\u8def\u884c\u9a76\u7684\u9a7e\u9a76\u4eba\uff0c\u9664\u6309\u89c4\u5b9a\u63a5\u53d7\u7f5a\u6b3e\u5916\uff0c\u8fd8\u8981\u53d7\u5230\u54ea\u79cd\u5904\u7406\nA. \u8ffd\u7a76\u5211\u4e8b\u8d23\u4efb\nB. \u540a\u9500\u9a7e\u9a76\u8bc1\nC. \u6682\u6263\u9a7e\u9a76\u8bc1\nD. \u590410\u65e5\u4ee5\u4e0b\u62d8\u7559\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3894203517844956, "meta-math/MetaMath-Mistral-7B": 0.49449087570800015, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9593825103611288, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.799396027766379, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u7b2c\u4e00\u5bb6\u4e13\u4e1a\u516c\u5171\u5173\u7cfb\u516c\u53f8\u6210\u7acb\u4e8e\nA. 1988\u5e74\nB. 1978\u5e74\nC. 1986\u5e74\nD. 1982\u5e74\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4087009733881603, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47340445350012167, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1952 \u5e74\uff0c\u5468\u6069\u6765\u5728\u300a\u6211\u4eec\u7684\u5916\u4ea4\u65b9\u9488\u548c\u4efb\u52a1\u300b\u4e2d\u8bf4\uff1a\u201c\u4e0d\u627f\u8ba4\u56fd\u6c11\u515a\u653f\u5e9c\u540c\u5404\u56fd\u5efa\u7acb\u7684\u65e7\u7684\u5916\u4ea4\u5173\u7cfb\uff0c\u800c\u8981\u5728\u65b0\u7684\u57fa\u7840\u4e0a\u540c\u5404\u56fd\u5efa\u7acb\u65b0\u7684\u5916\u4ea4\u5173\u7cfb\u3002\u5bf9\u4e8e\u9a7b\u5728\u65e7\u4e2d\u56fd\u7684\u5404\u56fd\u4f7f\u8282\uff0c\u6211\u4eec\u628a\u4ed6\u4eec\u5f53\u4f5c\u666e\u901a\u4fa8\u6c11\u5bf9\u5f85\uff0c\u4e0d\u5f53\u4f5c\u5916\u4ea4\u4ee3\u8868\u5bf9\u5f85\u3002\u201d\u5468\u6069\u6765\u5728\u6b64\u9610\u91ca\u7684\u5916\u4ea4\u653f\u7b56\u662f\nA. \u201c\u53e6\u8d77\u7089\u7076\u201d\nB. \u201c\u6253\u626b\u5e72\u51c0\u5c4b\u5b50\u518d\u8bf7\u5ba2\u201d\nC. \u201c\u4e00\u8fb9\u5012\u201d\nD. \u201c\u6c42\u540c\u5b58\u5f02\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5730552099681151, "itpossible/Chinese-Mistral-7B-v0.1": 0.4578709677499768, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7645963079095164, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5369443661507719}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6743\u5229\u548c\u4e49\u52a1\u7684\u8868\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u6743\u5229\u548c\u4e49\u52a1\u90fd\u4e0d\u53ef\u4ee5\u653e\u5f03\nB. \u4e49\u52a1\u53ef\u4ee5\u653e\u5f03\uff0c\u6743\u5229\u5fc5\u987b\u4eab\u6709\nC. \u6743\u5229\u548c\u4e49\u52a1\u90fd\u53ef\u4ee5\u653e\u5f03\nD. \u6743\u5229\u53ef\u4ee5\u653e\u5f03\uff0c\u4e49\u52a1\u5fc5\u987b\u5c65\u884c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.643426410895075, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c06\u7968\u636e\u6cd5\u7f16\u5165\u503a\u52a1\u6cd5\u5178\u5185\uff0c\u4f5c\u4e3a\u503a\u52a1\u6cd5\u7684\u4e00\u90e8\u5206\u5185\u5bb9\u7684\u56fd\u5bb6\u662f\nA. \u65e5\u672c\nB. \u82f1\u56fd\nC. \u745e\u58eb\nD. \u5fb7\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u6bd5\u4e1a\u6b4c\u300b\u7684\u66f2\u4f5c\u8005\u662f\nA. \u8d3a\u7eff\u6c40\nB. \u51bc\u661f\u6d77\nC. \u8042\u8033\nD. \u90d1\u5f8b\u6210\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u4e58\u5750\u516c\u4ea4\u8f66\u65f6\u56e0\u5230\u7ad9\u672a\u505c\u4e0e\u53f8\u673a\u53d1\u751f\u4e89\u6267\uff0c\u4e00\u6012\u4e4b\u4e0b\u62a2\u593a\u6b63\u5728\u884c\u9a76\u7684\u516c\u4ea4\u8f66\u65b9\u5411\u76d8\uff0c\u81f4\u516c\u4ea4\u8f66\u5931\u63a7\u649e\u5230\u8def\u8fb9\u7535\u7ebf\u6746\uff0c\u4e58\u5ba2\u53ca\u884c\u4eba\u53d7\u4f24\u3001\u516c\u4ea4\u8f66\u4e25\u91cd\u53d7\u635f\u3002\u7532\u7684\u884c\u4e3a\u6784\u6210\nA. \u4ea4\u901a\u8087\u4e8b\u7f6a\nB. \u5bfb\u8845\u6ecb\u4e8b\u7f6a\nC. \u4ee5\u5371\u9669\u65b9\u6cd5\u5371\u5bb3\u516c\u5171\u5b89\u5168\u7f6a\nD. \u5371\u9669\u9a7e\u9a76\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44259104110435216, "meta-math/MetaMath-Mistral-7B": 0.6064366688158908, "itpossible/Chinese-Mistral-7B-v0.1": 0.5025034636961727, "HuggingFaceH4/zephyr-7b-beta": 0.9940223250384942, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5771828683033712}}, {"question": "\u58f0\u97f5\u76f8\u62fc\u65f6\uff0c\u58f0\u97f5\u4e4b\u95f4\nA. \u5fc5\u987b\u505c\u987f\nB. \u4e0d\u80fd\u505c\u987f\nC. \u6709\u65f6\u53ef\u4ee5\u505c\u987f\nD. \u505c\u987f\u4e0d\u505c\u987f\u8981\u6839\u636e\u5177\u4f53\u97f3\u8282\u800c\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6210\u4eba\u4e0e\u5e7c\u513f\u5bf9\u4e00\u5e45\u753b\u7684\u77e5\u89c9\u6709\u660e\u663e\u5dee\u5f02\uff0c\u5e7c\u513f\u53ea\u4f1a\u770b\u5230\u8fd9\u5e45\u753b\u7684\u4e3b\u8981\u6784\u6210\uff0c\u800c\u6210\u4eba\u770b\u5230\u7684\u753b\u9762\u610f\u4e49\u3002\u8fd9\u53cd\u6620\u7684\u77e5\u89c9\u7279\u6027\u662f\nA. \u7406\u89e3\u6027\nB. \u6574\u4f53\u6027\nC. \u6052\u5e38\u6027\nD. \u9009\u62e9\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6e05\u6668\u8d77\u5e8a\u540e\u80cc\u8bf5\u4e1c\u897f\u6548\u679c\u5f88\u597d\uff0c\u8fd9\u662f\u56e0\u4e3a\u6ca1\u6709\uff08\uff09\u3002\nA. \u524d\u6444\u6291\u5236\nB. \u540e\u6444\u6291\u5236\nC. \u5206\u5316\u6291\u5236\nD. \u5916\u6291\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.64594175846687, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4805121962018544, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2016\u5e7412\u670816\u65e5\uff0c\u6559\u80b2\u90e8\u9881\u5e03\u4e86\u65b0\u4fee\u8ba2\u7684\u300a\u666e\u901a\u9ad8\u7b49\u5b66\u6821\u5b66\u751f\u7ba1\u7406\u89c4\u5b9a\u300b\uff0c\u5176\u4e2d\u7b2c55\u6761\u7b2c1\u6b3e\u89c4\u5b9a\uff1a\u201c\u5728\u5bf9\u5b66\u751f\u4f5c\u51fa\u5904\u5206\u6216\u8005\u5176\u4ed6\u4e0d\u5229\u51b3\u5b9a\u4e4b\u524d\uff0c\u5b66\u6821\u5e94\u5f53\u544a\u77e5\u5b66\u751f\u4f5c\u51fa\u51b3\u5b9a\u7684\u4e8b\u5b9e\u3001\u7406\u7531\u53ca\u4f9d\u636e\uff0c\u5e76\u544a\u77e5\u5b66\u751f\u4eab\u6709\u9648\u8ff0\u548c\u7533\u8fa9\u7684\u6743\u5229\uff0c\u542c\u53d6\u5b66\u751f\u7684\u9648\u8ff0\u548c\u7533\u8fa9\u3002\u201d\u8be5\u89c4\u5b9a\u96c6\u4e2d\u4f53\u73b0\u7684\u6cd5\u5f8b\u4ef7\u503c\u662f\nA. \u79e9\u5e8f\nB. \u5b89\u5168\nC. \u6548\u7387\nD. \u6b63\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9211944347012258, "meta-math/MetaMath-Mistral-7B": 0.9936369055192414, "itpossible/Chinese-Mistral-7B-v0.1": 0.8904808525989021, "HuggingFaceH4/zephyr-7b-beta": 0.9998556885636859, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9610859218575099, "meta-llama/Meta-Llama-3-8B": 0.9440829464836142, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9902094232775586}}, {"question": "\u514b\u6797\u987f\u653f\u5e9c\u4e3a\u52a0\u5f3a\u5bf9\u7ecf\u6d4e\u7684\u5b8f\u89c2\u8c03\u8282\u548c\u5e72\u9884\u6210\u7acb\u4e86\u4ec0\u4e48\u673a\u6784\nA. \u91d1\u878d\u76d1\u7ba1\u59d4\u5458\u4f1a\nB. \u56fd\u5bb6\u5b89\u5168\u59d4\u5458\u4f1a\nC. \u56fd\u5bb6\u7ecf\u6d4e\u59d4\u5458\u4f1a\nD. \u79d1\u5b66\u6280\u672f\u59d4\u5458\u4f1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.821367726339521, "meta-math/MetaMath-Mistral-7B": 0.776064044111661, "itpossible/Chinese-Mistral-7B-v0.1": 0.8061018370347124, "HuggingFaceH4/zephyr-7b-beta": 0.9713364707937007, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8826908717027331, "meta-llama/Meta-Llama-3-8B": 0.7802510724212307, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u83b7\u53d6\u4fe1\u606f\u7684\u9014\u5f84\u4e2d\u5c5e\u4e8e\u76f4\u63a5\u83b7\u53d6\u4fe1\u606f\u7684\u662f\nA. \u4ece\u4e8b\u91ce\u5916\u79d1\u5b66\u8003\u5bdf\nB. \u542c\u5e7f\u64ad\nC. \u770b\u7535\u89c6\nD. \u4e0a\u7f51\u6d4f\u89c8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5326190921020729, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u201c\u51fa\u82bd\u201d\u65b9\u5f0f\u4ece\u7ec6\u80de\u91ca\u653e\u7684\u75c5\u6bd2\u662f\nA. \u67ef\u8428\u5947\u75c5\u6bd2\nB. \u817a\u75c5\u6bd2\nC. \u9ebb\u75b9\u75c5\u6bd2\nD. ECHO \u75c5\u6bd2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34450439001334365, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8240230908452244, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.41342934899303657, "meta-llama/Meta-Llama-3-8B": 0.2966017332563094, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e01\u897f\u6797\u7684\u7b2c\u4e00\u90e8\u72ec\u5e55\u559c\u5267\u662f\nA. \u300a\u538b\u8feb\u300b\nB. \u300a\u5175\u53d8\u300b\nC. \u300a\u7ec8\u8eab\u5927\u4e8b\u300b\nD. \u300a\u4e00\u53ea\u9a6c\u8702\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2865061228366489, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7526151385466066, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3728641892601242}}, {"question": "\u5728\u6c11\u65cf\u516d\u8981\u7d20\u4e2d\uff0c\u5177\u6709\u76f8\u5f53\u7a33\u5b9a\u6027\u3001\u5386\u53f2\u8fde\u7eed\u6027\u548c\u6301\u4e45\u6027\u7684\u8981\u7d20\u662f\nA. \u5171\u540c\u98ce\u4fd7\u4e60\u60ef\nB. \u5171\u540c\u751f\u4ea7\u65b9\u5f0f\nC. \u5171\u540c\u6587\u5316\nD. \u5171\u540c\u5fc3\u7406\u8ba4\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53e4\u4ee3\u54f2\u5b66\u7cbe\u6c14\u6982\u5ff5\u7684\u4ea7\u751f\u662f\u6e90\u4e8e\nA. \u6c34\u5730\u8bf4\nB. \u5143\u6c14\u8bf4\nC. \u4e94\u65b9\u8bf4\nD. \u4e94\u6750\u8bf4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u5c06\u521a\u4e70\u56de\u7684\u5f69\u7535\u653e\u5728\u697c\u9053\u95e8\u53e3\uff0c\u4fbf\u4e0a\u697c\u5f00\u95e8\uff0c\u88ab\u6070\u597d\u8def\u8fc7\u7684\u4e59\u53d1\u73b0\u3002\u4e59\u62b1\u8d77\u5f69\u7535\u5c31\u8dd1\u3002\u4e59\u521a\u8dd1\u51fa10\u7c73\u8fdc\uff0c\u7532\u6b63\u597d\u4e0b\u697c\uff0c\u5c06\u4e59\u6293\u83b7\u3002\u5173\u4e8e\u4e59\u7684\u884c\u4e3a\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u6784\u6210\u76d7\u7a83\u7f6a\u672a\u9042\nB. \u6784\u6210\u76d7\u7a83\u7f6a\u65e2\u9042\nC. \u6784\u6210\u4fb5\u5360\u7f6a\u672a\u9042\nD. \u6784\u6210\u4fb5\u5360\u7f6a\u65e2\u9042\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3964338410612006, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5837308439298061, "meta-llama/Meta-Llama-3-8B": 0.2953920515320701, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u79f0\u4e3a\u5999\u5409\u7965\u7684\u83e9\u8428\u662f\nA. \u89c2\u97f3\nB. \u666e\u8d24\nC. \u51c6\u63d0\nD. \u6587\u6b8a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.36891085543308744, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f8e\u56fd1787\u5e74\u5baa\u6cd5\u89c4\u5b9a\uff0c\u56fd\u4f1a\u5206\u53c2\u3001\u4f17\u4e24\u9662\uff0e\u53c2\u8bae\u9662\u7531\u5404\u5dde\u8bae\u4f1a\u5206\u522b\u9009\u6d3e\u4e24\u540d\u53c2\u8bae\u5458\u7ec4\u6210\uff0c\u4f17\u8bae\u9662\u8bae\u5458\u6309\u4eba\u53e3\u6bd4\u4f8b\u7531\u5404\u5dde\u9009\u6c11\u666e\u9009\u4ea7\u751f\uff0c\u6bcf\u4e24\u5e74\u9009\u4e3e\u4e00\u6b21\u3002\u8fd9\u4e9b\u89c4\u5b9a\u4f53\u73b0\u4e86\nA. \u4f17\u8bae\u9662\u4e3b\u5bfc\u8054\u90a6\u7684\u7acb\u6cd5\u6743\u529b\nB. \u8054\u90a6\u5236\u4e0e\u83c7\u548c\u5236\u7684\u539f\u5219\nC. \u5404\u5dde\u6839\u636e\u4eba\u53e3\u5206\u4eab\u8054\u90a6\u6743\u529b\nD. \u53c2\u8bae\u9662\u4ee3\u8868\u5c11\u6570\u4eba\u5229\u76ca\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.319195801224909, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5015219447473117, "meta-llama/Meta-Llama-3-8B": 0.4109839681056239, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u674e\u53cc\u540c\u5b66\u6709\u6b63\u7248WPS2000\u8f6f\u4ef6\uff0c\u5f20\u534e\u540c\u5b66\u6ca1\u6709\uff0c\u4f46\u4ed6\u60f3\u62e5\u6709\u3002\u4f60\u8ba4\u4e3a\u5f20\u534e\u540c\u5b66\u53ef\u884c\u7684\u662f\nA. \u5230\u8f6f\u4ef6\u4e13\u5356\u5e97\u4e70\u4e00\u4efd\nB. \u8d81\u674e\u53cc\u540c\u5b66\u4e0d\u5728\u53bb\u590d\u5236\u4e00\u4efd\nC. \u5230\u5176\u5b83\u5730\u65b9\u590d\u5236\u4e00\u4efd\nD. \u7ecf\u674e\u53cc\u540c\u5b66\u540c\u610f\u540e\u624d\u590d\u5236\u4e00\u4efd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3660094373481115, "meta-math/MetaMath-Mistral-7B": 0.6431519519894604, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.48333194926754713, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6167229872809933, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u68ad\u4f26\u4fc3\u8fdb\u4e86\u4e24\u4e2a\u9636\u7ea7\u4e4b\u95f4\u7684\u59a5\u534f\u3002\u201d\u5982\uff0c\u68ad\u4f26\u6539\u9769\u867d\u5bf9\u571f\u5730\u6570\u91cf\u6709\u6240\u9650\u5236\uff0c\u4f46\u5374\u6ca1\u6709\u50cf\u5e73\u6c11\u6240\u5e0c\u671b\u7684\u90a3\u6837\u5265\u593a\u8d35\u65cf\u7684\u571f\u5730\u3002\u4e0b\u5217\u5404\u9879\uff0c\u80fd\u591f\u6b63\u786e\u53cd\u6620\u4e0a\u8ff0\u89c2\u70b9\u4e14\u7b26\u5408\u53f2\u5b9e\u7684\u662f\nA. \u65e2\u7167\u987e\u4e86\u5e73\u6c11\u5229\u76ca\u53c8\u7ef4\u62a4\u4e86\u8d35\u65cf\u90e8\u5206\u7279\u6743\nB. \u65e2\u5e9f\u9664\u4e86\u5e73\u6c11\u503a\u52a1\u53c8\u672a\u5f52\u8fd8\u5e73\u6c11\u539f\u6709\u571f\u5730\nC. \u65e2\u7981\u6b62\u4e86\u4eba\u8eab\u62b5\u62bc\u53c8\u672a\u89c4\u5b9a\u4e2a\u4eba\u571f\u5730\u9650\u989d\nD. \u65e2\u89e3\u653e\u4e86\u503a\u52a1\u5974\u96b6\u53c8\u4fdd\u7559\u4e86\u65e7\u7684\u793e\u4f1a\u7b49\u7ea7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8343804712310718, "meta-math/MetaMath-Mistral-7B": 0.9935713842059163, "itpossible/Chinese-Mistral-7B-v0.1": 0.7894832775691543, "HuggingFaceH4/zephyr-7b-beta": 0.9999822897934428, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9850635613536629, "meta-llama/Meta-Llama-3-8B": 0.6662656595944675, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.997249346840926}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u5236\u5b9a\u4f01\u4e1a\u7ecf\u8425\u51b3\u7b56\u7684\u91cd\u8981\u4f9d\u636e\u662f\nA. \u4f1a\u8ba1\u8d23\u4efb\nB. \u4f1a\u8ba1\u4fe1\u606f\nC. \u4f1a\u8ba1\u5de5\u4f5c\nD. \u4f1a\u8ba1\u6559\u80b2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6856572628784005, "meta-math/MetaMath-Mistral-7B": 0.844989491369028, "itpossible/Chinese-Mistral-7B-v0.1": 0.8343113289073045, "HuggingFaceH4/zephyr-7b-beta": 0.8578289769390582, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6848319903621987, "meta-llama/Meta-Llama-3-8B": 0.9338905331894853, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9876960564794846}}, {"question": "\u8f66\u8f86\u5728\u96e8\u5929\u4e34\u65f6\u505c\u8f66\u65f6\uff0c\u5e94\u5f00\u542f\u4ec0\u4e48\u706f\nA. \u5371\u9669\u62a5\u8b66\u95ea\u5149\u706f\nB. \u524d\u540e\u96fe\u706f\nC. \u5012\u8f66\u706f\nD. \u524d\u5927\u706f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u54ea\u4e9b\u5bf9\u300c\u7c7b\u578b 1\uff08Type-1\uff09\u300d\u548c\u300c\u7c7b\u578b 2\uff08Type-2\uff09\u300d\u9519\u8bef\u7684\u63cf\u8ff0\u662f\u9519\u8bef\u7684\nA. \u7c7b\u578b 1 \u9519\u8bef\u901a\u5e38\u5728\u5176\u662f\u6b63\u786e\u7684\u60c5\u51b5\u4e0b\u62d2\u7edd\u5047\u8bbe\u800c\u51fa\u73b0\nB. \u7c7b\u578b 1 \u901a\u5e38\u79f0\u4e4b\u4e3a\u5047\u6b63\u7c7b\uff0c\u7c7b\u578b 2 \u901a\u5e38\u79f0\u4e4b\u4e3a\u5047\u8d1f\u7c7b\nC. \u4ee5\u4e0a\u90fd\u662f\nD. \u7c7b\u578b 2 \u901a\u5e38\u79f0\u4e4b\u4e3a\u5047\u6b63\u7c7b\uff0c\u7c7b\u578b 1 \u901a\u5e38\u79f0\u4e4b\u4e3a\u5047\u8d1f\u7c7b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3670863716846745, "meta-math/MetaMath-Mistral-7B": 0.6805758932596514, "itpossible/Chinese-Mistral-7B-v0.1": 0.5374779662462528, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5640275033761161, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.49787407325630095}}, {"question": "\u6211\u56fd\u5728\u5c11\u6570\u6c11\u65cf\u805a\u5c45\u5730\u5b9e\u884c\u5c11\u6570\u6c11\u65cf\u533a\u57df\u81ea\u6cbb\uff0c\u4e0b\u5217\u884c\u653f\u7ea7\u522b\u5355\u4f4d\u4e2d\u4e0d\u5c5e\u4e8e\u6c11\u65cf\u533a\u57df\u81ea\u6cbb\u7684\u662f\nA. \u81ea\u6cbb\u53bf\uff08\u65d7\uff09\nB. \u6c11\u65cf\u4e61\nC. \u81ea\u6cbb\u5dde\uff08\u76df\uff09\nD. \u81ea\u6cbb\u533a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.32567743233302715, "meta-math/MetaMath-Mistral-7B": 0.46564297812935895, "itpossible/Chinese-Mistral-7B-v0.1": 0.40067966148457895, "HuggingFaceH4/zephyr-7b-beta": 0.9222641562698827, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5eb7\u8fbe\u516c\u53f8\u589e\u52a0\u4e86\u90ae\u5bc4\u548c\u8d85\u7ea7\u5e02\u573a\u4e24\u79cd\u6e20\u9053\uff0c\u5f15\u8d77\u4e86\u539f\u6709\u6e20\u9053\u7ecf\u9500\u5546\u7684\u5f3a\u70c8\u4e0d\u6ee1\u548c\u6297\u8bae\uff0c\u8fd9\u79cd\u6e20\u9053\u51b2\u7a81\u5c5e\u4e8e\nA. \u6f5c\u5728\u6e20\u9053\u51b2\u7a81\nB. \u5782\u76f4\u6e20\u9053\u51b2\u7a81\nC. \u6c34\u5e73\u6e20\u9053\u51b2\u7a81\nD. \u591a\u6e20\u9053\u51b2\u7a81\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35961757495166047, "meta-math/MetaMath-Mistral-7B": 0.42000512758000536, "itpossible/Chinese-Mistral-7B-v0.1": 0.4565159574845639, "HuggingFaceH4/zephyr-7b-beta": 0.910522998758754, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f71\u54cd\u8bb0\u8005\u5199\u4f5c\u5fc3\u7406\u72b6\u6001\u7684\u5ba2\u89c2\u56e0\u7d20\u4e0d\u5305\u62ec\nA. \u53d7\u4f17\nB. \u91c7\u8bbf\u5bf9\u8c61\nC. \u91c7\u8bbf\u7d20\u6750\nD. \u73af\u5883\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.40463997558826925}}, {"question": "\u679c\u8747\u751f\u6b96\u7ec6\u80de\u7684\u7ec6\u80de\u6838\u5185\u6709\u51e0\u6761\u67d3\u8272\u4f53\nA. 4\nB. 8\nC. 12\nD. 2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33186780620856055, "meta-math/MetaMath-Mistral-7B": 0.4320953178829167, "itpossible/Chinese-Mistral-7B-v0.1": 0.3403147063768956, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.31911523504877376, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9287927880315638}}, {"question": "\u57cb\u5728\u5e9f\u589f\u4e2d\u5982\u4f55\u7ef4\u6301\u751f\u547d\nA. \u6811\u7acb\u575a\u5b9a\u7684\u751f\u5b58\u4fe1\u5ff5\nB. \u4e0d\u8981\u5927\u54ed\u5927\u53eb\uff0c\u5e94\u4fdd\u5b58\u4f53\u529b\nC. \u4ee5\u4e0a\u90fd\u5bf9\nD. \u4e0d\u8981\u5750\u7acb\u4e0d\u5b89\uff0c\u52c9\u5f3a\u884c\u52a8\uff0c\u5c3d\u91cf\u4f11\u606f\uff0c\u95ed\u76ee\u517b\u795e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5044932834085539, "meta-math/MetaMath-Mistral-7B": 0.7442074388655858, "itpossible/Chinese-Mistral-7B-v0.1": 0.6554591160413797, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.625630131619127, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.550160962529149}}, {"question": "\u5b8c\u6574\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\u7531\uff08 \uff09\u7ec4\u6210\nA. \u8fd0\u7b97\u5668\u3001\u63a7\u5236\u5668\u3001\u5b58\u50a8\u5668\u3001\u8f93\u2f0a\u8bbe\u5907\u548c\u8f93\u51fa\u8bbe\u5907\nB. \u786c\u4ef6\u7cfb\u7edf\u548c\u8f6f\u4ef6\u7cfb\u7edf\nC. \u4e3b\u673a\u548c\u5916\u90e8\u8bbe\u5907\nD. \u4e3b\u673a\u7bb1\u3001\u663e\u793a\u5668\u3001\u952e\u76d8\u3001\u2fcf\u6807\u3001\u6253\u5370\u673a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7256605559314677, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d1f\u8f7d\u4e0a\u83b7\u5f97\u6700\u5927\u529f\u7387\u65f6\uff0c\u7535\u6e90\u7684\u5229\u7528\u7387\u5927\u7ea6\u4e3a\nA. 80%\nB. 50%\nC. 70%\nD. 60%\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3588823130168717, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.307023896814742, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5925811352753513}}, {"question": "\u4e2d\u56fd\u52a0\u5165\u653f\u5e9c\u95f4\u56fd\u9645\u7ec4\u7ec7\u7684\u6570\u91cf\u4ece1977\u5e74\u768421\u4e2a\u589e\u52a0\u52301996\u5e74\u768451\u4e2a\uff0c\u51e0\u4e4e\u52a0\u5165\u4e86\u8054\u5408\u56fd\u4f53\u7cfb\u4e2d\u7684\u6240\u6709\u91cd\u8981\u7684\u653f\u5e9c\u95f4\u7ec4\u7ec7\u3002\u4ece\u4e2d\u53ef\u4ee5\u770b\u51fa\nA. \u4e2d\u56fd\u79ef\u6781\u5f00\u5c55\u4ee5\u8054\u5408\u56fd\u4e3a\u4e2d\u5fc3\u7684\u591a\u8fb9\u5916\u4ea4\nB. \u4e2d\u56fd\u72ec\u7acb\u81ea\u4e3b\u5916\u4ea4\u65b9\u9488\u5177\u4f53\u8868\u73b0\u4e3a\u4e0d\u7ed3\u76df\nC. \u4e2d\u56fd\u79ef\u6781\u63a8\u52a8\u548c\u5f00\u521b\u65b0\u578b\u7684\u533a\u57df\u5408\u4f5c\u6a21\u5f0f\nD. \u4e2d\u56fd\u5f00\u59cb\u4ee5\u4e3b\u52a8\u59ff\u6001\u8c0b\u6c42\u5efa\u7acb\u65b0\u7684\u56fd\u9645\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4818199322913993, "itpossible/Chinese-Mistral-7B-v0.1": 0.5359319753038484, "HuggingFaceH4/zephyr-7b-beta": 0.8817187283409615, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8010968235255932, "meta-llama/Meta-Llama-3-8B": 0.5117501663048895, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7585740334330955}}, {"question": "\u5973\uff0c45\u5c81\u3002\u53cd\u590d\u5251\u7a81\u4e0b\u75bc\u75db3\u5e74\uff0c\u5455\u541010\u5929\uff0c\u5455\u5410\u7269\u6709\u9694\u591c\u5bbf\u98df\u3002\u8be5\u60a3\u8005\u6700\u6613\u53d1\u751f\u7684\u9178\u78b1\u5e73\u8861\u5931\u8c03\u662f\nA. \u4ee3\u8c22\u6027\u9178\u4e2d\u6bd2\nB. \u4ee3\u8c22\u6027\u78b1\u4e2d\u6bd2\nC. \u547c\u5438\u6027\u9178\u4e2d\u6bd2\nD. \u547c\u5438\u6027\u78b1\u4e2d\u6bd2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.33767879625406966, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c38\u4e50\u5927\u5178\u662f\u5728\u54ea\u91cc\u7f16\u6210\u7684\nA. \u56db\u5ddd\nB. \u4e0a\u6d77\nC. \u5357\u4eac\nD. \u5317\u4eac\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u516c\u5171\u5173\u7cfb\u7684\u56db\u5c42\u6db5\u4e49\u3001\nA. \u516c\u5173\u610f\u8bc6\u3001\u516c\u5173\u7406\u8bba\u3001\u516c\u5173\u884c\u4e3a\u3001\u516c\u5173\u72b6\u6001\nB. \u516c\u5173\u610f\u8bc6\u3001\u516c\u5173\u89c2\u5ff5\u3001\u516c\u5173\u6d3b\u52a8\u3001\u516c\u5173\u72b6\u6001\nC. \u516c\u5173\u72b6\u6001\u3001\u516c\u5173\u6d3b\u52a8\u3001\u516c\u5173\u89c2\u5ff5\u3001\u516c\u5173\u5b66\u79d1\nD. \u516c\u5173\u610f\u8bc6\u3001\u516c\u5173\u6d3b\u52a8\u3001\u516c\u5173\u4f20\u64ad\u3001\u516c\u5173\u72b6\u6001\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u53e4\u4ee3\u56db\u5927\u7f8e\u5973\u4e2d\u7684\u201c\u6c89\u9c7c\u201d\u662f\u7528\u6765\u5f62\u5bb9\u54ea\u4e00\u4f4d\nA. \u8c82\u8749\nB. \u897f\u65bd\nC. \u738b\u662d\u541b\nD. \u6768\u8d35\u5983\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8267979671332798, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6804111486326772, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6329240534748289}}, {"question": "\u5173\u4e8e\u4f5c\u2f64\u2f12\u548c\u53cd\u4f5c\u2f64\u2f12\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u5f39\u2f12\u4e0e\u6469\u64e6\u2f12\u90fd\u6709\u53cd\u4f5c\u2f64\u2f12\uff0c\u2f7d\u91cd\u2f12\u2f46\u53cd\u4f5c\u2f64\u2f12\nB. \u4f5c\u2f64\u2f12\u4e0e\u53cd\u4f5c\u2f64\u2f12\u7684\u5408\u2f12\u4e3a\u96f6\nC. \u4f5c\u2f64\u2f12\u4e0e\u53cd\u4f5c\u2f64\u2f12\u2f00\u5b9a\u662f\u6027\u8d28\u76f8\u540c\u7684\u2f00\u5bf9\u2f12\nD. \u5f53\u4f5c\u2f64\u2f12\u4ea7\u2f63\u540e\uff0c\u518d\u4ea7\u2f63\u53cd\u4f5c\u2f64\u2f12\uff1b\u5f53\u4f5c\u2f64\u2f12\u6d88\u5931\u540e\uff0c\u53cd\u4f5c\u2f64\u2f12\u624d\u6162\u6162\u7684\u6d88\u5931\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u5173\u4e8e\u8ba1\u7b97\u673a\u75c5\u6bd2\u4e0e\u8815\u866b\u7684\u7279\u70b9\u6bd4\u8f83\u7684\u53d9\u8ff0\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u5728\u4f20\u67d3\u673a\u5236\u4e2d\uff0c\u8815\u866b\u662f\u901a\u8fc7\u4e3b\u8981\u7a0b\u5e8f\u8fd0\u884c\u7684\nB. \u8815\u866b\u548c\u75c5\u6bd2\u90fd\u662f\u5bc4\u751f\u6a21\u5f0f\u751f\u5b58\nC. \u4e3a\u7cfb\u7edf\u6253\u8865\u4e01\uff0c\u80fd\u6709\u6548\u9884\u9632\u8815\u866b\uff0c\u4f46\u4e0d\u80fd\u6709\u6548\u9884\u9632\u75c5\u6bd2\nD. \u5728\u89e6\u53d1\u673a\u5236\u4e2d\uff0c\u8815\u866b\u7684\u89e6\u53d1\u8005\u662f\u8ba1\u7b97\u673a\u4f7f\u7528\u8005\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5341644735804267, "meta-llama/Meta-Llama-3-8B": 0.36078609119776345, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.606340174174777}}, {"question": "\u5728\u5927\u8111\u52a8\u8109\u73af\u6784\u6210\u4e2d\uff0c\u65e0\u4e0b\u5217\u54ea\u9879\u7ed3\u6784\nA. \u5927\u8111\u524d\u52a8\u8109\nB. \u9888\u5185\u52a8\u8109\nC. \u5927\u8111\u540e\u52a8\u8109\nD. \u5927\u8111\u4e2d\u52a8\u8109\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.39188406064306225, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38587209961415087, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5340458937685749}}, {"question": "\u5728Excel\u4e2d\uff0c\u4e0b\u6765\u65b9\u6cd5\u53ef\u5b9e\u73b0\u5feb\u901f\u67e5\u627e\u6ee1\u8db3\u6761\u4ef6\u7684\u6570\u636e\u5185\u5bb9\u7684\u662f\nA. \u5206\u7c7b\u6c47\u603b\nB. \u81ea\u52a8\u7b5b\u9009\nC. \u6392\u5e8f\nD. \u6570\u636e\u5355\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9319277143516455, "meta-math/MetaMath-Mistral-7B": 0.9942543110574904, "itpossible/Chinese-Mistral-7B-v0.1": 0.9443104514515602, "HuggingFaceH4/zephyr-7b-beta": 0.9989562869050339, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9936228228868761, "meta-llama/Meta-Llama-3-8B": 0.665060913036276, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9923173419383013}}, {"question": "\u751f\u7406\u5b66\u5dee\u5f02\u6027\u6784\u67b6\u4e0d\u540c\u7684\u7537\u5973\u56e0\u5e94\u6027\u751f\u7406\u53cd\u6620\u7684\u6027\u6210\u719f\u6709\u7740\u5404\u81ea\u7279\u5f02\u7684\nA. \u5fc3\u7406\u6bb5\nB. \u65f6\u95f4\u6bb5\nC. \u5e74\u9f84\u6bb5\nD. \u884c\u4e3a\u6bb5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34993229697144135, "meta-math/MetaMath-Mistral-7B": 0.4550475102366106, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4863146217654378, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8827996030317472}}, {"question": "\u660e\u521d\u5e9f\u884c\u7701\uff0c\u5730\u65b9\u5206\u8bbe\u4e09\u53f8\uff0c\u5206\u522b\u638c\u7ba1\u4e00\u5730\u6c11\u653f\u4e0e\u8d22\u653f\u3001\u53f8\u6cd5\u3001\u519b\u4e8b\uff0c\u76f4\u5c5e\u516d\u90e8\u3002\u660e\u4e2d\u53f6\u4ee5\u540e\uff0c\u7687\u5e1d\u4e34\u65f6\u6d3e\u9063\u7684\u5de1\u629a\u9010\u6e10\u6f14\u53d8\u4e3a\u4e09\u53f8\u4e4b\u4e0a\u7684\u5730\u65b9\u6700\u9ad8\u884c\u653f\u957f\u5b98\u3002\u8fd9\u4e00\u53d8\u5316\u6709\u52a9\u4e8e\nA. \u63d0\u9ad8\u5730\u65b9\u884c\u653f\u6548\u7387\nB. \u7f13\u89e3\u4e2d\u592e\u4e0e\u5730\u65b9\u7684\u5bf9\u7acb\nC. \u524a\u5f31\u516d\u90e8\u7684\u6743\u9650\nD. \u6269\u5927\u5730\u65b9\u884c\u653f\u6743\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38797403658066004, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u76d1\u89c6\u5668\u5411\u4ee3\u7406\u53d1\u51fa\u8bf7\u6c42\uff0c\u8be2\u95ee\u5b83\u6240\u9700\u8981\u7684\u4fe1\u606f\u503c\uff0c\u4ee3\u7406\u54cd\u5e94\u76d1\u89c6\u5668\u7684\u8bf7\u6c42\uff0c\u4ece\u5b83\u6240\u4fdd\u5b58\u7684\u7ba1\u7406\u4fe1\u606f\u5e93\u4e2d\u53d6\u51fa\u8bf7\u6c42\u7684\u503c\uff0c\u8fd4\u56de\u7ed9\u76d1\u89c6\u5668\u3002\u8fd9\u79cd\u901a\u4fe1\u673a\u5236\u53eb\u505a\nA. \u54cd\u5e94\nB. \u8bf7\u6c42\nC. \u4e8b\u4ef6\u62a5\u544a\nD. \u8f6e\u8be2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38671798231464344, "meta-llama/Meta-Llama-3-8B": 0.4527911628257128, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bfc\u81f4\u534e\u6c99\u6761\u7ea6\u7ec4\u7ec7\u6210\u7acb\u7684\u4e3b\u8981\u539f\u56e0\u662f\nA. 1955\u5e745\u67085\u65e5\u8054\u90a6\u5fb7\u56fd\u6b63\u5f0f\u52a0\u5165\u5317\u7ea6\nB. 1949\u5e74\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u6210\u7acb\nC. 1950\u5e74\u671d\u9c9c\u6218\u4e89\u7206\u53d1\nD. 1954\u5e74\u8d8a\u5357\u4eba\u6c11\u5f00\u5c55\u6297\u6cd5\u6218\u4e89\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c5e\u4e8e\u6c14\u5019\u8d44\u6e90\u7684\u662f\nA. \u964d\u6c34\nB. \u571f\u58e4\nC. \u5730\u4e0b\u6c34\nD. \u5730\u8868\u6c34\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3423962339378881, "meta-math/MetaMath-Mistral-7B": 0.38893668499518336, "itpossible/Chinese-Mistral-7B-v0.1": 0.5673844363985943, "HuggingFaceH4/zephyr-7b-beta": 0.8048417334368306, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6109588524261915, "meta-llama/Meta-Llama-3-8B": 0.6109588368402582, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.533732493081946}}, {"question": "\u4f4f\u5b85\u8d28\u91cf\u5206\u6237\u9a8c\u6536\u8868\u300b\u5e94\u7531\u54ea\u4e2a\u5355\u4f4d\u51fa\u5177\nA. \u5efa\u8bbe\u5355\u4f4d\nB. \u65bd\u5de5\u5355\u4f4d\nC. \u76d1\u7406\u5355\u4f4d\nD. \u653f\u5e9c\u8d28\u91cf\u76d1\u7763\u673a\u6784\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u7ed3\u6784\u4e2d\uff0c\u54ea\u4e2a\u4e0d\u5c5e\u4e8e\u5c48\u5149\u88c5\u7f6e\nA. \u8679\u819c\nB. \u6676\u72b6\u4f53\nC. \u89d2\u819c\nD. \u73bb\u7483\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.32871459371036227, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.31649016643535544, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6309\u968f\u673a\u65b9\u6cd5\u62bd\u53d6\u7684\u6837\u672c\u7279\u70b9\u662f:\nA. \u80fd\u6d88\u9664\u62bd\u6837\u8bef\u5dee\nB. \u80fd\u51cf\u5c11\u6837\u672c\u504f\u6027\nC. \u80fd\u6d88\u9664\u968f\u673a\u6d4b\u91cf\u8bef\u5dee\nD. \u80fd\u6d88\u9664\u7cfb\u7edf\u8bef\u5dee\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7456692049880786, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5963069382596721, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9587456571974292}}, {"question": "\u4e0d\u4ee5\u8fc7\u53bb\u7684\u5b9e\u9645\u5f00\u652f\u505a\u6807\u51c6\uff0c\u800c\u662f\u6839\u636e\u7ec4\u7ec7\u76ee\u524d\u7684\u9700\u6c42\u548c\u53d1\u5c55\u8d8b\u52bf\u91cd\u65b0\u4f30\u91cf\uff0c\u901a\u8fc7\u5bf9\u6bcf\u9879\u8d39\u7528\u5f00\u652f\u5408\u7406\u6027\u7684\u91cd\u65b0\u5ba1\u5b9a\u800c\u505a\u51fa\u9884\u7b97\u7684\u65b9\u6cd5\u662f\nA. \u201c\u603b\u9884\u7b97\u201d\u6cd5\nB. \u9879\u76ee\u9884\u7b97\u6cd5\nC. \u4f20\u7edf\u9884\u7b97\u6cd5\nD. \u96f6\u57fa\u9884\u7b97\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7064073296852915, "meta-math/MetaMath-Mistral-7B": 0.7667161398713124, "itpossible/Chinese-Mistral-7B-v0.1": 0.9418765197704505, "HuggingFaceH4/zephyr-7b-beta": 0.9615275308418677, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8938816903274005, "meta-llama/Meta-Llama-3-8B": 0.9007533515281587, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9919145334679113}}, {"question": "\u4e0b\u8868\u6240\u793a\u4e3a1956~1959\u5e74\u4e2d\u56fd\u90e8\u5206\u5916\u4ea4\u6d3b\u52a8\u3002\u4e2d\u56fd\u653f\u5e9c\u575a\u51b3\u652f\u6301\u57c3\u53ca\u4eba\u6c11\u6536\u56de\u82cf\u4f0a\u58eb\u8fd0\u6cb3\u4e3b\u6743\u7684\u6b63\u4e49\u6597\u4e89\u3002\u4e2d\u56fd\u9664\u4e86\u7ed9\u4e88\u67ec\u57d4\u5be8\u7ecf\u6d4e\u63f4\u52a9\u5916\uff0c\u8fd8\u4e0e\u5c3c\u6cca\u5c14\u3001\u9521\u5170\u3001\u7f05\u7538\u3001\u5370\u5c3c\u7b49\u56fd\u7b7e\u8ba2\u7ecf\u6d4e\u63f4\u52a9\u7684\u6b63\u5f0f\u534f\u5b9a\u3002\u4e2d\u56fd\u5148\u540e\u4e0e\u632a\u5a01\u3001\u8377\u5170\u3001\u5357\u65af\u62c9\u592b\u3001\u963f\u5bcc\u6c57\u3001\u5c3c\u6cca\u5c14\u3001\u57c3\u53ca\u3001\u53d9\u5229\u4e9a\u3001\u4e5f\u95e8\u3001\u9521\u5170\u7b49\u56fd\u5efa\u7acb\u4e86\u5927\u4f7f\u7ea7\u5916\u4ea4\u5173\u7cfb\uff0c\u540c\u82ac\u5170\u3001\u745e\u58eb\u3001\u4e39\u9ea6\u7531\u516c\u4f7f\u7ea7\u5347\u683c\u4e3a\u5927\u4f7f\u7ea7\u5916\u4ea4\u5173\u7cfb\u3002\u8fd9\u53ef\u7528\u4e8e\u8bf4\u660e\uff0c\u6b64\u65f6\u671f\u7684\u4e2d\u56fd\nA. \u8fce\u6765\u7b2c\u4e00\u6b21\u5efa\u4ea4\u9ad8\u6f6e\nB. \u79ef\u6781\u56e2\u7ed3\u53d1\u5c55\u4e2d\u56fd\u5bb6\nC. \u575a\u6301\u201c\u4e00\u8fb9\u5012\u201d\u7684\u5916\u4ea4\u65b9\u9488\nD. \u5916\u4ea4\u9886\u57df\u6253\u5f00\u65b0\u5c40\u9762\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.42971405836379356, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u82f1\u56fd\u7684\u6234\u897f\u63d0\u51fa\u4e86\u56fd\u9645\u53f8\u6cd5\u4e0a\u7684\nA. \u6cd5\u5219\u533a\u522b\u8bf4\nB. \u65e2\u5f97\u6743\u8bf4\nC. \u56fd\u9645\u793c\u8ba9\u8bf4\nD. \u610f\u601d\u81ea\u6cbb\u8bf4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2929037584813245, "meta-math/MetaMath-Mistral-7B": 0.4101046003501243, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u73b0\u5728\u7ecf\u5e38\u542c\u5230\u4e00\u79cd\u8bf4\u6cd5\uff0c\u667a\u5546\u4e0d\u654c\u60c5\u5546\u3002\u4e00\u4e2a\u4eba\u80fd\u8d70\u5230\u54ea\u91cc\uff0c\u53d6\u5f97\u4ec0\u4e48\u6210\u5c31\uff0c\u5173\u952e\u5728\u4e8e\u60c5\u5546\uff0c\u8fd9\u79cd\u201c\u60c5\u5546\u51b3\u5b9a\u8bba\u201d\u751a\u56a3\u5c18\u4e0a\uff0c\u4ee4\u4eba\u751f\u7591\u3002\u5728\u6211\u4eec\u8eab\u8fb9\uff0c\u65e0\u8bba\u662f\u751f\u6d3b\u4e2d\uff0c\u8fd8\u662f\u5de5\u4f5c\u4e2d\uff0c\u53ea\u8981\u4e00\u4e2a\u4eba\u7684\u4e8b\u60c5\u6ca1\u505a\u597d\uff0c\u90fd\u4f1a\u8ddf\u60c5\u5546\u4e0d\u9ad8\u6302\u8d77\u94a9\u6765\u3002\u4e8b\u5b9e\u4e0a\uff0c\u6211\u4eec\u90fd\u5938\u5927\u4e86\u60c5\u5546\u7684\u4f5c\u7528\uff0c\u5ffd\u89c6\u4e86\u667a\u5546\u3001\u5b9e\u529b\u3001\u52e4\u594b\uff0c\u4ee5\u4e3a\u53ea\u8981\u73a9\u8f6c\u60c5\u5546\u5c31\u80fd\u6b65\u6b65\u9ad8\u5347\uff0c\u5176\u5b9e\u4e0d\u7136\u3002\u4eba\u5728\u804c\u573a\uff0c\u60c5\u5546\u56fa\u7136\u91cd\u8981\uff0c\u4f46\u51b3\u5b9a\u6027\u56e0\u7d20\u8fd8\u662f\u667a\u5546\u548c\u624d\u534e\uff0c\u73a9\u60c5\u5546\u53ef\u4ee5\u8ba9\u4f60\u516b\u9762\u73b2\u73d1\uff0c\u4f46\u6bd5\u7adf\u73a9\u4e0d\u51fa\u5b9e\u9645\u4e1a\u7ee9\uff0c\u4e5f\u73a9\u4e0d\u51fa\u79d1\u7814\u6210\u679c\u3002\u4e00\u4e2a\u4eba\u6709\u771f\u624d\u5b9e\u5b66\uff0c\u672c\u9886\u8fc7\u786c\uff0c\u5de5\u4f5c\u52e4\u594b\uff0c\u90a3\u4e48\u60c5\u5546\u5176\u5b9e\u5c31\u662f\u9526\u4e0a\u6dfb\u82b1\u7684\u4e8b\u60c5\u4e86\u3002\u8fd9\u6bb5\u6587\u5b57\u610f\u5728\u8bf4\u660e\nA. \u60c5\u5546\u53ea\u662f\u9526\u4e0a\u6dfb\u82b1\uff0c\u667a\u5546\u65b9\u80fd\u51b3\u5b9a\u6210\u8d25\nB. \u201c\u60c5\u5546\u51b3\u5b9a\u8bba\u201d\u5e76\u4e0d\u6b63\u786e\uff0c\u60c5\u5546\u7684\u4f5c\u7528\u4e0d\u5e94\u88ab\u5938\u5927\nC. \u6210\u529f\u9760\u7684\u662f\u771f\u624d\u5b9e\u5b66\u3001\u8fc7\u786c\u672c\u9886\uff0c\u800c\u4e0d\u662f\u73a9\u8f6c\u60c5\u5546\nD. \u5728\u804c\u573a\u4e2d\uff0c\u667a\u5546\u6bd4\u60c5\u5546\u66f4\u52a0\u91cd\u8981\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u5173\u4e8e\u53cc\u5b50\u5ea7\u6d41\u661f\u96e8\u8bf4\u6cd5\u9519\u8bef\u7684\u662f\nA. \u6d41\u91cf\u5927\u4e14\u7a33\u5b9a\nB. \u6781\u5927\u51fa\u73b0\u572812\u6708\u4e2d\u65ec\nC. 2017\u5e74\u6708\u5149\u5bf9\u89c2\u6d4b\u7684\u5f71\u54cd\u4e0d\u5927\nD. \u6d41\u661f\u4f53\u901f\u5ea6\u8f83\u5feb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u8df3\u6c34\u754c\u6839\u636e\u8fd0\u52a8\u5458\u5c11\u5e74\u65f6\u671f\u4f53\u91cd\u8f7b\u3001\u5fc3\u7406\u8d1f\u8377\u5c0f\u7684\u7279\u70b9\uff0c\u8dc3\u8fdb\u5f0f\u7684\u53d1\u5c55\u96be\u5ea6\u6280\u672f\uff0c\u8fd9\u662f\u6709\u6548\u7684\u5904\u7406\u4e86\nA. \u89c4\u8303\u5316\u548c\u4e2a\u4f53\u5dee\u5f02\u7684\u5173\u7cfb\nB. \u7279\u957f\u6280\u672f\u548c\u5168\u9762\u6280\u672f\u7684\u5173\u7cfb\nC. \u5408\u7406\u7684\u5185\u90e8\u673a\u5236\u548c\u6b63\u786e\u7684\u5916\u90e8\u5f62\u6001\u7684\u5173\u7cfb\nD. \u5faa\u5e8f\u6e10\u8fdb\u4e0e\u96be\u70b9\u5148\u884c\u7684\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.38733735460863056, "meta-math/MetaMath-Mistral-7B": 0.4176007408374005, "itpossible/Chinese-Mistral-7B-v0.1": 0.32871459371036227, "HuggingFaceH4/zephyr-7b-beta": 0.9392794850154331, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6832000589645548, "meta-llama/Meta-Llama-3-8B": 0.43241138747909885, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e16\u754c\u4e0a\u7b2c\u4e00\u4e2a\u5728\u4f01\u4e1a\u5185\u90e8\u8bbe\u7acb\u516c\u5173\u90e8\u7684\u662f\nA. \u65e5\u672c\u4e30\u7530\u516c\u53f8\nB. \u7f8e\u56fd\u7535\u8bdd\u7535\u62a5\u516c\u53f8\nC. \u7f8e\u56fd\u798f\u7279\u6c7d\u8f66\u516c\u53f8\nD. \u65e5\u672c\u677e\u4e0b\u7535\u5668\u516c\u53f8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3649382996682295, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5052963379994225, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3742744353722653, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4184174010079213}}, {"question": "\u9762\u5411\u6570\u636e\u6316\u6398\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u4e3b\u8981\u89e3\u9ad8\u5c42\u5e94\u7528\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u81f4\u529b\u4e8e\u7814\u7a76\u5982\u4f55\u6839\u636e\u4e0d\u540c\u6570\u636e\u6316\u6398\u64cd\u4f5c\u7684\u7279\u5f81\u6765\u5b9e\u73b0\u5bf9\u9690\u79c1\u7684\u4fdd\u62a4\uff0c\u4ece\u6570\u636e\u6316\u7684\u89d2\u5ea6\uff0c\u4e0d\u5c5e\u4e8e\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u7684\u662f\nA. \u57fa\u4e8e\u6570\u636e\u533f\u540d\u5316\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\nB. \u57fa\u4e8e\u5fae\u636e\u5931\u771f\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\nC. \u57fa\u4e8e\u6570\u636e\u52a0\u5bc6\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\nD. \u57fa\u4e8e\u6570\u636e\u5206\u6790\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6521752050954049, "meta-math/MetaMath-Mistral-7B": 0.8082846380628782, "itpossible/Chinese-Mistral-7B-v0.1": 0.31483005318115603, "HuggingFaceH4/zephyr-7b-beta": 0.9856638030557069, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9515867797966364, "meta-llama/Meta-Llama-3-8B": 0.6034309579864038, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.894144327715385}}, {"question": "\u7b2c\u4e00\u4e2a\u89c4\u5b9a\u9886\u7a7a\u4e3b\u6743\u539f\u5219\u7684\u56fd\u9645\u516c\u7ea6\u662f\nA. \u300a\u56fd\u9645\u6c11\u7528\u822a\u7a7a\u516c\u7ea6\u300b\nB. \u300a\u7edf\u4e00\u56fd\u9645\u822a\u7a7a\u8fd0\u8f93\u67d0\u4e9b\u89c4\u5219\u7684\u516c\u7ea6\u300b\nC. \u300a\u6d77\u6d0b\u6cd5\u516c\u7ea6\u300b\nD. \u300a\u5df4\u9ece\u822a\u7a7a\u516c\u7ea6\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4260970579271916, "meta-math/MetaMath-Mistral-7B": 0.7843643793281065, "itpossible/Chinese-Mistral-7B-v0.1": 0.3582379679317444, "HuggingFaceH4/zephyr-7b-beta": 0.9992396606262552, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6255106909075493, "meta-llama/Meta-Llama-3-8B": 0.42855371553126287, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8004402044422574}}, {"question": "\u6211\u56fd\u8ba1\u7b97\u65e5\u5e73\u5747\u6d41\u91cf\u7684\u65e5\u5206\u754c\u662f\u4ece()\u65f6\u81f3()\u65f6\u3002\nA. 0~24\nB. 20~20\nC. 12~12\nD. 08~08\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7771009671070433, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u628a\u519c\u4e1a\u5f15\u2f0a\u57ce\u5e02\uff0c\u4f7f\u519c\u4e1a\u2f63\u4ea7\u4e0e\u57ce\u5e02\u7a7a\u95f4\u76f8\u878d\u5408\uff0c\u80fd\u591f\u6709\u6548\u6539\u5584\u5f53\u524d\u57ce\u5e02\u53d1\u5c55\u4e2d\u4ea7\u2f63\u7684\u95ee\u9898\uff0c\u6709\u5229\u4e8e\u57ce\u5e02\u53ef\u6301\u7eed\u53d1\u5c55\u3002\u8fd1\u5e74\u6765\uff0c\u6211\u56fd\u6709\u5b66\u8005\u63d0\u51fa\u4e86\u5c06\u519c\u4e1a\u4e0e\u57ce\u5e02\u4e2d\u7684\u793e\u533a\u76f8\u878d\u5408\u7684\u201c\u6709\u519c\u793e\u533a\u201d\u7684\u6784\u60f3\uff08\u7cfb\u7edf\uff09\u3002\u201c\u6709\u519c\u793e\u533a\u201d\u6700\u4e3b\u8981\u7684\u610f\u4e49\u662f\nA. \u7f8e\u5316\u793e\u533a\u7684\u2f63\u6d3b\u73af\u5883\nB. \u63d0\u4f9b\u4fbf\u6377\u7684\u2edd\u7269\u4f9b\u5e94\nC. \u6539\u53d8\u5c45\u2ea0\u7684\u2f63\u4ea7\u2f45\u5f0f\nD. \u5b9e\u73b0\u7269\u8d28\u7684\u5faa\u73af\u5229\u2f64\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3454901351726419, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7b2c\u4e00\u6b21\u4e16\u754c\u5927\u6218\u671f\u95f4\uff0c\u4e00\u4e9b\u9752\u5e74\u827a\u672f\u5bb6\u5728\u745e\u58eb\u7ec4\u6210\u827a\u672f\u7fa4\u4f53\u201c\u8fbe\u8fbe\u6d3e\u201d\u3002\u4ed6\u4eec\u7528\u7eb8\u7247\u3001\u62b9\u5e03\u3001\u7535\u8f66\u7968\u3001\u706b\u67f4\u76d2\u7b49\u8fdb\u884c\u521b\u4f5c\uff0c\u751a\u81f3\u628a\u74f7\u8d28\u7684\u5c0f\u4fbf\u5668\u547d\u540d\u4e3a\u201c\u55b7\u6cc9\u201d\u642c\u4e0a\u5c55\u89c8\u4f1a\u3002\u8fd9\u7c7b\u4f5c\u54c1\nA. \u9075\u5faa\u4e86\u5199\u5b9e\u539f\u5219\nB. \u7a81\u51fa\u4e86\u7406\u6027\u601d\u7ef4\nC. \u8868\u8fbe\u4e86\u5e7b\u706d\u53cd\u53db\nD. \u6292\u53d1\u4e86\u6d6a\u6f2b\u60c5\u6000\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8124991743109643, "meta-math/MetaMath-Mistral-7B": 0.9766052801279166, "itpossible/Chinese-Mistral-7B-v0.1": 0.8861745402615006, "HuggingFaceH4/zephyr-7b-beta": 0.9789232242299785, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8945906674502425, "meta-llama/Meta-Llama-3-8B": 0.9609459341186243, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9994772517080245}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\uff0c\u542b\u6709\u53cc\u5bbe\u8bed\u7684\u4e00\u53e5\u662f\nA. \u592b\u6649\u4f55\u53ad\u4e4b\u6709?\nB. \u91cd\u7232\u4e4b\u79ae\u800c\u6b78\u4e4b\u3002\nC. \u5154\u4e0d\u53ef\u5fa9\u5f97\uff0c\u800c\u8eab\u7232\u5b8b\u570b\u7b11\u3002\nD. \u751a\u77e3\uff0c\u6c5d\u4e4b\u4e0d\u60e0!\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3825322550539375, "HuggingFaceH4/zephyr-7b-beta": 0.6498069884250811, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u5f88\u65e9\u5c31\u6709\u4e86\u7a7f\u6728\u5c50\u7684\u76f8\u5173\u53f2\u4e66\u8bb0\u8f7d\uff0c\u4e0b\u9762\u7684\u978b\u5c31\u662f\u4e1c\u664b\u65f6\u8c22\u7075\u8fd0\u53d1\u751f\u7684\u201c\u8c22\u516c\u5c50\u201d\uff0c\u5b83\u5f53\u65f6\u7684\u7528\u9014\u662f\nA. \u821e\u978b\nB. \u767b\u5c71\u978b\nC. \u56de\u5bb6\u65f6\u7a7f\u7684\u978b\nD. \u4e0a\u671d\u7a7f\u7684\u671d\u978b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2920177551543942, "meta-math/MetaMath-Mistral-7B": 0.31555678880677207, "itpossible/Chinese-Mistral-7B-v0.1": 0.5425125249165893, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.29660173325630934, "meta-llama/Meta-Llama-3-8B": 0.32366165066143693, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f7f$log_{2}a>log_{3}27$\u6210\u2f74a\u7684\u53d6\u503c\u8303\u56f4\u662f\nA. $(3,+\\infty )$\nB. $(8,+\\infty )$\nC. $(0,+\\infty )$\nD. $(9,+\\infty )$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.27312720402870727, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.34993200875877273}}, {"question": "\u201c\u4e00\u4e94\u201d\u8ba1\u5212\u671f\u95f4\uff0c\u6211\u56fd\u4f4f\u5b85\u5efa\u8bbe\u5360\u57fa\u672c\u5efa\u8bbe\u6295\u8d44\u989d\u7684\u6bd4\u91cd\u4e0d\u65ad\u51cf\u5c11\uff0c\u5176\u4ed6\u975e\u751f\u4ea7\u6027\u5efa\u8bbe\u6295\u8d44\u4e5f\u5f00\u59cb\u53d7\u5230\u6291\u5236\u3002\u8fd9\u8868\u660e\u6211\u56fd\nA. \u81f4\u529b\u4e8e\u5960\u5b9a\u5de5\u4e1a\u5316\u57fa\u7840\nB. \u57ce\u5e02\u5316\u7684\u8fdb\u7a0b\u8d8b\u4e8e\u7f13\u6162\nC. \u56fd\u6c11\u7ecf\u6d4e\u7ed3\u6784\u81fb\u4e8e\u5e73\u8861\nD. \u5927\u529b\u538b\u7f29\u57fa\u672c\u5efa\u8bbe\u6295\u8d44\u89c4\u6a21\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4509742556161641, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1981\u5e74\u4e2d\u5171\u4e2d\u592e27\u53f7\u6587\u4ef6\u6307\u51fa\uff0c\u7ecf\u6d4e\u7279\u533a\u7684\u201c\u7279\u201d\u4e3b\u8981\u5728\u4e8e\u5b9e\u884c\u56fd\u5bb6\u89c4\u5b9a\u7684\u7279\u6b8a\u7ecf\u6d4e\u653f\u7b56\u548c\u7279\u6b8a\u7ecf\u6d4e\u7ba1\u7406\u4f53\u5236\uff0c\u5b83\u4e3b\u8981\u5305\u62ec\uff1aa\u8f83\u5927\u7684\u7ecf\u6d4e\u7ba1\u7406\u6743\u9650\uff1bb\u72ec\u7acb\u7684\u7ba1\u7406\u4f53\u5236\uff1bc\u793e\u4f1a\u4e3b\u4e49\u7ecf\u6d4e\u9886\u5bfc\u4e0b\u7684\u591a\u79cd\u7ecf\u6d4e\u6210\u5206\u5e76\u5b58\uff1bd\u5e02\u573a\u8c03\u8282\u4e3a\u4e3b\nA. abd\nB. acd\nC. bcd\nD. abc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.26560468668687814, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6a2a\u9053\u56fe\u8fdb\u5ea6\u8ba1\u5212\u4e2d\u6709\u5173\u65f6\u95f4\u8868\u793a\u7684\u8bf4\u6cd5\uff0c\u6b63\u786e\u7684\u662f\nA. \u6a2a\u9053\u56fe\u4e0d\u80fd\u8868\u793a\u51fa\u505c\u5de5\u65f6\u95f4\nB. \u6700\u5c0f\u7684\u65f6\u95f4\u5355\u4f4d\u662f\u5929\nC. \u65f6\u95f4\u5355\u4f4d\u53ef\u4ee5\u662f\u5de5\u4f5c\u65e5\nD. \u6a2a\u9053\u53ef\u8868\u793a\u5de5\u4f5c\u6700\u8fdf\u5f00\u59cb\u65f6\u95f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.581166826233746}}, {"question": "\u300a\u5df4\u95e8\u5c3c\u5fb7\u65af\u7bc7\u300b\u662f\u8c01\u7684\u4f5c\u54c1\uff1f\nA. \u5df4\u95e8\u5c3c\u5fb7\nB. \u82cf\u683c\u62c9\u5e95\nC. \u67cf\u62c9\u56fe\nD. \u5eb7\u5fb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b59\u4e2d\u5c71\u8ba4\u4e3a\uff0c\u9020\u6210\u6c11\u65cf\u7684\u6839\u672c\u539f\u56e0\u5728\u4e8e\nA. \u98ce\u4fd7\u4e60\u60ef\nB. \u81ea\u7136\u529b\nC. \u8840\u7edf\nD. \u8bed\u8a00\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7b80\u8c10\u632f\u52a8\u7684\u4e00\u4e2a\u632f\u52a8\u5468\u671f\u5185\nA. \u4ee5\u4e0a\u90fd\u4e0d\u5bf9\nB. \u632f\u52a8\u4f4d\u79fb\u4e0d\u76f8\u540c\nC. \u632f\u52a8\u76f8\u4f4d\u4e0d\u76f8\u540c\nD. \u632f\u52a8\u901f\u5ea6\u4e0d\u76f8\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.28966338381871215, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5168\u4e16\u754c\u6709\u8d85\u8fc715000\u4e2a\u6d77\u6d0b\u4fdd\u62a4\u533a\uff0c\u7edd\u5927\u591a\u6570\u4fdd\u62a4\u533a\u90fd\u5141\u8bb8\u5546\u4e1a\u6d3b\u52a8\uff0c\u4eba\u7c7b\u5df2\u7ecf\u5145\u5206\u5229\u7528\u751a\u81f3\u8fc7\u5ea6\u5f00\u53d1\u4e86\u5168\u740389%\u7684\u9c7c\u7c7b\u8d44\u6e90\uff0c\u5e76\u4e14\u6467\u6bc1\u4e86\u4e16\u754c\u4e0a\u8fd1\u534a\u6570\u7684\u73ca\u745a\u7901\u3002\u79d1\u5b66\u5bb6\u8ba4\u4e3a\uff0c\u4e3a\u4fdd\u62a4\u6d77\u6d0b\u751f\u7269\u5065\u5168\u7684\u591a\u6837\u6027\uff0c\u5168\u7403\u81f3\u5c1130%\u7684\u6d77\u6d0b\u9700\u8981\u5212\u5165\u4fdd\u62a4\u533a\u3002\u91cd\u8981\u7684\u662f\uff0c\u5927\u90e8\u5206\u4fdd\u62a4\u533a\u5e94\u8be5\u9760\u8fd1\u7e41\u534e\u7684\u6d77\u5cb8\u3002\u5982\u679c\u4e00\u7247\u6d77\u57df\u88ab\u9694\u79bb\u4e86\u8db3\u591f\u957f\u7684\u65f6\u95f4\uff0c\u9c7c\u7c7b\u548c\u751f\u7269\u591a\u5c31\u4f1a\u51fa\u73b0\u53cd\u5f39\u3002\u7e41\u76db\u7684\u9c7c\u7c7b\u4e5f\u4f1a\u9010\u6e10\u5411\u90bb\u8fd1\u7684\u6c34\u57df\u6269\u6563\u3002\u667a\u80fd\u5316\u7684\u4fdd\u62a4\u533a\u751a\u81f3\u53ef\u4ee5\u5728\u591a\u79cd\u538b\u529b\uff08\u5982\u6c61\u67d3\u3001\u53d8\u6696\u548c\u9178\u5316\uff09\u4e0b\u4f7f\u6d77\u6d0b\u751f\u6001\u5177\u6709\u66f4\u5f3a\u7684\u6062\u590d\u529b\u3002\u4ece\u8fd9\u6bb5\u6587\u5b57\u53ef\u4ee5\u770b\u51fa\u4f5c\u8005\u610f\u5728\u547c\u5401\nA. \u52a0\u5927\u6d77\u6d0b\u4fdd\u62a4\u533a\u5efa\u8bbe\nB. \u7981\u6b62\u6d77\u6d0b\u4fdd\u62a4\u533a\u5546\u4e1a\u6d3b\u52a8\nC. \u907f\u514d\u8fc7\u5ea6\u6355\u635e\uff0c\u4fdd\u62a4\u9c7c\u7c7b\nD. \u5927\u529b\u4fdd\u62a4\u73ca\u745a\u7901\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8784657168777057, "meta-math/MetaMath-Mistral-7B": 0.9970330877998425, "itpossible/Chinese-Mistral-7B-v0.1": 0.6628134322401206, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9762988035911381, "meta-llama/Meta-Llama-3-8B": 0.5909453769383215, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6723061044125166}}, {"question": "\u4e00\u672c\u6545\u4e8b\u4e66\u5171\u6709240\u9875\uff0c\u5982\u679c\u7b2c\u4e00\u5929\u770b\u4e86\u5168\u90e8\u76841/8\uff0c\u7b2c\u4e8c\u5929\u770b\u4e86\u5168\u90e8\u76841/6\uff0c\u7b2c\u4e09\u5929\u5e94\u8be5\u4ece\u7b2c()\u9875\u5f00\u59cb\u770b\nA. 75\nB. 76\nC. 70\nD. 71\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.565740119602058, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c49\u521d\uff0c\u5b98\u65b9\u7981\u6b62\u5546\u4eba\u201c\u8863\u4e1d\u4e58\u8f66\u201d\uff0c\u4f46\u540e\u6765\u4e00\u4e9b\u5546\u4eba\u201c\u5047\u4e8c\u5343\u77f3(\u5b98\u5458\u7ea7\u522b)\u8206\u670d\u5bfc\u4ece\u4f5c\u5021\u4e50\uff0c\u5962\u4f88\u65e5\u751a\u201d\u3002\u8fd9\u53cd\u6620\u51fa\nA. \u4f11\u517b\u751f\u606f\u9020\u6210\u6d88\u8d39\u89c2\u5ff5\u7684\u6539\u53d8\nB. \u539f\u6709\u89c4\u5236\u53d7\u5230\u5546\u4e1a\u53d1\u5c55\u7684\u6311\u6218\nC. \u671d\u5ef7\u7684\u6291\u5546\u653f\u7b56\u53d1\u751f\u4e86\u91cd\u5927\u8f6c\u53d8\nD. \u5b98\u5458\u4e0e\u5546\u4eba\u7684\u793e\u4f1a\u5730\u4f4d\u6e10\u8d8b\u4e00\u81f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8307841158479379, "meta-math/MetaMath-Mistral-7B": 0.9701141779736201, "itpossible/Chinese-Mistral-7B-v0.1": 0.7728672513062588, "HuggingFaceH4/zephyr-7b-beta": 0.9881033353232158, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9565217779770083, "meta-llama/Meta-Llama-3-8B": 0.83164994197528, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.992449842057917}}, {"question": "\u9152\u94a2\u662f\u56fd\u5bb6\u201c\u4e00\u4e94\u201d\u671f\u95f4\u91cd\u70b9\u5efa\u8bbe\u9879\u76ee\uff0c\u201c\u4e5d\u4e94\u201d\u671f\u95f4\u4e3a\u89e3\u51b3\u4ea7\u54c1\u7ed3\u6784\u4e0d\u5408\u7406\u548c\u6548\u76ca\u6d41\u5931\u95ee\u9898\uff0c\u9700\u6295\u8d4460\u4ebf\u5143\u3002\u7ecf\u8fc7\u591a\u65b9\u52aa\u529b\uff0c\u8fd8\u67097\u4ebf\u591a\u5143\u7f3a\u53e3\uff0c\u57fa\u672c\u5efa\u8bbe\u8d44\u91d1\u8fd8\u670920.6\u4ebf\u5143\u65e0\u6765\u6e90\u3002\u8fd9\u53cd\u6620\u51fa\nA. \u56fd\u4f01\u6539\u9769\u6ca1\u6709\u53d6\u5f97\u5b9e\u8d28\u6027\u6210\u6548\nB. \u8d44\u91d1\u532e\u4e4f\u5236\u7ea6\u4e86\u56fd\u4f01\u7684\u53d1\u5c55\nC. \u56fd\u4f01\u5185\u90e8\u7684\u8150\u8d25\u95ee\u9898\u8f83\u4e3a\u4e25\u91cd\nD. \u56fd\u5bb6\u5728\u52aa\u529b\u7f13\u89e3\u56fd\u4f01\u8d44\u91d1\u95ee\u9898\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.43458177596288955, "itpossible/Chinese-Mistral-7B-v0.1": 0.6920542802335496, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6235091887150793, "meta-llama/Meta-Llama-3-8B": 0.5705989998545299, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.46178021735633756}}, {"question": "\u300a\u7ec4\u7ec7\u90e8\u6765\u4e86\u4e2a\u5e74\u8f7b\u4eba\u300b\u4e2d\uff0c\u201c\u5c31\u662f\u90a3\u4e48\u56de\u4e8b\u201d\u662f\u8c01\u7684\u5904\u4e16\u54f2\u5b66\uff1f\nA. \u738b\u6e05\u6cc9\nB. \u5218\u4e16\u543e\nC. \u97e9\u5e38\u65b0\nD. \u6797\u9707\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.32205625344145955, "itpossible/Chinese-Mistral-7B-v0.1": 0.3306562312783846, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ea7\u751f\u9891\u7387\u5d29\u6e83\u7684\u539f\u56e0\u4e3a\nA. \u6709\u529f\u529f\u7387\u4e25\u91cd\u4e0d\u8db3\nB. \u7cfb\u7edf\u53d1\u751f\u77ed\u8def\nC. \u65e0\u529f\u529f\u7387\u4e25\u91cd\u4e0d\u8db3\nD. \u7cfb\u7edf\u53d7\u5230\u5c0f\u7684\u5e72\u6270\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8868\u8ff0\u4e0d\u6b63\u786e\u7684\u6709\nA. \u5357\u5357\u5408\u4f5c\u548c\u5357\u5317\u5bf9\u8bdd\u5c5e\u4e8e\u4e0d\u540c\u8303\u7574\nB. \u5357\u5317\u5bf9\u8bdd\u540c\u4e1c\u897f\u5bf9\u8bdd\u662f\u540c\u4e00\u6027\u8d28\u7684\u95ee\u9898\nC. \u5357\u5357\u5408\u4f5c\u4e0e\u5357\u5317\u5bf9\u8bdd\u7684\u76ee\u6807\u662f\u4e00\u81f4\u7684\nD. \u5357\u5357\u5408\u4f5c\u548c\u5357\u5317\u5bf9\u8bdd\u76f8\u8f85\u76f8\u6210\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6367400381228766, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5328076904409813, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6453211328389238}}, {"question": "\u53d1\u884c\u5bf9\u8c61\u4e8b\u5148\u6307\u5b9a\uff0c\u5728\u5c0f\u8303\u56f4\u5185\u53d1\u884c\u7684\u503a\u5238\u662f\nA. \u62b5\u62bc\u503a\u5238\nB. \u516c\u52df\u503a\u5238\nC. \u79c1\u52df\u503a\u5238\nD. \u65e0\u62b5\u62bc\u503a\u5238\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4993102450567097, "meta-math/MetaMath-Mistral-7B": 0.6557052251758128, "itpossible/Chinese-Mistral-7B-v0.1": 0.7598913157100926, "HuggingFaceH4/zephyr-7b-beta": 0.7958137314464059, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4488746704506832, "meta-llama/Meta-Llama-3-8B": 0.7050312422446416, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9039720455886664}}, {"question": "\u67d0\u5c0f\u7ec4\u83b7\u5f97\u4e00\u96cc\u96c4\u5f02\u682a\u690d\u682a\u7684\u7a81\u53d8\u4f53\uff0c\u5176\u7a81\u53d8\u6027\u72b6\u662f\u7531\u6b64\u690d\u682a\u4e00\u6761\u67d3\u8272\u4f53\u4e0a\u7684\u67d0\u4e2a\u57fa\u56e0\u7a81\u53d8\u4ea7\u751f\u7684\uff08\u5047\u8bbe\u7a81\u53d8\u6027\u72b6\u548c\u91ce\u751f\u6027\u72b6\u7531\u4e00\u5bf9\u7b49\u4f4d\u57fa\u56e0\u63a7\u5236\uff09\u3002\u73b0\u6b32\u786e\u5b9a\u7a81\u53d8\u57fa\u56e0\u7684\u663e\u9690\u6027\u53ca\u5176\u4f4d\u7f6e\uff0c\u8bbe\u8ba1\u5b9e\u9a8c\u5982\u4e0b\uff1a\u7528\u8be5\u7a81\u53d8\u96c4\u682a\u4e0e\u591a\u682a\u91ce\u751f\u7eaf\u5408\u96cc\u682a\u6742\u4ea4\uff1b\u89c2\u5bdf\u8bb0\u5f55\u5b50\u4ee3\u4e2d\u8868\u73b0\u7a81\u53d8\u6027\u72b6\u7684\u96c4\u682a\u5728\u5168\u90e8\u5b50\u4ee3\u96c4\u682a\u4e2d\u6240\u5360\u7684\u6bd4\u7387Q\uff0c\u5b50\u4ee3\u4e2d\u8868\u73b0\u7a81\u53d8\u6027\u72b6\u7684\u96cc\u682a\u5728\u5168\u90e8\u5b50\u4ee3\u96cc\u682a\u4e2d\u6240\u5360\u7684\u6bd4\u7387P\u3002\u4e0b\u5217\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f\nA. \u82e5\u7a81\u53d8\u57fa\u56e0\u4f4d\u4e8eX\u548cY\u7684\u540c\u6e90\u533a\u6bb5\u4e14\u4e3a\u663e\u6027\uff0c\u5219Q\u548cP\u503c\u5206\u522b\u4e3a1\u30011\nB. \u82e5\u7a81\u53d8\u57fa\u56e0\u4f4d\u4e8eX\u67d3\u8272\u4f53\u4e0a\u4e14\u4e3a\u663e\u6027\uff0c\u5219Q\u548cP\u503c\u5206\u522b\u4e3a0\u30011\nC. \u82e5\u7a81\u53d8\u57fa\u56e0\u4f4d\u4e8eY\u67d3\u8272\u4f53\u4e0a\uff0c\u5219Q\u548cP\u503c\u5206\u522b\u4e3a1\u30010\nD. \u82e5\u7a81\u53d8\u57fa\u56e0\u4f4d\u4e8e\u5e38\u67d3\u8272\u4f53\u4e0a\u4e14\u4e3a\u663e\u6027\uff0c\u5219Q\u548cP\u503c\u5206\u522b\u4e3a1/2\u30011/2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2866128117777278, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5fe7\u8005\u89c1\u4e4b\u5219\u5fe7\uff0c\u559c\u8005\u89c1\u4e4b\u5219\u559c\uff0c\u8fd9\u662f\u53d7\u4e00\u4e2a\u4eba\u7684()\u7684\u5f71\u54cd\u6240\u81f4\u3002\nA. \u70ed\u60c5\nB. \u5fc3\u5883\nC. \u5e94\u6fc0\nD. \u6fc0\u60c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7852355746542671, "meta-math/MetaMath-Mistral-7B": 0.9896070344624378, "itpossible/Chinese-Mistral-7B-v0.1": 0.8184174261838789, "HuggingFaceH4/zephyr-7b-beta": 0.9950323790409317, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9292435829793086, "meta-llama/Meta-Llama-3-8B": 0.8286491712227763, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9992722824975983}}, {"question": "\u793e\u4f1a\u4e3b\u4e49\u6539\u9769\u7684\u6839\u672c\u76ee\u7684\u5728\u4e8e\nA. \u89e3\u653e\u548c\u53d1\u5c55\u751f\u4ea7\u529b\nB. \u6539\u53d8\u793e\u4f1a\u4e3b\u4e49\u5236\u5ea6\nC. \u5b9e\u73b0\u793e\u4f1a\u516c\u5e73\nD. \u5b8c\u5584\u793e\u4f1a\u4e3b\u4e49\u5236\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4671162793039176, "meta-math/MetaMath-Mistral-7B": 0.8833684373385854, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.49678810275280577, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.378660454747423, "meta-llama/Meta-Llama-3-8B": 0.8104847551283823, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.673678475699324}}, {"question": "\u8bbe $\\boldsymbol{A}\uff0c \\boldsymbol{B}$ \u5747\u4e3a 2 \u9636\u77e9\u9635\uff0c $\\boldsymbol{A}^*\uff0c \\boldsymbol{B}^*$ \u5206\u522b\u4e3a $\\boldsymbol{A}\uff0c \\boldsymbol{B}$ \u7684\u4f34\u968f\u77e9\u9635\uff0c \u82e5 $|\\boldsymbol{A}|=2\uff0c|\\boldsymbol{B}|=3$\uff0c \u5219\u5206\u5757\u77e9\u9635 $\\left(\\begin{array}{ll}\\boldsymbol{O} & \\boldsymbol{A} \\\\ \\boldsymbol{B} & \\boldsymbol{O}\\end{array})$ \u7684\u4f34\u968f\u77e9\u9635\u4e3a\nA. $(\\begin{array}{cc}\\boldsymbol{O} & 3 \\boldsymbol{B}^* \\\\ 2 \\boldsymbol{A}^* & \\boldsymbol{O}\\end{array})$;\nB. $(\\begin{array}{cc}O & 2 B^* \\\\ 3 A^* & O\\end{array})$;\nC. $(\\begin{array}{cc}\\boldsymbol{O} & 3 \\boldsymbol{A}^* \\\\ 2 \\boldsymbol{B}^* & \\boldsymbol{O}\\end{array})$;\nD. $(\\begin{array}{cc}\\boldsymbol{O} & 2 \\boldsymbol{A}^* \\\\ 3 \\boldsymbol{B}^* & \\boldsymbol{O}\\end{array})$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ec4\u6210\u5b8c\u6574\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\u7684\u4e24\u4e2a\u90e8\u5206\u662f\nA. \u6559\u5b66\u8f6f\u4ef6\u548c\u8d22\u52a1\u8f6f\u4ef6\nB. \u64cd\u4f5c\u7cfb\u7edf\u548c\u5e94\u7528\u7cfb\u7edf\nC. \u7cfb\u7edf\u8f6f\u4ef6\u548c\u5e94\u7528\u8f6f\u4ef6\nD. \u786c\u4ef6\u7cfb\u7edf\u548c\u8f6f\u4ef6\u7cfb\u7edf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9072697389837224, "meta-math/MetaMath-Mistral-7B": 0.9883995517774014, "itpossible/Chinese-Mistral-7B-v0.1": 0.6051685550838927, "HuggingFaceH4/zephyr-7b-beta": 0.999995286619418, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9623903335258455, "meta-llama/Meta-Llama-3-8B": 0.8538473496613416, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.964367562655707}}, {"question": "\u7231\u56e0\u65af\u5766\u63d0\u51fa\u4e86\u8d28\u80fd\u2f45\u7a0b\uff0c\u63ed\u793a\u4e86\u8d28\u91cf\u4e0e\u80fd\u91cf\u7684\u5173\u7cfb\uff0c\u5173\u4e8e\u8d28\u80fd\u2f45\u7a0b\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. mc2\u662f\u7269\u4f53\u80fd\u591f\u653e\u51fa\u80fd\u91cf\u7684\u603b\u548c\nB. \u5f53\u7269\u4f53\u5411\u5916\u91ca\u653e\u80fd\u91cf\u65f6\uff0c\u5176\u8d28\u91cf\u5fc5\u5b9a\u51cf\u5c11\uff0c\u4e14\u51cf\u5c11\u7684\u8d28\u91cf\u0394m\u4e0e\u91ca\u653e\u7684\u80fd\u91cf\u0394E\u6ee1\u2f9c\u0394E\uff1d\u0394mc^2\nC. \u8d28\u91cf\u548c\u80fd\u91cf\u53ef\u4ee5\u76f8\u4e92\u8f6c\u5316\nD. \u5982\u679c\u7269\u4f53\u7684\u80fd\u91cf\u589e\u52a0\u4e86\u0394E\uff0c\u90a3\u4e48\u5b83\u7684\u8d28\u91cf\u76f8\u5e94\u51cf\u5c11\u0394m\uff0c\u5e76\u4e14\u0394E\uff1d\u0394mc^2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u822c\u60c5\u51b5\u4e0b\u5e94\u6536\u7968\u636e\u7684\u5165\u8d26\u4ef7\u503c\u662f\nA. \u9762\u503c\nB. \u8d34\u73b0\u6240\u5f97\u989d\nC. \u5230\u671f\u4ef7\u503c\nD. \u672a\u6765\u73b0\u91d1\u6d41\u91cf\u7684\u73b0\u503c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.45600289562246865, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7015583644195207, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.47255282078899086, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3466892061295826}}, {"question": "\u201c\u7a46\u516c\u8bbf\u8bf8\u8e47\u53d4\u201d\u4e2d\u7684\u201c\u8bbf\u201d\u610f\u601d\u662f\nA. \u62dc\u8bbf\nB. \u5f81\u8be2\nC. \u63a2\u671b\nD. \u8bbf\u95ee\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5256364062774826, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6487062136352535, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c49\u6b66\u5e1d\u65f6\u5f20\u9a9e\u51fa\u4f7f\u897f\u57df\uff0c\u8fdc\u81f3\u4eca\u4e2d\u4e9a\u963f\u59c6\u6cb3\u6d41\u57df\uff0c\u53d7\u5230\u6e34\u671b\u4e0e\u6c49\u901a\u4f7f\u5f80\u6765\u7684\u5927\u5b9b\u7b49\u56fd\u7684\u6b22\u8fce\u3002\u5176\u95f4\uff0c\u6c49\u8bbe\u7f6e\u6cb3\u897f\u56db\u90e1\uff0c\u6253\u901a\u4e86\u4e0e\u897f\u57df\u7684\u76f4\u63a5\u4ea4\u901a\u3002\u5f20\u9a9e\u5728\u51fa\u4f7f\u8fc7\u7a0b\u4e2d\u6240\u83b7\u5f97\u7684\u4fe1\u606f\u5bf9\u6253\u5f00\u4e1d\u7ef8\u4e4b\u8def\u548c\u5efa\u7acb\u4e2d\u56fd\u4e0e\u897f\u65b9\u7684\u8054\u7cfb\u8d77\u5230\u4e86\u5173\u952e\u4f5c\u7528\u3002\u636e\u6b64\u53ef\u77e5\uff0c\u5f20\u9a9e\u51fa\u4f7f\u897f\u57df\u7684\u529f\u7ee9\u662f\nA. \u5efa\u7acb\u4e86\u6c49\u671d\u4e0e\u897f\u65b9\u7684\u8054\u7cfb\nB. \u5f00\u8f9f\u4e86\u6c9f\u901a\u4e2d\u897f\u7684\u4e1d\u7ef8\u4e4b\u8def\nC. \u786e\u7acb\u4e86\u6c49\u671d\u5bf9\u897f\u57df\u7684\u7ba1\u8f96\u6743\nD. \u5f00\u542f\u4e86\u4e2d\u56fd\u4e0e\u4e2d\u4e9a\u7684\u4ea4\u5f80\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u91d1\u6208\u94c1\u9a6c\uff0c\u6c14\u541e\u4e07\u91cc\u5982\u864e\u201c\u662f\u8c01\u7684\u8bcd\u53e5\nA. \u5cb3\u98de\nB. \u674e\u6e05\u7167\nC. \u767d\u5c45\u6613\nD. \u8f9b\u5f03\u75be\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8445400506005202, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7889559144579807, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679c\u5bf9\u7b80\u5355\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u663e\u8457\u6027\u68c0\u9a8c\u7684\u7ed3\u679c\u662f\u4e0d\u80fd\u62d2\u7edd$H_0$\uff0c\u8fd9\u5c31\u610f\u5473\u7740\nA. \u8be5\u6a21\u578b\u65e0\u5e94\u7528\u4ef7\u503c\nB. \u8be5\u6a21\u578b\u6c42\u9519\u4e86\nC. X\u4e0eY\u4e4b\u95f4\u6beb\u65e0\u5173\u7cfb\nD. \u8be5\u6a21\u578b\u6709\u5e94\u7528\u4ef7\u503c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6500131023654009, "meta-math/MetaMath-Mistral-7B": 0.8472422241538881, "itpossible/Chinese-Mistral-7B-v0.1": 0.5283089511163849, "HuggingFaceH4/zephyr-7b-beta": 0.9994065815755381, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9634269858605738, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9831621460241762}}, {"question": "2009\u5e749\u670821\u65e5\u81f325\u65e5\uff0c\u80e1\u9526\u6d9b\u4e3b\u5e2d\u51fa\u5e2d\u4e86\u5728\u7ebd\u7ea6\u4e3e\u884c\u7684\u8054\u5408\u56fd\u6c14\u5019\u53d8\u5316\u5cf0\u4f1a\u3001\u7b2c64\u5c4a\u8054\u5408\u56fd\u5927\u4f1a\u4e00\u822c\u6027\u8fa9\u8bba\u3001\u5b89\u7406\u4f1a\u6838\u4e0d\u6269\u6563\u4e0e\u6838\u88c1\u519b\u5cf0\u4f1a\u548c\u5728\u5339\u5179\u5821\u4e3e\u884c\u7684\u4e8c\u5341\u56fd\u96c6\u56e2\u9886\u5bfc\u4eba\u7b2c\u4e09\u6b21\u91d1\u878d\u5cf0\u4f1a\u3002\u53d1\u8868\u7684\u591a\u4e2a\u91cd\u8981\u8bb2\u8bdd\u518d\u6b21\u5f15\u8d77\u56fd\u9645\u793e\u4f1a\u7684\u5e7f\u6cdb\u5173\u6ce8\u548c\u9ad8\u5ea6\u8bc4\u4ef7\u3002\u8fd9\u53cd\u6620\u4e86\u65b0\u65f6\u671f\u4e2d\u56fd\u5916\u4ea4\u653f\u7b56\u7684\u4e00\u4e2a\u7279\u70b9\u662f\nA. \u4e00\u8fb9\u5012\nB. \u53cd\u5bf9\u9738\u6743\u4e3b\u4e49\nC. \u79ef\u6781\u5f00\u5c55\u4ee5\u8054\u5408\u56fd\u4e3a\u4e2d\u5fc3\u7684\u591a\u8fb9\u5916\u4ea4\nD. \u79ef\u6781\u6d3b\u8dc3\u5728\u5730\u533a\u6027\u7684\u56fd\u9645\u7ec4\u7ec7\u5f53\u4e2d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8389281747860996, "meta-math/MetaMath-Mistral-7B": 0.9732519755953144, "itpossible/Chinese-Mistral-7B-v0.1": 0.8638175409942269, "HuggingFaceH4/zephyr-7b-beta": 0.9998733832052269, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9613254812635468, "meta-llama/Meta-Llama-3-8B": 0.8336636898431234, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9453119485871649}}, {"question": "\u4ee5\u4e0b\u4e0d\u5c5e\u4e8e\u6280\u672f\u4f20\u5a92\u7684\u4e00\u9879\u662f\nA. \u6821\u8bad\nB. \u6821\u520a\nC. \u6821\u56ed\u5e7f\u64ad\nD. \u6821\u62a5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6120843858478786, "itpossible/Chinese-Mistral-7B-v0.1": 0.4179598062764604, "HuggingFaceH4/zephyr-7b-beta": 0.846201960015774, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9735699343849367, "meta-llama/Meta-Llama-3-8B": 0.7576584279117513, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9760016246573361}}, {"question": "\u7532\u559d\u9189\u540e\u5728\u5bb6\u800d\u9152\u75af\uff0c\u968f\u624b\u5c06\u7b14\u8bb0\u672c\u7535\u8111\u4ece 15 \u697c\u6254\u51fa\u7a97\u5916\uff0c\u6b63\u597d\u7838\u4e2d\u5728\u5c0f\u533a\u6563\u6b65\u7684\u738b\u67d0\u5e76\u81f4\u5176\u91cd\u4f24\uff0c\u7532\u7684\u884c\u4e3a\u5e94\u8ba4\u5b9a\u4e3a\nA. \u8fc7\u5931\u81f4\u4eba\u91cd\u4f24\u7f6a\nB. \u4e0d\u6784\u6210\u72af\u7f6a\nC. \u6545\u610f\u4f24\u5bb3\u7f6a\nD. \u4ee5\u5371\u9669\u65b9\u6cd5\u5371\u5bb3\u516c\u5171\u5b89\u5168\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u897f\u65b9\u4e09\u5723\u662f\u6307\nA. \u963f\u5f25\u9640\u4f5b\uff0c\u5927\u52bf\u81f3\u83e9\u8428\uff0c\u5730\u85cf\u83e9\u8428\nB. \u963f\u5f25\u9640\u4f5b\uff0c\u89c2\u97f3\u83e9\u8428\uff0c\u5927\u52bf\u81f3\u83e9\u8428\nC. \u963f\u5f25\u9640\u4f5b\uff0c\u666e\u8d24\u83e9\u8428\uff0c\u89c2\u97f3\u83e9\u8428\nD. \u963f\u5f25\u9640\u4f5b\uff0c\u89c2\u97f3\u83e9\u8428\uff0c\u6587\u6b8a\u83e9\u8428\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4084128997908917, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9676\u74f7\u662f\u706b\u4e0e\u571f\u7684\u7ed3\u6676\uff0c\u662f\u4e2d\u534e\u6587\u660e\u7684\u8c61\u5f81\u4e4b\u4e00\uff0c\u5176\u5f62\u6210\u3001\u6027\u8d28\u4e0e\u5316\u5b66\u6709\u7740\u5bc6\u5207\u7684\u5173\u7cfb\u3002\u4e0b\u5217\u8bf4\u6cd5\u9519\u8bef\u7684\u662f\nA. \u9676\u74f7\u5316\u5b66\u6027\u8d28\u7a33\u5b9a\uff0c\u5177\u6709\u8010\u9178\u78b1\u4fb5\u8680\u3001\u6297\u6c27\u5316\u7b49\u4f18\u70b9\nB. \u9676\u74f7\u662f\u5e94\u7528\u8f83\u65e9\u7684\u4eba\u9020\u6750\u6599\uff0c\u4e3b\u8981\u5316\u5b66\u6210\u5206\u662f\u7845\u9178\u76d0\nC. \u201c\u96e8\u8fc7\u5929\u6674\u4e91\u7834\u5904\u201d\u6240\u63cf\u8ff0\u7684\u74f7\u5668\u9752\u8272\uff0c\u6765\u81ea\u6c27\u5316\u94c1\nD. \u95fb\u540d\u4e16\u754c\u7684\u79e6\u5175\u9a6c\u4fd1\u662f\u9676\u5236\u54c1\uff0c\u7531\u9ecf\u571f\u7ecf\u9ad8\u6e29\u70e7\u7ed3\u800c\u6210 \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u73b0\u5728\u7684\u5bb6\u5ead\u66b4\u529b\u4e0e\u8fc7\u53bb\u76f8\u6bd4\u51fa\u73b0\u7684\u65b0\u7279\u70b9\u662f\nA. \u65bd\u884c\u5bb6\u5ead\u66b4\u529b\u7684\u4e3b\u4f53\u662f\u519c\u6c11\u548c\u5de5\u4eba\nB. \u653b\u51fb\u6027\u589e\u5f3a\uff0c\u540e\u679c\u4e25\u91cd\nC. \u4e08\u592b\u6bb4\u6253\u8650\u5f85\u59bb\u5b50\uff0c\u59bb\u5b50\u591a\u4ee5\u5fcd\u8010\u5c45\u591a\nD. \u65bd\u884c\u5bb6\u5ead\u66b4\u529b\u7684\u4e3b\u4f53\u6587\u5316\u5c42\u6b21\u4f4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4984193308813859, "meta-math/MetaMath-Mistral-7B": 0.7834686226440432, "itpossible/Chinese-Mistral-7B-v0.1": 0.6034309418958488, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6936298614533141, "meta-llama/Meta-Llama-3-8B": 0.7193708717701206, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9263159811987002}}, {"question": "2004\u5e74\u6211\u56fd\u98df\u54c1\u5de5\u4e1a\u603b\u4ea7\u503c\u4e3a\nA. 16000\u4ebf\u5143\u5de6\u53f3\nB. 6000\u4ebf\u5143\u5de6\u53f3\nC. 10000\u4ebf\u5143\u5de6\u53f3\nD. 1000\u4ebf\u5143\u5de6\u53f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4375471551057792, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5836978317836389, "meta-llama/Meta-Llama-3-8B": 0.5408762550264521, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.726647086962447}}, {"question": "\u5468\u56f4\u8840\u7ba1\u5f81\u9633\u6027\u591a\u89c1\u4e8e\nA. \u4e3b\u52a8\u8109\u74e3\u5173\u95ed\u4e0d\u5168\nB. \u5fc3\u529b\u8870\u7aed\nC. \u5fc3\u5305\u79ef\u6db2\nD. \u4e8c\u5c16\u74e3\u72ed\u7a84\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.325455072595945, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u89e3\u51b3\u9690\u9a6c\u6a21\u578b\u4e2d\u9884\u6d4b\u95ee\u9898\u7684\u7b97\u6cd5\u662f\nA. \u524d\u5411\u7b97\u6cd5\nB. \u7ef4\u7279\u6bd4\u7b97\u6cd5\nC. Baum-Welch\u7b97\u6cd5\nD. \u540e\u5411\u7b97\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.863041568694983, "meta-math/MetaMath-Mistral-7B": 0.9817429600684424, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9879984594875403, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5568506818914258, "meta-llama/Meta-Llama-3-8B": 0.6875888069269153, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u4eec\u901a\u8fc7\u773c\u3001\u8033\u3001\u9f3b\u3001\u820c\u3001\u8eab\u5404\u79cd\u611f\u5b98\u611f\u89c9\u5230\u4e00\u4e2a\u68a8\u5b50\u7684\u5404\u79cd\u5c5e\u6027\uff0c\u5728\u610f\u8bc6\u4e2d\u628a\u5b83\u4eec\u8054\u7cfb\u8d77\u6765\u5f62\u6210\u4e86\u5173\u4e8e\u8fd9\u4e2a\u68a8\u5b50\u7684\u611f\u6027\u5f62\u8c61\uff0c\u8fd9\u79cd\u53cd\u6620\u5f62\u5f0f\u662f\nA. \u611f\u89c9\nB. \u8868\u8c61\nC. \u5206\u6790\nD. \u77e5\u89c9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4902071990254855, "HuggingFaceH4/zephyr-7b-beta": 0.8569831857119171, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u65bd\u5de5\u73b0\u573a\u8d28\u91cf\u68c0\u67e5\u65b9\u6cd5\u4e2d\uff0c\u5c5e\u4e8e\u7406\u5316\u8bd5\u9a8c\u65b9\u6cd5\u7684\u662f\nA. \u95e8\u7a97\u53e3\u5bf9\u89d2\u7ebf\u76f4\u5c3a\u68c0\u67e5\nB. \u6df7\u51dd\u571f\u6784\u4ef6\u6807\u9ad8\u6d4b\u91cf\nC. \u8d85\u58f0\u6ce2\u710a\u7f1d\u63a2\u4f24\nD. \u57fa\u6869\u9759\u8f7d\u8bd5\u9a8c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4526371315996681, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9994029888301882, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7195326370658149, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u83e9\u8428\u6559\u5316\u4f17\u751f\u5fc5\u987b\u535a\u5b66\u591a\u95fb\uff0c\u8981\u5177\u5907\u4e94\u660e\uff0c\u5176\u4e2d\u5185\u660e\u662f\u6307\nA. \u903b\u8f91\u5b66\nB. \u5de5\u827a\u5b66\nC. \u8bed\u8a00\u5b66 D:\u533b\u5b66\nD. \u4f5b\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5631221773222396, "meta-math/MetaMath-Mistral-7B": 0.9035763327831593, "itpossible/Chinese-Mistral-7B-v0.1": 0.7700666321175854, "HuggingFaceH4/zephyr-7b-beta": 0.9877476199678753, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.822340291204483, "meta-llama/Meta-Llama-3-8B": 0.9093873444304357, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9666853091635448}}, {"question": "\u5728\u56db\u683c\u8868 $\\chi^2$ \u68c0\u9a8c\u4e2d\uff0c\u82e5 $\\chi^2$ \u503c\u4e3a 6.86 \uff0c\u5219\nA. $\\mathrm{P}<0.01$\nB. $\\mathrm{P}=0.01$\nC. $\\mathrm{P}<0.05$\nD. $P>0.05$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.470321006446665, "meta-math/MetaMath-Mistral-7B": 0.7448792795479776, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7751107070041213, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.49431842833152895, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u521b\u4f5c\u4e86\u957f\u8bd7\u300a\u6bc1\u706d\u300b\u7684\u73b0\u4ee3\u8bd7\u4eba\u662f\nA. \u6731\u81ea\u6e05\nB. \u95fb\u4e00\u591a\nC. \u51af\u81f3\nD. \u827e\u9752\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u4eba\u7c7b\u8840\u578b\u9057\u4f20\u77e5\u8bc6\uff0c\u53ef\u4ee5\u9274\u522b\u4eb2\u5b50\u95f4\u7684\u8840\u7f18\u5173\u7cfb\u3002\u5df2\u77e5\u7236\u6bcd\u4e2d\u4e4b\u4e00\u65b9AB\u8840\u578b\uff0c\u53e6\u4e00\u65b9\u4e3a\uff2f\u8840\u578b\uff0c\u5176\u5b50\u5973\u7684\u8840\u578b\u53ef\u80fd\u662f\nA. O\u578b\nB. A\u578b\u6216B\u578b\nC. A\u578b\nD. AB\u578b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37778771354152557, "meta-math/MetaMath-Mistral-7B": 0.4258862039361537, "itpossible/Chinese-Mistral-7B-v0.1": 0.43103394594133804, "HuggingFaceH4/zephyr-7b-beta": 0.5902788890721203, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6869708506569197, "meta-llama/Meta-Llama-3-8B": 0.5501667601118084, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.43913052209150066}}, {"question": "\u4e09\u56fd\u6f14\u4e49\u6709\u201c\u671b\u6885\u6b62\u6e34\u201d\u7684\u5178\u6545\uff0c\u5ef6\u4f38\u81f3\u4e34\u5e8a\u7684\u60c5\u5fd7\u62a4\u7406\u662f\u5c5e\u4e8e\nA. \u6697\u793a\u7597\u6cd5\nB. \u91ca\u7591\u89e3\u60d1\nC. \u53d1\u6cc4\u89e3\u90c1\nD. \u8bf1\u5bfc\u7597\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.38754811993034344, "itpossible/Chinese-Mistral-7B-v0.1": 0.33075425723442, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.333183235354062, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5047\u8bbe$m_{40}^{(+)}=0.2\uff0cq_{40}^{\\prime(1)}=0.1$\uff0c\u5728\u591a\u51cf\u56e0\u6a21\u578b\u4e2d\u7684\u5404\u51cf\u56e0\u5bfc\u81f4\u7684\u51cf\u5c11\u4eba\u6570\u670d\u4ece\u5747\u5300\u5206\u5e03\uff0c\u5219$q_{40}^{\\prime(2)}=$\u3002\nA. 2/11\nB. 1/11\nC. 9/11\nD. 5/11\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.2997240426459713, "itpossible/Chinese-Mistral-7B-v0.1": 0.2689463386046896, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3160424181481998, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6df1\u5ea6\u4e0a\u5bf9\u5fc3\u7406\u5b66\u7684\u57fa\u672c\u7406\u8bba\u95ee\u9898\u8fdb\u884c\u7ec6\u81f4\u7814\u7a76\nA. \u53d1\u5c55\u5fc3\u7406\u5b66\nB. \u751f\u7406\u5fc3\u7406\u5b66\nC. \u7406\u8bba\u5fc3\u7406\u5b66\nD. \u666e\u901a\u5fc3\u7406\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8392787886944145, "meta-math/MetaMath-Mistral-7B": 0.9836997685136594, "itpossible/Chinese-Mistral-7B-v0.1": 0.6768640035832477, "HuggingFaceH4/zephyr-7b-beta": 0.998078395276584, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7327834399693766, "meta-llama/Meta-Llama-3-8B": 0.948639358867582, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9878487136806609}}, {"question": "\u8054\u884c\u5f80\u6765\u662f\u6307\nA. \u5546\u4e1a\u94f6\u884c\u4e4b\u95f4\u7684\u5f80\u6765\nB. \u5546\u4e1a\u94f6\u884c\u4e0e\u4e2d\u592e\u94f6\u884c\u4e4b\u95f4\u7684\u5f80\u6765\nC. \u5546\u4e1a\u94f6\u884c\u7cfb\u7edf\u5185\u90e8\u5404\u884c\u5904\u4e4b\u95f4\u7684\u5f80\u6765\nD. \u5546\u4e1a\u94f6\u884c\u4e0e\u975e\u94f6\u884c\u91d1\u878d\u673a\u6784\u4e4b\u95f4\u7684\u5f80\u6765\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.464380822850986, "meta-math/MetaMath-Mistral-7B": 0.8767757691068502, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7456929067112986, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.587340179069498, "meta-llama/Meta-Llama-3-8B": 0.5670184138430419, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7432487019076528}}, {"question": "\u4e0b\u5217\u5e38\u53d1\u751f\u8f6c\u79fb\u6027\u9499\u5316\u7684\u662f\nA. \u5fc3\u74e3\u819c\nB. \u80ba\u95f4\u8d28\nC. \u52a8\u8109\u7ca5\u6837\u786c\u5316\u6591\u5757\nD. \u9759\u8109\u5185\u5f62\u6210\u7684\u8840\u6813\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4167500334361728, "itpossible/Chinese-Mistral-7B-v0.1": 0.3632935652811919, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4633232958858545, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5f(x)\u662f\u5b9a\u4e49\u57df\u4e3a$(-\\infty ,+\\infty )$\u7684\u5947\u51fd\u6570\uff0c\u6ee1\u8db3f(1-x)=f(1+x)\u3002\u82e5f(1)=2\uff0c\u5219f(1)+f(2)+f(3)+...+f(50)\u7b49\u4e8e\nA. -50\nB. 0\nC. 2\nD. 50\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4464894206529084}}, {"question": "\u636e\u6e05\u4ee3\u6863\u6848\u300a\u6237\u90e8\u6c47\u9898\u5404\u7701\u6c11\u6570\u8c37\u6570\u6e05\u518c\u300b\u7edf\u8ba1\uff0c\u54b8\u4e30\u5143\u5e74\uff081851\u5e74\uff09\u5c1a\u6709\u5b8c\u6574\u7684\u5168\u56fd\u4eba\u53e3\u8bb0\u5f55\uff0c\u54b8\u4e30\u4e09\u5e74\u8d77\uff0c\u5357\u65b9\u591a\u7701\u518c\u62a5\u6b8b\u9619\uff0c\u4eba\u53e3\u907d\u964d\u3002\u9020\u6210\u8fd9\u79cd\u72b6\u51b5\u7684\u4e3b\u8981\u539f\u56e0\u662f\nA. \u5217\u5f3a\u53d1\u52a8\u4fb5\u534e\u6218\u4e89\uff0c\u6237\u7c4d\u7ba1\u7406\u53d7\u51b2\u51fb\nB. \u6e05\u653f\u5e9c\u8c03\u6574\u653f\u7b56\uff0c\u653e\u677e\u4e86\u6237\u7c4d\u7ba1\u7406\nC. \u519c\u6c11\u8d77\u4e49\u7206\u53d1\uff0c\u51b2\u51fb\u4e86\u65e7\u7684\u7edf\u6cbb\u79e9\u5e8f\nD. \u6e05\u671d\u540f\u6cbb\u8150\u8d25\uff0c\u6237\u7c4d\u7ba1\u7406\u6df7\u4e71\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3419216904345335, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5796964968697427, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8542731895935225, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c55\u5c81\u3002\u4e0a\u8179\u90e8\u88ab\u6c7d\u8f66\u649e\u4f242\u5c0f\u65f6\uff0c\u5267\u70c8\u8179\u75db\uff0c\u4f34\u6076\u5fc3\u5455\u5410\uff0c\u795e\u5fd7\u6de1\u6f20\u3002\u67e5\u4f53\uff1aP135\u6b21/\u5206\uff0cBP75/45mmHg\uff0c\u5168\u8179\u6709\u538b\u75db\u3001\u53cd\u8df3\u75db\u53ca\u808c\u7d27\u5f20\uff0c\u79fb\u52a8\u6027\u6d4a\u97f3\u53ef\u7591\u9633\u6027\uff0c\u80a0\u9e23\u97f3\u51cf\u5f31\u3002\u9996\u9009\u7684\u8bca\u65ad\u65b9\u6cd5\u662f\nA. \u8179\u8154\u52a8\u8109\u9020\u5f71\nB. \u8179\u90e8CT\u68c0\u67e5\nC. \u8179\u8154\u7a7f\u523a\nD. \u7acb\u4f4d\u8179\u5e73\u7247\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5451708313080267, "itpossible/Chinese-Mistral-7B-v0.1": 0.4117253023919197, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3867179964509093, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.526963047563597}}, {"question": "\u8bb0S_n\u4e3a\u7b49\u5dee\u6570\u5217a_n\u7684\u524dn\u9879\u548c\uff0c\u82e5a_2=3,a_3=9,\u5219S_6\u4e3a\nA. 32\nB. 28\nC. 24\nD. 36\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4117253023919197, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u4e0d\u5c5e\u4e8e\u4e16\u754c\u8d38\u6613\u7ec4\u7ec7\u7684\u57fa\u672c\u539f\u5219\u7684\u662f\nA. \u76f8\u4e92\u534f\u8c03\u539f\u5219\nB. \u8d38\u6613\u81ea\u7531\u5316\u539f\u5219\nC. \u975e\u6b67\u89c6\u6027\u539f\u5219\nD. \u516c\u5e73\u8d38\u6613\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.30765667748869174, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u91c7\u7528\u51fd\u8be2\u7684\u65b9\u5f0f\u54a8\u8be2\u4e13\u5bb6\u4eec\u7684\u5efa\u8bae\u4ee5\u505a\u51fa\u51b3\u7b56\u7684\u65b9\u6cd5\u662f\nA. \u5fb7\u5c14\u83f2\u6cd5\nB. \u65b9\u6848\u524d\u63d0\u5206\u6790\u6cd5\nC. \u5934\u8111\u98ce\u66b4\u6cd5\nD. \u6a21\u62df\u51b3\u7b56\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.43921756187877825, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.493091943319357, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9890782648578178}}, {"question": "\u300a\u5468\u6613\u2022\u7cfb\u8f9e\u300b\u548c\u300a\u8bf4\u6587\u89e3\u5b57\u300b\u8ba4\u4e3a\u201c\u516b\u5366\u201d\u4e3a\uff08\uff09\u6240\u4f5c\nA. \u90d1\u6a35\nB. \u4f0f\u727a\u6c0f\nC. \u795e\u519c\u6c0f\nD. \u4ed3\u9889\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8774918164918827, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.42588617478863083, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u76f4\u6d41\u5355\u81c2\u7535\u6865\u4e3b\u8981\u7528\u6765\u6d4b\u91cf\uff08\uff09\u5de6\u53f3\u7684\u4e2d\u6307\u7535\u963b\u3002\nA. 1\u6b27--100\u5146\u6b27\nB. 100\u6b27--1000\u5146\u6b27\nC. 1\u6b27--10\u5146\u6b27\nD. 10\u6b27--100\u5146\u6b27\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43495990387225064, "meta-math/MetaMath-Mistral-7B": 0.5899675802156314, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3837616204620929, "meta-llama/Meta-Llama-3-8B": 0.31712010892822357, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0d\u5c5e\u4e8e\u9884\u9632\u5fc3\u808c\u6897\u6b7b\uff0c\u6539\u5584\u9884\u540e\u7684\u836f\u7269\u662f\nA. \u4f9d\u6298\u9ea6\u5e03\nB. \u5432\u54da\u5e03\u82ac\nC. \u785d\u9178\u5f02\u5c71\u68a8\u916f\nD. \u7f8e\u6258\u6d1b\u5c14\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.31283638571410965, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4097932749405994}}, {"question": "\u4e3b\u5f20\u827a\u672f\u8d77\u6e90\u4e8e\u6a21\u4eff\u7684\u53e4\u5e0c\u814a\u54f2\u5b66\u5bb6\u662f\nA. \u5e2d\u52d2\u3001\u65af\u5bbe\u585e\nB. \u7231\u5fb7\u534e\u6cf0\u52d2\u3001\u8a79\u59c6\u65af\u5f17\u96f7\u6cfd\nC. \u514b\u7f57\u9f50\u3001\u67ef\u6797\u4f0d\u5fb7\nD. \u5fb7\u8c1f\u514b\u5229\u7279\u3001\u4e9a\u91cc\u58eb\u591a\u5fb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28190907984886393, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6520437798555617, "HuggingFaceH4/zephyr-7b-beta": 0.78154999166906, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8695741589330065, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9963742044793473}}, {"question": "\u5728\u4e2d\u534e\u6c11\u65cf\u60a0\u4e45\u7684\u5386\u53f2\u4e0a\uff0c\u542f\u8fea\u548c\u6307\u5f15\u5386\u4ee3\u4f18\u79c0\u4eba\u7269\u58ee\u4e3d\u4eba\u751f\u7684\u4e00\u4e2a\u5171\u540c\u601d\u60f3\u56e0\u7d20\u662f\nA. \u529f\u5229\u4e3b\u4e49\nB. \u793e\u4f1a\u4e3b\u4e49\nC. \u6c11\u4e3b\u4e3b\u4e49\nD. \u7231\u56fd\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8953119892578997, "meta-math/MetaMath-Mistral-7B": 0.9949573475834115, "itpossible/Chinese-Mistral-7B-v0.1": 0.9871076083696201, "HuggingFaceH4/zephyr-7b-beta": 0.9985414672929233, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.905168450149105, "meta-llama/Meta-Llama-3-8B": 0.7465231309845821, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9609584035816775}}, {"question": "\u8f66\u8f86\u5728\u4ea4\u53c9\u8def\u53e3\u6709\u4f18\u5148\u901a\u884c\u6743\u7684\uff0c\u9047\u6709\u8f66\u8f86\u62a2\u884c\u65f6\uff0c\u5e94\u600e\u6837\u505a\nA. \u51cf\u901f\u907f\u8ba9\uff0c\u5fc5\u8981\u65f6\u505c\u8f66\u8ba9\u884c\nB. \u63d0\u524d\u52a0\u901f\u901a\u8fc7\nC. \u6309\u4f18\u5148\u6743\u89c4\u5b9a\u6b63\u5e38\u884c\u9a76\u4e0d\u4e88\u907f\u8ba9\nD. \u62a2\u884c\u901a\u8fc7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7022052382198353, "meta-math/MetaMath-Mistral-7B": 0.9833137711862535, "itpossible/Chinese-Mistral-7B-v0.1": 0.9688743248958152, "HuggingFaceH4/zephyr-7b-beta": 0.9749008844303337, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7077921338276373, "meta-llama/Meta-Llama-3-8B": 0.726143071958984, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u56fd\u5bb6\u5784\u65ad\u8d44\u672c\u4e3b\u4e49\u4e3a\u5411\u793e\u4f1a\u4e3b\u4e49\u8fc7\u6e21\u51c6\u5907\u4e86\u7269\u8d28\u6761\u4ef6\u662f\u6307\nA. \u5168\u9762\u7684\u5546\u54c1\u5316\nB. \u5168\u9762\u7684\u5e02\u573a\u5316 \nC. \u5168\u9762\u7684\u793e\u4f1a\u5316\nD. \u5e02\u573a\u7684\u8ba1\u5212\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.42019965192620573, "meta-math/MetaMath-Mistral-7B": 0.4064580005969529, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6483662636972422, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6505757953045953, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8776365619037333}}, {"question": "\u9e3f\u96c1\u4f20\u4e66\u7684\u6545\u4e8b\u548c\u4e0b\u5217\u54ea\u4e2a\u4eba\u7269\u6709\u5173\nA. \u4e0d\u8fb1\u4f7f\u547d\u7684\u82cf\u6b66\nB. \u5e38\u80dc\u5c06\u519b\u970d\u53bb\u75c5\nC. \u53f2\u5b66\u5bb6\u53f8\u9a6c\u8fc1\nD. \u6c49\u6b66\u5e1d\u5218\u5f7b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5211708827951183, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d44\u672c\u4e3b\u4e49\u5730\u79df\u662f\nA. \u5784\u65ad\u5229\u6da6\u8f6c\u5316\u6765\u7684\nB. \u8d85\u989d\u5229\u6da6\u8f6c\u5316\u6765\u7684\nC. \u4f01\u4e1a\u5229\u6da6\u8f6c\u5316\u6765\u7684\nD. \u5e73\u5747\u5229\u6da6\u8f6c\u5316\u6765\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3408231061081189, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.35898421331362623, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3815746693525032, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5663882627280298}}, {"question": "\u4eba\u7c7b\u4e2d\u7684\u7ea2\u7eff\u8272\u76f2\u4e3a\u9690\u6027\u4f34\u6027\u9057\u4f20\uff0c\u5982\u679c\u4e00\u8272\u76f2\u643a\u5e26\u8005\uff08XCXc\uff09\u4e0e\u4e00\u6b63\u5e38\u7537\u6027\u7ed3\u5a5a\uff0c\u5b50\u4ee3\u4e2d\u7537\u5b69\u7684\u8868\u73b0\u578b\u662f\nA. \u00bd\u8272\u76f2\uff0c\u00bd\u6b63\u5e38\nB. \u5168\u4e3a\u8272\u76f2\nC. \u00be\u8272\u76f2\uff0c\u00bc\u6b63\u5e38\nD. \u5168\u4e3a\u6b63\u5e38\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3516042863233241, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.984830984497694}}, {"question": "\u652f\u6c14\u7ba1\u80ba\u6ce1\u704c\u6d17\u6db2\u5448\u5976\u767d\u8272\u3001PAS(+)\uff0c\u4e3b\u8981\u63d0\u793a\nA. \u7279\u53d1\u6027\u80ba\u7ea4\u7ef4\u5316\nB. \u55dc\u9178\u6027\u7c92\u7ec6\u80de\u80ba\u708e\nC. \u80ba\u6ce1\u86cb\u767d\u6c89\u7740\u75c7\nD. \u7ed3\u8282\u75c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3293945554888329, "meta-math/MetaMath-Mistral-7B": 0.4847303980580763, "itpossible/Chinese-Mistral-7B-v0.1": 0.38754811993034344, "HuggingFaceH4/zephyr-7b-beta": 0.9434411594356499, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5594256355999565, "meta-llama/Meta-Llama-3-8B": 0.5243223067260547, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u73b0\u5728\u8bb8\u591a\u8ba1\u7b97\u673a\u4e2d\u88c5\u6709\u9632\u706b\u5899\u3002\u4e0b\u5217\u63cf\u8ff0\u4e2d\u5c5e\u4e8e\u9632\u706b\u5899\u4f5c\u7528\u7684\u662f\nA. \u907f\u514d\u8ba1\u7b97\u673a\u7535\u5668\u8d77\u706b\nB. \u9632\u6b62\u9759\u7535 \nC. \u4fe1\u606f\u7684\u5b89\u5168\nD. \u4fe1\u606f\u7684\u81ea\u7531\u4ea4\u6362\u53f0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.976608550682946, "meta-math/MetaMath-Mistral-7B": 0.9983725434262739, "itpossible/Chinese-Mistral-7B-v0.1": 0.9431999440143555, "HuggingFaceH4/zephyr-7b-beta": 0.9997906483925105, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9944907549274323, "meta-llama/Meta-Llama-3-8B": 0.978928066089755, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9959151353086493}}, {"question": "\u6211\u56fd\u5386\u53f2\u4e0a\u4e00\u7bc7\u70f9\u996a\u7406\u8bba\u6587\u7ae0\u662f\nA. \u300a\u5415\u6c0f\u6625\u79cb \u672c\u5473\u7bc7\u300b\nB. \u300a\u968f\u56ed\u98df\u5355\u300b\nC. \u300a\u5343\u91d1\u8981\u65b9\u300b\nD. \u300a\u98df\u73cd\u5f55\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6695349079684202, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.474069820068834, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5211\u6cd5\u89c4\u5b9a\uff1a\u5171\u540c\u72af\u7f6a\u662f\u6307\u4e8c\u4eba\u4ee5\u4e0a\u5171\u540c\u6545\u610f\u72af\u7f6a\uff1b\u6559\u5506\u4e0d\u6ee118\u5468\u5c81\u7684\u4eba\u72af\u7f6a\u7684\uff0c\u5e94\u5f53\u4ece\u91cd\u5904\u7f5a\u3002\u6f58\u67d0\u6559\u550617\u5c81\u7684\u9648\u67d0\u76d7\u7a83\u4ed6\u4eba\u8d22\u72691\u4e07\u4f59\u5143\uff0c\u6cd5\u9662\u8ba4\u5b9a\u6f58\u67d0\u4e0e\u9648\u67d0\u5171\u540c\u6784\u6210\u76d7\u7a83\u7f6a\uff0c\u5e76\u5bf9\u6f58\u67d0\u4ece\u91cd\u5904\u7f5a\u3002\u8fd9\u4e00\u63a8\u7406\u5c5e\u4e8e\nA. \u6f14\u7ece\u63a8\u7406\nB. \u4ef7\u503c\u63a8\u7406\nC. \u7c7b\u6bd4\u63a8\u7406\nD. \u5f52\u7eb3\u63a8\u7406\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5045404902575912, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5372592341593382, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5c5e\u4e8e\u7a33\u5b9a\u7ec6\u80de\u7684\u662f\nA. \u9aa8\u9abc\u808c\u7ec6\u80de\nB. \u9aa8\u7ec6\u80de\nC. \u80f8\u819c\u95f4\u76ae\u7ec6\u80de\nD. \u76ae\u80a4\u8868\u76ae\u7ec6\u80de\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8588641777525983, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5262386711124636, "meta-llama/Meta-Llama-3-8B": 0.5140406183633489, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5812656556527531}}, {"question": "\u300a\u516c\u53f8\u6cd5\u300b\u7b2c154\u6761\u7b2c2\u6b3e\u89c4\u5b9a\uff1a\u516c\u53f8\u53d1\u884c\u516c\u53f8\u503a\u5238\u5e94\u5f53\u7b26\u5408\u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u8bc1\u5238\u6cd5\u300b\u89c4\u5b9a\u7684\u53d1\u884c\u6761\u4ef6\u3002\u8fd9\u4e00\u6761\u6587\u5c5e\u4e8e\u6cd5\u5f8b\u89c4\u5219\u4e2d\u7684\nA. \u51c6\u7528\u6027\u89c4\u5219\nB. \u6388\u6743\u6027\u89c4\u5219\nC. \u786e\u5b9a\u6027\u89c4\u5219\nD. \u7981\u6b62\u6027\u89c4\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u56fd\u5bb6\u5784\u65ad\u8d44\u672c\u4e3b\u4e49\u662f\nA. \u8d44\u4ea7\u9636\u7ea7\u56fd\u5bb6\u540c\u5784\u65ad\u8d44\u672c\u878d\u5408\u5728\u4e00\u8d77\u7684\u5784\u65ad\u8d44\u672c\u4e3b\u4e49\nB. \u6d88\u9664\u751f\u4ea7\u65e0\u653f\u5e9c\u72b6\u6001\u7684\u5784\u65ad\u8d44\u672c\u4e3b\u4e49\nC. \u5e26\u6709\u793e\u4f1a\u4e3b\u4e49\u56e0\u7d20\u7684\u5784\u65ad\u8d44\u672c\u4e3b\u4e49\nD. \u8d44\u4ea7\u9636\u7ea7\u56fd\u5bb6\u540c\u5784\u65ad\u8d44\u672c\u76f8\u4e92\u5206\u79bb\u7684\u5784\u65ad\u8d44\u672c\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65e5\u672c\u795e\u901a\u5ddd\u6d41\u57df\u7684\u201c\u9aa8\u75db\u75c5\u201d\u662f\u56e0\u4e3a\nA. \u9549\u4e2d\u6bd2\u5f15\u8d77\u3002\nB. \u94c5\u4e2d\u6bd2\u5f15\u8d77\u3002\nC. \u7837\u4e2d\u6bd2\u5f15\u8d77\u3002\nD. \u6c5e\u4e2d\u6bd2\u5f15\u8d77\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6079974812858236, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u57ce\u5e02\u5e02\u533a\u4eba\u53e3\u5c24\u5176\u662f\u5927\u57ce\u5e02\u5e02\u533a\u4eba\u53e3\u90ca\u533a\u5316\u3001\u5927\u57ce\u5e02\u5916\u56f4\u536b\u661f\u57ce\u9547\u5e03\u5c40\u5206\u6563\u5316\u7684\u57ce\u9547\u5316\uff0c\u88ab\u79f0\u4e3a\nA. \u540c\u6b65\u57ce\u9547\u5316\nB. \u9006\u57ce\u9547\u5316\nC. \u6ede\u540e\u57ce\u9547\u5316\nD. \u8fc7\u5ea6\u57ce\u9547\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3646426663899487, "itpossible/Chinese-Mistral-7B-v0.1": 0.3589842189651603, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3788721596190015, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6742\u6587\u300a\u6625\u672b\u95f2\u8c08\u300b\u4e2d\uff0c\u7ec6\u8170\u8702\u7684\u6545\u4e8b\u6240\u8981\u8868\u8fbe\u7684\u662f\nA. \u6b4c\u9882\u4eba\u7c7b\u7684\u79d1\u5b66\u7cbe\u795e\nB. \u63ed\u793a\u7edf\u6cbb\u9636\u7ea7\u9ebb\u75f9\u548c\u7981\u9522\u4eba\u6c11\u601d\u60f3\u7684\u65b9\u6cd5\nC. \u6279\u5224\u879f\u86c9\u7684\u61e6\u5f31\u65e0\u80fd\nD. \u8d5e\u7f8e\u7ec6\u8170\u8702\u7684\u806a\u660e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4689404746727647, "meta-math/MetaMath-Mistral-7B": 0.3900852433273489, "itpossible/Chinese-Mistral-7B-v0.1": 0.5553635924049048, "HuggingFaceH4/zephyr-7b-beta": 0.9947716174053993, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5998619922045508, "meta-llama/Meta-Llama-3-8B": 0.48398797934313104, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8233317524147731}}, {"question": "\u4e00\u90e8\u620f\u5267\u4f5c\u54c1\u7684\u57fa\u672c\u5355\u4f4d\u662f\nA. \u60c5\u5883\nB. \u6545\u4e8b\nC. \u4eba\u7269\nD. \u573a\u9762\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7559610160918779, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7285344432367833}}, {"question": "\u4ee5\u4e0b\u54ea\u4e00\u9879\u4e0d\u662f\u4e2d\u56fd\u4f20\u7edf\u7684\u5929\u6587\u4eea\u5668\nA. \u6d51\u4eea\nB. \u7b80\u4eea\nC. \u572d\u8868\nD. \u8c61\u9650\u4eea\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.38253298924946444, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9563566134380536, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6039641757135958, "meta-llama/Meta-Llama-3-8B": 0.8329241177550284, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9895956133701611}}, {"question": "\u521d\u8bca\u4e3a\u8840\u6813\u95ed\u585e\u6027\u8109\u7ba1\u708e\u7684\u60a3\u8005\uff0c\u6700\u91cd\u8981\u7684\u533b\u5631\u662f\nA. \u4f7f\u7528\u6b62\u75db\u836f\nB. \u5367\u5e8a\u4f11\u606f\nC. \u7acb\u5373\u6212\u70df\nD. \u6ce8\u610f\u4fdd\u6696\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3685317913748869, "meta-math/MetaMath-Mistral-7B": 0.4042843668189754, "itpossible/Chinese-Mistral-7B-v0.1": 0.7961421736176926, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8039228987994946, "meta-llama/Meta-Llama-3-8B": 0.9293235450916512, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9806978111919813}}, {"question": "\u300a\u9a6c\u514b\u601d\u4e3b\u4e49\u548c\u6c11\u65cf\u95ee\u9898\u300b\u4e00\u4e66\u7684\u4f5c\u8005\u662f\nA. \u52c3\u5217\u65e5\u6d85\u592b\nB. \u8d6b\u9c81\u6653\u592b\nC. \u5217\u5b81\nD. \u65af\u5927\u6797\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7528\u7ea2\u82b1\u7eaf\u5408\u4f53\u4e0e\u767d\u82b1\u7eaf\u5408\u4f53\u6742\u4ea4\uff0c\u6742\u79cdF1\u4ee3\u5168\u90e8\u4e3a\u7ea2\u82b1\uff0c\u56e0\u6b64\u7ea2\u82b1\u4e3a\nA. \u663e\u6027\u6027\u72b6\nB. \u8d28\u91cf\u6027\u72b6\nC. \u5355\u4f4d\u6027\u72b6\nD. \u9690\u6027\u6027\u72b6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35612055499370676, "meta-math/MetaMath-Mistral-7B": 0.38181830457859056, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9979212876627404, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7028037032376865, "meta-llama/Meta-Llama-3-8B": 0.5174255277540918, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5404096908098676}}, {"question": "\u7532\u5728\u4e61\u6751\u8def\u4e0a\u9ad8\u901f\u9a7e\u9a76\u62d6\u62c9\u673a\uff0c\u56e0\u89c6\u7ebf\u4e0d\u597d\u5c06\u6813\u5728\u8def\u8fb9\u7684\u8015\u725b\u649e\u6b7b\uff0c\u5bf9\u7532\u7684\u884c\u4e3a\nA. \u53ef\u4ee5\u514d\u4e88\u5211\u4e8b\u5904\u5206\nB. \u4ee5\u4ea4\u901a\u8087\u4e8b\u7f6a\u5b9a\u7f6a\u5904\u5206\nC. \u4e0d\u8ba4\u5b9a\u4e3a\u72af\u7f6a\nD. \u4ee5\u5371\u9669\u9a7e\u9a76\u7f6a\u5b9a\u7f6a\u5904\u5206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u90e8\u95e8\u6240\u5c5e\u578b\u7684\u516c\u5171\u5173\u7cfb\u90e8\u4e2d\uff0c\u628a\u516c\u5171\u5173\u7cfb\u804c\u80fd\u5b9a\u4f4d\u4e8e\u4f20\u64ad\u529f\u80fd\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u5c06\u516c\u5171\u5173\u7cfb\u90e8\u5f52\u5c5e\u4e8e\nA. \u529e\u516c\u5ba4\nB. \u63a5\u5f85\u90e8\u95e8\nC. \u5e7f\u544a\u5ba3\u4f20\u90e8\u95e8\nD. \u9500\u552e\u90e8\u95e8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6521752118558444, "meta-math/MetaMath-Mistral-7B": 0.6869708442481717, "itpossible/Chinese-Mistral-7B-v0.1": 0.6640014217407492, "HuggingFaceH4/zephyr-7b-beta": 0.9952572337471198, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8962043914720086, "meta-llama/Meta-Llama-3-8B": 0.3233250233605477, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7088663122469038}}, {"question": "\u201c\u4ec1\u4e49\u5728\u8eab\u800c\u8272\u4e0d\u4f10\u201d\u4e2d\u7684\u201c\u4f10\u201d\u662f\u6307\uff1a\nA. \u4ee5\u4e0a\u90fd\u4e0d\u5bf9\nB. \u9600\u95e8\nC. \u519b\u9600\nD. \u8fdb\u653b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3499320087587727, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u751c\u7389\u7c73\u7684\u91c7\u6536\u9002\u671f\u662f\nA. \u6dc0\u7c89\u542b\u91cf\u6700\u5c11\u3001\u7cd6\u542b\u91cf\u6700\u9ad8\u65f6\nB. \u6dc0\u7c89\u542b\u91cf\u6700\u5c11\u3001\u7cd6\u542b\u91cf\u6700\u4f4e\u65f6\nC. \u6dc0\u7c89\u542b\u91cf\u6700\u591a\u3001\u7cd6\u542b\u91cf\u6700\u9ad8\u65f6\nD. \u6dc0\u7c89\u542b\u91cf\u6700\u591a\u3001\u7cd6\u542b\u91cf\u6700\u4f4e\u65f6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4216544455741609, "meta-math/MetaMath-Mistral-7B": 0.5372592193409231, "itpossible/Chinese-Mistral-7B-v0.1": 0.4117253312653132, "HuggingFaceH4/zephyr-7b-beta": 0.4391305367718211, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4172048387870753, "meta-llama/Meta-Llama-3-8B": 0.4270449995121918, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5345735192631408}}, {"question": "\u4e0b\u5217\u88c5\u4fee\u505a\u6cd5\u5f62\u6210\u7684\u8377\u8f7d\u4f5c\u7528\uff0c\u5c5e\u4e8e\u7ebf\u8377\u8f7d\u7684\u662f\nA. \u60ac\u6302\u540a\u706f\nB. \u5b89\u653e\u5047\u5c71\nC. \u5c01\u95ed\u9633\u53f0\nD. \u94fa\u8bbe\u5730\u7816\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1000\u98976\u7b49\u661f\u805a\u96c6\u5728\u4e00\u8d77\u7684\u661f\u7b49\u8fd1\u4f3c\uff08\uff09\u7b49\nA. -1.5\nB. 1\nC. -0.5\nD. 6000\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3431547327443469, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c0f\u5f20\u6d3b\u6cfc\u597d\u52a8\uff0c\u53cd\u5e94\u8fc5\u901f\uff0c\u70ed\u7231\u4ea4\u9645\uff0c\u80fd\u8bf4\u4f1a\u9053\uff0c\u4f46\u60c5\u7eea\u4e0d\u7a33\u5b9a\uff0c\u6bd4\u8f83\u7c97\u679d\u5927\u53f6\u3002\u5176\u6c14\u8d28\u7c7b\u578b\u5c5e\u4e8e\nA. \u591a\u8840\u8d28\nB. \u9ecf\u6db2\u8d28\nC. \u6291\u90c1\u8d28\nD. \u80c6\u6c41\u8d28\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6439140608714028, "meta-math/MetaMath-Mistral-7B": 0.9657936811294685, "itpossible/Chinese-Mistral-7B-v0.1": 0.9092230498018781, "HuggingFaceH4/zephyr-7b-beta": 0.9789607127687847, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.748832125158183, "meta-llama/Meta-Llama-3-8B": 0.7481021734366468, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.994235145069008}}, {"question": "90\u5e74\u4ee3\u540e\u671f\uff0c\u5fb7\u56fd\u53d1\u884c\u91cf\u6700\u5927\u7684\u65e5\u62a5\u662f\nA. \u300a\u4e16\u754c\u62a5\u300b\nB. \u300a\u5357\u5fb7\u610f\u5fd7\u62a5\u300b\nC. \u300a\u897f\u5fb7\u610f\u5fd7\u62a5\u300b\nD. \u300a\u56fe\u7247\u62a5\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4124578775795746, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4792568758075599, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5404\u9879\u4e2d\uff0c\u5c5e\u4e8e\u4ea7\u54c1\u6210\u672c\u8ba1\u7b97\u65b9\u6cd5\u7684\u662f\nA. \u54c1\u79cd\u6cd5\nB. \u5b9a\u989d\u6bd4\u4f8b\u6cd5\nC. \u8ba1\u5212\u6210\u672c\u6cd5\nD. \u7ea6\u5f53\u4ea7\u91cf\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2821833983601388, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3528619482070927, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.42704501409611145, "meta-llama/Meta-Llama-3-8B": 0.31031319312730204, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e0b\u5217\u2f0f\u79cd\u5b58\u50a8\u5668\u4e2d\uff0cCPU\u53ef\u76f4\u63a5\u8bbf\u95ee\u7684\u662f\nA. \u4e3b\u5b58\u50a8\u5668\nB. \u78c1\u5e26\nC. \u5149\u76d8\nD. \u78c1\u76d8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.963546350095126, "meta-math/MetaMath-Mistral-7B": 0.9986556024539717, "itpossible/Chinese-Mistral-7B-v0.1": 0.925377735609727, "HuggingFaceH4/zephyr-7b-beta": 0.999967926073589, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9937526515077048, "meta-llama/Meta-Llama-3-8B": 0.9762669411362483, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.998947624681483}}, {"question": "\u9c7c\u7c7b\u662f\uff08\uff09\u7684\u826f\u597d\u6765\u6e90\nA. \u94be\nB. \u7ef4\u751f\u7d20K\nC. \u4e0d\u9971\u548c\u8102\u80aa\u9178\nD. \u81b3\u98df\u7ea4\u7ef4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8859001988922123, "meta-math/MetaMath-Mistral-7B": 0.9362085557428563, "itpossible/Chinese-Mistral-7B-v0.1": 0.7693337294804918, "HuggingFaceH4/zephyr-7b-beta": 0.7542082898713325, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9175019208301309, "meta-llama/Meta-Llama-3-8B": 0.7445404706407546, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9366505718349205}}, {"question": "\u539f\u59cb\u51ed\u8bc1\u6309\u5176\u6765\u6e90\u4e0d\u540c\uff0c\u53ef\u5206\u4e3a\nA. \u6c47\u603b\u539f\u59cb\u51ed\u8bc1\u548c\u8bb0\u8d26\u7f16\u5236\u51ed\u8bc1\nB. \u4e00\u6b21\u51ed\u8bc1\u548c\u7d2f\u8ba1\u51ed\u8bc1\nC. \u6536\u6b3e\u51ed\u8bc1\u548c\u4ed8\u6b3e\u51ed\u8bc1\nD. \u81ea\u5236\u539f\u59cb\u51ed\u8bc1\u548c\u5916\u6765\u539f\u59cb\u51ed\u8bc1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6010557798807481, "meta-math/MetaMath-Mistral-7B": 0.5230819041065484, "itpossible/Chinese-Mistral-7B-v0.1": 0.7936989398653402, "HuggingFaceH4/zephyr-7b-beta": 0.9999684766215012, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8546051084301903, "meta-llama/Meta-Llama-3-8B": 0.615943621382553, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u4e0d\u5c5e\u4e8e\u80c3\u98df\u7ba1\u53cd\u6d41\u75c5\u5e76\u53d1\u75c7\u7684\u662f\nA. \u98df\u7ba1\u72ed\u7a84\nB. \u98df\u7ba1\u61a9\u5ba4\nC. \u4e0a\u6d88\u5316\u9053\u51fa\u8840\nD. Barrett\u98df\u7ba1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7647891520577195, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6738681937811563, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201cIt\u2019s not just a planet\uff0c It\u2019s our home\uff0e\u201d\u8fd9\u662f\u5bf9\u5730\u7403\u5728\u5b87\u5b99\u4e2d\u5730\u4f4d\u6070\u5982\u5176\u5206\u7684\u8bc4\u4ef7\u3002\u5730\u7403\u7684\u7279\u6b8a\u6027\u8868\u73b0\u5728\nA. \u6709\u2f63\u547d\u7269\u8d28\u5b58\u5728\nB. \u8d28\u91cf\u548c\u4f53\u79ef\u9002\u4e2d\nC. \u53ea\u6709\u2f00\u9897\u536b\u661f\nD. \u2f83\u8f6c\u548c\u516c\u8f6c\u2f45\u5411\u76f8\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.41331540746354706, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3849546346032141, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8677360300367416}}, {"question": "1984\u5e741\u6708\uff0c\u4e2d\u56fd\u6c7d\u8f66\u7684\u7b2c\u4e00\u4e2a\u4e2d\u5916\u5408\u8d44\u4f01\u4e1a\u2014\u2014\u5317\u4eac\u5409\u666e\u6c7d\u8f66\u6709\u9650\u516c\u53f8\u8bde\u751f\u3002\u6b21\u5e74\uff0c\u4e2d\u5fb7\u5408\u8d44\u8f7f\u8f66\u751f\u4ea7\u4f01\u4e1a\u2014\u2014\u4e0a\u6d77\u5927\u4f17\u6c7d\u8f66\u6709\u9650\u516c\u53f8\u6210\u7acb\u3002\u4e2d\u56fd\u6c7d\u8f66\u5de5\u4e1a\u5f88\u5feb\u5c31\u8fdb\u5165\u4e86\u7b2c\u4e00\u8f6e\u7684\u5408\u8d44\u9ad8\u6f6e\u3002\u8fd9\u4e9b\u5408\u8d44\u6c7d\u8f66\u4f01\u4e1a\u7684\u51fa\u73b0\u53cd\u6620\u4e86\u4e2d\u56fd\nA. \u5e02\u573a\u7ecf\u6d4e\u4f53\u7cfb\u5f62\u6210\u65b0\nB. \u72ec\u7acb\u5de5\u4e1a\u4f53\u7cfb\u5f62\u6210\nC. \u7ecf\u6d4e\u6539\u9769\u4e0d\u65ad\u6df1\u5316\nD. \u6539\u9769\u5f00\u653e\u62c9\u5f00\u4e86\u5e8f\u5e55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5146284226049465, "meta-math/MetaMath-Mistral-7B": 0.5556877863351356, "itpossible/Chinese-Mistral-7B-v0.1": 0.5014157855779161, "HuggingFaceH4/zephyr-7b-beta": 0.9193824609230107, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5654461320949343, "meta-llama/Meta-Llama-3-8B": 0.710989256349572, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4840155913703345}}, {"question": "\u67d0\u5e02\u51fa\u79df\u8f66\u53f8\u673a\u7532\u4e3a\u4e86\u5c06\u75c5\u91cd\u7684\u9ad8\u4e2d\u751f\u53ca\u65f6\u9001\u5f80\u533b\u9662\uff0c\u8fde\u95ef\u4e24\u4e2a\u7ea2\u706f\u3002\u6309\u7167\u4ea4\u901a\u6cd5\u89c4\uff0c\u5bf9\u5176\u95ef\u7ea2\u706f\u884c\u4e3a\u5e94\u4e88\u4ee5\u6263\u5206\u5e76\u7f5a\u6b3e\uff0c\u4f46\u5e02\u516c\u5b89\u4ea4\u7ba1\u90e8\u95e8\u8ba4\u4e3a\uff0c\u7532\u7684\u505a\u6cd5\u7cfb\u6551\u4eba\u4e4b\u4e3e\uff0c\u51b3\u5b9a\u514d\u9664\u5bf9\u7532\u7684\u5904\u7f5a\u3002\u5bf9\u6b64\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u516c\u5b89\u4ea4\u7ba1\u90e8\u95e8\u4f5c\u51fa\u514d\u9664\u5904\u7f5a\u7684\u51b3\u5b9a\u8fdd\u53cd\u4e86\u6cd5\u5f8b\u9762\u524d\u4eba\u4eba\u5e73\u7b49\u539f\u5219\nB. \u7532\u4e0d\u53d7\u6cd5\u5f8b\u5236\u88c1\u5e76\u4e0d\u610f\u5473\u7740\u4ed6\u6ca1\u6709\u6cd5\u5f8b\u8d23\u4efb\nC. \u516c\u5b89\u4ea4\u7ba1\u90e8\u95e8\u4f5c\u51fa\u514d\u9664\u5904\u7f5a\u7684\u51b3\u5b9a\u8fd0\u7528\u7684\u662f\u6f14\u7ece\u63a8\u7406\nD. \u7532\u4e3a\u6551\u4eba\u800c\u95ef\u7ea2\u706f\u7684\u884c\u4e3a\u5e76\u4e0d\u8fdd\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5673844439301797, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4828010988312373, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.47733940202911507}}, {"question": "\u4eba\u8033\u80fd\u63a5\u53d7\u7684\u632f\u52a8\u9891\u7387\u662f\nA. 40~20000\u8d6b\u5179\nB. 20~20000\u8d6b\u5179\nC. 50~25000\u8d6b\u5179\nD. 30~25000\u8d6b\u5179\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4866406173335022, "meta-math/MetaMath-Mistral-7B": 0.8099519336073631, "itpossible/Chinese-Mistral-7B-v0.1": 0.32332502336054764, "HuggingFaceH4/zephyr-7b-beta": 0.9983581737601671, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5730151832370175, "meta-llama/Meta-Llama-3-8B": 0.5200545237355652, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u5b50\u7532\u4ee5\u4e00\u5973\u5b50\u7684\u53e3\u543b\u7ed9\u4e59\u7684\u4e08\u592b\u4e19\u5199\u4e86\u4e00\u5c01\u533f\u540d\u4fe1\uff0c\u8bc9\u8bf4\u4e59\u4e0e\u201c\u5176\u201d\u4e08\u592b\u6709\u4e0d\u6b63\u5f53\u7537\u5973\u5173\u7cfb\uff0c\u4e59\u56e0\u6b64\u4e0e\u4e08\u592b\u4e19\u4e4b\u95f4\u7684\u592b\u59bb\u611f\u60c5\u4e0d\u548c\u3002\u4e59\u540e\u6765\u5f97\u77e5\u6b64\u4fe1\u7cfb\u7537\u5b50\u7532\u6240\u5199\u3002\u636e\u6b64\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u7532\u4fb5\u72af\u4e86\u4e59\u7684\u9690\u79c1\u6743\nB. \u4e59\u6709\u6743\u5411\u7532\u4e3b\u5f20\u7cbe\u795e\u635f\u5bb3\u8d54\u507f\nC. \u7532\u4fb5\u72af\u4e86\u4e59\u7684\u914d\u5076\u6743\nD. \u7532\u4fb5\u72af\u4e86\u4e59\u7684\u540d\u8a89\u6743\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.42413356061927976}}, {"question": "\u5173\u4e8e\u6cd5\u7684\u6f14\u8fdb\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u5974\u96b6\u5236\u6cd5\u662f\u4eba\u7c7b\u5386\u53f2\u6700\u65e9\u51fa\u73b0\u7684\u6cd5\uff0c\u516c\u5143\u524d449\u5e74\u7684\u300a\u5341\u4e8c\u94dc\u8868\u6cd5\u300b\u662f\u53e4\u7f57\u9a6c\u4ee5\u539f\u4e60\u60ef\u6cd5\u4e3a\u57fa\u7840\u5236\u5b9a\u7684\u7b2c\u4e00\u90e8\u6210\u6587\u6cd5\nB. \u4e24\u5927\u6cd5\u7cfb\u90fd\u662f\u5728\u8d44\u4ea7\u9636\u7ea7\u5efa\u7acb\u653f\u6743\u4ee5\u540e\u624d\u5f62\u6210\u7684\nC. \u793e\u4f1a\u4e3b\u4e49\u6cd5\u662f\u4ee5\u5de5\u4eba\u9636\u7ea7\u4e3a\u9886\u5bfc\u7684\u5e7f\u5927\u4eba\u6c11\u5171\u540c\u610f\u5fd7\u548c\u6839\u672c\u5229\u76ca\u7684\u4f53\u73b0\uff0c\u56e0\u6b64\u793e\u4f1a\u4e3b\u4e49\u6cd5\u6ca1\u6709\u9636\u7ea7\u6027\nD. \u8d44\u672c\u4e3b\u4e49\u6cd5\u5f62\u6210\u7684\u6807\u5fd7\u662f\u5e26\u6709\u8d44\u672c\u4e3b\u4e49\u56e0\u7d20\u7684\u6cd5\u7684\u51fa\u73b0\uff0c\u5373\u5546\u6cd5\u7684\u5174\u8d77\u3001\u7f57\u9a6c\u6cd5\u7684\u590d\u5174\u3001\u8d44\u672c\u539f\u59cb\u79ef\u7d2f\u7684\u6cd5\u5f8b\u51fa\u73b0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.7566730593701396, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6286330321728708, "meta-llama/Meta-Llama-3-8B": 0.39677575732699655, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7433685919007734}}, {"question": "\u201c\u5175\u4e0d\u538c\u8bc8\u201d\u6700\u53ef\u80fd\u53cd\u6620\u7684\u5546\u4e1a\u4f26\u7406\u662f\nA. \u80a1\u4e1c\u4e0e\u7ba1\u7406\u5c42\u4f26\u7406\nB. \u6d88\u8d39\u8005\u5173\u7cfb\u4e2d\u7684\u4f26\u7406\nC. \u96c7\u5458\u4e0e\u5458\u5de5\u4f26\u7406\nD. \u5e02\u573a\u7ade\u4e89\u4e0e\u5408\u4f5c\u4f26\u7406\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43265743489266, "meta-math/MetaMath-Mistral-7B": 0.6323634852596559, "itpossible/Chinese-Mistral-7B-v0.1": 0.6274250707593, "HuggingFaceH4/zephyr-7b-beta": 0.9638829402665832, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6964101401371433, "meta-llama/Meta-Llama-3-8B": 0.47935414222805517, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6728\u661f\u8f68\u9053\u4f4d\u7f6e\u53ef\u4ee5\u89c2\u6d4b\u52304\u9897\u7c7b\u5730\u884c\u661f\u51cc\u65e5\uff0c\u90a3\u4e48\u53d1\u751f\u51cc\u65e5\u65f6\u4e0b\u5217\u54ea\u4e00\u4e2a\u7684\u89c6\u76f4\u5f84\u6700\u5927\nA. \u706b\u661f\nB. \u6c34\u661f\nC. \u5730\u7403\nD. \u91d1\u661f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30736334118347786, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.34611314921428016, "HuggingFaceH4/zephyr-7b-beta": 0.5121788714655076, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u7f51\u7edc\u5b89\u5168\u6cd5\u300b\u7b2c\u4e94\u5341\u516b\u6761\u660e\u786e\u89c4\u5b9a\uff0c\u56e0\u7ef4\u62a4\u56fd\u5bb6\u5b89\u5168\u548c\u793e\u4f1a\u516c\u5171\u79e9\u5e8f\uff0c\u5904\u7f6e\u91cd\u5927\u7a81\u53d1\u793e\u4f1a\u5b89\u5168\u4e8b\u4ef6\u7684\u9700\u8981\uff0c\u7ecf\uff08 \uff09\u51b3\u5b9a\u6216\u8005\u6279\u51c6\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u533a\u57df\u5bf9\u7f51\u7edc\u901a\u4fe1\u91c7\u53d6\u9650\u5236\u7b49\u4e34\u65f6\u63aa\u65bd\u3002\nA. \u7701\u7ea7\u4ee5\u4e0a\u4eba\u6c11\u653f\u5e9c\nB. \u7f51\u7edc\u670d\u52a1\u63d0\u4f9b\u5546\nC. \u56fd\u5bb6\u7f51\u4fe1\u90e8\u95e8\nD. \u56fd\u52a1\u9662\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e09\u7fa7\u9178\u5faa\u73af\u7684\u8d77\u59cb\u53cd\u5e94\u662f\nA. \u4e59\u9170\u8f85\u9176A\u4e0e\u4e8c\u6c27\u5316\u78b3\u7f29\u5408\nB. \u4e59\u9170\u8f85\u9176A\u4e0e\u8349\u9170\u4e59\u9178\u7f29\u5408\nC. \u4e19\u916e\u9178\u4e0e\u8349\u9170\u4e59\u9178\u7f29\u5408\nD. \u4e19\u916e\u9178\u4e0e\u4e8c\u6c27\u5316\u78b3\u7f29\u5408\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9a7e\u9a76\u7684\u8f66\u8f86\u6b63\u5728\u88ab\u5176\u4ed6\u8f66\u8f86\u8d85\u8d8a\u65f6\uff0c\u5e94\u600e\u6837\u505a\nA. \u7ee7\u7eed\u52a0\u901f\u884c\u9a76\nB. \u9760\u9053\u8def\u4e2d\u5fc3\u884c\u9a76\nC. \u51cf\u901f\uff0c\u9760\u53f3\u4fa7\u884c\u9a76\nD. \u52a0\u901f\u8ba9\u8def\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43170100835868963, "meta-math/MetaMath-Mistral-7B": 0.5667617237191023, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7384346417774533, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9233215657597625, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9919413901219802}}, {"question": "\u6839\u636e\u2f9a\u6f6e\u53d1\u2f63\u7684\u4e3b\u8981\u73af\u5883\u6761\u4ef6\u53ef\u4ee5\u5224\u65ad\uff0c\u5728\u6211\u56fd\u8f83\u6613\u53d1\u2f63\u2f9a\u6f6e\u7684\u6d77\u57df\u2f00\u822c\u662f a\u5357\u2f45\u8fd1\u5cb8\u6d77\u57df\uff1bb\u57ce\u5e02\u8fd1\u5cb8\u6d77\u57df\uff1bc\u5317\u2f45\u8fd1\u5cb8\u6d77\u57df\uff1bd\u4e61\u6751\u8fd1\u5cb8\u6d77\u57df\nA. ab\nB. cd\nC. ad\nD. bc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.331219054468835, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.28570956661341396, "HuggingFaceH4/zephyr-7b-beta": 0.3881179471198564, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.528308951116385, "meta-llama/Meta-Llama-3-8B": 0.3499320087587726, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4377363940461576}}, {"question": "\u2f29\u7403\u7531\u5730\u2faf\u7ad6\u76f4\u4e0a\u629b\uff0c\u4e0a\u5347\u7684\u6700\u2f24\u2fbc\u5ea6\u4e3aH\uff0c\u8bbe\u6240\u53d7\u963b\u2f12\u2f24\u2f29\u6052\u5b9a\uff0c\u5730\u2faf\u4e3a\u96f6\u52bf\u80fd\u2faf\u3002\u5728\u4e0a\u5347\u2f84\u79bb\u5730\u2fbc\u5ea6h\u5904\uff0c\u2f29\u7403\u7684\u52a8\u80fd\u662f\u52bf\u80fd\u76842\u500d\uff0c\u5728\u4e0b\u843d\u2f84\u79bb\u2fbc\u5ea6h\u5904\uff0c\u2f29\u7403\u7684\u52bf\u80fd\u662f\u52a8\u80fd\u76842\u500d\uff0c\u5219h\u7b49\u4e8e\nA. 4H/9\nB. 2H/9\nC. H/9\nD. 3H/9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.28966338381871215, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3646426766818066}}, {"question": "\u5173\u7cfb\u8425\u9500\u66f4\u6ce8\u610f\u7ef4\u7cfb\u73b0\u6709\u987e\u5ba2\uff0c\u4e0d\u8ba9\u987e\u5ba2\u6709\u79bb\u53bb\u7684\u673a\u4f1a\uff0c\u8fd9\u5c31\u662f\u4f01\u4e1a\u63a8\u884c\u7684\nA. \u5c11\u91cf\u987e\u5ba2\u80cc\u79bb\u8ba1\u5212\nB. \u633d\u7559\u987e\u5ba2\u8ba1\u5212\nC. \u96f6\u987e\u5ba2\u80cc\u79bb\u8ba1\u5212\nD. \u7559\u4f4f\u591a\u6570\u987e\u5ba2\u8ba1\u5212\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u4e0d\u5c5e\u4e8e\u9759\u529b\u6027\u529b\u91cf\u5de5\u7a0b\u7684\u662f\nA. \u5934\u624b\u5012\u7acb\nB. \u978d\u9a6c\nC. \u6e38\u6cf3\nD. \u4e3e\u91cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7905617542247215, "meta-math/MetaMath-Mistral-7B": 0.9573995088312195, "itpossible/Chinese-Mistral-7B-v0.1": 0.7333071604239528, "HuggingFaceH4/zephyr-7b-beta": 0.9000484932838625, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9601086803475617, "meta-llama/Meta-Llama-3-8B": 0.7517488797297758, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8222153889620132}}, {"question": "\u7532\u9a7e\u8f66\u95ef\u7ea2\u706f\uff0c\u5f53\u573a\u649e\u6b7b\u884c\u4eba\u738b\u67d0\u3002\u7532\u7684\u670b\u53cb\u4e59\u95fb\u8baf\u540e\u8ba9\u7532\u79bb\u5f00\uff0c\u5e76\u5728\u4ea4\u8b66\u8c03\u67e5\u65f6\u8c0e\u79f0\u662f\u81ea\u5df1\u5f00\u8f66\u8087\u4e8b\u3002\u4e59\u7684\u884c\u4e3a\u5e94\u8ba4\u5b9a\u4e3a\nA. \u4f2a\u8bc1\u7f6a\nB. \u4ea4\u901a\u8087\u4e8b\u7f6a\nC. \u5305\u5e87\u7f6a\nD. \u7a9d\u85cf\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6378661072188999, "meta-math/MetaMath-Mistral-7B": 0.9212575376211455, "itpossible/Chinese-Mistral-7B-v0.1": 0.4886653091658422, "HuggingFaceH4/zephyr-7b-beta": 0.9997194464945609, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8482041085724786, "meta-llama/Meta-Llama-3-8B": 0.6762997417580462, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.697050598380167}}, {"question": "\u54ea\u4e00\u5b63\u8282\u6700\u4e0d\u6613\u5f15\u53d1\u54ee\u5598\u75c5\nA. \u590f\nB. \u6625\nC. \u79cb\nD. \u51ac\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.30239379048437676, "HuggingFaceH4/zephyr-7b-beta": 0.48623791265367355, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f20\u64ad\u5b66\u754c\u8ba4\u4e3a\uff0c\u5728\u4e24\u6b21\u4e16\u754c\u5927\u6218\u4e4b\u95f4\u7684\u51e0\u5341\u5e74\u95f4\uff0c\u5173\u4e8e\u5927\u4f17\u4f20\u64ad\u5a01\u529b\u7814\u7a76\u4e2d\u6700\u6d41\u884c\u7684\u89c2\u70b9\u662f\nA. \u6700\u4f4e\u6548\u679c\u6cd5\u5219\nB. \u9b54\u5f39\u8bba\nC. \u9002\u5ea6\u6548\u679c\u8bba\nD. \u6709\u9650\u6548\u679c\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.43327400216389994, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2986334267609958, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f01\u4e1a\u5728\u5236\u5b9a\u8425\u9500\u653f\u7b56\u65f6\uff0c\u8981\u7edf\u7b79\u517c\u987e\u4e09\u65b9\u9762\u7684\u5229\u76ca\uff0c\u5373\u4f01\u4e1a\u5229\u6da6\u3001\u6d88\u8d39\u8005\u9700\u8981\u7684\u6ee1\u8db3\u548c\u793e\u4f1a\u5229\u76ca\uff0c\u8fd9\u79cd\u8425\u9500\u7ba1\u7406\u54f2\u5b66\u662f\nA. \u793e\u4f1a\u5e02\u573a\u8425\u9500\u89c2\u5ff5\nB. \u751f\u4ea7\u89c2\u5ff5\nC. \u63a8\u9500\u89c2\u5ff5\nD. \u5e02\u573a\u8425\u9500\u89c2\u5ff5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6636379751390967, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8105650337030155, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9824534262830347}}, {"question": "\u5bf9\u6cd5\u7684\u4f5c\u7528\u7684\u7247\u9762\u6216\u9519\u8bef\u8ba4\u8bc6\u4e4b\u4e00\u662f\nA. \u6cd5\u9700\u8981\u4eba\u7684\u6b63\u786e\u6267\u884c\u548c\u9002\u7528\nB. \u6cd5\u7684\u62bd\u8c61\u6027\u3001\u7a33\u5b9a\u6027\u4e0e\u73b0\u5b9e\u751f\u6d3b\u5b58\u5728\u4e00\u5b9a\u7684\u77db\u76fe\nC. \u6cd5\u662f\u8fbe\u5230\u4e00\u5b9a\u76ee\u7684\u7684\u552f\u4e00\u624b\u6bb5\u548c\u5de5\u5177\nD. \u6cd5\u5e76\u4e0d\u662f\u8c03\u6574\u793e\u4f1a\u5173\u7cfb\u7684\u552f\u4e00\u624b\u6bb5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4616704961742358, "meta-math/MetaMath-Mistral-7B": 0.3949756198399488, "itpossible/Chinese-Mistral-7B-v0.1": 0.8201038678194625, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6319849220230439, "meta-llama/Meta-Llama-3-8B": 0.6387009339729277, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u80be\u6240\u85cf\u4e4b\u7cbe\u662f\u6307\nA. \u5148\u5929\u4e4b\u7cbe\nB. \u5305\u62ec\u5148\u5929\u4e4b\u7cbe\u548c\u540e\u5929\u4e4b\u7cbe\nC. \u751f\u6b96\u4e4b\u7cbe\nD. \u540e\u5929\u4e4b\u7cbe\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3534716292209113, "meta-math/MetaMath-Mistral-7B": 0.5158824242932266, "itpossible/Chinese-Mistral-7B-v0.1": 0.3672766845447511, "HuggingFaceH4/zephyr-7b-beta": 0.6304847342344776, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5576165080406835, "meta-llama/Meta-Llama-3-8B": 0.5977709964227157, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8018738529822852}}, {"question": "\u4e0b\u9762\u7684\u53e5\u5b50\u5404\u5c5e\u4e8e\u4ec0\u4e48\u53e5\u5f0f\uff0c\u2460\u9648\u8ff0\u53e5\u2461\u7591\u95ee\u53e5\u2462\u53cd\u95ee\u53e5\uff0c\u9009\u62e9\u6b63\u786e\u7684\u7b54\u6848\u628a\u5e8f\u53f7\u586b\u5728\u6a2a\u7ebf\u5904___\u8fd9\u4e48\u8fdc\uff0c\u7bad\u54ea\u80fd\u5c04\u5230\u5462\uff1f___\u94fa\u8fd9\u4e48\u957f\u7684\u8def\u8981\u591a\u5c11\u5757\u77f3\u677f\u5462\uff1f___\u9648\u8d53\u7275\u7740\u90a3\u5339\u88ab\u75b2\u60eb\u7684\u7626\u9a6c\uff0c\u4e00\u6b65\u4e00\u6b65\u671d\u524d\u8d70\u7740\u3002\nA. \u2460\u2462\u2461\nB. \u2461\u2462\u2460\nC. \u2460\u2461\u2462\nD. \u2462\u2461\u2460\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c34\u8f6e\u673a\u5bfc\u8f74\u627f\u5b89\u88c5\u662f\u5728\uff08\uff09\u8fdb\u884c\u7684\u3002\nA. \u53d1\u7535\u673a\u8f6c\u5b50\u4e0e\u8f6c\u8f6e\u4e3b\u8f74\u8054\u76d8\u540e\nB. \u8f6c\u8f6e\u4e3b\u8f74\u540a\u5165\uff0c\u9876\u76d6\u5b89\u88c5\u597d\u540e\nC. \u673a\u7ec4\u8f74\u7ebf\u8c03\u6574\u5408\u683c\u53ca\u63a8\u529b\u8f74\u627f\u53d7\u529b\u8c03\u6574\u540e\nD. \u5bfc\u6c34\u673a\u6784\u5b89\u88c5\u597d\u540e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5052963528989117, "meta-math/MetaMath-Mistral-7B": 0.6085790183486356, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7512053491532268, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47995611658487314, "meta-llama/Meta-Llama-3-8B": 0.39564235197810266, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5746624269805186}}, {"question": "\u201c\u7389\u4e0d\u7422\uff0c\u4e0d\u6210\u5668;\u4eba\u4e0d\u5b66\uff0c\u4e0d\u77e5\u4e49\u201d\u63ed\u793a\u4e86\u6559\u80b2\u7684()\nA. \u7ecf\u6d4e\u529f\u80fd\nB. \u793e\u4f1a\u529f\u80fd\nC. \u4e2a\u4f53\u529f\u80fd\nD. \u653f\u6cbb\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6523692190520031, "meta-math/MetaMath-Mistral-7B": 0.794249549864923, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6701814378065085, "meta-llama/Meta-Llama-3-8B": 0.771511748417769, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8070839066164607}}, {"question": "\u5728Word\u4e2d\u8f93\u5165\u67d0\u4e9b\u7279\u6b8a\u7684\u7b26\u53f7\u65f6\uff0c\u53ef\u884c\u7684\u64cd\u4f5c\u65b9\u5f0f\u662f\nA. \u4f9d\u6b21\u5355\u51fb\u201c\u683c\u5f0f\u201d\u3001\u201c\u5b57\u4f53\u201d\nB. \u4f9d\u6b21\u5355\u51fb\u201c\u683c\u5f0f\u201d\u3001\u201c\u9879\u76ee\u7b26\u53f7\u548c\u7f16\u53f7\u201d\nC. \u4f9d\u6b21\u5355\u51fb\u201c\u63d2\u5165\u201d\u3001\u201c\u5bf9\u8c61\u201d \nD. \u5728\u7f16\u8f91\u533a\u53f3\u51fb\uff0c\u6253\u5f00\u5feb\u6377\u83dc\u5355\uff0c\u5355\u51fb\u5176\u4e2d\u201c\u7b26\u53f7\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6028971223887525, "meta-math/MetaMath-Mistral-7B": 0.9594333739243682, "itpossible/Chinese-Mistral-7B-v0.1": 0.8799713751631573, "HuggingFaceH4/zephyr-7b-beta": 0.922172423976871, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6721418427462084, "meta-llama/Meta-Llama-3-8B": 0.8208620488618871, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6543614153095292}}, {"question": "\u4e0b\u957f\u5761\u8fde\u7eed\u4f7f\u7528\u884c\u8f66\u5236\u52a8\u4f1a\u5bfc\u81f4\u4ec0\u4e48\nA. \u4f1a\u4f7f\u5236\u52a8\u5668\u6e29\u5ea6\u5347\u9ad8\u800c\u4f7f\u5236\u52a8\u6548\u679c\u6025\u5267\u4e0b\u964d\nB. \u5bb9\u6613\u9020\u6210\u8f66\u8f86\u503e\u7ffb\nC. \u4f1a\u7f29\u77ed\u53d1\u52a8\u673a\u5bff\u547d\nD. \u589e\u52a0\u9a7e\u9a76\u4eba\u7684\u52b3\u52a8\u5f3a\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7489447931647201, "meta-math/MetaMath-Mistral-7B": 0.9010879815436948, "itpossible/Chinese-Mistral-7B-v0.1": 0.9066587854100724, "HuggingFaceH4/zephyr-7b-beta": 0.9964684176936388, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9844759914950562, "meta-llama/Meta-Llama-3-8B": 0.8778650227025582, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9878257855766961}}, {"question": "\u5728\u6587\u6069\u56fe\u4e2d\uff0c\u7528\u6765\u6807\u8bb0\u7279\u79f0\u547d\u9898\u7684\u662f\u3002\nA. \u9634\u5f71\nB. \u661f\u53f7\nC. \u6570\u5b57\nD. \u5706\u5708\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.31283638571410965, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6786645175932259}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u805a\u4f17\u6597\u6bb4\u7f6a\u7684\u8bf4\u6cd5\uff0c\u6b63\u786e\u7684\u662f\nA. \u5728\u805a\u4f17\u6597\u6bb4\u8fc7\u7a0b\u4e2d\uff0c\u5982\u679c\u81f4\u4eba\u6b7b\u4ea1\uff0c\u90a3\u4e48\u5bf9\u9996\u8981\u5206\u5b50\u4e00\u5f8b\u6309\u7167\u6545\u610f\u6740\u4eba\u7f6a\u5b9a\u7f6a\u5904\u7f5a\nB. \u805a\u4f17\u6597\u6bb4\u7f6a\u53ea\u662f\u5904\u7f5a\u805a\u4f17\u6597\u6bb4\u7684\u9996\u8981\u5206\u5b50\uff0c\u5176\u4ed6\u53c2\u4e0e\u8005\u4e0d\u8d1f\u5211\u4e8b\u8d23\u4efb\nC. \u5728\u805a\u4f17\u6597\u6bb4\u8fc7\u7a0b\u4e2d\uff0c\u5982\u679c\u81f4\u4eba\u6b7b\u4ea1\uff0c\u90a3\u4e48\u76f4\u63a5\u81f4\u4f7f\u88ab\u5bb3\u4eba\u6b7b\u4ea1\u7684\u72af\u7f6a\u5206\u5b50\u5e94\u5f53\u6309\u7167\u6545\u610f\u6740\u4eba\u7f6a\u5b9a\u7f6a\u5904\u7f5a\nD. \u5728\u805a\u4f17\u6597\u6bb4\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5982\u679c\u81f4\u4eba\u6b7b\u4ea1\uff0c\u90a3\u4e48\u805a\u4f17\u6597\u6bb4\u53cc\u65b9\u6240\u6709\u4eba\u90fd\u5e94\u5f53\u6309\u7167\u6545\u610f\u6740\u4eba\u7f6a\u5b9a\u7f6a\u5904\u7f5a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6956245729200546, "meta-math/MetaMath-Mistral-7B": 0.9054003914415453, "itpossible/Chinese-Mistral-7B-v0.1": 0.5724807643524719, "HuggingFaceH4/zephyr-7b-beta": 0.9840691952594252, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8771933003988737, "meta-llama/Meta-Llama-3-8B": 0.8164577151364214, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9039040892798328}}, {"question": "\u6b66\u672f\u7684\u672c\u8d28\u7279\u6027\u662f\nA. \u5065\u8eab\u6027\nB. \u653b\u9632\u6280\u51fb\u6027\nC. \u6280\u672f\u6027\nD. \u827a\u672f\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.47471837021886126, "meta-math/MetaMath-Mistral-7B": 0.5060781025047146, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6590610634614799, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.47554750366739557}}, {"question": "\u5728\u8fd1\u4ee3\u6559\u80b2\u53f2\u4e0a\uff0c\u8ba4\u4e3a\u6559\u5b66\u4e3b\u8981\u5e94\u8bad\u7ec3\u5b66\u751f\u601d\u7ef4\u800c\u8f7b\u89c6\u4f20\u6388\u77e5\u8bc6\u7684\u7406\u8bba\u88ab\u79f0\u4e3a\u662f\nA. \u73b0\u4ee3\u6559\u80b2\u7406\u8bba\nB. \u4f20\u7edf\u6559\u80b2\u7406\u8bba\nC. \u5f62\u5f0f\u6559\u80b2\u7406\u8bba\nD. \u5b9e\u8d28\u6559\u80b2\u7406\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4858615050022016, "meta-math/MetaMath-Mistral-7B": 0.7359831479604672, "itpossible/Chinese-Mistral-7B-v0.1": 0.38745561900026004, "HuggingFaceH4/zephyr-7b-beta": 0.9490964451896458, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f01\u4e1a\u6807\u5fd7\u5728\u89c6\u89c9\u4f20\u8fbe\u4e2d\u7684\u57fa\u672c\u529f\u80fd\u662f\nA. \u5ef6\u4f38\u6027\nB. \u540c\u4e00\u6027\nC. \u9886\u5bfc\u6027\nD. \u8bc6\u522b\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u9009\u9879\u5f53\u4e2d\uff0c\u4e0d\u53ef\u4ee5\u88ab\u76f4\u63a5\u5438\u6536\u7684\u7269\u8d28\u662f\nA. \u679c\u7cd6\nB. \u9499\u79bb\u5b50\nC. \u7ef4\u751f\u7d20C\nD. \u4e73\u7cd6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.685094989299643, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6784\u6210\u6cd5\u5f8b\u90e8\u95e8\u7684\u6700\u57fa\u672c\u7ec6\u80de\u662f\nA. \u89c4\u8303\u6027\u6cd5\u5f8b\u6587\u4ef6\nB. \u6cd5\u5f8b\u5236\u5ea6\nC. \u6cd5\u5f8b\u89c4\u8303\nD. \u6cd5\u5f8b\u4f53\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7687638914749744, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9698911833866121, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5483660391344901}}, {"question": "\u6839\u636e\u300a\u4e13\u5229\u6cd5\u300b\u7684\u6709\u5173\u89c4\u5b9a\uff0c\u4e0b\u5217\u9009\u9879\u4e2d\u53ef\u4ee5\u6388\u4e88\u4e13\u5229\u6743\u7684\u662f\nA. \u4e59\u53d1\u660e\u4e86\u5bf9\u7cd6\u5c3f\u75c5\u7279\u6709\u7684\u6cbb\u7597\u65b9\u6cd5\nB. \u4e01\u53d1\u660e\u4e86\u67d0\u690d\u7269\u65b0\u54c1\u79cd\u7684\u751f\u4ea7\u65b9\u6cd5\nC. \u4e19\u5bf9\u4e24\u79cd\u5e73\u9762\u5370\u5237\u54c1\u7684\u56fe\u6848\u7684\u7ed3\u5408\u4f5c\u51fa\u7684\u4e3b\u8981\u8d77\u6807\u8bc6\u4f5c\u7528\u7684\u8bbe\u8ba1\nD. \u7532\u53d1\u660e\u4e86\u7528\u4ee5\u8eb2\u907f\u76d1\u63a7\u673a\u52a8\u8f66\u8d85\u901f\u884c\u9a76\u7684\u96f7\u8fbe\u7684\u9884\u8b66\u7535\u5b50\u72d7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2885095257630687, "HuggingFaceH4/zephyr-7b-beta": 0.8595793517084866, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u4e0d\u5c5e\u4e8e\u968f\u56ed\u83dc\u7279\u70b9\u7684\u662f\nA. \u6ce8\u91cd\u539f\u6599\u9009\u62e9\u548c\u642d\u914d\nB. \u8bb2\u7a76\u8272\u3001\u9999\u3001\u5473\u3001\u5f62\u3001\u5668\u53ca\u8fdb\u98df\u827a\u672f\nC. \u6781\u5176\u8bb2\u7a76\u5bcc\u8d35\u548c\u6ecb\u8865\nD. \u6ce8\u91cd\u7b75\u5e2d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5e38\u89c1\u7684\u6050\u6016\u6d3b\u52a8\u5f62\u5f0f\u4e4b\u4e00\u662f\nA. \u9ed1\u793e\u4f1a\nB. \u6c11\u65cf\u4e3b\u4e49\nC. \u6781\u5de6\u7ffc\nD. \u52ab\u6301\u98de\u673a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u72af\u7f6a\u5206\u5b50\u5c06\u81ea\u5df1\u76d7\u7a83\u7684\u6570\u989d\u8f83\u5927\u7684\u8d22\u7269\u4e88\u4ee5\u9500\u552e\u7684\uff0c\u6784\u6210\nA. \u63a9\u9970\u3001\u9690\u7792\u72af\u7f6a\u6240\u5f97\u3001\u72af\u7f6a\u6240\u5f97\u6536\u76ca\u7f6a\nB. \u59a8\u5bb3\u8bc1\u636e\u7f6a\nC. \u76d7\u7a83\u7f6a\u4e00\u7f6a\nD. \u76d7\u7a83\u7f6a\u548c\u63a9\u9970\u3001\u9690\u7792\u72af\u7f6a\u6240\u5f97\u3001\u72af\u7f6a\u6240\u5f97\u6536\u76ca\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0e\u2f83\u7136\u2f1f\u58e4\u76f8\u2f50\uff0c\u2f54\u7a3b\u2f1f\nA. \u7ed3\u6784\u66f4\u590d\u6742\nB. \u4e0d\u542b\u8150\u6b96\u8d28\nC. \u67af\u679d\u843d\u53f6\u5c42\u589e\u539a\nD. \u80a5\u2f12\u2f54\u5e73\u4e0b\u964d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.44623685971274624, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.37286420319786495}}, {"question": "\u4e0b\u5217\u4e0d\u5c5e\u4e8e\u6539\u5584\u75c5\u60c5\u7684\u6297\u98ce\u6e7f\u836f\u7269\u662f\nA. \u6c2f\u55b9\nB. \u7532\u6c28\u8776\u5464\nC. \u67f3\u6c2e\u78fa\u5421\u5576\nD. \u8418\u666e\u751f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5297390867488198, "meta-math/MetaMath-Mistral-7B": 0.8457102523086818, "itpossible/Chinese-Mistral-7B-v0.1": 0.307023896814742, "HuggingFaceH4/zephyr-7b-beta": 0.9645360443041713, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9461989161785108, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e3a\u673a\u4f53\u5408\u6210\u8102\u80aa\u9178\u63d0\u4f9b NADPH \u7684\u662f\nA. \u67e0\u6aac\u9178-\u4e19\u916e\u9178\u5faa\u73af\nB. \u4e19\u6c28\u9178-\u8461\u8404\u7cd6\u5faa\u73af\nC. \u4e09\u7fa7\u9178\u5faa\u73af\nD. \u4e73\u9178\u5faa\u73af\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4184173992585321, "meta-math/MetaMath-Mistral-7B": 0.5454638292731577, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5737569499317772, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u5b50\u4e2d\u6210\u8bed\u4f7f\u7528\u4e0d\u6070\u5f53\u7684\u4e00\u9879\u662f\nA. \u4e00\u4e2a\u80f8\u895f\u72ed\u9698\u7684\u4eba\uff0c\u4e00\u5b9a\u662f\u4e00\u4e2a\u559c\u6b22\u89c1\u98ce\u4f7f\u8235\uff0c\u65f6\u523b\u51c6\u5907\u8ba9\u81ea\u5df1\u5fc3\u7075\u4e0b\u8dea\u7684\u4eba\u3002\nB. \u4ed6\u603b\u662f\u6d25\u6d25\u4e50\u9053\u4e8e\u81ea\u5df1\u5df2\u7ecf\u53d6\u5f97\u7684\u6210\u7ee9\u3002\nC. \u4ece\u98ce\u683c\u770b\uff0c\u674e\u767d\u7684\u662f\u98d8\u9038\u8c6a\u653e\uff0c\u675c\u752b\u7684\u662f\u6c89\u90c1\u987f\u632b\uff0c\u5404\u6709\u5343\u79cb\u3002\nD. \u674e\u660e\u753b\u7684\u8fd9\u5e45\u753b\u5938\u5f20\u800c\u4f20\u795e\uff0c\u8fde\u4ed6\u7238\u7238\u770b\u4e86\u90fd\u5fcd\u4fca\u4e0d\u7981\u5730\u5927\u7b11\u8d77\u6765\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3133232324172161, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6521479990820787, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u9664\u54ea\u9879\u5916\uff0c\u5747\u4e3a\u996e\u98df\u505c\u6ede\u578b\u8179\u75db\u7684\u4e3b\u75c7\nA. \u55f3\u8150\u541e\u9178\nB. \u8118\u8179\u80c0\u6ee1\nC. \u75bc\u75db\u62d2\u6309\nD. \u820c\u7ea2\u5c11\u82d4\uff0c\u8109\u7ec6\u6570\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3034791597392292, "meta-math/MetaMath-Mistral-7B": 0.6267179884152353, "itpossible/Chinese-Mistral-7B-v0.1": 0.4336342941897161, "HuggingFaceH4/zephyr-7b-beta": 0.9690890183500743, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.534968023130534, "meta-llama/Meta-Llama-3-8B": 0.4201996442380645, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4743223039725629}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u4e0d\u5c5e\u4e8e\u4f01\u4e1a\u80a1\u4e1c\u6743\u5229\u7684\u662f\nA. \u5bf9\u4f01\u4e1a\u8d44\u4ea7\u7684\u63a7\u5236\u6743\nB. \u5176\u4ed6\u6709\u5173\u6cd5\u89c4\u3001\u89c4\u5219\u89c4\u5b9a\u7684\u5176\u4ed6\u6743\u5229\nC. \u62e5\u6709\u5269\u4f59\u7d22\u53d6\u6743\nD. \u4f01\u4e1a\u7ae0\u7a0b\u7684\u6743\u5229\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ece14\u4e16\u7eaa\u5f00\u59cb\uff0c\u82f1\u56fd\u5927\u6cd5\u5b98\u6cd5\u9662\u7684\u6cd5\u5b98\u4eec\u6839\u636e\u516c\u5e73\u6b63\u4e49\u539f\u5219\u548c\u89c4\u5219\u5bf9\u666e\u901a\u6cd5\u8fdb\u884c\u4fee\u6b63\u3001\u8865\u5145\u800c\u51fa\u73b0\u548c\u53d1\u5c55\u8d77\u6765\u7684\u4e00\u79cd\u6cd5\u5f8b\uff0c\u79f0\u4e3a\nA. \u666e\u901a\u6cd5\u4fee\u6b63\nB. \u8861\u5e73\u6cd5\nC. \u666e\u901a\u6cd5\nD. \u4e60\u60ef\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7352035700116759, "meta-math/MetaMath-Mistral-7B": 0.7804280194003337, "itpossible/Chinese-Mistral-7B-v0.1": 0.757318661373762, "HuggingFaceH4/zephyr-7b-beta": 0.9799389405153232, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9035763069587396, "meta-llama/Meta-Llama-3-8B": 0.9730933868884841, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.997268566101645}}, {"question": "\u6709\u98ce\u9669\u6761\u4ef6\u4e0b\u7684\u51b3\u7b56\uff0c\u5176\u51b3\u7b56\u6761\u4ef6\u662f\nA. \u51b3\u7b56\u7684\u53ef\u80fd\u7ed3\u679c\u6709\u591a\u4e2a\uff0c\u4f46\u4e0d\u77e5\u9053\u6bcf\u79cd\u7ed3\u679c\u53d1\u751f\u7684\u6982\u7387\nB. \u51b3\u7b56\u7684\u53ef\u80fd\u7ed3\u679c\u6709\u591a\u4e2a\uff0c\u4e14\u6bcf\u79cd\u7ed3\u679c\u7684\u53d1\u751f\u6982\u7387\u4e3a\u5df2\u77e5\nC. \u51b3\u7b56\u7684\u53ef\u80fd\u7ed3\u679c\u6709\u591a\u4e2a\nD. \u51b3\u7b56\u7684\u53ef\u80fd\u7ed3\u679c\u53ea\u6709\u4e00\u4e2a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6508189389767961}}, {"question": "\u6d59\u6c5f\u7701\u7684\u6d2a\u707e\u591a\u53d1\u4e8e\nA. 7\u6708\u52308\u6708\nB. 7\u6708\u52309\u6708\nC. 6\u6708\u52308\u6708\nD. 6\u6708\u52309\u6708\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3160427140241836, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3815746737409844, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u54ea\u91cc\u6ca1\u6709\u6cd5\u5f8b\uff0c\u54ea\u91cc\u5c31\u6ca1\u6709\u81ea\u7531\u3002\u201d\u5173\u4e8e\u8fd9\u53e5\u8bdd\uff0c\u4e0b\u5217\u7406\u89e3\u6b63\u786e\u7684\u662f\nA. \u6cd5\u5f8b\u4ee5\u4fdd\u969c\u4e2a\u4eba\u81ea\u7531\u4e3a\u552f\u4e00\u7684\u4ef7\u503c\u548c\u76ee\u6807\nB. \u82e5\u6ca1\u6709\u6cd5\u5f8b\u7684\u4fdd\u969c\u548c\u7ea6\u675f\uff0c\u81ea\u7531\u4fbf\u8361\u7136\u65e0\u5b58\nC. \u4eba\u751f\u800c\u81ea\u7531\uff0c\u56e0\u6b64\u81ea\u7531\u4e0e\u6cd5\u5f8b\u65e0\u5173\nD. \u81ea\u7531\u610f\u5473\u7740\u4eba\u53ef\u4ee5\u4ece\u4e8b\u4efb\u4f55\u81ea\u5df1\u60f3\u505a\u7684\u4e8b\u60c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8338228243430463, "meta-math/MetaMath-Mistral-7B": 0.9868035613399707, "itpossible/Chinese-Mistral-7B-v0.1": 0.80313078939558, "HuggingFaceH4/zephyr-7b-beta": 0.9999051875367938, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9958852168662692, "meta-llama/Meta-Llama-3-8B": 0.9358894066964323, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9915082778998642}}, {"question": "\u82e5\u2f17\u516d\u8fdb\u6570\u5faeAC.B\uff0c\u5219\u5176\u2f17\u8fdb\u5236\u6570\u4e3a\nA. 254.54\nB. 172.6875\nC. 172.625\nD. 2763\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.33482349867891054, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.32545507259594497, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u539f\u53d1\u6027\u5fae\u5c0f\u75c5\u53d8\u6027\u80be\u75c5\u7684\u4e3b\u8981\u7279\u70b9\u662f\nA. \u80be\u5c0f\u7403\u57fa\u5e95\u819c\u660e\u663e\u589e\u539a\uff0c\u5448\u9489\u72b6\u7a81\u8d77\nB. \u4e0a\u547c\u5438\u9053\u611f\u67d3\u540e 1-3 \u5929\u51fa\u73b0\u8840\u5c3f\nC. \u53cc\u4e0b\u80a2\u5bf9\u79f0\u51fa\u8840\u70b9\u4f34\u5927\u91cf\u86cb\u767d\u5c3f\u548c\u4f4e\u86cb\u767d\u8840\u75c7\nD. \u53cd\u590d\u53d1\u4f5c\u7684\u5927\u91cf\u86cb\u767d\u5c3f\uff0c\u4e0d\u4f34\u8840\u5c3f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4791329022432429, "meta-math/MetaMath-Mistral-7B": 0.6206149452596967, "itpossible/Chinese-Mistral-7B-v0.1": 0.37887214559235705, "HuggingFaceH4/zephyr-7b-beta": 0.9995597280673343, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3349017728881003, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9a6c\u94c3\u66ae\u5757\u830e\u50a8\u85cf\u4e0d\u5f53\u4f1a\u51fa\u73b0\u9178\u5473\uff0c\u8fd9\u79cd\u73b0\u8c61\u4e0e\u9a6c\u94c3\u57fa\u5757\u830e\u7ec6\u80de\u7684\u65e0\u6c27\u547c\u5438\u6709\u5173\u3002\u4e0b\u5217\u53d9\u8ff0\u6b63\u786e\u7684\u662f\nA. \u9a6c\u94c3\u85af\u5757\u830e\u7ec6\u80de\u65e0\u6c27\u547c\u5438\u4ea7\u751f\u7684\u4e73\u9178\u662f\u7531\u4e19\u916e\u9178\u8f6c\u5316\u800c\u6765\nB. \u9a6c\u94c3\u85af\u5757\u830e\u7ec6\u80de\u65e0\u6c27\u547c\u5438\u4ea7\u751f\u4e19\u916e\u9178\u7684\u8fc7\u7a0b\u4e0d\u80fd\u751f\u6210ATP\nC. \u9a6c\u94c3\u85af\u5757\u830e\u7ec6\u80de\u65e0\u6c27\u547c\u5438\u7684\u4ea7\u7269\u662f\u4e73\u9178\u548c\u8461\u8404\nD. \u9a6c\u94c3\u85af\u5757\u830e\u50a8\u85cf\u5e93\u4e2d\u6c27\u6c14\u6d53\u5ea6\u7684\u5347\u9ad8\u4f1a\u589e\u52a0\u9178\u5473\u7684\u4ea7\u751f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30150657848650314, "meta-math/MetaMath-Mistral-7B": 0.4713948392283368, "itpossible/Chinese-Mistral-7B-v0.1": 0.4513194825315068, "HuggingFaceH4/zephyr-7b-beta": 0.8796793547514377, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4965194293553775, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7991656604792249}}, {"question": "\u5728\u5012\u6bcd\u7ebf\u65f6\uff0c\u5728\u62c9\u5f00\u6bcd\u8054\u65ad\u8def\u5668\u64cd\u4f5c\u7535\u6e90\u524d\u5e94\u5148\u6295\u5165\nA. \u6c9f\u901a\u4e09\u8df3\u538b\u677f\nB. \u8fc7\u6d41\u4fdd\u62a4\u538b\u677f\nC. \u5145\u7535\u4fdd\u62a4\u538b\u677f\nD. \u4e92\u8054\u538b\u677f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.303006867405966, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1968\u5e74\u6d77\u7259\u300a\u5173\u4e8e\u4ea4\u901a\u4e8b\u6545\u6cd5\u5f8b\u9002\u7528\u7684\u516c\u7ea6\u300b\u89c4\u5b9a\uff0c\u4ea4\u901a\u4e8b\u6545\u539f\u5219\u4e0a\u9002\u7528\nA. \u8f66\u8f86\u767b\u8bb0\u5730\u6cd5\nB. \u5f53\u4e8b\u4eba\u60ef\u5e38\u5c45\u6240\u5730\u6cd5\nC. \u8f66\u8f86\u7ecf\u5e38\u505c\u653e\u5730\u6cd5\nD. \u4e8b\u6545\u53d1\u751f\u5730\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3436615088034304, "meta-math/MetaMath-Mistral-7B": 0.5337436524773918, "itpossible/Chinese-Mistral-7B-v0.1": 0.6979565796986209, "HuggingFaceH4/zephyr-7b-beta": 0.991697664357184, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4655094774097466, "meta-llama/Meta-Llama-3-8B": 0.3966658933538001, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8fa9\u8bc1\u6cd5\u7684\u5426\u5b9a\u5373\u201c\u626c\u5f03\u201d\uff0c\u5b83\u7684\u542b\u4e49\u662f\u6307\nA. \u629b\u5f03\nB. \u4e8b\u7269\u4e2d\u597d\u7684\u65b9\u9762\u548c\u574f\u7684\u65b9\u9762\u7684\u7ec4\u5408\nC. \u7eaf\u7cb9\u7684\u5426\u5b9a\nD. \u65e2\u514b\u670d\u53c8\u4fdd\u7559\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4946643038647148, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4645499962284441, "HuggingFaceH4/zephyr-7b-beta": 0.9078111858177031, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5512818068718115, "meta-llama/Meta-Llama-3-8B": 0.9791098504008157, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9665548475786216}}, {"question": "\u7269\u4f53\u7531\u4e8e\u8fd0\u52a8\u800c\u5177\u6709\u7684\u80fdf\u79f0\u4e3a\nA. \u673a\u68b0\u80fd\nB. \u52a8\u80fd\nC. \u52bf\u80fd\nD. \u60ef\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8668546594927592, "meta-math/MetaMath-Mistral-7B": 0.9184454671182447, "itpossible/Chinese-Mistral-7B-v0.1": 0.7302946123829648, "HuggingFaceH4/zephyr-7b-beta": 0.9206019242344947, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8733796754927926, "meta-llama/Meta-Llama-3-8B": 0.7386320574421967, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7843449132105711}}, {"question": "\u7a81\u51fa\u8868\u660e\u897f\u6b27\u5bf9\u5357\u5317\u95ee\u9898\u91c7\u53d6\u8f83\u4e3a\u5f00\u660e\u7acb\u573a\u7684\u662f\nA. \u6d1b\u7f8e\u534f\u5b9a\nB. \u7b2c\u56db\u70b9\u8ba1\u5212\nC. \u8d6b\u5c14\u8f9b\u57fa\u6700\u540e\u6587\u4ef6\nD. \u798f\u7530\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2828645123477969, "meta-math/MetaMath-Mistral-7B": 0.3075818303735863, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u63a8\u955c\u5934\u662f\u6307\nA. \u6444\u50cf\u673a\u968f\u7269\u4f53\u8fd0\u52a8\u800c\u8fdb\u884c\u62cd\u6444\u7684\u955c\u5934\nB. \u6444\u50cf\u673a\u56fa\u5b9a\u5728\u4e00\u4e2a\u652f\u70b9\u4e0a\uff0c\u6cbf\u6c34\u5e73\u8f74\u6216\u5782\u76f4\u8f74\u8fd0\u52a8\u62cd\u6444\u7684\u955c\u5934\nC. \u6444\u50cf\u673a\u53d8\u5316\u7126\u8ddd\u4f7f\u753b\u9762\u6846\u67b6\u9010\u6e10\u8fdc\u79bb\u88ab\u6444\u5bf9\u8c61\u7684\u955c\u5934\nD. \u4e0e\u62c9\u955c\u5934\u8fd0\u52a8\u65b9\u5411\u76f8\u53cd\u7684\u4e00\u79cd\u62cd\u6444\u65b9\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5fae\u673a\u4e2d1K\u5b57\u8282\u8868\u793a\u7684\u5b57\u8282\u6570\u662f\nA. 1024\nB. 8X1024\nC. 1000\nD. 8X1000\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5257541375612462, "meta-math/MetaMath-Mistral-7B": 0.8922287248270252, "itpossible/Chinese-Mistral-7B-v0.1": 0.6570540818518295, "HuggingFaceH4/zephyr-7b-beta": 0.76949784547381, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8962680596956744, "meta-llama/Meta-Llama-3-8B": 0.605985134989143, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9939222493039982}}, {"question": "\u5728\u6211\u56fd\uff0c\u627f\u62c5\u8fdd\u5baa\u8d23\u4efb\u3001\u627f\u53d7\u8fdd\u5baa\u5236\u88c1\u7684\u4e3b\u4f53\u4e3b\u8981\u662f\nA. \u56fd\u5bb6\u673a\u5173\u53ca\u5176\u9886\u5bfc\u4eba\u5458\nB. \u56fd\u5bb6\u673a\u5173\u516c\u804c\u4eba\u5458\nC. \u56fd\u5bb6\u673a\u5173\u53ca\u5176\u516c\u804c\u4eba\u5458\nD. \u56fd\u5bb6\u673a\u5173\u9886\u5bfc\u4eba\u5458\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ece\u8fdd\u6cd5\u884c\u4e3a\u7684\u6784\u6210\u8981\u7d20\u770b\uff0c\u5224\u65ad\u67d0\u4e00\u884c\u4e3a\u662f\u5426\u8fdd\u6cd5\u7684\u5173\u952e\u56e0\u7d20\u662f\u4ec0\u4e48\uff1f\nA. \u8be5\u884c\u4e3a\u5728\u6cd5\u5f8b\u4e0a\u88ab\u786e\u8ba4\u4e3a\u8fdd\u6cd5\nB. \u8be5\u884c\u4e3a\u7531\u5177\u6709\u8d23\u4efb\u80fd\u529b\u7684\u4e3b\u4f53\u4f5c\u51fa\nC. \u8be5\u884c\u4e3a\u4fb5\u72af\u4e86\u6cd5\u5f8b\u6240\u4fdd\u62a4\u7684\u67d0\u79cd\u793e\u4f1a\u5173\u7cfb\u548c\u793e\u4f1a\u5229\u76ca\nD. \u8be5\u884c\u4e3a\u6709\u6545\u610f\u6216\u8005\u8fc7\u5931\u7684\u8fc7\u9519\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u51fd\u6570\u4e2d\uff0c\u5468\u671f\u662f$\\pi$\uff0c\u4e14\u5728$[0,\\pi/2]$\u4e0a\u7684\u51cf\u51fd\u6570\u662f\nA. $y=\\sin(2x)$\nB. $y=\\cos(2x)$\nC. $y=\\sin(x+\\pi/4)$\nD. $y=\\cos(x+\\pi/4)$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2821833983601387, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ed3\u5408\u8bed\u5883\uff0c\u5c06\u4e0b\u5217\u8bed\u53e5\u586b\u2f0a\u6570\u5b57\u5904\uff0c\u6700\u6070\u5f53\u7684\u2f00\u9879\u662f\uff1a\u2edb\u5439\u8fc7\u8349\u539f\uff0c\u2f46\u8fb9\u7684\u8349\u53f6\u6839\u830e\u76f8\u8fde\uff0c\u8f7b\u8f7b\u6447\u66f3\uff0c\u4e1d\u4e1d\u594f\u9e23\uff0c\u7eff\u6ce2\u5fae\u6f3e\uff0c\u9001\u6765\u7f15\u7f15\u6e05\u2fb9\uff0c\u90a3\u662fi. \u3002\u2edb\u5219\u663e\u5f97\u66f4\u6709\u8010\u2f3c\uff0c\u5b83\u4eec\u2f00\u5343\u5e74\u2f1c\u2f00\u5343\u5e74\u5730\u4ece\u8349\u5c16\u2f09\u8f7b\u8f7b\u63a0\u8fc7\uff0c\u4e3a\u7684\u53ea\u662f\u7b49\u5f85\u2f00\u4e2a\u673a\u4f1a\uff0cii. \uff0c\u5439\u5f00\u2f0f\u2f5a\u8584\u8584\u7684\u4e91\u5f69\u3002\nA. i.\u2f63\u547d\u4f18\u96c5\u7684\u6c89\u9189\u4e0e\u900d\u9065 ii.\u6380\u8d77\u2f00\u5c42\u677e\u677e\u7684\u6ce5\u2f1f\nB. i.\u2f63\u547d\u4f18\u96c5\u7684\u6c89\u9189\u4e8e\u900d\u9065 ii.\u5e26\u2f9b\u2f00\u70b9\u7ec6\u7ec6\u7684\u6ce5\u2f1f\nC. i.\u8fbd\u9614\u8349\u539f\u7684\u55a7\u54d7\u4e0e\u8e81\u52a8 ii.\u5e26\u2f9b\u2f00\u70b9\u7ec6\u7ec6\u7684\u6ce5\u2f1f\nD. i.\u8fbd\u9614\u8349\u539f\u7684\u55a7\u54d7\u4e0e\u8e81\u52a8 ii.\u6380\u8d77\u2f00\u5c42\u677e\u677e\u7684\u6ce5\u2f1f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28141066483141647, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3819020634097665, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u81ea\u89c9\u5c65\u884c\u793e\u4f1a\u8d23\u4efb\uff0c\u5173\u7231\u4ed6\u4eba\u3001\u6276\u5371\u6d4e\u56f0\u3001\u6276\u8d2b\u5e2e\u56f0\u3001\u70ed\u5fc3\u516c\u76ca\uff0c\u901a\u8fc7\u5404\u79cd\u5f62\u5f0f\u4e3a\u4eba\u6c11\u591a\u505a\u597d\u4e8b\uff0c\u4e3a\u793e\u4f1a\u591a\u505a\u8d21\u732e\uff0c\u62b5\u5236\u548c\u53cd\u5bf9\u8fdd\u80cc\u4eba\u6c11\u5229\u76ca\u548c\u613f\u671b\u7684\u601d\u60f3\u548c\u884c\u4e3a\u3002\u8fd9\u4f53\u73b0\u4e86\u4ee5\u201c\u516b\u8363\u516b\u803b\u201d\u4e3a\u4e3b\u8981\u5185\u5bb9\u7684\u793e\u4f1a\u4e3b\u4e49\u8363\u8fb1\u89c2\u4e2d\nA. \u201c\u4ee5\u670d\u52a1\u4eba\u6c11\u4e3a\u8363\uff0c\u4ee5\u80cc\u79bb\u4eba\u6c11\u4e3a\u803b\u201d\u7684\u8981\u6c42\nB. \u201c\u4ee5\u8270\u82e6\u594b\u6597\u4e3a\u8363\uff0c\u4ee5\u9a84\u5962\u6deb\u9038\u4e3a\u803b\u201d\u7684\u8981\u6c42\nC. \u201c\u4ee5\u5d07\u5c1a\u79d1\u5b66\u4e3a\u8363\uff0c\u4ee5\u611a\u6627\u65e0\u77e5\u4e3a\u803b\u201d\u7684\u8981\u6c42\nD. \u201c\u4ee5\u9075\u7eaa\u5b88\u6cd5\u4e3a\u8363\uff0c\u4ee5\u8fdd\u6cd5\u4e71\u7eaa\u4e3a\u803b\u201d\u7684\u8981\u6c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.7159693387504984, "itpossible/Chinese-Mistral-7B-v0.1": 0.3785916633064982, "HuggingFaceH4/zephyr-7b-beta": 0.9431430065654948, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9375645332531762, "meta-llama/Meta-Llama-3-8B": 0.6824601466612732, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5460498918912866}}, {"question": "\u201c\u5b5d\u516c\u65e2\u89c1\u536b\u9785\uff0c\u8bed\u4e8b\u826f\u4e45\uff0c\u5b5d\u516c\u65f6\u65f6\u7761\uff0c\u5f17\u542c\u3002\u201d\u4e2d\u7684\u201c\u7761\u201d\u610f\u601d\u662f\nA. \u7761\u9192\nB. \u7761\u89c9\nC. \u5750\u7740\u6253\u76f9\nD. \u7761\u7740\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5b57\u4e2d\u5b8c\u5168\u5c5e\u4e8e\u4f1a\u610f\u5b57\u7684\u4e00\u7ec4\u662f\nA. \u4e0a\u8fc7\u672c\nB. \u6212\u53ca\u708e\nC. \u6821\u4fe1\u539f\nD. \u53cb\u6b66\u53d4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28650612283664895, "meta-math/MetaMath-Mistral-7B": 0.4479780596806147, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8457\u6709\u300a\u5927\u4f17\u4f20\u64ad\u7684\u6548\u679c\u300b\u4e00\u4e66\uff0c\u5e76\u5c06\u53d7\u4f17\u5bf9\u4fe1\u606f\u63a5\u6536\u7684\u9009\u62e9\u6027\u884c\u4e3a\u8fdb\u884c\u5177\u4f53\u5206\u89e3\u7684\u4f20\u64ad\u5b66\u8005\u662f\nA. \u65bd\u62c9\u59c6\nB. \u970d\u592b\u5170\nC. \u674e\u666e\u66fc\nD. \u514b\u62c9\u5e15\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.31712010892822357, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1960\u5e74\uff0c\u65e5\u3001\u7f8e\u4fee\u65391951\u5e74\u7b7e\u8ba2\u7684\u519b\u4e8b\u6761\u7ea6\uff0c\u5220\u9664\u4e86\u539f\u6761\u7ea6\u4e2d\u9a7b\u65e5\u7f8e\u519b\u53ef\u4ee5\u7528\u4e8e\u9547\u538b\u65e5\u672c\u5185\u4e71\u7684\u6761\u6b3e\uff0c\u4fdd\u7559\u4e86\u539f\u6761\u7ea6\u4e2d\u7f8e\u519b\u9a7b\u624e\u65e5\u672c\u3001\u53cc\u65b9\u5171\u540c\u5e94\u5bf9\u5bf9\u65e5\u672c\u9886\u571f\u4e0a\u7684\u4efb\u4f55\u4e00\u65b9\u7684\u6b66\u529b\u8fdb\u653b\u7b49\u5185\u5bb9\uff0c\u7b7e\u8ba2\u4e86\u65b0\u7684\u65e5\u7f8e\u5b89\u5168\u6761\u7ea6\u3002\u8fd9\u4e00\u6761\u7ea6\u7684\u7b7e\u8ba2\nA. \u53cd\u6620\u51fa\u65e5\u672c\u5df2\u7ecf\u6210\u4e3a\u7ecf\u6d4e\u5927\u56fd\nB. \u6807\u5fd7\u7740\u4e9a\u592a\u5730\u533a\u7684\u529b\u91cf\u5bf9\u6bd4\u6539\u53d8\nC. \u610f\u5473\u7740\u7f8e\u56fd\u5728\u4e9a\u6d32\u7ef4\u6301\u51b7\u6218\u653f\u7b56\nD. \u8868\u660e\u65e5\u7f8e\u519b\u4e8b\u540c\u76df\u53d7\u5230\u524a\u5f31\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4186261171639434, "meta-math/MetaMath-Mistral-7B": 0.621653183350971, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5775706301421175, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c47\u5c81\uff0c\u8179\u80c0\u3001\u7eb3\u5dee\u534a\u5e74\uff0c6\u5c0f\u65f6\u524d\u7a81\u53d1\u5455\u8840\u6765\u8bca\u3002\u65e2\u5f80HBsAg(+)\u3002\u67e5\u4f53\uff1aP125\u6b21/\u5206\uff0cBP70/50mmHg\uff0c\u5de9\u819c\u8f7b\u5ea6\u9ec4\u67d3\uff0c\u809d\u813e\u808b\u4e0b\u672a\u89e6\u53ca\uff0c\u79fb\u52a8\u6027\u6d4a\u97f3(+)\uff0c\u4e0b\u80a2\u53ef\u51f9\u6027\u6c34\u80bf\u3002\u5e94\u9996\u9009\u7684\u5904\u7406\u63aa\u65bd\u662f\nA. \u5256\u8179\u63a2\u67e5\nB. \u4e09\u8154\u4e8c\u56ca\u7ba1\u538b\u8feb\nC. \u7d27\u6025\u8f93\u6db2\u548c\u8f93\u8840\nD. \u6025\u8bca\u80c3\u955c\u6b62\u8840\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5799368176803408, "meta-math/MetaMath-Mistral-7B": 0.74873316360111, "itpossible/Chinese-Mistral-7B-v0.1": 0.3460058095032025, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5918385478818752, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u519c\u6751\u5e38\u642d\u5efa\u6a2a\u622a\u2faf\u4e3a\u534a\u5706\u5f62\u7684\u5168\u5c01\u95ed\u5851\u6599\u8584\u819c\u852c\u83dc\u2f24\u68da\u3002\u5982\u679c\u4e0d\u8003\u8651\u5851\u6599\u8584\u819c\u57cb\u5728\u2f1f\u2fa5\u7684\u90e8\u5206\uff0c\u90a3\u4e48\u642d\u5efa\u2f00\u4e2a\u957f32m\uff0c\u5bbd4\u7c73\u7684\u852c\u83dc\u2f24\u68da\u9700\u8981\u2f64\u5851\u6599\u8584\u819c\u7684\u2faf\u79ef\u662f\nA. $72\\pi $\nB. $68\\pi $\nC. $64\\pi $\nD. $69\\pi $\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.35686329776860143, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9633\u865a\u5bd2\u51dd\u6240\u81f4\u7684\u4fbf\u79d8\uff0c\u5b9c\u9009\u7528\u7684\u4e2d\u6210\u836f\u662f\nA. \u6842\u9644\u5730\u9ec4\u4e38\u4e0e\u9ebb\u4ec1\u6ecb\u813e\nB. \u6e05\u5b81\u4e38\nC. \u69df\u6994\u56db\u6d88\u4e38\nD. \u9ebb\u4ec1\u6da6\u80a0\u4e38\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.29539205153207015, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5789936156902675, "meta-llama/Meta-Llama-3-8B": 0.27312720402870727, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9ad8\u4e09\u5b66\u751f\u5c0f\u8f89\u56e0\u4e00\u6b21\u6a21\u62df\u8003\u8bd5\u5931\u8d25\uff0c\u5c31\u8ba4\u5b9a\u81ea\u5df1\u8003\u4e0d\u4e0a\u7406\u60f3\u4e2d\u7684\u5927\u5b66\uff0c\u611f\u89c9\u524d\u9014\u65e0\u671b\uff0c\u6839\u636e\u7406\u6027\u60c5\u7eea\u7597\u6cd5\u539f\u7406\uff0c\u5c0f\u8f89\u7684\u8fd9\u79cd\u4e0d\u5408\u7406\u4fe1\u5ff5\u5c5e\u4e8e\nA. \u4e3b\u89c2\u8981\u6c42\nB. \u7247\u9762\u5316\nC. \u76f8\u5bf9\u5316\nD. \u7cdf\u7cd5\u81f3\u6781\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f57\u9a6c\u6cd5\u590d\u5174\u65f6\u671f\u51fa\u73b0\u7684\u4ee5\u7814\u7a76\u548c\u6062\u590d\u7f57\u9a6c\u6cd5\u4e3a\u6838\u5fc3\u7684\u6cd5\u5b66\u6d41\u6d3e\u662f\nA. \u5206\u6790\u5b9e\u8bc1\u4e3b\u4e49\u6cd5\u5b66\u6d3e\nB. \u6ce8\u91ca\u6cd5\u5b66\u6d3e\nC. \u7f57\u9a6c\u6cd5\u5b66\u6d3e\nD. \u793e\u4f1a\u6cd5\u5b66\u6d3e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8170242945795215, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6756117668904983, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6761\u6b3e\u7b26\u5408\u6cd5\u5f8b\u89c4\u5b9a\u7684\u662f\nA. \u6dd8\u5b9d\u7f51\u67d0\u670d\u9970\u5e97\u8868\u793a\uff1a\u672c\u5e97\u5546\u54c1\u4e00\u7ecf\u552e\u51fa\uff0c\u6982\u4e0d\u9000\u8d27\nB. \u67d0\u996d\u5e97\u5e97\u5802\u544a\u77e5\uff1a\u201c\u8bf7\u4fdd\u7ba1\u597d\u968f\u8eab\u7269\u54c1\uff0c\u4e22\u5931\u6982\u4e0d\u8d1f\u8d23\u201d\nC. \u67d0\u5546\u573a\u5395\u6240\u95e8\u53e3\u83b7\u6551\u724c\u201c\u5730\u6ed1\u5c0f\u5fc3\u6454\u5012\uff0c\u5426\u5219\u6982\u4e0d\u8d1f\u8d23\u4efb\u201d\nD. \u67d0\u5e72\u6d17\u5e97\u7533\u660e\uff1a\u8863\u7269\u4e22\u5931\uff0c\u53ea\u8d54\u4ed8\u6d17\u8863\u8d39\u4e8c\u500d\u7684\u4ef7\u94b1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2994509091275925, "meta-math/MetaMath-Mistral-7B": 0.3918190362819597, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8861458078539196, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.511193498646212, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5468430113366054}}, {"question": "\u7537\u6027\uff0c50 \u5c81\u3002\u4f53\u91cd 60kg\uff0c\u884c\u80c3\u764c\u6839\u6cbb\u672f\uff0c\u5176\u672f\u540e\u6bcf\u65e5\u9700\u8981\u6700\u5408\u9002\u7684\u70ed\u91cf\u662f\nA. 1500kcal\nB. 1650kcal\nC. 1950kcal\nD. 1800kcal\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.29107338094791696, "meta-math/MetaMath-Mistral-7B": 0.44670212761218436, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3589842246166939, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.42391840727597474}}, {"question": "\u5bf9\u4e8e\u96cc\u6fc0\u7d20\u7684\u751f\u7406\u4f5c\u7528\uff0c\u54ea\u9879\u662f\u4e0d\u6070\u5f53\u7684\nA. \u6709\u52a9\u4e8e\u5375\u5de2\u79ef\u50a8\u80c6\u56fa\u9187\nB. \u4fc3\u8fdb\u94a0\u4e0e\u6c34\u7684\u6392\u6cc4\nC. \u4fc3\u4f7f\u5b50\u5bab\u53d1\u80b2\u5e76\u4f7f\u5b50\u5bab\u6536\u7f29\u529b\u589e\u5f3a\nD. \u52a0\u5f3a\u8f93\u5375\u7ba1\u8282\u5f8b\u6027\u6536\u7f29\u7684\u632f\u5e45\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5382465022064461, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.44864321801920987, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9803590650549828}}, {"question": "\u5c0f\u674e\u548c\u5c0f\u738b\u662f\u540c\u4e8b\uff0c\u6700\u8fd1\u5fc3\u7406\u4e0a\u5374\u662f\u51b0\u706b\u4e24\u91cd\u5929\u3002\u5c0f\u674e\u56e0\u5706\u6ee1\u5b8c\u6210\u5de5\u4f5c\u5b9a\u989d\u4efb\u52a1\uff0c\u4e0d\u4ec5\u9886\u5230\u4e86\u5168\u989d\u5de5\u8d44\uff0c\u8fd8\u5f97\u5230\u4e86\u76f8\u5f53\u4e8e\u6708\u5de5\u8d4425\uff05\u7684\u5956\u52b1\u3002\u5c0f\u738b\u5219\u56e0\u672a\u5b8c\u6210\u6708\u5de5\u4f5c\u5b9a\u989d\u4efb\u52a1\uff0c\u53ea\u9886\u5230\u4e8680\uff05\u7684\u6708\u5de5\u8d44\u3002\u4ed6\u4eec\u6240\u5728\u4f01\u4e1a\u7684\u5de5\u8d44\u5236\u5ea6\u5373\u6cf0\u52d2\u6240\u5021\u5bfc\u7684\nA. \u8ba1\u4ef6\u5de5\u8d44\u5236\nB. \u5dee\u522b\u8ba1\u4ef6\u5de5\u8d44\u5236\nC. \u5c97\u4f4d\u7ed3\u6784\u5de5\u8d44\u5236\nD. \u7ee9\u6548\u5de5\u8d44\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5808696715925966, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53\u4e00\u65e5\u5185\u6c34\u4f4d\u53d8\u5316\u4e0d\u5927\u65f6\uff0c\u8ba1\u7b97\u65e5\u5e73\u5747\u6c34\u4f4d\u5e94\u91c7\u7528()\nA. \u51e0\u4f55\u5e73\u5747\u6cd5\nB. \u52a0\u6743\u5e73\u5747\nC. \u9762\u79ef\u5305\u56f4\u6cd5\nD. \u7b97\u672f\u5e73\u5747\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5569969472489128, "meta-math/MetaMath-Mistral-7B": 0.7690529212599256, "itpossible/Chinese-Mistral-7B-v0.1": 0.4310339459413379, "HuggingFaceH4/zephyr-7b-beta": 0.9871520031140493, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8120113953023, "meta-llama/Meta-Llama-3-8B": 0.3306562312783846, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6469\u64e6\u529b\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u76f8\u4e92\u7d27\u538b\u7684\u7c97\u7cd9\u7269\u4f53\u4e4b\u95f4\u6709\u76f8\u5bf9\u6ed1\u52a8\u65f6\uff0c\u624d\u53d7\u6ed1\u52a8\u6469\u64e6\u529b\nB. \u76f8\u4e92\u538b\u7d27\u7684\u7c97\u7cd9\u7269\u4f53\u95f4\u4e00\u5b9a\u5b58\u5728\u6469\u64e6\u529b\nC. \u8fd0\u52a8\u7684\u7269\u4f53\u4e00\u5b9a\u53d7\u5230\u6ed1\u52a8\u6469\u64e6\u529b\nD. \u9759\u6b62\u7684\u7269\u4f53\u4e00\u5b9a\u53d7\u5230\u9759\u6469\u64e6\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.41393699828782254, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5463490701810393}}, {"question": "\u201c\u5f81\u201d\u548c\u201c\u5fb5\u201d\u7684\u5173\u7cfb\u662f\nA. \u7e41\u7b80\u5b57\nB. \u53e4\u4eca\u5b57\nC. \u5f02\u4f53\u5b57\nD. \u901a\u5047\u5b57\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2712019384407753, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u516c\u5e73\u4ea4\u6613\u7684\u5b9e\u8d28\u662f\nA. \u4e92\u60e0\u4e92\u5229\nB. \u7b49\u4ef7\u4ea4\u6362\nC. \u81ea\u613f\nD. \u8bda\u5b9e\u65e0\u6b3a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.33868745066245093}}, {"question": "\u201c\u795e\u5b97\u65e2\u7528\u738b\u5b89\u77f3\u4e3a\u53c2\u77e5\u653f\u4e8b\uff0c\u5b89\u77f3\u4e3a\u5e1d\u8a00\u5929\u4e0b\u8d22\u5229\u6240\u5f53\u5f00\u8f9f\u655b\u6563\u8005\uff0c\u5e1d\u7136\u5176\u8bf4\uff0c\u9042\u521b\u7acb\u5236\u7f6e\u4e09\u53f8\u6761\u4f8b\u53f8\u201d\u201c\u4ee5\u901a\u5929\u4e0b\u4e4b\u5229\u201d\u3002\u8fd9\u8bf4\u660e\uff0c\u738b\u5b89\u77f3\u53d8\u6cd5\u521d\u671f\u7684\u6539\u9769\u91cd\u70b9\u5728\u4e8e\nA. \u52a0\u5f3a\u7687\u6743\nB. \u52a0\u5f3a\u519b\u4e8b\u5b9e\u529b\nC. \u5206\u6563\u5bb0\u76f8\u6743\u529b\nD. \u589e\u52a0\u8d22\u653f\u6536\u5165\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7380824738634926, "meta-math/MetaMath-Mistral-7B": 0.8579023321378149, "itpossible/Chinese-Mistral-7B-v0.1": 0.9408691533981337, "HuggingFaceH4/zephyr-7b-beta": 0.9255430525144055, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8989241713633987, "meta-llama/Meta-Llama-3-8B": 0.8447587719490628, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.948909666741824}}, {"question": "\u9732\u70b9\u8868\u793a\u7a7a\u6c14\u7684[ ]\u3002\nA. \u6e7f\u5ea6\nB. \u900f\u660e\u5ea6\nC. \u6e29\u5ea6\nD. \u5bc6\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7068155793798324, "meta-math/MetaMath-Mistral-7B": 0.9770179781997059, "itpossible/Chinese-Mistral-7B-v0.1": 0.695160718919085, "HuggingFaceH4/zephyr-7b-beta": 0.5473686851336849, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7963622454176149, "meta-llama/Meta-Llama-3-8B": 0.9352197957466506, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9790986923030626}}, {"question": "\u8ba1\u7b97\u673a\u8fd0\u7b97\u901f\u5ea6\u7684\u5355\u4f4d\u662f\nA. MHZ\nB. MB\nC. MIPS\nD. MTBF\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8785411287548028, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u300a\u5efa\u8bbe\u5de5\u7a0b\u6587\u4ef6\u5f52\u6863\u89c4\u8303\u300b\uff0c\u5173\u4e8e\u65bd\u5de5\u6587\u4ef6\u7acb\u5377\u7684\u8bf4\u6cd5\uff0c\u6b63\u786e\u7684\u662f\nA. \u6587\u5b57\u6750\u6599\u6309\u4e8b\u9879\u3001\u4e13\u4e1a\u987a\u5e8f\u6392\u5217\nB. \u58f0\u50cf\u8d44\u6599\u5e94\u4e0e\u7eb8\u8d28\u6587\u4ef6\u5728\u6848\u5377\u8bbe\u7f6e\u4e0a\u4e00\u81f4\nC. \u5377\u5185\u65e2\u6709\u6587\u5b57\u6750\u6599\u53c8\u6709\u56fe\u7eb8\u8d44\u6599\u65f6\uff0c\u56fe\u7eb8\u6392\u5217\u5728\u524d\nD. \u4e13\u4e1a\u5206\u5305\u7684\u5206\u90e8\u5de5\u7a0b\uff0c\u5e94\u5e76\u5165\u76f8\u5e94\u5355\u4f4d\u5de5\u7a0b\u7acb\u5377\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4209626777184045, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53e4\u4ee3\u7684\u5f88\u591a\u6210\u8bed\u3001\u8c1a\u8bed\u90fd\u8574\u542b\u7740\u5f88\u591a\u79d1\u5b66\u77e5\u8bc6\uff0c\u4e0b\u5217\u5bf9\u6210\u8bed\u3001\u8c1a\u8bed\u7684\u89e3\u91ca\u6b63\u786e\u7684\u662f\nA. \u201c\u7389\u4e0d\u7422\u4e0d\u6210\u5668\u201d\uff0c\u201c\u767e\u70bc\u65b9\u80fd\u6210\u94a2\u201d\u53d1\u751f\u7684\u5747\u4e3a\u5316\u5b66\u53d8\u5316\nB. \u201c\u706b\u6811\u94f6\u82b1\u201d\u4e2d\u7684\u7130\u706b\u5b9e\u8d28\u4e0a\u662f\u91d1\u5c5e\u5143\u7d20\u7684\u7130\u8272\u53cd\u5e94\nC. \u201c\u7518\u4e4b\u5982\u9974\u201d\u8bf4\u660e\u7cd6\u7c7b\u5747\u6709\u751c\u5473\nD. \u201c\u7206\u7af9\u58f0\u4e2d\u9664\u65e7\u5c81\uff0c\u6625\u98ce\u9001\u6696\u5165\u5c60\u82cf\u201d\u3002\u7206\u7af9\u7206\u70b8\u53d1\u751f\u7684\u662f\u5206\u89e3\u53cd\u5e94\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3148300531811561, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e3a\u4e86\u63a2\u8ba8\u67d0\u75c5\u5728\u4e0d\u540c\u56fd\u5bb6\u4eba\u7fa4\u4e2d\u7684\u53d1\u75c5\u5dee\u5f02\u662f\u73af\u5883\u56e0\u7d20\u6216\u9057\u4f20\u56e0\u7d20\u6240\u81f4\u5e38\u5e94\u7528\nA. \u9057\u4f20\u6d41\u884c\u75c5\u5b66\nB. \u63cf\u8ff0\u6d41\u884c\u75c5\u5b66\nC. \u79fb\u6c11\u6d41\u884c\u75c5\u5b66\nD. \u8840\u6e05\u6d41\u884c\u75c5\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.45611255094160863, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.49720789817168065, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8435524170412749, "meta-llama/Meta-Llama-3-8B": 0.545170816528483, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6309\u7167\u300a\u7a81\u53d1\u4e8b\u4ef6\u5e94\u5bf9\u6cd5\u300b\u7684\u89c4\u5b9a\uff0c\u7a81\u53d1\u4e8b\u4ef6\u53ef\u5206\u4e3a()\u548c\u793e\u4f1a\u5b89\u5168\u4e8b\u4ef6\nA. \u81ea\u7136\u707e\u5bb3\u3001\u4e8b\u6545\u707e\u96be\u3001\u516c\u5171\u536b\u751f\u4e8b\u4ef6\nB. \u81ea\u7136\u707e\u5bb3\u3001\u5b89\u5168\u4e8b\u6545\u3001\u536b\u751f\u4e8b\u6545\nC. \u81ea\u7136\u707e\u5bb3\u3001\u4eba\u4e3a\u4e8b\u6545\nD. \u5730\u9707\u707e\u96be\u3001\u4eba\u4e3a\u706b\u707e\u3001\u4ea4\u901a\u4e8b\u6545\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8645940742059304, "meta-math/MetaMath-Mistral-7B": 0.806138493255571, "itpossible/Chinese-Mistral-7B-v0.1": 0.8207135285833415, "HuggingFaceH4/zephyr-7b-beta": 0.9997834162153928, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8935948508016982, "meta-llama/Meta-Llama-3-8B": 0.810205269342551, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9587581754924227}}, {"question": "\u5728\u786e\u7acb\u7406\u60f3\u548c\u8ffd\u6c42\u7406\u60f3\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5f53\u53d1\u73b0\u7406\u60f3\u548c\u73b0\u5b9e\u7684\u77db\u76fe\u65f6\uff0c\u5e94\u8be5\u91c7\u53d6\u7684\u6b63\u786e\u6001\u5ea6\u662f\nA. \u4e0d\u52a0\u5206\u6790\u5730\u5168\u76d8\u8ba4\u540c\u5f53\u4e0b\u7684\u73b0\u5b9e\nB. \u5bf9\u73b0\u5b9e\u5927\u5931\u6240\u671b\uff0c\u9003\u907f\u6216\u53cd\u5bf9\u73b0\u5b9e\nC. \u5bf9\u7406\u60f3\u5931\u53bb\u4fe1\u5fc3\uff0c\u201c\u544a\u522b\u7406\u60f3\u201d\nD. \u575a\u5b9a\u4fe1\u5ff5\uff0c\u5728\u5b9e\u8df5\u4e2d\u628a\u7406\u60f3\u53d8\u4e3a\u73b0\u5b9e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9821135256464079, "meta-math/MetaMath-Mistral-7B": 0.9992710171789065, "itpossible/Chinese-Mistral-7B-v0.1": 0.9820244139028231, "HuggingFaceH4/zephyr-7b-beta": 0.9994793850054431, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9979421107511838, "meta-llama/Meta-Llama-3-8B": 0.9839178337682565, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9914735465247956}}, {"question": "\u5168\u666f\u5f0f\u53cd\u6620\u4e86\u6297\u6218\u80dc\u5229\u524d\u540e\u5149\u660e\u4e0e\u9ed1\u6697\uff0c\u6b63\u4e49\u4e0e\u90aa\u6076\u640f\u6597\u7684\u8270\u82e6\u5c81\u6708\u7684\u5267\u4f5c\u662f\u7530\u6c49\u7684\nA. \u300a\u6b66\u677e\u300b\nB. \u300a\u4e3d\u4eba\u884c\u300b\nC. \u300a\u98ce\u4e91\u513f\u5973\u300b\nD. \u300a\u4e00\u81f4\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5782\u76f4\u7ebf\u6bb5\u4e0e\u6c34\u5e73\u7ebf\u6bb5\u7b49\u957f\uff0c\u4f46\u770b\u8d77\u6765\u5782\u76f4\u7ebf\u6bb5\u6bd4\u6c34\u5e73\u7ebf\u6bb5\u957f\uff0c\u8fd9\u662f\u9519\u89c9\u4e2d\u7684\nA. \u83f2\u514b\u9519\u89c9\nB. \u5e9e\u4f50\u9519\u89c9\nC. \u5384\u4efb\u65af\u5766\u9519\u89c9\nD. \u9ed1\u7075\u9519\u89c9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.35002889446029034, "HuggingFaceH4/zephyr-7b-beta": 0.5942906974848866, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4648272279902475, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6309\u7167\u5bc6\u7801\u7cfb\u7edf\u5bf9\u660e\u6587\u7684\u5904\u7406\u65b9\u6cd5\uff0c\u5bc6\u7801\u7cfb\u7edf\u53ef\u4ee5\u5206\u4e3a\nA. \u5bf9\u79f0\u5bc6\u7801\u7cfb\u7edf\u548c\u975e\u5bf9\u79f0\u5bc6\u7801\u7cfb\u7edf\nB. \u6570\u636e\u52a0\u5bc6\u7cfb\u7edf\u548c\u6570\u5b57\u7b7e\u540d\u7cfb\u7edf\nC. \u5206\u7ec4\u5bc6\u7801\u7cfb\u7edf\u548c\u5e8f\u5217\u5bc6\u7801\u7cfb\u7edf\nD. \u5bf9\u79f0\u5bc6\u7801\u7cfb\u7edf\u548c\u516c\u94a5\u5bc6\u7801\u7cfb\u7edf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u9738\u738b\u522b\u59ec\u300b\u662f\u4eac\u5267\u4e2d\u7684\u540d\u6bb5\uff0c\u201c\u865e\u59ec\u201d\u8fd9\u4e00\u89d2\u8272\u5728\u4eac\u5267\u4e2d\u5c5e\u4e8e\nA. \u9752\u8863\nB. \u6b66\u751f\nC. \u6587\u4e11\nD. \u82b1\u65e6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c25\u5c81\uff0c1\u5468\u6765\u65e0\u660e\u663e\u8bf1\u56e0\u53d1\u70ed\u3001\u53cc\u819d\u5173\u8282\u75bc\u75db\u4f34\u76ae\u80a4\u51fa\u8840\u70b9\uff0c\u81ea\u6d4b\u4f53\u6e29\u6700\u9ad838.8\u2103\uff0c\u65e0\u5bd2\u6218\u3002\u65e2\u5f80\u4f53\u5065\u3002\u67e5\u4f53\uff1aT38.1\u2103\uff0c\u56db\u80a2\u76ae\u80a4\u53ef\u89c1\u51fa\u8840\u70b9\uff0c\u53e3\u8154\u9ecf\u819c\u89c1\u4e24\u5904\u6e83\u75a1\u3002\u5fc3\u3001\u80ba\u3001\u8179\u65e0\u660e\u663e\u5f02\u5e38\u3002\u5316\u9a8c\u8840\uff1aHb102g/L\uff0cWBC5.2\u00d7109/L\uff0cPlt24\u00d7109/L\uff0c\u7f51\u7ec7\u7ea2\u7ec6\u80de4.9%\uff0c\u5c3f\u86cb\u767d(++)\u3002\u8be5\u60a3\u8005\u6700\u53ef\u80fd\u7684\u8bca\u65ad\u4e3a\nA. \u7cfb\u7edf\u6027\u786c\u5316\u75c7\nB. \u7cfb\u7edf\u6027\u7ea2\u6591\u72fc\u75ae\nC. \u8d1d\u8d6b\u5207\u7279\u75c5\nD. \u7c7b\u98ce\u6e7f\u6027\u5173\u8282\u708e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30799675544530336, "meta-math/MetaMath-Mistral-7B": 0.37578680063918496, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.49309197311599057, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u54ea\u4e2a\u4e0d\u662f\u4e2d\u4e16\u7eaa\u5b97\u6559\u88c1\u5224\u6240\u8feb\u5bb3\u7684\u6240\u8c13\u201c\u5f02\u7aef\u201d\nA. \u4fe1\u5949\u4ed6\u6559\u4e4b\u4eba\nB. \u201c\u5deb\u58eb\u201d\u548c\u72b9\u592a\u4eba\nC. \u5177\u6709\u65b0\u601d\u60f3\u7684\u79d1\u5b66\u5148\u9a71\nD. \u6301\u4e0d\u540c\u5b97\u6559\u89c1\u89e3\u8005\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0d\u5c5e\u4e8e\u9ab6\u4e1b\u7684\u795e\u7ecf\u662f\nA. \u80a1\u795e\u7ecf\nB. \u81c0\u4e0b\u795e\u7ecf\nC. \u81c0\u4e0a\u795e\u7ecf\nD. \u5750\u9aa8\u795e\u7ecf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4228468970115174, "itpossible/Chinese-Mistral-7B-v0.1": 0.37354501646787847, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.25756620677129083, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5655585313699831}}, {"question": "\u7c7b\u6bd4\u662f\u4e00\u79cd\u6709\u6548\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f52\u7c7b\u548c\u6bd4\u8f83\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u548c\u638c\u63e1\u65b0\u6982\u5ff5\u3001\u65b0\u77e5\u8bc6\uff0e\u4e0b\u5217\u7c7b\u6bd4\u4e0d\u6b63\u786e\u7684\u662f\nA. \u7535\u78c1\u6ce2\u53ef\u4ee5\u4e0e\u673a\u68b0\u6ce2\u7c7b\u6bd4\uff0c\u90fd\u53ef\u4ee5\u53d1\u751f\u5e72\u6d89\u3001\u884d\u5c04\u73b0\u8c61\nB. \u70b9\u7535\u8377\u53ef\u4ee5\u4e0e\u8d28\u70b9\u7c7b\u6bd4\uff0c\u90fd\u662f\u7406\u60f3\u5316\u6a21\u578b\nC. \u7535\u573a\u529b\u505a\u529f\u53ef\u4ee5\u4e0e\u91cd\u529b\u505a\u529f\u7c7b\u6bd4\uff0c\u4e24\u79cd\u529b\u505a\u529f\u90fd\u4e0e\u8def\u5f84\u65e0\u5173\nD. \u7535\u573a\u7ebf\u53ef\u4ee5\u4e0e\u78c1\u611f\u7ebf\u7c7b\u6bd4\uff0c\u90fd\u662f\u7528\u5047\u60f3\u7684\u66f2\u7ebf\u5f62\u8c61\u5316\u5730\u63cf\u7ed8\u201c\u573a\u201d\u7684\u5ba2\u89c2\u5b58\u5728\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. I\u7684\u539f\u5b50\u534a\u5f84\u5927\u4e8eBr\uff0cHI\u6bd4HBr\u7684\u70ed\u7a33\u5b9a\u6027\u5f3a\nB. P\u7684\u975e\u91d1\u5c5e\u6027\u5f3a\u4e8eSi\uff0cH3PO4\u6bd4H2SiO3\u7684\u9178\u6027\u5f3a\nC. Al2O3\u548cMgO\u5747\u53ef\u4e0eNaOH\u6eb6\u6db2\u53cd\u5e94\nD. SO2\u548cSO3\u6df7\u5408\u6c14\u4f53\u901a\u5165Ba(NO3)2\u6eb6\u6db2\u53ef\u5f97\u5230BaSO3\u548cBaSO4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2821833983601388, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": ".\u6839\u636e\u6587\u4ef6\u540d\u5224\u65ad\u4ee5\u4e0b\u54ea\u4e9b\u6587\u4ef6\u4e0d\u662f\u56fe\u7247\u6587\u4ef6\nA. \u56fe\u7247.bmp\nB. \u56fe\u7247.gif\nC. \u56fe\u7247.jpg\nD. \u56fe\u7247.doc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9392524285083874, "meta-math/MetaMath-Mistral-7B": 0.9882652256945628, "itpossible/Chinese-Mistral-7B-v0.1": 0.8035894785286571, "HuggingFaceH4/zephyr-7b-beta": 0.9999600402083023, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9858169332728466, "meta-llama/Meta-Llama-3-8B": 0.9751511655326555, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9962979185214045}}, {"question": "\u5df2\u77e5\u67d0\u968f\u673a\u53d8\u91cf$X$\u7684\u751f\u5b58\u51fd\u6570\u4e3a$S(x)=-\\frac{1}{60^3} x^3+1$\uff0c\u4e14$0 \\leqslant x \\leqslant 60$\uff0c\u5e76\u6709$E(X)=45$\uff0c\u5219${ }_3 m_4=( )$\u3002\nA. 0.000054\nB. 0.000027\nC. 0.000026\nD. 0.00043\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u5c5e\u4e8e\u5468\u5ef6\u8bcd\u9879\u7684\u662f\nA. SAP\u8c13\u9879\nB. SAP\u4e3b\u9879\nC. SIP\u4e3b\u9879\nD. SOP\u4e3b\u9879\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.35686329776860143, "itpossible/Chinese-Mistral-7B-v0.1": 0.27416108226793107, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2925894411256113, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6cd5\u56fd\u653f\u6cbb\u601d\u60f3\u5bb6\u6258\u514b\u7ef4\u5c14\u5728\u300a\u8bba\u7f8e\u56fd\u7684\u6c11\u4e3b\u300b\u4e2d\u8bf4\uff1a\u201c\u7f8e\u56fd\u7684\u8054\u90a6\u5baa\u6cd5\uff0c\u597d\u50cf\u80fd\u5de5\u5de7\u5320\u521b\u9020\u7684\u4e00\u4ef6\u53ea\u80fd\u4f7f\u53d1\u660e\u4eba\u6210\u540d\u53d1\u8d22\uff0c\u800c\u843d\u5230\u4ed6\u4eba\u4e4b\u624b\u5c31\u53d8\u6210\u4e00\u65e0\u7528\u5904\u7684\u7f8e\u4e3d\u827a\u672f\u54c1\u3002\u201d\u8fd9\u53e5\u8bdd\u7740\u91cd\u5f3a\u8c03\u7f8e\u56fd\u8054\u90a6\u5baa\u6cd5\nA. \u5177\u6709\u501f\u9274\u610f\u4e49\nB. \u521b\u9020\u4e86\u65b0\u7684\u5baa\u6cd5\u4f53\u5236\nC. \u4fc3\u8fdb\u4e86\u8d44\u672c\u4e3b\u4e49\u53d1\u5c55\nD. \u4e0d\u5177\u6709\u666e\u9002\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3534719558397804, "meta-math/MetaMath-Mistral-7B": 0.7503974308082844, "itpossible/Chinese-Mistral-7B-v0.1": 0.5340639057355574, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4125156818135334, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0d\u5c5e\u4e8e\u9888\u5916\u52a8\u8109\u7684\u5206\u652f\u662f\nA. \u9762\u52a8\u8109\nB. \u8111\u819c\u4e2d\u52a8\u8109\nC. \u820c\u52a8\u8109\nD. \u989e\u6d45\u52a8\u8109\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.31283638571410965, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9ad8\u5206\u5316\u9cde\u764c\u4e2d\u6700\u5178\u578b\u7684\u7279\u5f81\u662f\nA. \u7ec6\u80de\u5206\u88c2\u8c61\u5c11\nB. \u51fa\u73b0\u89d2\u5316\u73e0\nC. \u5b9e\u8d28\u4e0e\u95f4\u8d28\u5206\u754c\u6e05\u695a\nD. \u5f62\u6210\u764c\u5de2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30856495466412287, "meta-math/MetaMath-Mistral-7B": 0.3986893610065384, "itpossible/Chinese-Mistral-7B-v0.1": 0.45169543862159645, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3875481340777786, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f71\u54cd\u6559\u80b2\u4e8b\u4e1a\u7684\u53d1\u5c55\u89c4\u6a21\u548c\u901f\u5ea6\u7684\u4e3b\u8981\u56e0\u7d20\u662f\u793e\u4f1a\nA. \u751f\u4ea7\u529b\nB. \u7ecf\u6d4e\u5236\u5ea6\nC. \u610f\u8bc6\u5f62\u6001\nD. \u653f\u6cbb\u5236\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6633024707452448, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2010\u5e749\u670826\u65e5\u81f328\u65e5\uff0c\u4fc4\u603b\u7edf\u6885\u5fb7\u97e6\u6770\u592b\u5bf9\u4e2d\u56fd\u8fdb\u884c\u6b63\u5f0f\u8bbf\u95ee\u3002\u53cc\u65b9\u5c31\u56fd\u9645\u5f62\u52bf\u548c\u53cc\u8fb9\u5173\u7cfb\u8fbe\u6210\u4e86\u8bb8\u591a\u5171\u8bc6\uff0c\u5176\u4e2d\u5305\u62ec\u53cc\u65b9\u52a0\u5f3a\u8d38\u6613\u5408\u4f5c\u3002\u4e2d\u4fc4\u52a0\u5f3a\u8d38\u6613\u5408\u4f5c\u6709\u5229\u4e8e\u53cc\u65b9a\u5171\u540c\u5e94\u5bf9\u91d1\u878d\u5371\u673a\u7684\u51b2\u51fb b\u4e92\u901a\u6709\u65e0\uff0c\u5b9e\u73b0\u8d44\u6e90\u4f18\u5316\u914d\u7f6ec\u53d1\u6325\u6bd4\u8f83\u4f18\u52bf\uff0c\u63d0\u9ad8\u7ecf\u6d4e\u6548\u76ca d\u5f7b\u5e95\u6d88\u9664\u8d38\u6613\u6469\u64e6\uff0c\u4fc3\u8fdb\u8d38\u6613\u5e73\u8861\nA. acd\nB. bcd\nC. abd\nD. abc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30889363229415834, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6024171000519769, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47203052758493513, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u5de5\u667a\u80fd\u7684\u6570\u636e\u5784\u65ad\u4e3a\u53cd\u5784\u65ad\u7406\u8bba\u63d0\u51fa\u4e86\u65b0\u7684\u5b9e\u8df5\u95ee\u9898\uff0c\u6362\u53e5\u8bdd\u8bf4\uff0c\u53cd\u5784\u65ad\u6cd5\u7684\u7406\u8bba\u521b\u65b0\u662f\u4e00\u79cd\u73b0\u5b9e\u7684\u5b9e\u8df5\u9700\u6c42\u3002\u5177\u4f53\u800c\u8a00\uff0c\u53cd\u5784\u65ad\u6cd5\u7684\u7406\u8bba\u521b\u65b0\u9700\u6c42\u4e3b\u8981\u6709\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a\u7b2c\u4e00\uff0c\u53cc\u8fb9\u5e02\u573a\u7406\u8bba\u3002\u4e0e\u5355\u8fb9\u5e02\u573a\u76f8\u5bf9\u5e94\uff0c\u7ecf\u6d4e\u5b66\u5bb6\u6240\u63d0\u51fa\u7684\u53cc\u8fb9\u5e02\u573a\u7406\u8bba\uff0c\u80fd\u591f\u4e3a\u591a\u8fb9\u6027\u6570\u636e\u5e02\u573a\u7684\u53cd\u5784\u65ad\u6267\u6cd5\u63d0\u4f9b\u6709\u76ca\u601d\u8def\u3002\u5f53\u524d\u7684\u53cc\u8fb9\u5e02\u573a\u7406\u8bba\uff0c\u4e00\u65b9\u9762\u9700\u8981\u5173\u6ce8\u5b9e\u8df5\u52a8\u5411\u8fdb\u4e00\u6b65\u5b8c\u5584\u7406\u8bba\u81ea\u8eab\uff0c\u53e6\u4e00\u65b9\u9762\u9700\u8981\u63a2\u7d22\u53cc\u8fb9\u5e02\u573a\u7406\u8bba\u5982\u4f55\u8f6c\u5316\u4e3a\u53ef\u4f9b\u64cd\u4f5c\u9002\u7528\u7684\u53cd\u5784\u65ad\u6cd5\u5f8b\u6cd5\u89c4\u3002\u7b2c\u4e8c\uff0c\u76f8\u5173\u5e02\u573a\u7406\u8bba\u3002\u4eba\u5de5\u667a\u80fd\u7684\u6570\u636e\u53cd\u5784\u65ad\uff0c\u9700\u8981\u63a2\u8ba8\u5355\u72ec\u754c\u5b9a\u201c\u6570\u636e\u5e02\u573a\u201d\u7684\u5fc5\u8981\u6027\u548c\u53ef\u884c\u6027\uff0c\u8fd8\u9700\u8981\u6b63\u89c6\u4f20\u7edf\u76f8\u5173\u5e02\u573a\u754c\u5b9a\u5de5\u5177\u5728\u6570\u636e\u5e02\u573a\u7684\u65e0\u80fd\u4e3a\u529b\uff0c\u8fdb\u800c\u79ef\u6781\u63a2\u7d22\u53ef\u66ff\u4ee3\u6027\u7684\u5206\u6790\u5de5\u5177\u3002\u63a5\u4e0b\u6765\u6700\u53ef\u80fd\u8bb2\u8ff0\u7684\u662f\u6570\u636e\u5784\u65ad\u7684\nA. \u7acb\u6cd5\u95ee\u9898\nB. \u6cd5\u5f8b\u6e0a\u6e90\nC. \u6cd5\u5f8b\u754c\u5b9a\nD. \u6267\u6cd5\u95ee\u9898\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.39923914424370804}}, {"question": "\u6839\u636e\u300a\u6c11\u6cd5\u603b\u5219\u300b\u89c4\u5b9a\uff0c\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u5c5e\u4e8e\u7279\u522b\u6cd5\u4eba\u7684\u662f\nA. \u793e\u4f1a\u670d\u52a1\u673a\u6784\nB. \u519c\u6751\u96c6\u4f53\u7ecf\u6d4e\u7ec4\u7ec7\nC. \u57fa\u91d1\u4f1a\nD. \u80a1\u4efd\u6709\u9650\u516c\u53f8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6d41\u884c\u75c5\u5b66\u7684\u63cf\u8ff0\u6027\u7814\u7a76\u4e0d\u5305\u62ec\nA. \u75c5\u4f8b\u5bf9\u7167\u7814\u7a76\nB. \u73b0\u51b5\u7814\u7a76\nC. \u666e\u67e5\nD. \u62bd\u6837\u8c03\u67e5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.29972404264597124, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.37457369550229136, "meta-llama/Meta-Llama-3-8B": 0.655467996133395, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5561950429616114}}, {"question": "\u201c\u89c2\u4e4e\u5929\u6587\uff0c\u89c2\u4e4e\u4eba\u6587\u201d\u51fa\u81ea\nA. \u300a\u8bba\u8bed\u300b\nB. \u300a\u5c1a\u4e66\u300b\nC. \u300a\u9053\u5fb7\u7ecf\u300b\nD. \u300a\u5468\u6613\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e13\u95e8\u6027\u516c\u5171\u5173\u7cfb\u6d3b\u52a8\u662f\u6307\u5728\u4e00\u5b9a\uff08\uff09\u5185\uff0c\u6709\u8ba1\u5212\u3001\u6709\u7cfb\u7edf\u5730\u8fd0\u7528\u516c\u5171\u5173\u7cfb\u6280\u672f\u548c\u65b9\u6cd5\u6765\u8fbe\u5230\u67d0\u79cd\u516c\u5171\u5173\u7cfb\u76ee\u7684\u7684\u4e13\u95e8\u6027\u6d3b\u52a8\u3002\nA. \u65f6\u65e5\nB. \u65f6\u671f\nC. \u65f6\u5019\nD. \u65f6\u95f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.47917745777789683, "itpossible/Chinese-Mistral-7B-v0.1": 0.6941733490316472, "HuggingFaceH4/zephyr-7b-beta": 0.6272272737320346, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8442374930302705}}, {"question": "\u98df\u54c1\u5728\u51b7\u5374\u8fc7\u7a0b\u4e2d\u7684\u70ed\u4ea4\u6362\u91cf\u4e3a\nA. Q=GWwr\nB. A+B+C\nC. Q=GC0(T\u521d-T\u7ec8)\nD. Q=G\u03b2F(P\u7269-P\u7a7a)\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5721299144622339, "meta-math/MetaMath-Mistral-7B": 0.8159110831898411, "itpossible/Chinese-Mistral-7B-v0.1": 0.4932869192130161, "HuggingFaceH4/zephyr-7b-beta": 0.9853046477683395, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8035269951259373, "meta-llama/Meta-Llama-3-8B": 0.48473038317081263, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8375637767614855}}, {"question": "\u901a\u8fc7\u6536\u96c6\u73b0\u5b58\u7684\u4ee5\u6587\u5b57\u3001\u6570\u5b57\u3001\u7b26\u53f7\u3001\u753b\u9762\u7b49\u4fe1\u606f\u5f62\u5f0f\u51fa\u73b0\u7684\u6587\u732e\u8d44\u6599\uff0c\u5206\u6790\u548c\u63a2\u8ba8\u5404\u79cd\u4e2a\u4eba\u4e0e\u793e\u4f1a\u7684\u5173\u7cfb\u53ca\u793e\u4f1a\u73b0\u8c61\u7684\u4e00\u79cd\u7814\u7a76\u65b9\u6cd5\uff0c\u88ab\u79f0\u505a\nA. \u95ee\u5377\u8c03\u67e5\nB. \u5b9e\u5730\u7814\u7a76\nC. \u793e\u4f1a\u5b9e\u9a8c\nD. \u975e\u4ecb\u5165\u6027\u7814\u7a76\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.532430401784156, "meta-math/MetaMath-Mistral-7B": 0.4678876638620665, "itpossible/Chinese-Mistral-7B-v0.1": 0.6555480920588554, "HuggingFaceH4/zephyr-7b-beta": 0.9659824472792955, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4991427931318576, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7412062773621508}}, {"question": "\u201c\u5b83\u7ad9\u8d77\u6765\u53cd\u5bf918\u4e16\u7eaa\u7684\u53e4\u5178\u4e3b\u4e49\u3001\u542f\u8499\u601d\u60f3\u3001\u7406\u6027\u601d\u60f3\u4ee5\u53ca\u8fd9\u4e9b\u9886\u57df\u5185\u7684\u79e9\u5e8f\u201d\uff0c\u5f3a\u8c03\u201c\u5bf9\u81ea\u7136\u4e4b\u7231\u3001\u4e0a\u5e1d\u7cbe\u795e\u5b58\u5728\u548c\u611f\u60c5\u529b\u91cf\u4e4b\u95f4\u7684\u8054\u7cfb\u201d\u3002\u6700\u7b26\u5408\u4e0a\u8ff0\u521b\u4f5c\u503e\u5411\u7684\u4f5c\u54c1\u662f\nA. \u300a\u6bcd\u4eb2\u300b\nB. \u300a\u5df4\u9ece\u5723\u6bcd\u9662\u300b\nC. \u300a\u4eba\u95f4\u559c\u5267\u300b\nD. \u300a\u8001\u4eba\u4e0e\u6d77\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u72af\u7f6a\u6cd5\u5b9a\u6700\u9ad8\u5211\u4e3a5\u5e74\u6709\u671f\u5f92\u5211\u7684\uff0c\u5176\u8ffd\u8bc9\u671f\u9650\u4e3a\nA. 5\u5e74\nB. 15\u5e74\nC. 10\u5e74\nD. 20\u5e74\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4483229614064345, "meta-math/MetaMath-Mistral-7B": 0.5461997764756944, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7985574888080197, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3376787962540697, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f1a\u8f66\u4e2d\u9047\u5230\u5bf9\u65b9\u6765\u8f66\u884c\u8fdb\u6709\u56f0\u96be\u9700\u501f\u9053\u65f6\uff0c\u5e94\u600e\u6837\u505a\nA. \u4e0d\u4fb5\u5360\u5bf9\u65b9\u9053\u8def\uff0c\u6b63\u5e38\u884c\u9a76\nB. \u5c3d\u91cf\u793c\u8ba9\u5bf9\u65b9\u5148\u884c\nC. \u793a\u610f\u5bf9\u65b9\u505c\u8f66\u8ba9\u884c\nD. \u9760\u53f3\u4fa7\u52a0\u901f\u884c\u9a76\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6733762970471698, "meta-math/MetaMath-Mistral-7B": 0.9456101602818064, "itpossible/Chinese-Mistral-7B-v0.1": 0.8449126765142433, "HuggingFaceH4/zephyr-7b-beta": 0.998797972635395, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9639452411855609, "meta-llama/Meta-Llama-3-8B": 0.6061547623194425, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8426747757683843}}, {"question": "\u4e0b\u5217\u8bf4\u6cd5\u9519\u8bef\u7684\u662f\nA. \u201c\u753b\u56fe\u201d\u8f6f\u4ef6\u53ef\u4ee5\u6253\u5f00BMP\u4f4d\u56fe\u56fe\u50cf\u6587\u4ef6\uff0c\u8fd8\u53ef\u4ee5\u6253\u5f00JPG\u3001GIF\u7b49\u56fe\u50cf\u6587\u4ef6\nB. Winodws\u9644\u4ef6\u4e2d\u7684Midia Player(\u5a92\u4f53\u64ad\u653e\u5668)\u53ef\u4ee5\u64ad\u653eWAV\u7b49\u97f3\u4e50\u6587\u4ef6\uff0c\u4e5f\u53ef\u4ee5\u64ad\u653eAVI\u7b49\u89c6\u9891\u6587\u4ef6\nC. Word\u8f6f\u4ef6\u53ef\u4ee5\u627e\u5f00DOC\u6587\u4ef6\uff0c\u4e5f\u53ef\u4ee5\u6253\u5f00TXT\u6587\u4ef6\nD. PowerPoint\u7528\u4e8e\u6253\u5f00MP3\u7b49\u97f3\u4e50\u6587\u4ef6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9044122830194845, "meta-math/MetaMath-Mistral-7B": 0.9759640699150781, "itpossible/Chinese-Mistral-7B-v0.1": 0.7785442855479133, "HuggingFaceH4/zephyr-7b-beta": 0.9996684254876226, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9926504900685108, "meta-llama/Meta-Llama-3-8B": 0.9503269900675408, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8951721442925187}}, {"question": "\u6709\u4e9b\u6d77\u8680\u6d1e\u7684\u201c\u5929\u82b1\u677f\u201d\u4e0a\u8fd8\u6709\u4e2a\u5927\u7a9f\u7abf\uff0c\u9633\u5149\u7167\u8fdb\u6765\uff0c\u5c31\u50cf\u7f57\u9a6c\u7684\u4e07\u795e\u6bbf\u4e00\u6837\u3002\u8fd9\u53ef\u80fd\u662f\u56e0\u4e3a\u9876\u90e8\u5ca9\u77f3\u672c\u8eab\u8d28\u5730\u8106\u5f31\uff0c\u66f4\u6613\u88ab__\u3002\u4e5f\u53ef\u80fd\u662f\u6d77\u6c34\u6324\u5165\u5ca9\u77f3\u88c2\u9699\u65f6\uff0c\u539f\u672c\u5728\u91cc\u9762\u7684\u7a7a\u6c14\u88ab\u538b\u7f29\uff0c\u6c34\u586b\u6ee1\u6d1e\u9699\u4e0b\u65b9\uff0c\u538b\u7f29\u7a7a\u6c14\u4fbf__\u6d1e\u9876\uff0c\u76f4\u81f3\u5c06\u6d1e\u9876\u51fb\u7a7f\uff0c\u5f62\u6210\u5929\u7a97\u3002\u4f9d\u6b21\u586b\u5165\u5212\u6a2a\u7ebf\u90e8\u5206\u6700\u6070\u5f53\u7684\u4e00\u9879\u662f\nA. \u4fb5\u8680\u51b2\u51fb\nB. \u8150\u8680\u5165\u4fb5\nC. \u8150\u5316\u5145\u76c8\nD. \u4fb5\u88ad\u653b\u51fb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5389884490697788}}, {"question": "\u534a\u8eab\u4e0d\u9042\uff0c\u53e3\u773c\u6b6a\u659c\uff0c\u820c\u5f3a\u8bed\u8e47\uff0c\u504f\u8eab\u9ebb\u6728\uff0c\u5934\u6655\u76ee\u7729\uff0c\u820c\u8d28\u6697\u6de1\uff0c\u820c\u82d4\u767d\u817b\uff0c\u8109\u5f26\u6ed1\u3002\u8bc1\u5c5e\nA. \u75f0\u70ed\u8151\u5b9e\uff0c\u98ce\u75f0\u4e0a\u6270\nB. \u809d\u9633\u66b4\u4ea2\uff0c\u98ce\u706b\u4e0a\u6270\nC. \u5143\u795e\u8d25\u8131\uff0c\u795e\u660e\u6563\u4e71\nD. \u98ce\u75f0\u7600\u8840\uff0c\u75f9\u963b\u8109\u7edc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2865061228366489, "meta-math/MetaMath-Mistral-7B": 0.31712010892822357, "itpossible/Chinese-Mistral-7B-v0.1": 0.3500288944602904, "HuggingFaceH4/zephyr-7b-beta": 0.8972155096062374, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4295625177800357, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c06\u771f\u7406\u544a\u77e5\u5927\u4f17\u5728\u4f5b\u8bed\u4e2d\u79f0\u4f5c\nA. \u5fc3\u65bd\nB. \u884c\u65bd\nC. \u8d22\u65bd\nD. \u6cd5\u65bd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6830368242103134, "HuggingFaceH4/zephyr-7b-beta": 0.9445241881718925, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9650995110717052, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8009512697524199}}, {"question": "\u4e0b\u5217\u8bed\u53e5\u4f5c\u4e3a\u5212\u5206\uff0c\u6b63\u786e\u7684\u662f\nA. \u8fd9\u4e2a\u73ed\u7684\u5b66\u751f\uff0c\u9664\u4e86\u4e03\u540d\u5973\u5b66\u751f\u5916\uff0c\u5176\u4f59\u90fd\u662f\u7537\u751f\nB. \u4e00\u5e74\u53ef\u4ee5\u5206\u4e3a\u6625\u3001\u590f\u3001\u79cb\u3001\u51ac\u56db\u5b63\nC. \u6218\u4e89\u5206\u4e3a\u5e38\u89c4\u6218\u4e89\u548c\u4e16\u754c\u5927\u6218\nD. \u6982\u5ff5\u5206\u4e3a\u666e\u904d\u6982\u5ff5\u3001\u5355\u72ec\u6982\u5ff5\u548c\u6b63\u6982\u5ff5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30138122892344665, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.36385828438381157, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f5c\u4e3an-6\u7cfb\u5217\u8102\u80aa\u9178\u7684\u524d\u4f53\u53ef\u8f6c\u53d8\u6210\u03b3-\u4e9a\u9ebb\u9178\u3001\u82b1\u751f\u56db\u70ef\u9178\u7684\u5fc5\u9700\u8102\u80aa\u9178\u662f\nA. \u4e9a\u9ebb\u9178\nB. \u03b1-\u4e9a\u9ebb\u9178\nC. \u4e9a\u6cb9\u9178\nD. \u4e8c\u5341\u78b3\u4e94\u70ef\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.36321228488384677, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6297\u751f\u7d20\u53ef\u4f5c\u4e3a\u598a\u5a20\u671f\u6025\u6027\u80be\u76c2\u80be\u708e\u60a3\u8005\u9996\u9009\u7684\u662f\nA. \u963f\u5947\u9709\u7d20\nB. \u5e86\u5927\u9709\u7d20\nC. \u6c28\u82c4\u897f\u6797\nD. \u6c27\u6c1f\u6c99\u661f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5439689867810821, "meta-math/MetaMath-Mistral-7B": 0.6512744711665343, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9707809116191485, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5158824367875585, "meta-llama/Meta-Llama-3-8B": 0.38812073416459414, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5152278213320992}}, {"question": "\u809d\u6e90\u6027\u6c34\u80bf\u7684\u7279\u70b9\u662f\nA. \u5e38\u6709\u8e1d\u90e8\u6c34\u80bf\nB. \u5e38\u4ece\u989c\u9762\u90e8\u5f00\u59cb\nC. \u5e38\u4e3a\u975e\u51f9\u9677\u6027\u6c34\u80bf\nD. \u5e38\u4f34\u9888\u9759\u8109\u5145\u76c8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.35286194274668226, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u54ea\u4e2a\u6210\u8bed\u548c\u66f9\u64cd\u6709\u5173\nA. \u7834\u91dc\u6c89\u821f\nB. \u671b\u6885\u6b62\u6e34\nC. \u753b\u997c\u5145\u9965\nD. \u5b88\u682a\u5f85\u5154\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5375\u53d7\u7cbe\u7684\u90e8\u4f4d\u5728\nA. \u8f93\u5375\u7ba1\u6f0f\u6597\nB. \u8f93\u5375\u7ba1\u5b50\u5bab\u90e8\nC. \u8f93\u5375\u7ba1\u58f6\u8179\u90e8\nD. \u8f93\u5375\u7ba1\u5ce1\u90e8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.46128369975373273, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.2810882504428991, "meta-llama/Meta-Llama-3-8B": 0.32205625344145955, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679c\u67d0\u7269\u8d28\u5728\u80be\u52a8\u8109\u4e2d\u6709\u4e00\u5b9a\u6d53\u5ea6\uff0c\u800c\u5728\u80be\u9759\u8109\u4e2d\u51e0\u4e4e\u4e3a\u96f6\uff0c\u5176\u6e05\u9664\u7387\nA. \u7b49\u4e8e\u6bcf\u5206\u949f\u80be\u8840\u6d41\u91cf\nB. \u7b49\u4e8e\u6bcf\u5206\u949f\u80be\u8840\u6d46\u6d41\u91cf\nC. \u7b49\u4e8e\u80be\u5c0f\u7403\u6ee4\u8fc7\u7387\nD. \u7b49\u4e8e\u96f6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3614417084329609, "itpossible/Chinese-Mistral-7B-v0.1": 0.34478590299221606, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u7269\u8d28\u5728\u4f53\u5185\u6c27\u5316\u6210CO2\u53caH2O\u65f6\u751f\u6210ATP\u6700\u591a\u7684\u662f\nA. \u4e73\u9178\nB. \u4e19\u916e\u9178\nC. \u8c37\u6c28\u9178\nD. \u8349\u9170\u4e59\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.350104390476052, "itpossible/Chinese-Mistral-7B-v0.1": 0.325455072595945, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u75be\u75c5\u4e2d\u4e0d\u6613\u53d1\u751f\u764c\u53d8\u7684\u662f\nA. \u7ed3\u80a0\u591a\u53d1\u6027\u817a\u7624\nB. \u5341\u4e8c\u6307\u80a0\u6e83\u75a1\nC. \u840e\u7f29\u6027\u80c3\u708e\nD. \u4ea4\u754c\u75e3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.28661281177772774, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.47363509707314366}}, {"question": "\u5b9c\u540e\u4e0b\u7684\u996e\u7247\u662f\nA. \u897f\u7ea2\u82b1\nB. \u96f7\u4e38\nC. \u963f\u80f6\nD. \u7802\u4ec1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28244158352462884, "meta-math/MetaMath-Mistral-7B": 0.3060136256597631, "itpossible/Chinese-Mistral-7B-v0.1": 0.2821833983601388, "HuggingFaceH4/zephyr-7b-beta": 0.40202865642572594, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6240\u9009\u6027\u72b6\u4e0a\uff0c\u7559\u79cd\u7fa4\u4e0e\u5168\u7fa4\u5e73\u5747\u503c\u4e4b\u5dee\u79f0\u4e3a\nA. \u9009\u62e9\u8fdb\u5c55\nB. \u9009\u62e9\u5f3a\u5ea6\nC. \u9009\u62e9\u53cd\u5e94\nD. \u9009\u62e9\u5dee\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5784327948657404, "HuggingFaceH4/zephyr-7b-beta": 0.8998560698391802, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4430448139737192, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8878706395955163}}, {"question": "\u5728\u897f\u65b9\u7f8e\u5b66\u53f2\u4e0a\uff0c\u63d0\u51fa\u201c\u5185\u5728\u611f\u5b98\u8bf4\u201d\u7684\u7f8e\u5b66\u5bb6\u662f\nA. \u8377\u52a0\u5179\nB. \u54c8\u5947\u751f\nC. \u590f\u592b\u5179\u535a\u91cc\nD. \u535a\u514b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2988885982467767, "meta-math/MetaMath-Mistral-7B": 0.35002889983337143, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9709016092962476, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.36727669046039974, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9a7e\u9a76\u673a\u52a8\u8f66\u901a\u8fc7\u6ca1\u6709\u4ea4\u901a\u4fe1\u53f7\u7684\u4ea4\u53c9\u8def\u53e3\u600e\u6837\u884c\u9a76\nA. \u5de6\u4fa7\u8f66\u8f86\u5148\u884c\nB. \u51cf\u901f\u6162\u884c\nC. \u52a0\u901f\u901a\u8fc7\nD. \u5927\u578b\u8f66\u5148\u884c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6793098109586048, "meta-math/MetaMath-Mistral-7B": 0.8526210989974984, "itpossible/Chinese-Mistral-7B-v0.1": 0.8875043303234075, "HuggingFaceH4/zephyr-7b-beta": 0.9957114730346669, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9210051000377038, "meta-llama/Meta-Llama-3-8B": 0.5586499695882374, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u6c34\u6676\u5de5\u827a\u54c1\u5728\u7f8e\u56fd\u4e2d\u4f4e\u4ef7\u4f4d\u7684\u5e02\u573a\u5360\u6709\u7387\u8d85\u8fc7\u4e8680%\u3002\u5047\u5b9a2010\u5e74\u67d0\u6b3e\u6c34\u6676\u5de5\u827a\u54c1\u4ef7\u503c\u7528\u4eba\u6c11\u5e01\u8868\u793a\u4e3a1 596\u5143\uff0c\u4eba\u6c11\u5e01\u6c47\u7387\u4e3a1\u7f8e\u5143\uff1d6.7\u5143\u4eba\u6c11\u5e01\u3002\u5982\u679c2011\u5e74\u751f\u4ea7\u8be5\u6b3e\u6c34\u6676\u5de5\u827a\u54c1\u7684\u793e\u4f1a\u52b3\u52a8\u751f\u4ea7\u7387\u63d0\u9ad820%\uff0c\u4e14\u4eba\u6c11\u5e01\u5bf9\u7f8e\u5143\u5347\u503c5%\uff0c\u5176\u4ed6\u6761\u4ef6\u4e0d\u53d8\uff0c\u8be5\u6b3e\u6c34\u6676\u5de5\u827a\u54c1\u4ee5\u7f8e\u5143\u6807\u4ef7\u5e94\u4e3a\nA. 192\u7f8e\u5143\nB. 209.5\u7f8e\u5143\nC. 199.5\u7f8e\u5143\nD. 180.15\u7f8e\u5143\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3162591581373374, "meta-math/MetaMath-Mistral-7B": 0.36029124167861426, "itpossible/Chinese-Mistral-7B-v0.1": 0.31283638571410965, "HuggingFaceH4/zephyr-7b-beta": 0.7344442786771846, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.511193498646212, "meta-llama/Meta-Llama-3-8B": 0.4349598745742093, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.532531112295531}}, {"question": "\uff08\uff09\u6307\u4eba\u4eec\u8eab\u4f53\u90e8\u4f4d\u4f5c\u51fa\u8868\u73b0\u67d0\u79cd\u5177\u4f53\u542b\u4e49\u7684\u52a8\u4f5c\u7b26\u53f7\u3002\nA. \u60c5\u6001\u8bed\u8a00\nB. \u52a8\u6001\u8bed\u8a00\nC. \u9759\u6001\u8bed\u8a00\nD. \u8eab\u52bf\u8bed\u8a00\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8957287180279342, "meta-math/MetaMath-Mistral-7B": 0.989574764172003, "itpossible/Chinese-Mistral-7B-v0.1": 0.7937704729668342, "HuggingFaceH4/zephyr-7b-beta": 0.9999975999081949, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.981698142895992, "meta-llama/Meta-Llama-3-8B": 0.5143150755407668, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9844170080237455}}, {"question": "\u68b5\u9ad8\u662f\u54ea\u4e2a\u56fd\u5bb6\u8457\u540d\u753b\u5bb6\u3002\nA. \u82f1\u56fd\nB. \u4e39\u9ea6\nC. \u8377\u5170\nD. \u6cd5\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.9174669716500281, "itpossible/Chinese-Mistral-7B-v0.1": 0.8425630454988846, "HuggingFaceH4/zephyr-7b-beta": 0.5168870199371415, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5133130705913631, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c49\u5b57\u7684\u62fc\u2fb3\u8f93\u2f0a\u7801\u5c5e\u4e8e\u6c49\u5b57\u7684\nA. \u6807\u51c6\u7801\nB. ASCII\u7801\nC. \u5185\u7801\nD. \u5916\u7801\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7814\u7a76\u8005\u5728\u67d0\u79cd\u7a0b\u5ea6\u4e0a\u7f6e\u8eab\u4e8e\u89c2\u5bdf\u5bf9\u8c61\u7684\u73af\u5883\u548c\u793e\u4f1a\u6d3b\u52a8\u4e2d\uff0c\u4f7f\u81ea\u5df1\u6210\u4e3a\u88ab\u7814\u7a76\u7fa4\u4f53\u4e2d\u4e00\u5458\uff0c\u8fd9\u79cd\u89c2\u5bdf\u65b9\u6cd5\u88ab\u79f0\u4e3a\nA. \u6587\u732e\u7814\u7a76\nB. \u793e\u4f1a\u5b9e\u9a8c\nC. \u53c2\u4e0e\u89c2\u5bdf\nD. \u95ee\u5377\u8c03\u67e5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.968856918476299, "meta-math/MetaMath-Mistral-7B": 0.9913033051682194, "itpossible/Chinese-Mistral-7B-v0.1": 0.9611989466845521, "HuggingFaceH4/zephyr-7b-beta": 0.9547062426381857, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.97752153660854, "meta-llama/Meta-Llama-3-8B": 0.9522186436137098, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9294303229817591}}, {"question": "\u4ee5\u4e0b\u51e0\u79cd\u771f\u6838\u751f\u7269\uff0c\u9057\u4f20\u5b66\u5bb6\u5df2\u5e7f\u6cdb\u7814\u7a76\u7684\u5305\u62ec\nA. \u4ee5\u4e0a\u9009\u9879\u5747\u662f\nB. \u7389\u7c73\nC. \u9175\u6bcd\nD. \u679c\u8747\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8622002803813621, "meta-math/MetaMath-Mistral-7B": 0.9699705035728107, "itpossible/Chinese-Mistral-7B-v0.1": 0.672362783331339, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8974167557204555, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7055651785627199}}, {"question": "\u793e\u4f1a\u552f\u540d\u8bba\u7684\u4e3b\u8981\u4ee3\u8868\u4eba\u7269\u662f\nA. \u97e6\u4f2f\nB. \u6d82\u5c14\u5e72\nC. \u9a6c\u514b\u601d\nD. \u5e03\u52b3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3788074063714755, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.2712019384407753, "meta-llama/Meta-Llama-3-8B": 0.3362011175603434, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728WINDOWS\u7f51\u7edc\u73af\u5883\u4e2d\uff0c\u8981\u8bbf\u95ee\u5176\u4ed6\u8ba1\u7b97\u673a\uff0c\u53ef\u4ee5\u6253\u5f00\nA. \u6211\u7684\u6587\u6863\nB. \u7f51\u4e0a\u90bb\u5c45\nC. \u63a7\u5236\u9762\u677f\nD. \u6211\u7684\u7535\u8111\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.9756934779664991, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6386716778579941, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7053941973320507}}, {"question": "\u8d44\u672c\u4e3b\u4e49\u653f\u515a\u5236\u5ea6\u7684\u5b9e\u8d28\u662f\nA. \u4e0d\u53d7\u8d44\u672c\u4e3b\u4e49\u56fd\u5bb6\u653f\u6743\u7684\u8d44\u672c\u4e3b\u4e49\u6027\u8d28\u5236\u7ea6\nB. \u5141\u8bb8\u9a6c\u514b\u601d\u4e3b\u4e49\u653f\u515a\u72ec\u7acb\u6267\u653f\nC. \u5141\u8bb8\u5de5\u4eba\u9636\u7ea7\u53ca\u5176\u653f\u515a\u53c2\u4e0e\u56fd\u5bb6\u653f\u6cbb\u751f\u6d3b\nD. \u8d44\u4ea7\u9636\u7ea7\u9009\u62e9\u81ea\u5df1\u7684\u56fd\u5bb6\u7ba1\u7406\u8005\uff0c\u5b9e\u73b0\u5176\u5185\u90e8\u5229\u76ca\u5e73\u8861\u7684\u653f\u6cbb\u673a\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.833040570331799, "meta-math/MetaMath-Mistral-7B": 0.77621066907429, "itpossible/Chinese-Mistral-7B-v0.1": 0.7615138676549951, "HuggingFaceH4/zephyr-7b-beta": 0.9935562073434326, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6864827823128609, "meta-llama/Meta-Llama-3-8B": 0.8782256282115265, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6836220689882139}}, {"question": "\u73b0\u4ee3\u5316\u517b\u9e21\u751f\u4ea7\u4e2d\uff0c\u86cb\u9e21\u7684\u9972\u517b\u5468\u671f\u4e00\u822c\u4e3a\nA. 2\u5e74\nB. 72\u5468\nC. 1\u5e74\nD. 64\u5468\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41166576187381965, "meta-math/MetaMath-Mistral-7B": 0.4644163307489673, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9953762014464854, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4133000687727783, "meta-llama/Meta-Llama-3-8B": 0.5103194200913526, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65e9\u5728\u53e4\u4ee3\uff0c\u6211\u56fd\u4eba\u6c11\u5c31\u79ef\u7d2f\u4e86\u4e0d\u5c11\u5bf9\u5316\u5b66\u7269\u8d28\u53d8\u5316\u7684\u8ba4\u8bc6\u3002\u4f8b\u5982\uff0c\u664b\u4ee3\u70bc\u4e39\u5bb6\u3001\u533b\u5b66\u5bb6\u845b\u6d2a\u6240\u8457\u300a\u62b1\u6734\u5b50\u300b\u4e00\u4e66\u4e2d\u8bb0\u8f7d\u6709\u201c\u4e39\u7802\u70e7\u4e4b\u6210\u6c34\u94f6\uff0c\u79ef\u53d8\u53c8\u6210\u4e39\u7802\u201d\u3002\u8fd9\u53e5\u8bdd\u4e2d\u7684\u4e39\u7802\u6307\u7684\u662fHgS\uff0c\u4e0b\u5217\u5173\u4e8e\u8fd9\u53e5\u8bdd\u7684\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u201c\u4e39\u7802\u70e7\u4e4b\u6210\u6c34\u94f6\u201d\u8fc7\u7a0b\u4e2d\u8fd8\u53ef\u80fd\u4ea7\u751fSO2\nB. \u8fd9\u4e2a\u8fc7\u7a0b\u662f\u53ef\u9006\u53cd\u5e94\nC. \u8fd9\u4e2a\u8fc7\u7a0b\u53d1\u751f\u4e86\u590d\u5206\u89e3\u53cd\u5e94\nD. \u8fd9\u4e2a\u8fc7\u7a0b\u53ea\u6d89\u53ca\u7269\u7406\u53d8\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.33136355362667563, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7887821438214074}}, {"question": "\u6839\u636e\u8bba\u8bc1\u7684\u524d\u63d0\u4e0e\u7ed3\u8bba\u7684\u652f\u6301\u5173\u7cfb\uff0c\u8c2c\u8bef\u53ef\u4ee5\u5206\u4e3a\nA. \u524d\u63d0\u8c2c\u8bef\nB. \u652f\u6301\u8c2c\u8bef\nC. \u4ee5\u4e0a\u90fd\u5bf9\nD. \u76f8\u5e72\u8c2c\u8bef\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.35776608755353506, "HuggingFaceH4/zephyr-7b-beta": 0.3982518517054595, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.39418480059104805, "meta-llama/Meta-Llama-3-8B": 0.7863472025896655, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.858519330292749}}, {"question": "\u4f7f\u75282\u5757\u51f8\u900f\u955c\u53ef\u4ee5\u5236\u4f5c\u4e00\u67b6\u5f00\u666e\u52d2\u5f0f\u671b\u8fdc\u955c\u3002\u5728\u7269\u955c\u548c\u76ee\u955c\u4e4b\u95f4\u63d2\u5165\u4e00\u5757\u51f9\u900f\u955c\u80fd\u591f\u8fdb\u4e00\u6b65\u63d0\u9ad8\u671b\u8fdc\u955c\u7684\u653e\u5927\u500d\u6570\u3002\u4f7f\u7528\u8fd9\u67b6\u671b\u8fdc\u955c\u89c2\u5bdf\u8fdc\u5904\u6b63\u7acb\u7684\u5b57\u6bcdp\uff0c\u5728\u76ee\u955c\u4e2d\u770b\u5230\u7684\u5f62\u72b6\u662f\nA. q\nB. p\nC. b\nD. d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5321079209939151, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e09\u89d2\u5f62ABC\u4e2d\uff0csinA:sinB:sinC=3:2:4\uff0c\u5219cosC\u7684\u503c\u4e3a\nA. -2/3\nB. -1/4\nC. 2/3\nD. 1/4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e0b\u5217\u53d9\u8ff0\u4e2d\uff0c\u4e0d\u4f1a\u53d1\u751f\u7684\u5316\u751f\u662f\nA. \u7ea4\u7ef4\u7ec4\u7ec7\u5316\u751f\u4e3a\u8f6f\u9aa8\u7ec4\u7ec7\nB. \u67f1\u72b6\u4e0a\u76ae\u5316\u751f\u4e3a\u9cde\u72b6\u4e0a\u76ae\nC. \u9cde\u72b6\u4e0a\u76ae\u5316\u751f\u4e3a\u7ea4\u7ef4\u7ec4\u7ec7\nD. \u80c3\u817a\u4e0a\u76ae\u5316\u751f\u4e3a\u80a0\u817a\u4e0a\u76ae\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5754652887634157, "meta-math/MetaMath-Mistral-7B": 0.9255919478037234, "itpossible/Chinese-Mistral-7B-v0.1": 0.6284660047233402, "HuggingFaceH4/zephyr-7b-beta": 0.996354078087888, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7054705545531453, "meta-llama/Meta-Llama-3-8B": 0.44304481397371914, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.501117463389654}}, {"question": "\u201c\u8f9f\u201d\u548c\u201c\u907f\u201d\u7684\u5173\u7cfb\u662f\nA. \u7e41\u7b80\u5b57\nB. \u901a\u5047\u5b57\nC. \u53e4\u4eca\u5b57\nD. \u5f02\u4f53\u5b57\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u751f\u65e6\u51c0\u672b\u4e11\u201d\u662f\u4eac\u5267\u7684\u884c\u5f53\uff0c\u5176\u4e2d\u201c\u51c0\u201d\u662f\nA. \u8001\u4eba\nB. \u5b69\u5b50\nC. \u5973\u89d2\nD. \u7537\u89d2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.30601362565976303, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u4e2a\u4f53\u53d1\u751f\u4e86\u9690\u6027\u7a81\u53d8\uff0c\u5728\u81ea\u4ea4\u540e\u4ee3\u4e2d\uff0c\u7a81\u53d8\u6027\u72b6\u7684\u8868\u73b0\u548c\u68c0\u51fa\u7a81\u53d8\u7eaf\u5408\u4f53\u5206\u522b\u662f\nA. \u7b2c\u4e8c\u4ee3\u8868\u73b0\uff0c\u7b2c\u4e09\u4ee3\u7eaf\u5408\nB. \u7b2c\u4e8c\u4ee3\u8868\u73b0\uff0c\u7b2c\u4e8c\u4ee3\u7eaf\u5408\nC. \u7b2c\u4e00\u4ee3\u8868\u73b0\uff0c\u7b2c\u4e8c\u4ee3\u7eaf\u5408\nD. \u7b2c\u4e00\u4ee3\u8868\u73b0\uff0c\u7b2c\u4e00\u4ee3\u7eaf\u5408\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.28966338381871215, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4965194293553775}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u67d0\u4e00\u4e2a\u4f53\u80f0\u5c9bB\u7ec6\u80de\u548cB\u7ec6\u80de\u7684\u8868\u8ff0\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u90fd\u80fd\u8bc6\u522b\u4fe1\u606f\u5206\u5b50\nB. \u90fd\u5177\u6709\u53d1\u8fbe\u7684\u9ad8\u5c14\u57fa\u4f53\nC. \u90fd\u80fd\u8f6c\u5f55\u51fa\u6307\u5bfcATP\u9176\u5408\u6210\u7684mRNA\nD. \u90fd\u542b\u6709\u63a7\u5236\u80f0\u5c9b\u7d20\u5408\u6210\u7684\u57fa\u56e0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u56fd\u5bb6\u5de5\u4f5c\u4eba\u5458\u8096\u67d0\u5728\u7ecf\u6d4e\u5f80\u6765\u4e2d\uff0c\u8fdd\u53cd\u56fd\u5bb6\u89c4\u5b9a\uff0c\u6536\u53d7\u67d0\u516c\u53f8\u56de\u62634\u4e07\u5143\uff0c\u5f52\u4e2a\u4eba\u6240\u6709\u3002\u8096\u67d0\u7684\u884c\u4e3a\u6784\u6210\nA. \u53d7\u8d3f\u7f6a\nB. \u8d2a\u6c61\u7f6a\u548c\u53d7\u8d3f\u7f6a\nC. \u4ecb\u7ecd\u8d3f\u8d42\u7f6a\nD. \u8d2a\u6c61\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u53ea64M\u7684\u4f18\u76d8\u7684\u5bb9\u91cf\u76f8\u5f53\u4e8e\nA. 640000KB\nB. 1000KB\nC. 64KB\nD. 65536KB\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5741308511450051, "meta-math/MetaMath-Mistral-7B": 0.3984020769951305, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9999897822673732, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5431960487581493, "meta-llama/Meta-Llama-3-8B": 0.36329356528119183, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7535\u89c6\u5267\u300a\u56db\u4e16\u540c\u5802\u300b\u662f\u6839\u636e\uff08\uff09\u7684\u957f\u7bc7\u5c0f\u8bf4\u6539\u7f16\u7684\u3002\nA. \u9a6c\u91d1\nB. \u8001\u820d\nC. \u9c81\u8fc5\nD. \u8305\u76fe\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44469512951113976, "meta-math/MetaMath-Mistral-7B": 0.6036570520735217, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8273416790338792, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.750827256277493, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6278602375560275}}, {"question": "\u5fc3\u7406\u72b6\u6001\u662f\u6307\u4eba\u5728\u67d0\u4e00\u65f6\u523b\u7684\u5fc3\u7406\u6d3b\u52a8\u6c34\u5e73\u3002\u4ee5\u4e0b\u5c5e\u4e8e\u5fc3\u7406\u72b6\u6001\u7684\u662f\nA. \u52e4\u52b3\nB. \u5fc3\u5883\nC. \u6001\u5ea6\nD. \u8bb0\u5fc6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4218896111607803, "meta-math/MetaMath-Mistral-7B": 0.9140179173844585, "itpossible/Chinese-Mistral-7B-v0.1": 0.6978847966603663, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7713460525933812, "meta-llama/Meta-Llama-3-8B": 0.6649269145547185, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9991152546402303}}, {"question": "\u8fd0\u52a8\u505c\u6b62\u540e\uff0c\u5f53\u8fd0\u52a8\u4e2d\u6240\u6d88\u8017\u7684\u80fd\u6e90\u7269\u8d28\u6062\u590d\u5230\u539f\u6765\u6c34\u5e73\u540e\uff0c\u7ee7\u7eed\u6301\u7eed\u5230\u8d85\u8fc7\u539f\u6765\u7684\u6c34\u5e73\uff0c\u8fd9\u79cd\u73b0\u8c61\u79f0\u4e3a\nA. \u6b63\u5e38\u6062\u590d\nB. \u8d85\u91cf\u6062\u590d\nC. \u6301\u7eed\u6062\u590d\nD. \u7f13\u6162\u6062\u590d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6227888797937347, "meta-math/MetaMath-Mistral-7B": 0.9777168659758487, "itpossible/Chinese-Mistral-7B-v0.1": 0.9051377968651286, "HuggingFaceH4/zephyr-7b-beta": 0.912878719676828, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8710342950634933, "meta-llama/Meta-Llama-3-8B": 0.7909437106745426, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9828418717537074}}, {"question": "\u201c\u4e00\u5bf9\u82b1\u74f6\u201d\u3001\u201c\u4f60\u8bf4\u7684\u5bf9\u201d\u3001\u201c\u9762\u5bf9\u672a\u6765\u201d\u4e2d\u4e09\u4e2a\u201c\u5bf9\u201d\u4ee3\u8868\nA. \u4e09\u4e2a\u8bcd\nB. \u53ea\u4ee3\u8868\u97f3\u8282\uff0c\u4e0d\u4ee3\u8868\u8bcd\nC. \u4e24\u4e2a\u8bcd\nD. \u4e00\u4e2a\u8bcd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.33863640659411287, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5294346483932203}}, {"question": "\u4f01\u4e1a\u4e0e\u5458\u5de5\u7684\u6743\u5229\u4e0e\u4e49\u52a1\u53ef\u4ee5\u5728\u96c7\u4f63\u5408\u540c\u4e2d\u5f97\u5230\u89c4\u5b9a\uff0c\u5305\u62ec\uff08\uff09a\u5de5\u8d44\u5f85\u9047\uff1bb\u5de5\u4f5c\u6027\u8d28\uff1bc\u5de5\u4f5c\u65f6\u95f4\uff1bd\u9000\u4f11\u91d1\nA. abcd\nB. acd\nC. bcd\nD. abc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5327389249581185, "meta-math/MetaMath-Mistral-7B": 0.5986807290665849, "itpossible/Chinese-Mistral-7B-v0.1": 0.6889519854895805, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5071199722748061, "meta-llama/Meta-Llama-3-8B": 0.4861861135360488, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7888975127709995}}, {"question": "\u7531\u4e8e\u73af\u5883\u6761\u4ef6\u4e2d\u7684\u4e0d\u9002\u56e0\u7d20\u6240\u9020\u6210\u7684\u6682\u505c\u53d1\u82bd\u3001\u751f\u957f\u7684\u73b0\u8c61\u53eb\nA. \u9650\u5236\u56e0\u7d20\nB. \u81ea\u53d1\u4f11\u7720\nC. \u751f\u7406\u4f11\u7720\nD. \u5f3a\u8feb\u4f11\u7720\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u975e\u56fd\u5bb6\u5de5\u4f5c\u4eba\u5458\u53d7\u8d3f\u7f6a\u7684\u8bf4\u6cd5\uff0c\u9519\u8bef\u7684\u662f\nA. \u975e\u56fd\u5bb6\u5de5\u4f5c\u4eba\u5458\u53d7\u8d3f\u7f6a\u7684\u4e3b\u89c2\u65b9\u9762\u8868\u73b0\u4e3a\u6545\u610f\nB. \u975e\u56fd\u5bb6\u5de5\u4f5c\u4eba\u5458\u53d7\u8d3f\u7f6a\u7684\u72af\u7f6a\u4e3b\u4f53\u662f\u7279\u6b8a\u4e3b\u4f53\uff0c\u53ea\u80fd\u662f\u975e\u56fd\u6709\u516c\u53f8\u3001\u4f01\u4e1a\u7684\u5de5\u4f5c\u4eba\u5458\nC. \u975e\u56fd\u5bb6\u5de5\u4f5c\u4eba\u5458\u53d7\u8d3f\u7f6a\u7684\u5ba2\u89c2\u65b9\u9762\u8868\u73b0\u4e3a\u5229\u7528\u804c\u52a1\u4e0a\u7684\u4fbf\u5229\uff0c\u7d22\u53d6\u4ed6\u4eba\u8d22\u7269\u6216\u975e\u6cd5\u6536\u53d7\u4ed6\u4eba\u8d22\u7269\uff0c\u4e3a\u4ed6\u4eba\u8c0b\u53d6\u5229\u76ca\uff0c\u6570\u989d\u8f83\u5927\u7684\u884c\u4e3a\nD. \u975e\u56fd\u5bb6\u5de5\u4f5c\u4eba\u5458\u53d7\u8d3f\u7f6a\u7684\u5ba2\u4f53\u662f\u56fd\u5bb6\u5bf9\u516c\u53f8\u3001\u4f01\u4e1a\u5de5\u4f5c\u4eba\u5458\u804c\u52a1\u6d3b\u52a8\u7684\u7ba1\u7406\u5236\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5694584663410744, "HuggingFaceH4/zephyr-7b-beta": 0.49964028725623305, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8264732460754123, "meta-llama/Meta-Llama-3-8B": 0.8741928838317023, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53\u4ee3\u4e16\u754c\u5546\u54c1\u7ed3\u6784\u590d\u6742\u5316\uff0c\u4e3b\u8981\u8868\u73b0\u5728\nA. \u51fa\u73b0\u4e86\u8865\u507f\u8d38\u6613\u3001\u79df\u8d41\u8d38\u6613\u3001\u52a0\u5de5\u8d38\u6613\u7b49\u65b0\u5f62\u5f0f\nB. \u540c\u4e00\u5546\u54c1\u7684\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u5f80\u5f80\u662f\u5728\u4e0d\u540c\u7684\u56fd\u5bb6\u751f\u4ea7\u7684\nC. \u5546\u54c1\u6280\u672f\u542b\u91cf\u65e5\u76ca\u63d0\u9ad8\uff0c\u9644\u52a0\u503c\u5927\u5e45\u5ea6\u589e\u52a0\nD. \u5236\u6210\u54c1\u8d38\u6613\u6269\u5927\uff0c\u521d\u7ea7\u4ea7\u54c1\u8d38\u6613\u51cf\u5c11\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5bf9\u2f42\u5316\u3001\u2f42\u5b66\u5e38\u8bc6\u7684\u8868\u8ff0\u4e0d\u6b63\u786e\u7684\u2f00\u9879\u662f\nA. \u300a\u5de6\u4f20\u300b\u5373\u300a\u6625\u79cb\u5de6\u2f52\u4f20\u300b\uff0c\u2f1c\u79f0\u300a\u5de6\u2f52\u6625\u79cb\u300b\uff0c\u662f\u4e2d\u56fd\u53e4\u4ee3\u7684\u53f2\u5b66\u548c\u2f42\u5b66\u540d\u8457\uff0c\u65e7\u4f20\u4e3a\u6625\u79cb\u65f6 \u671f\u5de6\u4e18\u660e\u6240\u8457\uff0c\u8fd1\u2f08\u8ba4\u4e3a\u662f\u6218\u56fd\u65f6\u2f08\u6240\u7f16\u3002\u201c\u2f00\u2fce\u4f5c\u2f53'\u51fa\u2f83\u300a\u5de6\u4f20\u300b\u3002\nB. \u300a\u6211\u7684\u53d4\u53d4\u4e8e\u52d2\u300b\u7684\u4f5c\u8005\u83ab\u6cca\u6851\uff0c\u662f\u6cd5\u56fd\u4f18\u79c0\u7684\u6279\u5224\u73b0\u5b9e\u4e3b\u4e49\u4f5c\u5bb6\u3002\u4ed6\u4e0e\u4fc4\u56fd\u7684\u5951\u8bc3\u592b\u3001\u7f8e\u56fd\u7684\u6b27\u00b7\u4ea8\u5229\u5e76\u79f0\u4e3a\u201c\u4e16\u754c\u4e09\u2f24\u77ed\u7bc7\u2f29\u8bf4\u4e4b\u738b\u201d\u3002 \nC. \u201c\u2f46\u4e1d\u2f75\u4e4b\u4e71\u2f7f\u201d\u4e2d\u7684\u201c\u4e1d\u2f75\u201d\u6cdb\u6307\u2fb3\u4e50\uff1b\u53e4\u4ee3\u4f4f\u5b85\u65c1\u8fb9\u5e38\u683d\u79cd\u6851\u6811\u548c\u6893\u6811\uff0c\u540e\u6765\u5c31\u2f64\u201c\u6851\u6893\u201d\u6307\u4ee3\u5bb6\u4e61\uff1b\u5c01\u5efa\u541b\u4e3b\u796d\u793e\u7a37\uff0c\u7948\u6c42\u4e30\u5e74\uff0c\u540e\u6765\u5c31\u628a\u201c\u793e\u7a37\u201d\u4f5c\u4e3a\u56fd\u5bb6\u7684\u4ee3\u79f0\u3002\nD. \u6731\u2f83\u6e05\uff0c\u8457\u540d\u6563\u2f42\u5bb6\u3001\u8bd7\u2f08\u3001\u5b66\u8005\uff0c\u8457\u6709\u6563\u2f42\u96c6\u300a\u80cc\u5f71\u300b\u300a\u6b27\u6e38\u6742\u8bb0\u300b\u7b49\u3002\u6211\u4eec\u5b66\u8fc7\u4ed6\u7684\u6563\u2f42\u300a\u6d4e\u5357\u7684\u51ac\u5929\u300b\u300a\u80cc\u5f71\u300b\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3468666740605408, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4323268768868835, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u5956\u9500\u552e\u8fd9\u4e00\u4fc3\u9500\u5de5\u5177\u9488\u5bf9\u7684\u662f\nA. \u6d88\u8d39\u8005\nB. \u63a8\u9500\u4eba\u5458\nC. \u96f6\u552e\u5546\nD. \u4e2d\u95f4\u5546\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.48866530043490325, "HuggingFaceH4/zephyr-7b-beta": 0.9311560326058148, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8621541653165783, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5373957711427493}}, {"question": "\u5173\u4e8e\u94a2\u7b4b\u52a0\u5de5\u7684\u8bf4\u6cd5\uff0c\u6b63\u786e\u7684\u662f\nA. \u5f2f\u6298\u8fc7\u7a0b\u4e2d\u53ef\u52a0\u70ed\u94a2\u7b4b\nB. \u94a2\u7b4b\u7684\u5207\u65ad\u53e3\u4e0d\u5e94\u6709\u8d77\u5f2f\u73b0\u8c61\nC. \u5f2f\u6298\u8fc7\u5ea6\u7684\u94a2\u7b4b\uff0c\u53ef\u56de\u5f2f\nD. \u4e00\u6b21\u5f2f\u6298\u4e0d\u5230\u4f4d\uff0c\u53ef\u53cd\u590d\u5f2f\u6298\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.48069472740675945, "meta-math/MetaMath-Mistral-7B": 0.6859881238641746, "itpossible/Chinese-Mistral-7B-v0.1": 0.7070475955802874, "HuggingFaceH4/zephyr-7b-beta": 0.9979369049176045, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7262607153812723, "meta-llama/Meta-Llama-3-8B": 0.652639518695817, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9953512469367616}}, {"question": "\u80b1\u9aa8\u5916\u79d1\u9888\u9aa8\u6298\u6613\u635f\u4f24\nA. \u6861\u795e\u7ecf\nB. \u5c3a\u795e\u7ecf\nC. \u814b\u795e\u7ecf\nD. \u6b63\u4e2d\u795e\u7ecf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7cbe\u5fc3\u5730\u62a4\u6301\u81ea\u5df1\u7684\u5584\u610f\uff0c\u7cbe\u5fc3\u5730\u4fdd\u6301\u81ea\u5df1\u7684\u5584\u884c\uff0c\u201c\u4e0d\u4ee5\u5584\u5c0f\u800c\u4e0d\u4e3a\uff0c\u4e0d\u4ee5\u6076\u5c0f\u800c\u4e3a\u4e4b\u201d\uff0c\u4f7f\u5176\u4e0d\u65ad\u5730\u79ef\u7d2f\u6210\u4e3a\u4e2a\u4eba\u54c1\u5fb7\u3002\u8fd9\u79cd\u9053\u5fb7\u4fee\u517b\u65b9\u6cd5\u662f\nA. \u62e9\u5584\u800c\u4ece\nB. \u79ef\u5584\u6210\u5fb7\nC. \u7701\u5bdf\u514b\u6cbb\nD. \u9676\u51b6\u60c5\u64cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7043866301630514, "meta-math/MetaMath-Mistral-7B": 0.9917472782619786, "itpossible/Chinese-Mistral-7B-v0.1": 0.939442026707675, "HuggingFaceH4/zephyr-7b-beta": 0.9186277698862407, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.962863399934769, "meta-llama/Meta-Llama-3-8B": 0.893734668948191, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9967387115160139}}, {"question": "\u7535\u6c14\u8bbe\u5907\u7740\u706b\uff0c\u5e94\u4f7f\u7528\uff08\uff09\u706d\u706b\u3002\nA. \u6ce1\u6cab\u706d\u706b\u5668\nB. \u6e7f\u68c9\u88ab\nC. \u5e72\u7c89\u706d\u706b\u5668\nD. \u9ec4\u6ce5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3887245648358076, "meta-math/MetaMath-Mistral-7B": 0.6235777838683897, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8723623333034127, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6649267334943828, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f4e\u4e8e\u7ef4\u6301\u8eab\u4f53\u6709\u6548\u6d3b\u52a8\u6700\u4f4e\u751f\u5b58\u6307\u6807\u7684\u8d2b\u56f0\u72b6\u6001\uff0c\u88ab\u79f0\u4e3a\nA. \u7269\u8d28\u8d2b\u56f0\nB. \u76f8\u5bf9\u8d2b\u56f0\nC. \u7edd\u5bf9\u8d2b\u56f0\nD. \u7cbe\u795e\u8d2b\u56f0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7709208072523184, "meta-math/MetaMath-Mistral-7B": 0.8593242766463802, "itpossible/Chinese-Mistral-7B-v0.1": 0.7166787162391968, "HuggingFaceH4/zephyr-7b-beta": 0.983575079558981, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7765051737312384, "meta-llama/Meta-Llama-3-8B": 0.8989788871926181, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9474028646302338}}, {"question": "\u81ea\u75c5\u539f\u4f53\u4fb5\u5165\u673a\u4f53\u5230\u4e34\u5e8a\u75c7\u72b6\u6700\u65e9\u51fa\u73b0\u7684\u4e00\u6bb5\u65f6\u95f4\u79f0\u4e3a\nA. \u524d\u9a71\u671f\nB. \u6f5c\u4f0f\u671f\nC. \u53d1\u75c5\u524d\u671f\nD. \u4f20\u67d3\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7122610970716055, "meta-math/MetaMath-Mistral-7B": 0.9657267537967383, "itpossible/Chinese-Mistral-7B-v0.1": 0.8461282361596566, "HuggingFaceH4/zephyr-7b-beta": 0.9994051367741348, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9595710372788143, "meta-llama/Meta-Llama-3-8B": 0.77382751235237, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6554814936168126}}, {"question": "\u5a74\u5e7c\u513f\u6700\u4e3b\u8981\u7684\u611f\u77e5\u89c9\u662f\u89c6\u89c9\u3001\u542c\u89c9\u3001\u89e6\u89c9\u7b49\uff0c\u5176\u4e2d\u53d1\u5c55\u6700\u65e9\u7684\u662f\nA. \u542c\u89c9\nB. \u5473\u89c9\nC. \u89c6\u89c9\nD. \u89e6\u89c9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3376787962540697, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8def\u53e3\u8f6c\u5f2f\u8fc7\u7a0b\u4e2d\uff0c\u6301\u7eed\u5f00\u542f\u8f6c\u5411\u706f\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u4ec0\u4e48\nA. \u8ba9\u5176\u4ed6\u9a7e\u9a76\u4eba\u77e5\u9053\u60a8\u6b63\u5728\u8d85\u8f66\nB. \u5b8c\u6210\u8f6c\u5f2f\u52a8\u4f5c\u524d\uff0c\u5173\u95ed\u8f6c\u5411\u706f\u662f\u4e60\u60ef\u52a8\u4f5c\nC. \u5b8c\u6210\u8f6c\u5f2f\u52a8\u4f5c\u524d\uff0c\u5173\u95ed\u8f6c\u5411\u706f\u4f1a\u5bf9\u8f66\u8f86\u9020\u6210\u635f\u5bb3\nD. \u8ba9\u5176\u4ed6\u9a7e\u9a76\u4eba\u77e5\u9053\u60a8\u6b63\u5728\u8f6c\u5f2f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9027343041708452, "meta-math/MetaMath-Mistral-7B": 0.9914804128900122, "itpossible/Chinese-Mistral-7B-v0.1": 0.9209289714942985, "HuggingFaceH4/zephyr-7b-beta": 0.999888499611261, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9977206971085357, "meta-llama/Meta-Llama-3-8B": 0.9480754349600158, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6cd5\u5f8b\u89e3\u91ca\u7684\u8868\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u5386\u53f2\u89e3\u91ca\u65b9\u6cd5\u65e2\u53ef\u7528\u4e8e\u6b63\u5f0f\u89e3\u91ca\uff0c\u4e5f\u53ef\u7528\u4e8e\u975e\u6b63\u5f0f\u89e3\u91ca\nB. \u56fd\u5bb6\u673a\u5173\u5bf9\u6cd5\u5f8b\u6240\u4f5c\u7684\u89e3\u91ca\u5747\u4e3a\u6709\u6743\u89e3\u91ca\nC. \u6211\u56fd\u6cd5\u5f8b\u89e3\u91ca\u4f53\u7cfb\u5305\u62ec\u7acb\u6cd5\u89e3\u91ca\u548c\u53f8\u6cd5\u89e3\u91ca\u4e24\u79cd\u5f62\u5f0f\nD. \u6309\u89e3\u91ca\u5c3a\u5ea6\u7684\u4e0d\u540c\u53ef\u4ee5\u5c06\u6cd5\u5f8b\u89e3\u91ca\u5206\u4e3a\u6587\u4e49\u89e3\u91ca\u4e0e\u4f53\u7cfb\u89e3\u91ca\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u62a5\u544a\u6587\u5b66\u300a\u5305\u8eab\u5de5\u300b\u628a\u5305\u8eab\u5de5\u7684\u751f\u6d3b\u6d53\u7f29\u4e3a\u4e00\u5929\u4e2d\u7684\u4e09\u4e2a\u573a\u666f\uff0c\u5b83\u4eec\u662f\nA. \u8d77\u5e8a\u3001\u4e0a\u5de5\u3001\u4e0b\u5de5\nB. \u4e0a\u5de5\u3001\u5348\u9910\u3001\u4e0b\u5de5\nC. \u65e9\u9910\u3001\u4e0a\u5de5\u3001\u4e0b\u5de5\nD. \u8d77\u5e8a\u3001\u65e9\u9910\u3001\u4e0a\u5de5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.33544561004293627, "HuggingFaceH4/zephyr-7b-beta": 0.590123040378136, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5c5e\u4e8e\u6cd5\u5f8b\u5236\u88c1\u7684\u662f\nA. \u5b66\u751f\u674e\u67d0\u56e0\u4e25\u91cd\u8fdd\u53cd\u6821\u89c4\u88ab\u5b66\u6821\u901a\u62a5\u6279\u8bc4\nB. \u67d0\u7701\u4eba\u5927\u5e38\u59d4\u4f1a\u5236\u5b9a\u7684\u5730\u65b9\u6027\u6cd5\u89c4\u56e0\u4e0e\u5baa\u6cd5\u76f8\u62b5\u89e6\uff0c\u88ab\u5168\u56fd\u4eba\u5927\u5e38\u59d4\u4f1a\u64a4\u9500\nC. \u7532\u3001\u4e59\u4e24\u516c\u53f8\u7b7e\u8ba2\u6709\u5408\u4f5c\u534f\u8bae\uff0c\u540e\u7532\u516c\u53f8\u8fdd\u7ea6\uff0c\u7ecf\u4e59\u516c\u53f8\u8058\u8bf7\u7684\u5f8b\u5e08\u4e0e\u4e4b\u8fdb\u884c\u4ea4\u6d89\uff0c\u5e76\u4ee5\u63d0\u8d77\u8bc9\u8bbc\u76f8\u5a01\u80c1\uff0c\u7532\u516c\u53f8\u88ab\u8feb\u4ed8\u7ed9\u4e59\u516c\u53f8\u4e00\u7b14\u8fdd\u7ea6\u91d1\nD. \u515a\u5458\u5f20\u67d0\u56e0\u8fdd\u53cd\u515a\u7eaa\u53d7\u5230\u515a\u5185\u4e25\u91cd\u8b66\u544a\u5904\u5206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4156383487946337, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8f93\u5375\u7ba1\u7ed3\u624e\u672f\u4e8e\u8f93\u5375\u7ba1\uff08 \uff09\u8fdb\u884c\nA. \u5ce1\u90e8\nB. \u5b50\u5bab\nC. \u6f0f\u6597\u90e8\nD. \u58f6\u8179\u90e8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4242631597241338, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7082735238002503}}, {"question": "\u4e0b\u5217\u90a3\u79cd\u60c5\u51b5\u4e0d\u80fd\u7528MMT\u8bc4\u5b9a\nA. \u4e0b\u8fd0\u52a8\u795e\u7ecf\u5143\u635f\u4f24\nB. \u4e0a\u8fd0\u52a8\u795e\u7ecf\u5143\u635f\u4f24\u7684\u5f1b\u7f13\u671f\nC. \u4e0a\u8fd0\u52a8\u795e\u7ecf\u5143\u635f\u4f24\u7684\u6062\u590d\u671f\nD. \u4e0a\u8fd0\u52a8\u795e\u7ecf\u5143\u635f\u4f24\u7684\u75c9\u631b\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b87\u822a\u5458\u5728\u56f4\u7ed5\u5730\u7403\u505a\u5300\u901f\u5706\u5468\u8fd0\u52a8\u7684\u822a\u5929\u98de\u673a\u4e2d\uff0c\u5904\u4e8e\u5b8c\u5168\u5931\u91cd\u72b6\u6001\uff0c\u90a3\u4e48\u4ee5\u4e0b\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u9009\u9879\u662f\nA. \u5b87\u822a\u5458\u4e0d\u53d7\u91cd\u529b\u4f5c\u7528\nB. \u5b87\u822a\u5458\u53ea\u53d7\u91cd\u529b\u7684\u4f5c\u7528\nC. \u5b87\u822a\u5458\u53d7\u5230\u5e73\u8861\u529b\u7684\u4f5c\u7528\nD. \u5b87\u822a\u5458\u6240\u53d7\u7684\u91cd\u529b\u4ea7\u751f\u5411\u5fc3\u52a0\u901f\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2014\u5e747\u670815\u65e5\uff0c\u91d1\u7816\u56fd\u5bb6\u9886\u5bfc\u4eba\u7b2c\u516d\u6b21\u4f1a\u6664\u5728\u5df4\u897f\u798f\u5854\u83b1\u8428\u4e3e\u884c\u3002\u4e94\u56fd\u9886\u5bfc\u4eba\u56f4\u7ed5\u201c\u5b9e\u73b0\u5305\u5bb9\u6027\u589e\u957f\u7684\u53ef\u6301\u7eed\u89e3\u51b3\u65b9\u6848\u201d\u4e3b\u9898\uff0c\u5c31\u5f53\u524d\u4e16\u754c\u7ecf\u6d4e\u5f62\u52bf\u3001\u56fd\u9645\u653f\u6cbb\u5b89\u5168\u95ee\u9898\u4ea4\u6362\u610f\u89c1\uff0c\u8fbe\u6210\u5e7f\u6cdb\u5171\u8bc6\uff0c\u53d6\u5f97\u91cd\u8981\u6210\u679c\u3002\u4e0b\u9762\u5bf9\u4e8e\u201c\u91d1\u7816\u56fd\u5bb6\u201d\u7406\u89e3\u6b63\u786e\u7684\u662f\nA. \u5df4\u897f\u3001\u5370\u5ea6\u3001\u4e2d\u56fd\u662f\u591a\u6781\u5316\u4e2d\u7684\u91cd\u8981\u4e00\u6781\nB. \u91d1\u7816\u56fd\u5bb6\u201d\u662f\u4e00\u4e2a\u533a\u57df\u4e00\u4f53\u5316\u7ecf\u6d4e\u7ec4\u7ec7\nC. \u4e2d\u56fd\u5c06\u968f\u7740\u56fd\u529b\u7684\u589e\u5f3a\u53ef\u4e3a\u4e16\u754c\u505a\u51fa\u66f4\u5927\u7684\u8d21\u732e\nD. \u56fd\u9645\u7ade\u4e89\u65e5\u76ca\u6fc0\u70c8\uff0c\u4f7f\u4e16\u754c\u8d8a\u6765\u8d8a\u52a8\u8361\u4e0d\u5b89\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5386460863920159, "meta-math/MetaMath-Mistral-7B": 0.7585408816165218, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6991354928680856, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6278602458197603, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.547896672846641}}, {"question": "\u5df4\u91d1\u7684\u5c0f\u8bf4\u300a\u5bb6\u300b\u6700\u521d\u53d1\u8868\u4e8e\nA. 20\u4e16\u7eaa20\u5e74\u4ee3\u540e\u671f\nB. 20\u4e16\u7eaa30\u5e74\u4ee3\u540e\u671f\nC. 20\u4e16\u7eaa30\u5e74\u4ee3\u521d\u671f\nD. 20\u4e16\u7eaa20\u5e74\u4ee3\u521d\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36052080752550003, "meta-math/MetaMath-Mistral-7B": 0.49344398112330995, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5560487754489158, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.35347162922091135, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679c\u6309\u53d8\u5740\u2f45\u5f0f\u8bfb\u53d6\u64cd\u4f5c\u6570\uff0c\u5219\u6709\u6548\u5730\u5740\u662f\u6307\nA. \u53d8\u5740\u8ba1\u7b97\u83b7\u5f97\u7684\u5730\u5740\nB. \u57fa\u5740\u5bc4\u5b58\u5668\u4e2d\u5b58\u653e\u7684\u5730\u5740\nC. \u6307\u4ee4\u4e2d\u76f4\u63a5\u7ed9\u51fa\u7684\u5730\u5740\nD. \u53d8\u5740\u5bc4\u5b58\u5668\u4e2d\u5b58\u653e\u7684\u5730\u5740\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6416602220831158, "meta-math/MetaMath-Mistral-7B": 0.8414218788000853, "itpossible/Chinese-Mistral-7B-v0.1": 0.37480846975763354, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.745664842219304, "meta-llama/Meta-Llama-3-8B": 0.5898528489687563, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8114534494631416}}, {"question": "\u201c\u5f92\u5584\u4e0d\u8db3\u4ee5\u4e3a\u653f\uff0c\u5f92\u6cd5\u4e0d\u8db3\u4ee5\u81ea\u884c\u3002\u201d\u8fd9\u8bf4\u660e\u4e86\nA. \u6cd5\u7684\u5c40\u9650\u6027\nB. \u6cd5\u7684\u4e0d\u786e\u5b9a\u6027\nC. \u6cd5\u7684\u9636\u7ea7\u6027\nD. \u6cd5\u7684\u7ee7\u627f\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8480134695882089, "meta-math/MetaMath-Mistral-7B": 0.9803596032774984, "itpossible/Chinese-Mistral-7B-v0.1": 0.7553384866708841, "HuggingFaceH4/zephyr-7b-beta": 0.9997768692142198, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.987112395392982, "meta-llama/Meta-Llama-3-8B": 0.9333982273248684, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6306588885911427}}, {"question": "\u8bbe\u4e8b\u4ef6 $A\uff0c B$ \u540c\u65f6\u53d1\u751f\u65f6\uff0c \u4e8b\u4ef6 $C$ \u4e00\u5b9a\u53d1\u751f\uff0c \u5219 ( )\nA. $P(C) \\geq P(A)+P-1$\nB. $P=P(A B)$\nC. $P(C) \\leq P+P(B)-1$\nD. $P(C)=P(A \\cup B)$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6d88\u8d39\u8005\u5c06\u5176\u4e2a\u4eba\u53ef\u652f\u914d\u6536\u5165\u5728\u6d88\u8d39\u548c\u50a8\u84c4\u4e4b\u95f4\u5206\u914d\uff0c\u662f\u6d88\u8d39\u8005\u7684\nA. \u8d44\u6e90\u518d\u5206\u914d\u9009\u62e9\nB. \u8d2d\u4e70\u9009\u62e9\nC. \u8d44\u6e90\u521d\u6b21\u5206\u914d\u9009\u62e9\nD. \u8d44\u6e90\u6d88\u8d39\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u82cf\u683c\u62c9\u5e95\u4e3b\u5f20\uff0c\u201c\u51e1\u662f\u4e3a\u4e00\u4e2a\u4eba\u81ea\u5df1\u7684\u7406\u667a\u6240\u5ba3\u5224\u4e3a\u9519\u8bef\u7684\u4e1c\u897f\uff0c\u5c31\u4e0d\u5e94\u8be5\u53bb\u60f3\u3001\u4e0d\u5e94\u8be5\u53bb\u505a\uff0c\u54ea\u6015\u53d7\u5230\u5f53\u6743\u8005\u6216\u4efb\u4f55\u6cd5\u5ead\u7684\u5f3a\u8feb\uff0c\u4e5f\u8981\u4e0d\u60dc\u4efb\u4f55\u4ee3\u4ef7\u4e88\u4ee5\u62b5\u5236\u3002\u201d\u5176\u4e3b\u8981\u610f\u56fe\u662f\u9f13\u52b1\u4eba\u4eec\nA. \u4fdd\u6301\u6000\u7591\u7cbe\u795e\u3001\u6279\u5224\u773c\u5149\nB. \u53d1\u73b0\u81ea\u6211\uff0c\u8ffd\u6c42\u4eba\u8eab\u81ea\u7531\nC. \u53cd\u6297\u96c5\u5178\u6c11\u4e3b\u653f\u6cbb\u548c\u6cd5\u5f8b\nD. \u4ee5\u4e2a\u4eba\u611f\u53d7\u4f5c\u4e3a\u4ef7\u503c\u6807\u51c6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8327231768860606, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9968684376916728}}, {"question": "\u4e0b\u5217\u9009\u9879\u5c5e\u4e8eB\u65cf\u7ef4\u751f\u7d20\u7684\u662f\nA. \u89c6\u9ec4\u919b\nB. \u751f\u80b2\u915a\nC. \u6838\u9ec4\u7d20\nD. \u80c6\u56fa\u9187\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3939287965633906, "HuggingFaceH4/zephyr-7b-beta": 0.6011171625013711, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.532531112295531}}, {"question": "\u6839\u636e\u57fa\u8377\u673a\u7ec4\u7684\u8bf4\u6cd5\uff0c\u4e0b\u9762\u6b63\u786e\u7684\u662f\nA. \u6838\u7535\u673a\u7ec4\u627f\u62c5\nB. \u706b\u7535\u673a\u7ec4\u53ef\u4ee5\u627f\u62c5\nC. \u71c3\u6c14\u8f6e\u673a\u7ec4\u53ef\u4ee5\u627f\u62c5\nD. \u7ecf\u6d4e\u6027\u597d\u7684\u6c34\u7535\u673a\u7ec4\u53ef\u4ee5\u627f\u62c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.36150852560561064}}, {"question": "\u201c\u79d1\u5b66\u7ba1\u7406\u4e4b\u7236\u201d\u662f\u6307\nA. \u6cf0\u52d2\nB. \u6b27\u6587\nC. \u6cd5\u7ea6\u5c14\nD. \u97e6\u4f2f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.9640450320061394, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9369413355526035, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6307443974873569}}, {"question": "\u5fae\u578b\u8ba1\u7b97\u673a\u7684\u5185\u5b58\u5bb9\u91cf\u4e3b\u8981\u6307\uff08 \uff09\u7684\u5bb9\u91cf\nA. RAM\nB. ROM\nC. CMOS\nD. Cache\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7028857180223405, "meta-math/MetaMath-Mistral-7B": 0.8372633091244865, "itpossible/Chinese-Mistral-7B-v0.1": 0.7215580034498073, "HuggingFaceH4/zephyr-7b-beta": 0.999435685736593, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9326442115461757, "meta-llama/Meta-Llama-3-8B": 0.9658548575040646, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9996473128625672}}, {"question": "\u4e0b\u5217\u7269\u8d28\u4e2d\uff0c\u53ef\u4ee5\u7528\u6765\u5438\u9644\u4e59\u70ef\u7684\u662f\nA. \u9ad8\u9530\u9178\u94be\nB. \u6d88\u77f3\u7070\nC. \u78b3\u9178\u94be\nD. \u78b1\u6eb6\u6db2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2920177551543942, "meta-math/MetaMath-Mistral-7B": 0.3994860964260413, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3520821795518118, "meta-llama/Meta-Llama-3-8B": 0.40397671198254576, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6408866383033657}}, {"question": "\u6559\u80b2\u8d77\u6e90\u4e8e()\nA. \u4eba\u7c7b\u7684\u751f\u4ea7\u52b3\u52a8\nB. \u751f\u7269\u754c\u5e74\u957f\u52a8\u7269\u5bf9\u5e7c\u5c0f\u52a8\u7269\u7684\u7167\u6599\nC. \u4eba\u7c7b\u65e9\u671f\u7684\u5fc3\u7406\u6a21\u4eff\nD. \u95ee\u9898\u8fd8\u6709\u5f85\u8fdb\u4e00\u6b65\u7814\u7a76\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33121905446883504, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5597097093846791, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5401621751871098, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f53\u8d28\u6307\u6570(BMI)26\u8005\u4e3a\nA. \u6d88\u7626\nB. \u4e2d\u5ea6\u80a5\u80d6\nC. \u8f7b\u5ea6\u80a5\u80d6\nD. \u6b63\u5e38\u8303\u56f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4907382516513249, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5011174347098099, "meta-llama/Meta-Llama-3-8B": 0.43477594177030093, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5228067533510005}}, {"question": "\u559c\u6b22\u51b7\u9762\u3001\u6253\u7cd5\u3001\u677e\u997c\u5e76\u559c\u6b22\u4ee5\u6c64\u3001\u9171\u3001\u6ce1\u83dc\u4e3a\u526f\u98df\u7684\u6c11\u65cf\u662f\nA. \u671d\u9c9c\u65cf\nB. \u767d\u65cf\nC. \u6ee1\u65cf\nD. \u8d6b\u54f2\u65cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.9385680397784733, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8786866043619966, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.990679325530231}}, {"question": "\u5c40\u57df\u2f79\u91c7\u2f64\u7684\u53cc\u7ede\u7ebf\u4e3a\nA. 6\u7c7bUTP\nB. 3\u7c7bUTP\nC. 4\u7c7bUTP\nD. 5\u7c7bUTP\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5\u5173\u4e8ex\u7684\u5206\u5f0f\u2f45\u7a0b$\\frac{x}{x-1}-2=\\frac{k}{1-x}$\u7684\u89e3\u4e3a\u6b63\u6570\uff0c\u5219k\u7684\u53d6\u503c\u8303\u56f4\u4e3a\nA. k\uff1e-2 \nB. k\uff1e-2\u4e14k\u2260-1\nC. -2\uff1ck\uff1c0\nD. k\uff1c2\u4e14k\u22601\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.37993597849898714, "itpossible/Chinese-Mistral-7B-v0.1": 0.33053091950088104, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3925874646159021, "meta-llama/Meta-Llama-3-8B": 0.26560468668687814, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9093\u8d24\u5728\u300a\u51b3\u5b9a\u4e2d\u56fd\u547d\u8fd0\u7684700\u5929\u300b\u4e00\u4e66\u4e2d\u8fd9\u6837\u63cf\u8ff0\uff1a\u201c(\u5f3a\u6e21\u9ec4\u6cdb\u533a\u4ee5\u540e)\u6700\u60ca\u9669\u7684\u4e00\u5e55\u53d1\u751f\u5728\u6dee\u6cb3\u5cb8\u8fb9\u3002\u6e21\u53e3\u6ca1\u6709\u8239\uff0c\u4e5f\u6ca1\u6709\u6865\uff0c\u6cb3\u6c34\u53c8\u6df1\u53c8\u6025\u96be\u4ee5\u5f92\u6d89\uff0c\u5c3e\u968f\u7684\u8ffd\u5175\u5df2\u7ecf\u8d76\u6765\uff0c\u540e\u536b\u963b\u51fb\u6218\u5168\u9762\u7206\u53d1\u3002\u7d27\u6025\u4e4b\u4e2d\uff0c\u53f8\u4ee4\u5458\u5218\u4f2f\u627f\u4eb2\u81ea\u591c\u63a2\u6dee\u6cb3\uff0c\u7ecf\u8fc7\u5bfb\u8bbf\u7adf\u7136\u627e\u5230\u4e00\u5904\u5f53\u5730\u4eba\u8fc7\u6cb3\u7684\u6d45\u6ee9\uff0c\u4e3a\u5927\u519b\u5f00\u8f9f\u4e00\u6761\u751f\u8def\u3002\u201d\u8fd9\u6bb5\u63cf\u8ff0\u6240\u53cd\u6620\u7684\u5386\u53f2\u4e8b\u4ef6\nA. \u53d6\u5f97\u4e86\u6e21\u6c5f\u6218\u5f79\u7684\u51b3\u5b9a\u6027\u80dc\u5229\nB. \u57fa\u672c\u6d88\u706d\u4e86\u56fd\u6c11\u515a\u519b\u961f\u7684\u4e3b\u529b\nC. \u8d62\u5f97\u4e86\u8fd1\u767e\u5e74\u6765\u7b2c\u4e00\u6b21\u53cd\u4fb5\u7565\u6597\u4e89\u7684\u80dc\u5229xkb\nD. \u63ed\u5f00\u4e86\u4eba\u6c11\u89e3\u653e\u519b\u6218\u7565\u53cd\u653b\u7684\u5e8f\u5e55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9189814469883151, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b8b\u660e\u7406\u5b66\u666e\u904d\u5021\u5bfc\u201c\u5b58\u7406\u53bb\u6b32\u201d\u7684\u4fee\u517b\u8bba\uff0c\u201c\u683c\u7269\u201d\u7684\u8ba4\u8bc6\u8bba\uff0c\u201c\u6210\u8d24\u6210\u5723\u201d\u7684\u5883\u754c\u8bba\uff0c\u201c\u9f50\u5bb6\u5e73\u5929\u4e0b\u201d\u7684\u529f\u540d\u8bba\uff0c\u5176\u6839\u672c\u51fa\u53d1\u70b9\u662f\nA. \u57f9\u517b\u7ecf\u4e16\u81f4\u7528\u7684\u4eba\u624d\nB. \u6811\u7acb\u7406\u5b66\u7684\u7edf\u6cbb\u5730\u4f4d\nC. \u4fee\u8eab\u517b\u6027\uff0c\u63d0\u9ad8\u4e2a\u4eba\u4fee\u517b\nD. \u89c4\u8303\u793e\u4f1a\u79e9\u5e8f\uff0c\u5b9e\u73b0\u793e\u4f1a\u548c\u8c10\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4fee\u8def\u961f\u4fee\u4e00\u6bb5\u8def\uff0c\u7b2c\u4e00\u5929\u4fee\u4e86\u5168\u7a0b\u76842/5\uff0c\u7b2c\u4e8c\u5929\u4fee\u4e86240\u7c73\uff0c\u5b8c\u6210\u4e86\u5168\u90e8\u4fee\u8def\u4efb\u52a1\uff0c\u7b2c\u4e00\u5929\u4fee\u4e86\u591a\u5c11\u7c73\uff0c\u6b63\u786e\u7684\u7b97\u5f0f\u662f\nA. 240/(1-2/5)\nB. 240/(1-2/5)*(2/5)\nC. 240/(1+2/5)\nD. 240*(1+2/5)\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.29473196255740886, "meta-math/MetaMath-Mistral-7B": 0.43137938225753636, "itpossible/Chinese-Mistral-7B-v0.1": 0.44334536548408165, "HuggingFaceH4/zephyr-7b-beta": 0.6784187751552203, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.30601362565976303, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9505\u7089\u5b89\u5168\u9600\u7684\u68c0\u9a8c\u5468\u671f\u4e3a\nA. \u534a\u5e74\nB. 2\u5e74\nC. 3\u5e74\nD. 1\u5e74\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.546814614439449, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.46183045580841453, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "35KV\u4e2d\u6027\u70b9\u4e0d\u63a5\u5730\u7cfb\u7edf\uff0c\u6b63\u5e38\u8fd0\u884c\u65f6\uff0c\u4e09\u76f8\u5bf9\u5730\u7535\u5bb9\u7535\u6d41\u5747\u4e3a10\uff0c\u5f53A\u76f8\u53d1\u751f\u91d1\u5c5e\u6027\u63a5\u5730\u65f6\uff0cA\u76f8\u63a5\u5730\u7535\u6d41\u4e3a\nA. 30A\nB. 15A\nC. 10A\nD. 20A\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31541697987042605, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.3527247741233186, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6117450621057695, "meta-llama/Meta-Llama-3-8B": 0.31483005318115603, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679c\u8fde\u7eed\u591a\u4ee3\u56de\u4ea4\uff0c\u5219\u56de\u4ea4\u540e\u4ee3\u57fa\u56e0\u578b\u5c06\u8d8b\u4e8e\nA. \u6742\u79cdF1\nB. \u6bcd\u672c\nC. \u975e\u8f6e\u56de\u4eb2\u672c\nD. \u8f6e\u56de\u4eb2\u672c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31975602452905333, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.39083046067037613, "HuggingFaceH4/zephyr-7b-beta": 0.783723525428538, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.555363562967974, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6570543379278662}}, {"question": "\u5316\u5b66\u4e0e\u6750\u6599\u3001\u751f\u6d3b\u548c\u73af\u5883\u5bc6\u5207\u76f8\u5173\uff0c\u4e0b\u5217\u6709\u5173\u8bf4\u6cd5\u4e2d\u9519\u8bef\u7684\u662f\nA. \u5927\u529b\u5b9e\u65bd\u77ff\u7269\u71c3\u6599\u8131\u786b\u8131\u785d\u6280\u672f\uff0c\u80fd\u51cf\u5c11\u786b\u3001\u6c2e\u6c27\u5316\u7269\u7684\u6392\u653e\nB. \u660e\u77fe\u51c0\u6c34\u65f6\u53d1\u751f\u4e86\u5316\u5b66\u53ca\u7269\u7406\u53d8\u5316\uff0c\u80fd\u8d77\u5230\u51c0\u6c34\u4f5c\u7528\uff0c\u800c\u6ca1\u6709\u6740\u83cc\u3001\u6d88\u6bd2\u7684\u4f5c\u7528\nC. \u67d0\u65b0\u578b\u822a\u5929\u670d\u6750\u6599\u4e3b\u8981\u6210\u5206\u662f\u7531\u78b3\u5316\u7845\u3001\u9676\u74f7\u548c\u78b3\u7ea4\u7ef4\u590d\u5408\u800c\u6210\uff0c\u5b83\u662f\u4e00\u79cd\u65b0\u578b\u65e0\u673a\u975e\u91d1\u5c5e\u6750\u6599\nD. \u98df\u54c1\u888b\u4e2d\u5e38\u653e\u6709\u7845\u80f6\u548c\u94c1\u7c89\uff0c\u90fd\u80fd\u8d77\u5230\u5e72\u71e5\u7684\u4f5c\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5340363637269787, "meta-math/MetaMath-Mistral-7B": 0.5818229279946404, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4268441117365493, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.95284839524521}}, {"question": "2007\u5e741\u6708\uff0c\u6b63\u5f0f\u5c06\u9632\u536b\u5385\u5347\u683c\u4e3a\u9632\u536b\u7701\uff0c\u8fc8\u51fa\u7a81\u7834\u201c\u548c\u5e73\u5baa\u6cd5\u201d\u5173\u952e\u4e00\u6b65\u7684\u662f\u4ec0\u4e48\u5185\u9601\nA. \u5b89\u500d\u5185\u9601\nB. \u5c0f\u6cc9\u5185\u9601\nC. \u798f\u7530\u5185\u9601\nD. \u6e21\u8fb9\u5185\u9601\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5165797095148945, "meta-math/MetaMath-Mistral-7B": 0.6837774182769837, "itpossible/Chinese-Mistral-7B-v0.1": 0.7213857231148938, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.36150852560561064, "meta-llama/Meta-Llama-3-8B": 0.5632763502929296, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u4e2d\u592e\u519b\u4e8b\u59d4\u5458\u4f1a\u4e0e\u4e2d\u56fd\u5171\u4ea7\u515a\u4e2d\u592e\u519b\u4e8b\u59d4\u5458\u4f1a\u7684\u5173\u7cfb\u662f\nA. \u4e00\u4e2a\u673a\u6784\uff0c\u4e24\u4e2a\u540d\u79f0\nB. \u4e00\u4e2a\u673a\u6784\uff0c\u4e00\u4e2a\u540d\u79f0\nC. \u4e24\u4e2a\u673a\u6784\uff0c\u4e24\u4e2a\u540d\u79f0\nD. \u4e24\u4e2a\u673a\u6784\uff0c\u4e00\u4e2a\u540d\u79f0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2925894411256113, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4424193552192134, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u5728\u5730\u7403\u8868\u9762\u89c2\u6d4b\uff0c\u4f7f\u7528\u5929\u6587\u671b\u8fdc\u955c\u65e0\u6cd5\u770b\u5230\u7684\u662f\nA. \u6c34\u661f\u76f8\u4f4d\nB. \u592a\u9633\u65e5\u5195\nC. \u6708\u9762\u83ab\u65af\u79d1\u6d77\nD. \u706b\u661f\u536b\u661f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u8bf4\u6cd5\u6b63\u786e\u7684\u9009\u9879\u662f\nA. \u628a1Kg\u7684\u7269\u4f53\u5300\u901f\u4e3e\u9ad81m\uff0c\u4e3e\u529b\u505a\u529f\u4e3a1J\nB. \u529b\u5bf9\u7269\u4f53\u4e0d\u505a\u529f\uff0c\u8bf4\u660e\u7269\u4f53\u6ca1\u6709\u4f4d\u79fb\nC. \u628a\u91cd1N\u7684\u7269\u4f53\u5300\u901f\u4e3e\u9ad81m\uff0c\u514b\u670d\u91cd\u529b\u505a\u529f\u4e3a1J\nD. \u529b\u5bf9\u7269\u4f53\u505a\u529f\u8d8a\u591a\uff0c\u8bf4\u660e\u7269\u4f53\u6240\u53d7\u7684\u529b\u8d8a\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.49294317718054964, "meta-math/MetaMath-Mistral-7B": 0.5246138729305269, "itpossible/Chinese-Mistral-7B-v0.1": 0.42426315972413386, "HuggingFaceH4/zephyr-7b-beta": 0.9950997232313876, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6396525413248012, "meta-llama/Meta-Llama-3-8B": 0.5998024648933816, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.749017130295649}}, {"question": "\u4fee\u8def\u961f\u4fee\u4e00\u6bb5\u8def\uff0c\u7b2c\u4e00\u5929\u4fee\u4e86\u5168\u7a0b\u76841/4\uff0c\u7b2c\u4e8c\u5929\u4fee\u4e86\u5168\u7a0b\u76841/5\uff0c\u5df2\u77e5\u7b2c\u4e00\u5929\u4e0e\u7b2c\u4e8c\u5929\u4e00\u5171\u4fee\u4e861800\u7c73\uff0c\u8fd9\u6bb5\u8def\u7684\u5168\u957f\u662f()\u7c73\nA. 1800/(1/4+1/5)\nB. 1800/(1/4\u20141/5)\nC. 1800*(1/4+1/5)\nD. 1800/(1\u20141/4\u20141/5)\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.40434616446529315, "meta-math/MetaMath-Mistral-7B": 0.6869711912995837, "itpossible/Chinese-Mistral-7B-v0.1": 0.47206039094314045, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4345817568835351, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7181548997900271}}, {"question": "\u8102\u80aa\u662f\u5bb6\u5154\u80fd\u91cf\u6765\u6e90\u548c\u6c89\u79ef\u4f53\u8102\u7684\u8425\u517b\u7269\u8d28\u4e4b\u4e00\u3002\u5bb6\u5154\u65e5\u7cae\u4e2d\u7c97\u8102\u80aa\u7684\u542b\u91cf\u5e94\u4e3a\nA. 1\uff5e\uff12\uff05\nB. 2\uff5e3\uff05\nC. 4\uff5e5\uff05\nD. 3\uff5e4\uff05\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2920177551543942, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.308176776288574, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3949756319152888, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5\u96c6\u5408$A={x|y=\\sqrt{4-x^4}}$\uff0c\u96c6\u5408$B={x|x>=a}$\uff0c\u5219$A\\subseteq B$\u7684\u4e00\u4e2a\u5145\u5206\u4e0d\u5fc5\u8981\u6761\u4ef6\u662f\nA. (-\\inf,-2)\nB. [2,+\\inf)\nC. (-\\inf,-2]\nD. (2,+?)\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679c\u7ba1\u7406\u8005\u80fd\u591f\u9884\u6d4b\u51fa\u6267\u884c\u51b3\u7b56\u5c06\u6765\u53ef\u80fd\u4f1a\u5f97\u51fa\u51e0\u79cd\u7ed3\u679c\u548c\u6bcf\u79cd\u7ed3\u679c\u7684\u6982\u7387\u662f\u591a\u5c11\uff0c\u8fd9\u79cd\u6761\u4ef6\u4e0b\u7684\u51b3\u7b56\u53eb\u505a\nA. \u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u7684\u51b3\u7b56\nB. \u6709\u98ce\u9669\u6761\u4ef6\u4e0b\u7684\u51b3\u7b56\nC. \u786e\u5b9a\u6761\u4ef6\u4e0b\u7684\u51b3\u7b56\nD. \u65e0\u98ce\u9669\u6761\u4ef6\u4e0b\u7684\u51b3\u7b56\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6386078179424369, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9991170660280735, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.507341694976629, "meta-llama/Meta-Llama-3-8B": 0.6064368185957144, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c32\u5c81\u3002\u996e\u9152\u540e\u4e2d\u4e0a\u8179\u6301\u7eed\u6027\u75bc\u75db6\u5c0f\u65f6\uff0c\u5e76\u9010\u6e10\u52a0\u5267\uff0c\u5411\u80a9\u3001\u80cc\u90e8\u653e\u5c04\uff0c\u4f34\u6076\u5fc3\u3001\u5455\u5410\u3002\u67e5\u4f53\uff1aP118\u6b21/\u5206\uff0cBP90/75mmHg\uff0c\u6025\u6027\u9762\u5bb9\uff0c\u8868\u60c5\u75db\u82e6\uff0c\u5168\u8179\u538b\u75db\uff0c\u5c24\u4ee5\u4e2d\u4e0a\u8179\u4e3a\u8457\uff0c\u8f7b\u5ea6\u808c\u7d27\u5f20\u548c\u53cd\u8df3\u75db\uff0c\u809d\u533a\u672a\u89e6\u53ca\u80bf\u5757\u3002\u5916\u5468\u8840WBC15\u00d7109/L\uff0c\u4e2d\u6027\u7c92\u7ec6\u80de81%\u3002\u660e\u786e\u8bca\u65ad\u540e\uff0c\u5bf9\u51b3\u5b9a\u6cbb\u7597\u6700\u6709\u4ef7\u503c\u7684\u8bca\u65ad\u68c0\u67e5\u662f\nA. \u8179\u90e8B\u8d85\nB. \u8840\u3001\u5c3f\u6dc0\u7c89\u9176\u6d4b\u5b9a\nC. \u8840\u9499\u6d4b\u5b9a\nD. \u8179\u90e8\u589e\u5f3aCT\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8613258220125634, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4577567506435653, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u4f8b\u53e5\u201c\u95ee\u201d\u5b57\u7528\u5176\u672c\u4e49\u7684\u662f\nA. \u662d\u738b\u5357\u5f81\u800c\u4e0d\u590d\uff0c\u5be1\u4eba\u662f\u95ee\u3002\nB. \u65e2\u800c\u7f81\u5bd3\u4eac\u5e08\uff0c\u4e45\u65e0\u5bb6\u95ee\u3002\nC. \u662d\u738b\u4e4b\u4e0d\u590d\uff0c\u541b\u5176\u95ee\u8bf8\u6c34\u6ee8\u3002\nD. \u4f2f\u725b\u6709\u75be\uff0c\u5b50\u95ee\u4e4b\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.32486943359923176, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.40101787230328095}}, {"question": "\u5728\u4e00\u6b21\u5fc3\u7406\u77e5\u8bc6\u6d4b\u8bd5\u4e2d\uff0c\u5173\u4e8e\u77ed\u65f6\u8bb0\u5fc6\u7684\u5bb9\u91cf\u5355\u4f4d\uff0c\u5b66\u751f\u4eec\u7684\u7b54\u6848\u6d89\u53ca\u4ee5\u4e0b\u56db\u79cd\uff0c\u5176\u4e2d\u6b63\u786e\u7684\u9009\u9879\u662f\nA. \u6bd4\u7279\nB. \u5b57\u8282\nC. \u8bcd\u7ec4\nD. \u7ec4\u5757\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4588066873620157, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53d1\u751f\u5730\u9707\u540e\u5e94\u4e0d\u987e\u4e00\u5207\u7684\u4fdd\u62a4\u54ea\u4e2a\u90e8\u4f4d\nA. \u80f8\u90e8\nB. \u9888\u90e8\nC. \u8179\u90e8\nD. \u5934\u90e8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5109596852532194, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.999913828449703, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.755895829001925, "meta-llama/Meta-Llama-3-8B": 0.8430603565233286, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.44661210736942575}}, {"question": "\u201c\u6df1\u6d45\u201d\u8fd9\u4e2a\u8bcd\u7531\u201c\u6df1\u201d\u4e0e\u201c\u6d45\u201d\u8fd9\u4e00\u5bf9\u53cd\u4e49\u8bcd\u7ec4\u6210\uff0c\u4e0b\u9762\u8bcd\u8bed\u4e2d\u4e0e\u5b83\u76f8\u8fd1\u7684\u4e00\u7ec4\u662f\nA. \u6b63\u53cd \u524d\u540e \u5feb\u6162 \u826f\u597d \u8f93\u8d62\nB. \u597d\u574f \u6b7b\u6d3b \u540e\u9000 \u6625\u79cb \u66f2\u76f4\nC. \u4e0a\u4e0b \u4e2d\u5916 \u6c34\u706b \u7ea2\u7eff \u957f\u8fdc\nD. \u51fa\u5165 \u5f00\u5173 \u539a\u8584 \u4e1c\u897f \u88c5\u5378\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u533a\u57df\u7ecf\u6d4e\u96c6\u56e2\u7ec4\u7ec7\u4e2d\uff0c\u7ecf\u6d4e\u4e00\u4f53\u5316\u7a0b\u5ea6\u6700\u9ad8\u7684\u662f\nA. \u6b27\u6d32\u8054\u76df\nB. \u4e1c\u5357\u4e9a\u56fd\u5bb6\u8054\u76df\nC. \u5317\u7f8e\u81ea\u7531\u8d38\u6613\u533a\nD. \u897f\u975e\u56fd\u5bb6\u7ecf\u6d4e\u5171\u540c\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43713229525595415, "meta-math/MetaMath-Mistral-7B": 0.6119269839428713, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5696282611728103, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5340705836233488, "meta-llama/Meta-Llama-3-8B": 0.8878179130011536, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9108397503522502}}, {"question": "\u5973\u6027\uff0c19\u5c81\u3002\u6d77\u5916\u7559\u5b66\u56de\u56fd\u4f11\u5047\uff0c\u56e0\u6708\u7ecf\u4e0d\u8c03\u5c31\u8bca\u3002\u60a3\u8005\u8bc9\u51fa\u56fd\u540e\u6708\u7ecf\u4e0d\u6b63\u5e38\uff0c\u62c5\u5fc3\u56e0\u4e0d\u9002\u5e94\u5f53\u5730\u6c14\u5019\u8eab\u4f53\u51fa\u73b0\u95ee\u9898\u3002\u6b64\u65f6\u6700\u4f73\u7684\u6c9f\u901a\u662f\nA. \u5173\u6ce8\u6708\u7ecf\u5f02\u5e38\u60c5\u51b5\u5e76\u4e88\u4ee5\u8bca\u6cbb\nB. \u5173\u6ce8\u6708\u7ecf\u60c5\u51b5\uff0c\u4e86\u89e3\u60a3\u8005\u5728\u56fd\u5916\u7684\u5fc3\u7406\u72b6\u51b5\u5e76\u4e88\u4ee5\u8bca\u6cbb\nC. \u5173\u6ce8\u6708\u7ecf\u60c5\u51b5\uff0c\u4e86\u89e3\u60a3\u8005\u5728\u56fd\u5916\u7684\u793e\u4f1a\u9002\u5e94\u6027\u72b6\u51b5\u5e76\u4e88\u4ee5\u8bca\u6cbb\nD. \u5173\u6ce8\u6708\u7ecf\u60c5\u51b5\uff0c\u4e86\u89e3\u60a3\u8005\u5728\u56fd\u5916\u7684\u5fc3\u7406\u548c\u793e\u4f1a\u9002\u5e94\u6027\u72b6\u51b5\u5e76\u4e88\u4ee5\u8bca\u6cbb\u548c\u5b89\u6170\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9589121092566435, "meta-math/MetaMath-Mistral-7B": 0.9973831449943982, "itpossible/Chinese-Mistral-7B-v0.1": 0.8602799738193391, "HuggingFaceH4/zephyr-7b-beta": 0.9999405701570804, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9680083987024082, "meta-llama/Meta-Llama-3-8B": 0.877865038679288, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8403583027540181}}, {"question": "\u603b\u8d26\u548c\u5e8f\u65f6\u8d26\u4e00\u822c\u5e94\u91c7\u7528\nA. \u5907\u67e5\u8d26\u7c3f\nB. \u8ba2\u672c\u5f0f\u8d26\u7c3f\nC. \u5361\u7247\u5f0f\u8d26\u7c3f\nD. \u6d3b\u9875\u5f0f\u8d26\u7c3f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.32770783487659344, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u725b\u987f\u73af\u5b9e\u9a8c\u88c5\u7f6e\u662f\u7528\u4e00\u5e73\u51f8\u900f\u955c\u7f6e\u4e8e\u4e00\u5e73\u677f\u73bb\u7483\u4e0a\u3002\u4eca\u4ee5\u5e73\u884c\u5355\u8272\u5149\u4ece\u4e0a\u5411\u4e0b\u5782\u76f4\u5165\u5c04\uff0c\u5e76\u4ece\u4e0a\u5411\u4e0b\u89c2\u5bdf\uff0c\u770b\u5230\u6709\u8bb8\u591a\u660e\u6697\u76f8\u95f4\u7684\u540c\u5fc3\u5706\u73af\uff0c\u8fd9\u4e9b\u5706\u73af\u7684\u7279\u70b9\u4e3a\nA. \u63a5\u89e6\u70b9\u662f\u660e\u7684\uff0c\u540c\u5fc3\u5706\u73af\u662f\u7b49\u8ddd\u79bb\u7684\nB. \u63a5\u89e6\u70b9\u662f\u6697\u7684\uff0c\u540c\u5fc3\u5706\u73af\u662f\u4e0d\u7b49\u8ddd\u79bb\u7684\nC. \u63a5\u89e6\u70b9\u662f\u6697\u7684\uff0c\u540c\u5fc3\u5706\u73af\u662f\u7b49\u8ddd\u79bb\u7684\nD. \u63a5\u89e6\u70b9\u662f\u660e\u7684\uff0c\u540c\u5fc3\u5706\u73af\u662f\u4e0d\u7b49\u8ddd\u79bb\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.34239623393788804, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u8ba1\u7b97\u673a\u4e2d\uff0cbit\u7684\u4e2d\u6587\u542b\u4e49\u662f\nA. \u5b57\u8282\nB. \u5b57\nC. \u4e8c\u8fdb\u5236\u4f4d\nD. \u50cf\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9864481224118155, "meta-math/MetaMath-Mistral-7B": 0.9996326688302196, "itpossible/Chinese-Mistral-7B-v0.1": 0.9682479071337511, "HuggingFaceH4/zephyr-7b-beta": 0.9963985489884334, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9982626518779469, "meta-llama/Meta-Llama-3-8B": 0.9791098492578733, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9987966835067824}}, {"question": "\u6ca5\u9752\u9762\u5c42\u538b\u5b9e\u5ea6\u662f\u6307\nA. \u73b0\u573a\u5b9e\u9645\u5bc6\u5ea6\u4e0e\u5ba4\u5185\u6807\u51c6\u5bc6\u5ea6\u4e4b\u6bd4\nB. \u73b0\u573a\u5b9e\u9645\u5e72\u5bc6\u5ea6\u4e0e\u5ba4\u5185\u6807\u51c6\u5bc6\u5ea6\u4e4b\u6bd4\nC. \u73b0\u573a\u5b9e\u9645\u5e72\u5bc6\u5ea6\u4e0e\u5ba4\u5185\u51fb\u5b9e\u8bd5\u9a8c\u6700\u5927\u5e72\u5bc6\u5ea6\u4e4b\u6bd4\nD. \u73b0\u573a\u5b9e\u9645\u6e7f\u5bc6\u5ea6\u4e0e\u5ba4\u5185\u51fb\u5b9e\u8bd5\u9a8c\u6700\u5927\u6e7f\u5bc6\u5ea6\u4e4b\u6bd4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u5904\u4e8e\u8fd8\u539f\u72b6\u6001\u7684Windows\u5e94\u2f64\u7a0b\u5e8f\u7a97\u2f1d\uff0c\u4e0d\u80fd\u5b9e\u73b0\u7684\u64cd\u4f5c\u662f\nA. \u79fb\u52a8\nB. \u65cb\u8f6c\nC. \u6700\u2f29\u5316\nD. \u6700\u2f24\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2616492824408659, "meta-math/MetaMath-Mistral-7B": 0.4309501337456245, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9380146279388444, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53\u81ea\u7531\u5ea6\u8d8b\u5411\u65e0\u7a77\u5927\uff0c\u4e14$\\pi$\u4e0d\u63a5\u8fd1\u4e8e0\u4e5f\u4e0d\u63a5\u8fd1\u4e8e1\u65f6\uff0c\u4e8c\u9879\u5206\u5e03\u8d8b\u5411\u4e8e\nA. \u6b64\u6001\u5206\u5e03\nB. $t$\u5206\u5e03\nC. $F$\u5206\u5e03\nD. $\\chi^2$\u5206\u5e03\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u66b4\u529b\u5e72\u6d89\u5a5a\u59fb\u81ea\u7531\u7f6a\u7684\u8bf4\u6cd5\uff0c\u6b63\u786e\u7684\u6709\nA. \u66b4\u529b\u5e72\u6d89\u5a5a\u59fb\u81ea\u7531\u7f6a\u662f\u544a\u8bc9\u7684\u624d\u5904\u7406\uff0c\u5373\u4f7f\u88ab\u5bb3\u4eba\u6b7b\u4ea1\uff0c\u4e5f\u53ea\u80fd\u6309\u7167\u81ea\u8bc9\u6848\u4ef6\u5904\u7406\nB. \u66b4\u529b\u5e72\u6d89\u5a5a\u59fb\u81ea\u7531\u7f6a\u4e2d\u81f4\u4f7f\u88ab\u5bb3\u4eba\u6b7b\u4ea1\uff0c\u53ea\u9650\u4e8e\u8fc7\u5931\uff0c\u4e0d\u5305\u62ec\u6545\u610f\u81f4\u4f7f\u88ab\u5bb3\u4eba\u6b7b\u4ea1\u7684\u60c5\u5f62\nC. \u8be5\u7f6a\u7684\u72af\u7f6a\u4e3b\u4f53\u662f\u7279\u6b8a\u4e3b\u4f53\uff0c\u53ea\u80fd\u662f\u548c\u88ab\u5bb3\u4eba\u6709\u4eb2\u5c5e\u5173\u7cfb\u7684\u4eba\nD. \u8be5\u7f6a\u5c5e\u4e8e\u59a8\u5bb3\u793e\u4f1a\u7ba1\u7406\u79e9\u5e8f\u72af\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8092192396593034, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u66f2\u7ebf\u8fd0\u52a8\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u66f2\u7ebf\u8fd0\u52a8\u4e5f\u53ef\u4ee5\u662f\u901f\u5ea6\u4e0d\u53d8\u7684\u8fd0\u52a8\nB. \u53d8\u901f\u8fd0\u52a8\u2f00\u5b9a\u662f\u66f2\u7ebf\u8fd0\u52a8\nC. \u66f2\u7ebf\u8fd0\u52a8\u2f00\u5b9a\u662f\u53d8\u901f\u8fd0\u52a8\nD. \u901f\u7387\u4e0d\u53d8\u7684\u66f2\u7ebf\u8fd0\u52a8\u662f\u5300\u901f\u8fd0\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7b2c\u4e8c\u6b21\u4e16\u754c\u5927\u6218\u540e\u5c40\u90e8\u6218\u4e89\u4e3b\u8981\u53d1\u751f\u5728\nA. \u793e\u4f1a\u4e3b\u4e49\u56fd\u5bb6\u4e4b\u95f4\nB. \u4e9a\u975e\u62c9\u5730\u533a\nC. \u6b27\u6d32\nD. \u793e\u4f1a\u4e3b\u4e49\u540c\u8d44\u672c\u4e3b\u4e49\u56fd\u5bb6\u4e4b\u95f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4358945674046615, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7220500727886657, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9589163718587316}}, {"question": "\u4e2d\u533b\u5b66\u201c\u8bc1\u201d\u7684\u6982\u5ff5\u662f\nA. \u5bf9\u75be\u75c5\u75c7\u72b6\u4e0e\u4f53\u5f81\u7684\u5206\u6790\u8fc7\u7a0b\nB. \u5bf9\u75be\u75c5\u75c7\u72b6\u4e0e\u4f53\u5f81\u7684\u8c03\u67e5\u8fc7\u7a0b\nC. \u75be\u75c5\u7684\u75c7\u72b6\u4e0e\u4f53\u5f81\nD. \u75be\u75c5\u53d1\u5c55\u8fc7\u7a0b\u4e2d\u67d0\u4e00\u9636\u6bb5\u7684\u75c5\u7406\u6982\u62ec\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6274250951215825, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5127221787687647, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6309\u4ea7\u54c1\u5206\u6d3e\u63a8\u9500\u4eba\u5458\u7684\u4f18\u70b9\u662f\u6709\u5229\u4e8e\nA. \u660e\u786e\u63a8\u9500\u4eba\u5458\u8d23\u4efb\nB. \u660e\u786e\u63a8\u9500\u5bf9\u8c61\nC. \u6280\u672f\u6027\u4ea7\u54c1\u7684\u63a8\u9500\nD. \u8282\u7701\u63a8\u9500\u8d39\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6309\u4e3b\u4f53\u6807\u51c6\u5212\u5206\u7684\u5404\u79cd\u6cd5\u5f8b\u610f\u8bc6\u4e2d\uff0c\u6700\u4e3a\u590d\u6742\u7684\u662f\nA. \u7fa4\u4f53\u6cd5\u5f8b\u610f\u8bc6\nB. \u56e2\u4f53\u6cd5\u5f8b\u610f\u8bc6\nC. \u4e2a\u4eba\u6cd5\u5f8b\u610f\u8bc6\nD. \u793e\u4f1a\u6cd5\u5f8b\u610f\u8bc6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5993774291410426, "meta-math/MetaMath-Mistral-7B": 0.7989058472226052, "itpossible/Chinese-Mistral-7B-v0.1": 0.450388688906963, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9524164171601649, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4287707853742485}}, {"question": "\u4e0b\u5217\u8868\u8ff0\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u56fd\u5bb6\u5f3a\u5236\u529b\u662f\u4fdd\u8bc1\u6cd5\u7684\u5b9e\u65bd\u7684\u552f\u4e00\u529b\u91cf\nB. \u4e3a\u4eba\u4eec\u7684\u884c\u4e3a\u63d0\u4f9b\u6a21\u5f0f\u3001\u6807\u51c6\u6216\u65b9\u5411\uff0c\u8fd9\u4e00\u7279\u6027\u6307\u7684\u662f\u6cd5\u7684\u7edf\u4e00\u6027\nC. \u6cd5\u662f\u8c03\u6574\u4eba\u4eec\u884c\u4e3a\u7684\u89c4\u8303\u548c\u6cd5\u662f\u8c03\u6574\u793e\u4f1a\u5173\u7cfb\u7684\u89c4\u8303\uff0c\u8fd9\u4e24\u79cd\u8bf4\u6cd5\u6ca1\u6709\u672c\u8d28\u4e0a\u7684\u533a\u522b\nD. \u975e\u89c4\u8303\u6027\u6cd5\u5f8b\u6587\u4ef6\u867d\u7136\u6ca1\u6709\u89c4\u8303\u6027\uff0c\u4f46\u6709\u6cd5\u5f8b\u6548\u529b\uff0c\u56e0\u800c\u4e5f\u5c5e\u4e8e\u6cd5\u5f8b\u7684\u8303\u7574\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u9152\u7c7b\u751f\u4ea7\u4f01\u4e1a\u51b3\u5b9a\u5c06\u5176\u65b0\u4e1a\u52a1\u6269\u5c55\u5230\u6c7d\u8f66\u751f\u4ea7\u9886\u57df\uff0c\u8fd9\u79cd\u591a\u5143\u5316\u589e\u957f\u65b9\u5f0f\u662f\nA. \u540c\u5fc3\u591a\u5143\u5316\nB. \u96c6\u56e2\u591a\u5143\u5316\nC. \u6c34\u5e73\u591a\u5143\u5316\nD. \u5782\u76f4\u591a\u5143\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.37887215961900156, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\uff08 \uff09\u662f\u5e02\u573a\u7ecf\u6d4e\u4e2d\u6700\u6838\u5fc3\uff0c\u6700\u7a81\u51fa\u5730\u4f4d\u4e5f\u662f\u6700\u4e3b\u8981\u7684\u9053\u5fb7\nA. \u5148\u4e49\u540e\u5229\nB. \u8bda\u4fe1\nC. \u8d35\u548c\u5bbd\u5bb9 \nD. \u4e92\u5229\u4e92\u60e0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7612904881493026, "meta-math/MetaMath-Mistral-7B": 0.9887673695630975, "itpossible/Chinese-Mistral-7B-v0.1": 0.8622004236319559, "HuggingFaceH4/zephyr-7b-beta": 0.9999084412173812, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9000452472159981, "meta-llama/Meta-Llama-3-8B": 0.5837308368711829, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8305500640784702}}, {"question": "\u8981\u628a\u793e\u4f1a\u95ee\u9898\u9632\u6cbb\u7684\u6218\u7565\u4efb\u52a1\u7eb3\u5165\u56fd\u5bb6\u7ecf\u6d4e\u548c\u793e\u4f1a\u53d1\u5c55\u89c4\u5212\uff0c\u8fd9\u662f\u793e\u4f1a\u95ee\u9898\u9632\u6cbb\u7684\nA. \u5c40\u90e8\u6027\u601d\u60f3\nB. \u957f\u671f\u6027\u601d\u60f3\nC. \u5168\u7403\u6027\u601d\u60f3\nD. \u5168\u5c40\u6027\u601d\u60f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7831093877528891, "meta-math/MetaMath-Mistral-7B": 0.7869379111514461, "itpossible/Chinese-Mistral-7B-v0.1": 0.8748272162572707, "HuggingFaceH4/zephyr-7b-beta": 0.790315799341269, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5954319028081566, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5325704396591361}}, {"question": "\u8bd7\u6b4c\u300a\u96e8\u5df7\u300b\u4e2d\u501f\u7528\u201c\u96e8\u5df7\u201d\u548c\u201c\u4e01\u9999\u201d\u7b49\u610f\u8c61\u6240\u8868\u8fbe\u7684\u201c\u6211\u201d\u7684\u60c5\u7eea\u662f\nA. \u6c89\u95f7\u7edd\u671b\nB. \u4e50\u89c2\u5f00\u6717\nC. \u5b64\u72ec\u4f24\u611f\nD. \u5e73\u548c\u6de1\u6cca\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5697859381003946, "meta-math/MetaMath-Mistral-7B": 0.5438030003831581, "itpossible/Chinese-Mistral-7B-v0.1": 0.4127033114901247, "HuggingFaceH4/zephyr-7b-beta": 0.9939461112920333, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5844157879008216, "meta-llama/Meta-Llama-3-8B": 0.8361370356639284, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8001083060935039}}, {"question": "\u2f00\u4e2a\u7531\u9759\u2f4c\u5f00\u59cb\u505a\u5300\u52a0\u901f\u76f4\u7ebf\u8fd0\u52a8\u7684\u7269\u4f53\uff0c\u5982\u679c\u8fd0\u52a8\u7684\u7b2c1s\u5185\u7684\u4f4d\u79fb\u662f2m\uff0c\u5219\u7269\u4f53\u5728\u8fd0\u52a8\u7684\u7b2c2s\u5185\u7684\nA. 8m\nB. 10.8m\nC. 4m\nD. 3m\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df4\u95e8\u5c3c\u5fb7\u4ece\u54ea\u91cc\u51fa\u53d1\u53bb\u8ffd\u95ee\u4e8b\u7269\u7684\u672c\u8d28\nA. \u4ece\u4e8b\u7269\u7684\u5916\u89c2\u4e0a\u8ffd\u6c42\u5173\u4e8e\u4e8b\u7269\u7684\u77e5\u8bc6\nB. \u4ece\u6982\u5ff5\u7684\u89d2\u5ea6\u8ffd\u6c42\u5173\u4e8e\u4e8b\u7269\u7684\u77e5\u8bc6\nC. \u4ece\u6784\u6210\u4e8b\u7269\u7684\u6750\u6599\u4e0a\u8ffd\u6c42\u5173\u4e8e\u4e8b\u7269\u7684\u77e5\u8bc6\nD. \u4ece\u65f6\u95f4\u4e0a\u8ffd\u6c42\u5173\u4e8e\u4e8b\u7269\u7684\u77e5\u8bc6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8781563095115479, "meta-llama/Meta-Llama-3-8B": 0.6487062281175798, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9496276634810027}}, {"question": "\u5965\u65af\u672c\u63d0\u51fa\u4e86\nA. \u8ba4\u77e5\u5b66\u4e60\u5206\u7c7b\u7406\u8bba\nB. \u6559\u5b66\u76ee\u6807\u5206\u7c7b\u7406\u8bba\nC. \u5148\u884c\u7ec4\u7ec7\u8005\u7684\u6559\u5b66\u7b56\u7565\nD. \u5934\u8111\u98ce\u66b4\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u86cb\u767d\u8d28\u7684\u4e92\u8865\u4f5c\u7528\u662f\u6307\nA. \u8102\u548c\u86cb\u767d\u8d28\u7684\u6df7\u5408\u98df\u7528\uff0c\u4ee5\u63d0\u9ad8\u8425\u517b\u4ef7\u503c\nB. \u4e0d\u540c\u79cd\u7c7b\u7684\u86cb\u767d\u8d28\u6df7\u5408\u98df\u7528\uff0c\u4ee5\u63d0\u9ad8\u8425\u517b\u4ef7\u503c\nC. \u7cd6\u548c\u86cb\u767d\u8d28\u7684\u6df7\u5408\u98df\u7528\uff0c\u4ee5\u63d0\u9ad8\u8425\u517b\u4ef7\u503c\nD. \u7cd6\u548c\u8102\u7684\u6df7\u5408\u98df\u7528\uff0c\u4ee5\u63d0\u9ad8\u8425\u517b\u4ef7\u503c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7907794373512914, "meta-math/MetaMath-Mistral-7B": 0.9821975320705286, "itpossible/Chinese-Mistral-7B-v0.1": 0.7921786121317199, "HuggingFaceH4/zephyr-7b-beta": 0.9257843591360131, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8457102523086817, "meta-llama/Meta-Llama-3-8B": 0.8682049795279104, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8900182307225472}}, {"question": "\u2f64\u2f00\u675f\u7d2b\u5916\u7ebf\u7167\u5c04\u67d0\u2fa6\u5c5e\u65f6\u4e0d\u80fd\u4ea7\u2f63\u5149\u7535\u6548\u5e94\uff0c\u53ef\u80fd\u4f7f\u8be5\u2fa6\u5c5e\u4ea7\u2f63\u5149\u7535\u6548\u5e94\u7684\u63aa\u65bd\u662f\nA. \u6539\u2f64\u9891\u7387\u66f4\u2f29\u7684\u7d2b\u5916\u7ebf\u7167\u5c04\nB. \u5ef6\u2ed3\u539f\u7d2b\u5916\u7ebf\u7684\u7167\u5c04\u65f6\u95f4\nC. \u6539\u2f64\u5f3a\u5ea6\u66f4\u2f24\u7684\u539f\u7d2b\u5916\u7ebf\u7167\u5c04\nD. \u6539\u2f64X\u5c04\u7ebf\u7167\u5c04\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3534716292209113, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9992\u5934\u7684\u9996\u521b\u8005\u636e\u6c11\u95f4\u4f20\u8bf4\u662f\nA. \u5b54\u5b50\nB. \u675c\u752b\nC. \u8bf8\u845b\u4eae\nD. \u674e\u767d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3354456100429363, "HuggingFaceH4/zephyr-7b-beta": 0.4641850551029963, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4258862039361537, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4304106798960247}}, {"question": "\u8d44\u672c\u4e3b\u4e49\u56fd\u5bb6\u7684\u9009\u4e3e\u7684\u5b9e\u8d28\u662f\nA. \u6bcf\u4e2a\u516c\u6c11\u90fd\u80fd\u901a\u8fc7\u7ade\u9009\u53c2\u4e0e\u653f\u6cbb\u6d3b\u52a8\uff0c\u8868\u8fbe\u81ea\u5df1\u7684\u613f\u671b\u548c\u8981\u6c42\nB. \u534f\u8c03\u7edf\u6cbb\u9636\u7ea7\u5185\u90e8\u5229\u76ca\u5173\u7cfb\u548c\u77db\u76fe\u7684\u91cd\u8981\u63aa\u65bd \nC. \u4eba\u6c11\u5f53\u5bb6\u4f5c\u4e3b\nD. \u8d44\u4ea7\u9636\u7ea7\u548c\u65e0\u4ea7\u9636\u7ea7\u5206\u6743\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6469488270291331, "meta-math/MetaMath-Mistral-7B": 0.6032070200404243, "itpossible/Chinese-Mistral-7B-v0.1": 0.7876690121159861, "HuggingFaceH4/zephyr-7b-beta": 0.9923442701816451, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7922458010932123, "meta-llama/Meta-Llama-3-8B": 0.7734117470824448, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b8c\u5168\u7ade\u4e89\u5e02\u573a\u4e0a\uff0c\u5355\u4e2a\u5382\u5546\u7684\u751f\u4ea7\u8981\u7d20\u9700\u6c42\u66f2\u7ebf\u5411\u53f3\u4e0b\u65b9\u503e\u659c\u7684\u539f\u56e0\u662f\nA. \u8981\u7d20\u7684\u8fb9\u9645\u4ea7\u91cf\u9012\u51cf\nB. \u7b49\u4ea7\u91cf\u66f2\u7ebf\u5411\u53f3\u4e0b\u65b9\u503e\u659c\nC. \u8981\u7d20\u6240\u751f\u4ea7\u4ea7\u54c1\u7684\u8fb9\u9645\u6548\u7528\u9012\u51cf\nD. \u751f\u4ea7\u7684\u89c4\u6a21\u6536\u76ca\u9012\u51cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u660e\u672b\u601d\u60f3\u5bb6\u674e\u8d3d\u662f\u4e00\u4f4d\u72c2\u72f7\u4e4b\u58eb:\u4ed6\u5243\u5149\u5934\u53d1\u7559\u7740\u957f\u987b\uff0c\u201c\u5112\u5e3d\u88f9\u50e7\u5934\u201d\uff0c\u7a7f\u7740\u4ea6\u50e7\u4ea6\u5112\u7684\u602a\u5f02\u670d\u88c5;\u4ed6\u8bb2\u5b66\u4f20\u9053\uff0c\u5374\u6536\u5973\u5f1f\u5b50\u3002\u7531\u6b64\u53ef\u89c1\uff0c\u674e\u8d3d\nA. \u529b\u56fe\u51b2\u7834\u5c01\u5efa\u4f20\u7edf\u7684\u675f\u7f1a\nB. \u81f4\u529b\u4e8e\u5112\u5b66\u548c\u4f5b\u6559\u7684\u878d\u5408\nC. \u6e34\u671b\u5f97\u5230\u8d85\u7136\u7269\u5916\u7684\u81ea\u7531\nD. \u8ba4\u53ef\u660e\u4ee3\u5987\u5973\u5730\u4f4d\u7684\u53d8\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6670183781859708, "meta-math/MetaMath-Mistral-7B": 0.9705469660479568, "itpossible/Chinese-Mistral-7B-v0.1": 0.5259719832127508, "HuggingFaceH4/zephyr-7b-beta": 0.9976804659226239, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7881067746144761, "meta-llama/Meta-Llama-3-8B": 0.520108629305867, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u98ce\u9669\u6295\u8d44\u51b3\u7b56\u4e2d\uff0c\u672f\u8bed\u201c\u7ed3\u679c\u201d\u662f\u6307\nA. \u7279\u5b9a\u7684\u7b56\u7565\u548c\u81ea\u7136\u72b6\u6001\u76f8\u7ed3\u5408\u6240\u4ea7\u751f\u7684\u5f97\u6216\u5931\nB. \u5c06\u6765\u53ef\u80fd\u5b58\u5728\u7684\u73af\u5883\u6761\u4ef6\nC. \u7528\u6765\u5b9e\u73b0\u7ba1\u7406\u76ee\u6807\u7684\u884c\u52a8\u65b9\u6848\nD. \u5c06\u6765\u53ef\u80fd\u7684\u65b9\u6848\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6237893511247913, "meta-math/MetaMath-Mistral-7B": 0.8331281856422529, "itpossible/Chinese-Mistral-7B-v0.1": 0.7734045874078646, "HuggingFaceH4/zephyr-7b-beta": 0.999541082442695, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.902240092357795, "meta-llama/Meta-Llama-3-8B": 0.8594742403368707, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9469035958780645}}, {"question": "\u4eba\u7c7b\u793e\u4f1a\u9700\u8981\u9053\u5fb7\uff0c\u4e5f\u4ea7\u751f\u548c\u53d1\u5c55\u4e86\u9053\u5fb7\u3002\u4e0b\u5217\u9009\u9879\u4e2d\u79d1\u5b66\u8bf4\u660e\u4e86\u9053\u5fb7\u8d77\u6e90\u7684\u662f\nA. \u9053\u5fb7\u8d77\u6e90\u4e8e\u4eba\u6027\u4e2d\u7684\u60c5\u611f\u6216\u6b32\u671b\nB. \u9053\u5fb7\u8d77\u6e90\u4e8e\u4eba\u5148\u5929\u5177\u6709\u7684\u67d0\u79cd\u826f\u77e5\nC. \u9053\u5fb7\u8d77\u6e90\u4e8e\u201c\u5929\u201d\u7684\u610f\u5fd7\u6216\u201c\u795e\u201d\u7684\u542f\u793a\nD. \u9053\u5fb7\u4ea7\u751f\u4e8e\u4eba\u7c7b\u5386\u53f2\u7684\u53d1\u5c55\u548c\u4eba\u4eec\u7684\u793e\u4f1a\u5b9e\u8df5\u4e2d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.938872347652905, "meta-math/MetaMath-Mistral-7B": 0.9921724685047904, "itpossible/Chinese-Mistral-7B-v0.1": 0.9664077433414411, "HuggingFaceH4/zephyr-7b-beta": 0.998930107757558, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.980540746316707, "meta-llama/Meta-Llama-3-8B": 0.962056011884629, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9643160531441619}}, {"question": "\u6211\u4eec\u515a\u548c\u56fd\u5bb6\u6c11\u65cf\u653f\u7b56\u4e2d\u7684\u603b\u653f\u7b56\u662f\nA. \u6c11\u65cf\u5e73\u7b49\u56e2\u7ed3\nB. \u6c11\u65cf\u533a\u57df\u81ea\u6cbb\nC. \u6c11\u65cf\u6587\u5316\u6559\u80b2\u53d1\u5c55\nD. \u6c11\u65cf\u7ecf\u6d4e\u53d1\u5c55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8834058867666478, "meta-math/MetaMath-Mistral-7B": 0.9961280266015503, "itpossible/Chinese-Mistral-7B-v0.1": 0.9329003169123983, "HuggingFaceH4/zephyr-7b-beta": 0.9999454654486339, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9781807880838147, "meta-llama/Meta-Llama-3-8B": 0.5435763405214848, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u4e9b\u53d9\u8ff0\u662f\u6b63\u786e\u7684\nA. \u5916\u663e\u5b50\u5728\u57fa\u56e0\u7ec4\u548c cDNA \u4e2d\u987a\u5e8f\u76f8\u540c\nB. \u4eba\u4f53\u4e2d\u7684\u6240\u6709\u7ec6\u80de\u8868\u8fbe\u76f8\u540c\u7684\u4e00\u5957\u57fa\u56e0\nC. \u4eba\u4f53\u4e2d\u7684\u6240\u6709\u7ec6\u80de\u5747\u6309\u76f8\u540c\u7684\u65b9\u5f0f\u62fc\u63a5\u6bcf\u4e2a\u57fa\u56e0\u7684 RNA\nD. \u5185\u542b\u5b50\u901a\u5e38\u88ab\u7ffb\u8bd1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3903562657474929, "meta-math/MetaMath-Mistral-7B": 0.7042091633287456, "itpossible/Chinese-Mistral-7B-v0.1": 0.48631464933537477, "HuggingFaceH4/zephyr-7b-beta": 0.3984643205193326, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6289865612997857, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8ba1\u7b97\u673a\u53d6\u8bc1\u662f\u6307\u80fd\u591f\u4e3a\u6cd5\u5ead\u6240\u63a5\u53d7\u7684\u3001\u5b58\u5728\u4e8e\u8ba1\u7b97\u673a\u548c\u76f8\u5173\u8bbe\u5907\u4e2d\u7684\u7535\u5b50\u8bc1\u636e\u7684\u786e\u8ba4\u3001\u4fdd\u62a4\u3001\u63d0\u53d6\u548c\u5f52\u6863\u7684\u8fc7\u7a0b\u3002\u4ee5\u4e0b\u5173\u4e8e\u8ba1\u7b97\u673a\u53d6\u8bc1\u7684\u63cf\u8ff0\u4e2d\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u8ba1\u7b97\u673a\u53d6\u8bc1\u9700\u8981\u91cd\u6784\u72af\u7f6a\u884c\u4e3a\nB. \u4e3a\u4e86\u4fdd\u8bc1\u8c03\u67e5\u5de5\u5177\u7684\u5b8c\u6574\u6027\uff0c\u9700\u8981\u5bf9\u6240\u6709\u5de5\u5177\u8fdb\u884c\u52a0\u5bc6\u5904\u7406\nC. \u8ba1\u7b97\u673a\u53d6\u8bc1\u4e3b\u8981\u662f\u56f4\u7ed5\u7535\u5b50\u8bc1\u636e\u8fdb\u884c\u7684\nD. \u7535\u5b50\u8bc1\u636e\u5177\u6709\u65e0\u5f62\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7198238178531812, "meta-math/MetaMath-Mistral-7B": 0.9023848051394328, "itpossible/Chinese-Mistral-7B-v0.1": 0.5165796946301178, "HuggingFaceH4/zephyr-7b-beta": 0.997289428557842, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9289084987532366, "meta-llama/Meta-Llama-3-8B": 0.5808696715925967, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5404\u9879\u4e2d\uff0c\u4f53\u73b0\u8c28\u614e\u539f\u5219\u8981\u6c42\u7684\u662f\nA. \u5bf9\u5e94\u6536\u8d26\u6b3e\u8ba1\u63d0\u574f\u8d26\u51c6\u5907\nB. \u4e25\u683c\u5212\u5206\u6536\u76ca\u6027\u652f\u51fa\u4e0e\u8d44\u672c\u6027\u652f\u51fa\nC. \u53d1\u51fa\u5b58\u8d27\u6210\u672c\u7684\u8ba1\u7b97\u91c7\u7528\u5148\u8fdb\u5148\u51fa\u6cd5\nD. \u5f53\u671f\u9500\u552e\u6536\u5165\u4e0e\u5176\u76f8\u5173\u6210\u672c\u8d39\u7528\u914d\u6bd4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3387392219693854, "HuggingFaceH4/zephyr-7b-beta": 0.7352255181760049, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5113041468242412, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u82f1\u56fd\u6700\u65e9\u7684\u5b9a\u671f\u62a5\u520a\u662f1621\u5e74\u51fa\u7248\u7684\nA. \u300a\u5404\u5730\u89c1\u95fb\u300b\nB. \u300a\u62a5\u7eb8\u300b\nC. \u300a\u65b0\u95fb\u62a5\u9053\u300b\nD. \u300a\u6bcf\u5468\u65b0\u95fb\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.36891085543308744, "HuggingFaceH4/zephyr-7b-beta": 0.38754810578290844, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5448\u4e8c\u5341\u9762\u4f53\u7acb\u4f53\u5bf9\u79f0\u7684DNA\u75c5\u6bd2\u662f\nA. \u817a\u75c5\u6bd2\nB. \u6b63\u7c98\u75c5\u6bd2\nC. \u98ce\u75b9\u75c5\u6bd2\nD. \u5f39\u72b6\u75c5\u6bd2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u75be\u75c5\u4e2d\u6025\u9700\u624b\u672f\u63a2\u67e5\u7684\u662f\nA. \u5f20\u529b\u6027\u6c14\u80f8\nB. \u95ed\u5408\u6027\u6c14\u80f8\nC. \u8fdb\u884c\u6027\u8840\u6c14\u80f8\nD. \u5f00\u653e\u6027\u6c14\u80f8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3589842189651603, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d1d\u591a\u82ac\u7684\u7b2c\u4e5d\u4ea4\u54cd\u66f2\u662f\nA. \u300a\u7530\u56ed\u300b\nB. \u300a\u5408\u5531\u300b\nC. \u300a\u82f1\u96c4\u300b\nD. \u300a\u547d\u8fd0\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7480252656110611, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u79cd\u60c5\u51b5\u4e0b\u5c3f\u4e2d\u51fa\u73b0\u86cb\u767d\u8d28\nA. \u6ee4\u8fc7\u819c\u4e0a\u88c2\u5b54\u7d20\u6570\u91cf\u51cf\u5c11\nB. \u6ee4\u8fc7\u5206\u6570\u589e\u52a0\nC. \u5c0f\u7ba1\u6db2\u6d41\u901f\u52a0\u901f\nD. GFR\u589e\u52a0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.32675113387898663, "HuggingFaceH4/zephyr-7b-beta": 0.8112302203643977, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45625586697652004, "meta-llama/Meta-Llama-3-8B": 0.43241138747909885, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6901668902553265}}, {"question": "\u5173\u4e8e\u7535\u5bb9\u5668\u7684\u7535\u5bb9\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u7535\u5bb9\u5668\u4e0d\u5e26\u7535\u65f6\uff0c\u5176\u7535\u5bb9\u4e3a\u96f6\nB. \u7535\u5bb9\u5668\u5e26\u7535\u8377\u91cf\u8d8a\u591a\uff0c\u5176\u7535\u5bb9\u8d8a\u5927\nC. \u7535\u5bb9\u5668\u7684\u7535\u5bb9\u53ea\u7531\u5b83\u672c\u8eab\u7684\u6027\u8d28\u51b3\u5b9a\nD. \u7535\u5bb9\u5668\u4e24\u6781\u677f\u95f4\u7535\u538b\u8d8a\u4f4e\uff0c\u5176\u7535\u5bb9\u8d8a\u5c0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.46259142594551483, "meta-math/MetaMath-Mistral-7B": 0.9330916769872293, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6052750976086512, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5960535545152842, "meta-llama/Meta-Llama-3-8B": 0.37354501646787847, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5013085964245465}}, {"question": "\u4e0b\u5217\u8fd0\u52a8\u4e2d\uff0c\u5c5e\u4e8e\u7b80\u8c10\u632f\u52a8\u7684\u662f\nA. \u5355\u6446\u7684\u6446\u52a8\nB. \u659c\u629b\u8fd0\u52a8\nC. \u5e73\u629b\u8fd0\u52a8\nD. \u5730\u9707\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.83796478091662, "meta-math/MetaMath-Mistral-7B": 0.988377976363598, "itpossible/Chinese-Mistral-7B-v0.1": 0.8317155111751975, "HuggingFaceH4/zephyr-7b-beta": 0.9996441693977776, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9602935102048291, "meta-llama/Meta-Llama-3-8B": 0.9624037581911119, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.96711105014518}}, {"question": "\u67d0\u98df\u7269\u4e2d\u6d4b\u5f97\u5176\u6c28\u57fa\u9178\u542b\u91cf\u6700\u4f4e\u4e3a\u4e1d\u6c28\u9178\uff0c\u5176\u6b21\u662f\u8d56\u6c28\u9178\uff0c\u518d\u6b21\u662f\u82cf\u6c28\u9178\uff0c\u5176\u4ed6\u6c28\u57fa\u9178\u542b\u91cf\u4e0e\u9e21\u86cb\u767d\u6bd4\u8f83\u76f8\u8fd1\uff0c\u5219\u8be5\u98df\u7269\u5f53\u4e2d\u7684\nA. \u82cf\u6c28\u9178\u4e3a\u7b2c\u4e09\u9650\u5236\u6c28\u57fa\u9178\nB. \u4e1d\u6c28\u9178\u4e3a\u7b2c\u4e00\u9650\u5236\u6c28\u57fa\u9178\nC. \u8d56\u6c28\u9178\u4e3a\u7b2c\u4e00\u9650\u5236\u6c28\u57fa\u9178\nD. \u8d56\u6c28\u9178\u4e3a\u7b2c\u4e8c\u9650\u5236\u6c28\u57fa\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.46606421409142573, "meta-math/MetaMath-Mistral-7B": 0.5013271168967722, "itpossible/Chinese-Mistral-7B-v0.1": 0.3220562534414596, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6bcf\u5e74\u768411\u67089\u65e5\u88ab\u786e\u5b9a\u4e3a\nA. \u7981\u6bd2\u65e5\nB. \u62a4\u58eb\u8282\nC. \u5b89\u5168\u751f\u4ea7\u65e5\nD. \u6d88\u9632\u5b89\u5168\u65e5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3985567637814715, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.503775358573754, "HuggingFaceH4/zephyr-7b-beta": 0.9978418123822212, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4703069298158966, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c50\u5c81\u3002\u80c3\u6e83\u75a1\u75c5\u53f2 10 \u5e74\uff0c\u8fd12\u4e2a\u6708\u60a3\u8005\u8179\u75db\u52a0\u91cd\uff0c\u5931\u53bb\u89c4\u5f8b\uff0c\u7ecf\u591a\u79cd\u836f\u7269\u6cbb\u7597\u65e0\u6548\u4f53\u91cd\u4e0b\u964d\u3002\u67e5\u4f53:\u6d45\u8868\u6dcb\u5df4\u7ed3\u65e0\u80bf\u5927\uff0c\u8179\u5e73\u8f6f\uff0c\u4e0a\u8179\u90e8\u6709\u538b\u75db\u3002\u4e3a\u660e\u786e\u8bca\u65ad\uff0c\u6700\u6709\u610f\u4e49\u7684\u68c0\u67e5\u662f\nA. \u7c7b\u9690\u8840\u8bd5\u9a8c\nB. \u80c3\u955c\u68c0\u67e5\nC. \u9910\u9020\u5f71\nD. \u8840\u6e05\u80c3\u6ccc\u7d20\u6d4b\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6613273999034529, "meta-math/MetaMath-Mistral-7B": 0.9468547778608747, "itpossible/Chinese-Mistral-7B-v0.1": 0.798283669863847, "HuggingFaceH4/zephyr-7b-beta": 0.999397296466336, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9565351471919048, "meta-llama/Meta-Llama-3-8B": 0.5635695896871141, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8042765299081994}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6cd5\u7684\u672c\u8d28\u548c\u7279\u5f81\u7684\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u6cd5\u662f\u53d7\u793e\u4f1a\u7269\u8d28\u751f\u6d3b\u6761\u4ef6\u51b3\u5b9a\u7684\nB. \u6cd5\u662f\u7531\u793e\u4f1a\u8206\u8bba\u4fdd\u969c\u5b9e\u65bd\u7684\nC. \u6cd5\u4f53\u73b0\u7684\u662f\u793e\u4f1a\u5168\u4f53\u6210\u5458\u7684\u610f\u5fd7\nD. \u6cd5\u4e0d\u53d7\u4e00\u56fd\u5386\u53f2\u4f20\u7edf\u548c\u98ce\u4fd7\u4e60\u60ef\u7684\u5f71\u54cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u4ece\u7ade\u4e89\u5bf9\u624b\u7684\u7f51\u7edc\u5e97\u94fa\u5927\u91cf\u865a\u5047\u8d2d\u4e70\u5546\u54c1\uff0c\u7535\u5546\u5e73\u53f0\u56e0\u6b64\u8ba4\u5b9a\u8be5\u5e97\u94fa\u6076\u610f\u5237\u5355\uff0c\u7ed9\u4e88\u8be5\u5e97\u94fa\u4ee5\u641c\u7d22\u964d\u6743\u5904\u7406\uff0c\u5bfc\u81f4\u8be5\u5e97\u94fa\u7684\u5546\u54c1\u96be\u4ee5\u88ab\u6d88\u8d39\u8005\u68c0\u7d22\u5230\u9020\u6210\u7ecf\u6d4e\u635f\u5931 35 \u4e07\u5143\uff0c\u7532\u7684\u884c\u4e3a\u5e94\u8ba4\u5b9a\u4e3a\nA. \u4e0d\u6784\u6210\u72af\u7f6a\nB. \u975e\u6cd5\u4fb5\u5165\u8ba1\u7b97\u673a\u4fe1\u606f\u7cfb\u7edf\u7f6a\nC. \u7834\u574f\u751f\u4ea7\u7ecf\u8425\u7f6a\nD. \u635f\u5bb3\u5546\u4e1a\u4fe1\u7528\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9999\u6e2f\u7684\u7b2c\u4e00\u4e2a\u6587\u5b66\u793e\u56e2\u201c\u5c9b\u4e0a\u793e\u201d\u6210\u7acb\u7684\u65f6\u95f4\u662f\nA. 1930\u5e74\nB. 1929\u5e74\nC. 1927\u5e74\nD. 1928\u5e74\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.29539231000588123, "meta-math/MetaMath-Mistral-7B": 0.5062980458803837, "itpossible/Chinese-Mistral-7B-v0.1": 0.3825322550539375, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.31649016643535544, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4369335963233848}}, {"question": "\u4e0b\u5217\u54ea\u4e00\u9879\u4e0d\u5c5e\u4e8e\u5f90\u5fd7\u6469\u8bd7\u6b4c\u7684\u827a\u672f\u7279\u8272\nA. \u4e30\u5bcc\u7684\u60f3\u8c61\uff0c\u795e\u5947\u7684\u5938\u5f20\nB. \u97f5\u5f8b\u548c\u8c10\uff0c\u5bcc\u4e8e\u97f3\u4e50\u7f8e\nC. \u6784\u601d\u7cbe\u5de7\uff0c\u610f\u8c61\u65b0\u9896\nD. \u8f9e\u85fb\u534e\u7f8e\uff0c\u98ce\u683c\u660e\u4e3d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4242697619719605, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5942154672356329}}, {"question": "\u4e0b\u5217\u5404\u7ec4\u8bbe\u5907\u4e2d\uff0c\u5168\u90e8\u5c5e\u4e8e\u8f93\u5165\u8bbe\u5907\u7684\u4e00\u7ec4\u662f\nA. \u786c\u76d8\u3001\u6253\u5370\u673a\u548c\u952e\u76d8\nB. \u952e\u76d8\u3001\u78c1\u76d8\u548c\u6253\u5370\u673a\nC. \u952e\u76d8\u3001\u626b\u63cf\u4eea\u548c\u9f20\u6807\nD. \u952e\u76d8\u3001\u9f20\u6807\u548c\u663e\u793a\u5668\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6439142530546369, "meta-math/MetaMath-Mistral-7B": 0.8081328394796226, "itpossible/Chinese-Mistral-7B-v0.1": 0.38253225505393756, "HuggingFaceH4/zephyr-7b-beta": 0.552643048017546, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7435855789606813, "meta-llama/Meta-Llama-3-8B": 0.6560790509001797, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f5b\u6559\u63d0\u51fa\u4e0d\u51c0\u89c2\u3001\u5e03\u65bd\u5bf9\u6cbb\nA. \u8d2a\nB. \u6389\u6094\nC. \u7761\nD. \u55d4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.44864321801920987, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5552458624836129, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45505424870416383, "meta-llama/Meta-Llama-3-8B": 0.579334566587193, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u521d\u6e29\u76f8\u540c\uff0c\u8d28\u91cf\u4e5f\u76f8\u540c\u7684\u2f54\u548c\u94dc\u5757\uff0c\u5438\u6536\u76f8\u7b49\u7684\u70ed\u91cf\u540e\uff0c\u518d\u5c06\u94dc\u5757\u6295\u2f0a\u2f54\u4e2d\uff0c\u5219\u4f1a\u51fa\u73b0\nA. \u94dc\u5757\u5438\u70ed\uff0c\u2f54\u653e\u70ed\nB. \u2f54\u7684\u5185\u80fd\u4f20\u9012\u5230\u94dc\u5757\u4e0a\nC. \u94dc\u5757\u4e0e\u2f54\u4e4b\u95f4\u4e0d\u53d1\u2f63\u70ed\u4f20\u9012\nD. \u94dc\u5757\u653e\u70ed\uff0c\u2f54\u5438\u70ed\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3646363956912232, "meta-math/MetaMath-Mistral-7B": 0.49651941445493847, "itpossible/Chinese-Mistral-7B-v0.1": 0.3919626083521393, "HuggingFaceH4/zephyr-7b-beta": 0.6177599338144183, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.43439969997822314, "meta-llama/Meta-Llama-3-8B": 0.3704758839685164, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6743\u5229\u6240\u4fdd\u969c\u7684\u8d21\u732e\uff0c\u4e5f\u5c31\u662f\u6743\u529b\u6240\u4fdd\u969c\u7684\u4ed6\u4eba\u5e94\u5f97\u7684\u5229\u76ca\uff0c\u6211 \u4eec\u79f0\u4e4b\u4e3a\nA. \u5951\u7ea6\nB. \u6cd5\u5f8b\nC. \u6743\u5229\nD. \u4e49\u52a1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728Word\u7f16\u8f91\u72b6\u6001\u4e0b\uff0c\u5f53\u524d\u7f16\u8f91\u7684\u6587\u6863\u662fC\u76d8\u4e2d\u7684\u6587\u6863\uff0c\u8981\u5c06\u8be5\u6587\u6863\u4fdd\u5b58\u5230\u8f6f\u76d8\uff0c\u5e94\u5f53\u4f7f\u7528\u201c\u6587\u4ef6\u201d\u83dc\u5355\u4e2d\u7684\nA. \u53d1\u9001\nB. \u65b0\u5efa\nC. \u53e6\u5b58\u4e3a \nD. \u4fdd\u5b58\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7464045127295633, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.974282016469351}}, {"question": "\u4e01\u58a8\u5728\u5979\u7684\u5c0f\u8bf4\u300a\u4ed6\u6765\u4e86\uff0c\u8bf7\u95ed\u773c\u300b\u4e2d\uff0c\u5851\u9020\u4e86\u9ad8\u667a\u5546\u4e25\u8c28\u548c\u5584\u4e8e\u63a8\u7406\u7684\u72af\u7f6a\u5fc3\u7406\u5b66\u4e13\u5bb6\u8584\u9773\u8a00\u7684 \u5f62\u8c61\u3002\u8fd9\u5c5e\u4e8e\nA. \u521b\u9020\u60f3\u8c61\nB. \u65e0\u610f\u60f3\u8c61\nC. \u518d\u9020\u60f3\u8c61\nD. \u65e0\u610f\u8bb0\u5fc6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4168952680018965, "meta-math/MetaMath-Mistral-7B": 0.7712815627246994, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8345974946827432, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8295867814889655, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6775018450835628}}, {"question": "\u660e\u671d\u5b8b\u5e94\u661f\u300a\u5929\u5de5\u5f00\u7269\u300b\u8bb0\u8f7d\uff1a\u201c\u5176\u5e9f\u7eb8\u6d17\u53bb\u6731\u58a8\u3001\u6c61\u79fd\uff0c\u6d78\u70c2\u5165\u69fd\u518d\u9020\uff0c\u5168\u7701\u4ece\u524d\u716e\u6d78\u4e4b\u529b\uff0c\u4f9d\u7136\u6210\u7eb8\uff0c\u8017\u4ea6\u4e0d\u591a\u3002\u6c5f\u5357\u7af9\u8d31\u4e4b\u56fd\uff0c\u4e0d\u4ee5\u4e3a\u7136\uff0c\u5317\u65b9\u5373\u5bf8\u6761\u7247\u89d2\u5728\u5730\uff0c\u968f\u624b\u62fe\u8d77\u518d\u9020\uff0c\u540d\u66f0\u8fd8\u9b42\u7eb8\u3002\u201d\u7531\u6b64\u53ef\u77e5\uff0c\u8fd8\u9b42\u7eb8\uff1aa\u751f\u4ea7\u904d\u5e03\u5168\u56fd\u5404\u5730\uff1bb\u964d\u4f4e\u4e86\u751f\u4ea7\u6210\u672c\uff1bc\u5236\u9020\u5de5\u827a\u66f4\u52a0\u590d\u6742\uff1bd\u6269\u5927\u4e86\u539f\u6599\u6765\u6e90\nA. ab\nB. bd\nC. cd\nD. ac\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5783\u573e\u586b\u57cb\u573a\u662f\u538c\u6c27\u7ec6\u83cc\u7684\u6ecb\u751f\u5730\uff0c\u4f1a\u4ea7\u751f\u5927\u91cf\u7532\u70f7\u3002\u7532\u70f7\u7684\u6e29\u5ba4\u6548\u5e94\u6bd4\u4e8c\u6c27\u5316\u78b3\u66f4\u5f3a\uff0c\u5c3d\u7ba1\u53ef\u4ee5\u5c06\u5176\u6355\u6349\u5e76\u8f6c\u5316\u4e3a\u80fd\u6e90\uff0c\u4f46\u662f\u5373\u4f7f\u6700\u9ad8\u6548\u7684\u56de\u6536\u7cfb\u7edf\uff0c\u4ecd\u6709\u9ad8\u8fbe10%\u7684\u7532\u70f7\u53d1\u751f\u9003\u9038\u3002\u7531\u4e8e\u5783\u573e\u586b\u57cb\u573a\u4ea7\u751f\u7684\u7532\u70f7\u5927\u90e8\u5206\u6765\u81ea\u6709\u673a\u5e9f\u5f03\u7269\uff0c\u56e0\u6b64\u53ef\u4ee5\u7528\u66f4\u7eff\u8272\u7684\u65b9\u5f0f\u5904\u7406\uff0c\u6700\u7b80\u5355\u7684\u65b9\u6cd5\u5c31\u662f\u5806\u80a5\u3002\u5b9e\u9645\u4e0a\uff0c\u5783\u573e\u586b\u57cb\u573a\u91cc\u4e09\u5206\u4e4b\u4e8c\u7684\u5783\u573e\u53ef\u4ee5\u5806\u80a5\uff0c\u8fd9\u5c31\u5927\u5927\u51cf\u5c11\u4e86\u7532\u70f7\u7684\u4ea7\u91cf\u3002\u7531\u4e8e\u5806\u80a5\u8fc7\u7a0b\u4e2d\u4f1a\u4ea7\u751f\u9ad8\u6e29\u3001\u9ad8\u538b\uff0c\u6709\u673a\u5e9f\u5f03\u7269\u4e2d\u7684\u78b3\u53ef\u80fd\u90e8\u5206\u8f6c\u5316\u4e3a\u4e8c\u6c27\u5316\u78b3\u548c\u4e00\u6c27\u5316\u78b3\u3002\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u8003\u8651\u5c06\u8fd9\u4e9b\u6709\u673a\u7269\u91cd\u65b0\u7ec4\u5408\u6210\u6db2\u6001\u71c3\u6599\uff08\u4e59\u9187\u6216\u7532\u9187\uff09\uff0c\u6216\u7528\u4f5c\u5176\u4ed6\u5de5\u4e1a\u539f\u6599\u3002\u8fd9\u6bb5\u6587\u5b57\u4e3b\u8981\u4ecb\u7ecd\u4e86\nA. \u7528\u5806\u80a5\u65b9\u5f0f\u5904\u7406\u5783\u573e\u586b\u57cb\u573a\u7684\u5783\u573e\u80fd\u6709\u6548\u51cf\u5c11\u7532\u70f7\u7684\u751f\u6210\nB. \u7532\u70f7\u7684\u6e29\u5ba4\u6548\u5e94\u6bd4\u4e8c\u6c27\u5316\u78b3\u66f4\u751a\nC. \u5806\u80a5\u8fd9\u79cd\u5783\u573e\u5904\u7406\u65b9\u5f0f\u9700\u8981\u5c06\u5176\u751f\u6210\u7684\u6709\u673a\u5e9f\u5f03\u7269\u91cd\u65b0\u7ec4\u5408\nD. \u73b0\u9636\u6bb5\u7684\u7532\u70f7\u56de\u6536\u7cfb\u7edf\u5bf9\u7532\u70f7\u7684\u6355\u6349\u5229\u7528\u4f9d\u65e7\u6709\u9650\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5723588274769094, "meta-math/MetaMath-Mistral-7B": 0.9682485657099423, "itpossible/Chinese-Mistral-7B-v0.1": 0.41738513912002245, "HuggingFaceH4/zephyr-7b-beta": 0.9920570288643986, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9234602518723187, "meta-llama/Meta-Llama-3-8B": 0.6184469525169949, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9337902906419245}}, {"question": "\u5728Rip\u8def\u7531\u4e2d\u8bbe\u7f6e\u7ba1\u7406\u8ddd\u79bb\u662f\u8861\u91cf\u2f00\u4e2a\u8def\u7531\u53ef\u4fe1\u5ea6\u7684\u7b49\u7ea7\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u5b9a\u4e49\u7ba1\u7406\u8ddd\u79bb\u6765\u533a\u522b\u4e0d\u540c\uff08\uff09\u6765\u6e90\u3002\u8def\u7531\u5668\u603b\u662f\u6311\u9009\u5177\u6709\u6700\u4f4e\u7ba1\u7406\u8ddd\u79bb\u7684\u8def\u7531\nA. \u2f79\u7edc\u7ed3\u6784\u4fe1\u606f\nB. \u6570\u636e\u4ea4\u6362\u4fe1\u606f\nC. \u62d3\u6251\u4fe1\u606f\nD. \u8def\u7531\u4fe1\u606f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u76ee\u524d\u4e2d\u56fd\u4e00\u5171\u6709\u5bb6\u5e7f\u7535\u96c6\u56e2\nA. 15\nB. 30\nC. 32\nD. 20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u516c\u5171\u5173\u7cfb\u7684\u7ec4\u7ec7\u673a\u6784\u662f\u4e13\u95e8\u6267\u884c\u516c\u5173\u4efb\u52a1\u3001\u5b9e\u73b0\u516c\u5173\u529f\u80fd\u7684\u884c\u4e3a\uff08\uff09\uff0c\u662f\u516c\u5171\u5173\u7cfb\u5de5\u4f5c\u7684\u4e13\u4e1a\u804c\u80fd\u673a\u6784\u3002\nA. \u4e3b\u4f53\nB. \u624b\u6bb5\nC. \u5ba2\u4f53\nD. \u8fc7\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5163687995759115, "meta-math/MetaMath-Mistral-7B": 0.9622304517536351, "itpossible/Chinese-Mistral-7B-v0.1": 0.4234425939401968, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.595831261711555, "meta-llama/Meta-Llama-3-8B": 0.7227809520238907, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4086155784869918}}, {"question": "\u6211\u56fd\u7684\u6839\u672c\u653f\u6cbb\u5236\u5ea6\u662f\nA. \u6c11\u65cf\u533a\u57df\u81ea\u6cbb\u5236\u5ea6\nB. \u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u5236\u5ea6\nC. \u5171\u4ea7\u515a\u9886\u5bfc\u7684\u591a\u515a\u5408\u4f5c\u548c\u653f\u6cbb\u534f\u5546\u5236\u5ea6\nD. \u4eba\u6c11\u6c11\u4e3b\u4e13\u653f\u5236\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u55c5\u795e\u7ecf\u7a7f\u8fc7\uff08 \uff09\u5165\u9885\u8154\nA. \u7b5b\u5b54\nB. \u5375\u5706\u5b54\nC. \u9888\u9759\u8109\u5b54\nD. \u5706\u5b54\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4243460062580423, "itpossible/Chinese-Mistral-7B-v0.1": 0.34611314921428016, "HuggingFaceH4/zephyr-7b-beta": 0.37327773153027777, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u4f8b\u5b50\u4e2d\uff0c\u4e0d\u662f\u533a\u522b\u8bcd\u7684\u6709\nA. \u6025\u6027\nB. \u9178\u6027 \nC. \u6682\u65f6\nD. \u91ce\u751f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u77ff\u7269\u8d28\u4e2d\u5c5e\u4e8e\u5e38\u91cf\u77ff\u7269\u8d28\u7684\u662f\nA. \u94a0\nB. \u7898\nC. \u94c1\nD. \u950c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3204796408282626, "meta-math/MetaMath-Mistral-7B": 0.3787890008316441, "itpossible/Chinese-Mistral-7B-v0.1": 0.49720789729800535, "HuggingFaceH4/zephyr-7b-beta": 0.4780218313291958, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38957468522358635, "meta-llama/Meta-Llama-3-8B": 0.37238013739438847, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6587\u5b66\u6d3b\u52a8\u4e2d\uff0c\u5f53\u60c5\u4e0e\u7406\u4e0d\u4e00\u81f4\u7684\u65f6\u5019\uff0c\u6070\u5f53\u7684\u505a\u6cd5\u662f\nA. \u7275\u60c5\u5c31\u7406\nB. \u7275\u7406\u5c31\u60c5\nC. \u5b58\u60c5\u53bb\u7406\nD. \u5b58\u7406\u53bb\u60c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41738543315715604, "meta-math/MetaMath-Mistral-7B": 0.6441842104158786, "itpossible/Chinese-Mistral-7B-v0.1": 0.34239623393788804, "HuggingFaceH4/zephyr-7b-beta": 0.7645887635219202, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.46788766053770214, "meta-llama/Meta-Llama-3-8B": 0.34040633907578, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d44\u672c\u4e3b\u4e49\u7ecf\u6d4e\u5371\u673a\u7684\u5b9e\u8d28\u662f\nA. \u751f\u4ea7\u4e0d\u8db3\u7684\u5371\u673a\nB. \u751f\u4ea7\u8fc7\u5269\u7684\u5371\u673a\nC. \u751f\u4ea7\u7edd\u5bf9\u8fc7\u5269\u7684\u5371\u673a\nD. \u751f\u4ea7\u76f8\u5bf9\u8fc7\u5269\u7684\u5371\u673a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6578425623133668, "meta-math/MetaMath-Mistral-7B": 0.8549933052704288, "itpossible/Chinese-Mistral-7B-v0.1": 0.42439567363214664, "HuggingFaceH4/zephyr-7b-beta": 0.9723697827092217, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6743619658263726, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53ef\u4f5c\u4e3a\u8ba1\u7b97\u673a\u4e2d\u6587\u4ef6\u540d\u901a\u914d\u7b26\u7684\u662f\nA. +\u548c-\nB. *\u548c#\nC. >\u548c<\nD. *\u548c\uff1f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.682029179686628, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9946247803878138, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u80fd\u6e90\u7684\u5f62\u6210\u4e0e\u592a\u9633\u8f90\u5c04\u80fd\u6709\u5173\u7684\u662f\nA. \u5730\u70ed\u80fd\nB. \u6838\u80fd\nC. \u7164\u70ad\u77f3\u6cb9\nD. \u6f6e\u6c50\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4287316500515485, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5265984790634277, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.603921316396055, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u75c5\u60c5\u548c\u836f\u6548\uff0c\u5065\u80c3\u836f\u5b9c\nA. \u65e9\u9910\u540e\u670d\nB. \u996d\u540e\u670d\nC. \u7761\u524d\u670d\nD. \u7a7a\u8179\u670d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.40067969011095805, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3160424181481997, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u56fd\u5bb6\u6709\u5173\u89c4\u5b9a\uff0c\u4e0b\u5217\u54ea\u79cd\u4e1a\u6001\u4e0d\u5c5e\u4e8e\u9910\u996e\u670d\u52a1\u8bb8\u53ef\u7684\u8303\u56f4\nA. \u5b66\u6821\u98df\u5802\nB. \u96c6\u4f53\u7528\u9910\u914d\u9001\u5355\u4f4d\nC. \u98df\u54c1\u644a\u8d29\nD. \u5c0f\u5403\u5e97\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5285677652440335, "meta-math/MetaMath-Mistral-7B": 0.8277896331834894, "itpossible/Chinese-Mistral-7B-v0.1": 0.5357296045136983, "HuggingFaceH4/zephyr-7b-beta": 0.8960519535738916, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4240043125952056, "meta-llama/Meta-Llama-3-8B": 0.6054069948533094, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7582598032250131}}, {"question": "\u300a\u7d20\u95ee\u00b7\u81f3\u771f\u8981\u5927\u8bba\u300b\u4e2d\u5bf9\u201c\u7559\u8005\u201d\u75c5\u8bc1\u7684\u6cbb\u7597\u5b9c\u91c7\u7528\nA. \u653b\u4e4b\nB. \u6563\u4e4b\nC. \u524a\u4e4b\nD. \u9664\u4e4b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u64cd\u4f5c\u7cfb\u7edf\u6309\u5176\u529f\u80fd\u5173\u7cfb\u5206\u4e3a\u7cfb\u7edf\u5c42\u3001\u7ba1\u7406\u5c42\u548c\uff08 \uff09\u4e09\u4e2a\u5c42\u6b21\nA. \u2f64\u6237\u5c42\nB. \u903b\u8f91\u5c42\nC. \u5e94\u2f64\u5c42\nD. \u6570\u636e\u5c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4839005738618998, "meta-math/MetaMath-Mistral-7B": 0.6275365958002748, "itpossible/Chinese-Mistral-7B-v0.1": 0.4015743715695098, "HuggingFaceH4/zephyr-7b-beta": 0.6618939672109311, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5087466008540894}}, {"question": "\u56fd\u4ea7\u6ca5\u9752\u7684\u7279\u70b9\u662f\nA. \u6bd4\u91cd\u5927\nB. \u5ef6\u5ea6\u8f83\u5c0f\nC. \u542b\u8721\u91cf\u9ad8\nD. \u8f6f\u5316\u70b9\u9ad8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4700651741502618, "meta-math/MetaMath-Mistral-7B": 0.5111934907698467, "itpossible/Chinese-Mistral-7B-v0.1": 0.3723801373943884, "HuggingFaceH4/zephyr-7b-beta": 0.9694346702000836, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5784327714315733, "meta-llama/Meta-Llama-3-8B": 0.3788721596190015, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8514946933160991}}, {"question": "1979\u20141981\u5e74\uff0c\u4e2d\u56fd\u51cf\u5c11\u7cae\u98df\u64ad\u79cd\u9762\u79ef5000\u4e07\u4ea9\uff0c\u6709\u8ba1\u5212\u5730\u6269\u5927\u4e86\u7ecf\u6d4e\u4f5c\u7269\u7684\u79cd\u690d\u9762\u79ef\uff0c\u5728\u6709\u6761\u4ef6\u7684\u5730\u65b9\u8fd8\u5f00\u59cb\u9010\u6b65\u9000\u8015\u8fd8\u6797\u8fd8\u6536\uff0c\u9f13\u52b1\u519c\u6751\u5728\u7ecf\u6d4e\u5408\u7406\u539f\u5219\u4e0b\u4e3e\u529e\u793e\u961f\u4f01\u4e1a\u3002\u8fd9\u4e9b\u653f\u7b56\nA. \u5065\u5168\u4e86\u5e02\u573a\u7ecf\u6d4e\u4f53\u5236\nB. \u52a0\u5feb\u4e86\u79c1\u8425\u4f01\u4e1a\u53d1\u5c55\nC. \u63a8\u52a8\u4e86\u519c\u6751\u7ecf\u6d4e\u7ed3\u6784\u7684\u8c03\u6574\nD. \u5b8c\u5584\u4e86\u5bb6\u5ead\u8054\u4ea7\u627f\u5305\u8d23\u4efb\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8025464856721046, "meta-math/MetaMath-Mistral-7B": 0.9763917491019926, "itpossible/Chinese-Mistral-7B-v0.1": 0.8900295692394239, "HuggingFaceH4/zephyr-7b-beta": 0.9859778908851401, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8747986948172242, "meta-llama/Meta-Llama-3-8B": 0.49551718132619427, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9402168534161081}}, {"question": "\u8d2d\u7f6e\u56fa\u5b9a\u8d44\u4ea7\u652f\u4ed8\u7684\u603b\u4ef7\u6b3e\uff0c\u79f0\u4e3a\u8be5\u8d44\u4ea7\u7684\nA. \u53ef\u53d8\u73b0\u51c0\u503c\nB. \u516c\u5141\u4ef7\u503c\nC. \u91cd\u7f6e\u6210\u672c\nD. \u5386\u53f2\u6210\u672c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6694333479551923, "meta-math/MetaMath-Mistral-7B": 0.8845116566294844, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9996222649217088, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8136303881808643, "meta-llama/Meta-Llama-3-8B": 0.6331435102702518, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7676367615379337}}, {"question": "\u300a\u5468\u6613\u300b\u7684\u4e09\u5c42\u4e49\u7406\u4e0d\u5305\u62ec\nA. \u4e0d\u6613\nB. \u53d8\u6613\nC. \u7b80\u6613\nD. \u4fbf\u6613\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.44304481397371914, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7782784422861092, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6630399093050576}}, {"question": "\u5168\u9176\u662f\u6307\nA. \u9176-\u6291\u5236\u5242\u590d\u5408\u7269\nB. \u9176\u86cb\u767d-\u8f85\u52a9\u56e0\u5b50\u590d\u5408\u7269\nC. \u9176-\u5e95\u7269\u590d\u5408\u7269\nD. \u9176-\u522b\u6784\u5242\u590d\u5408\u7269\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3704758839685164, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53f6\u5723\u9676\u5c0f\u8bf4\u5851\u9020\u7684\u5351\u7410\u7070\u8272\u7684\u5c0f\u77e5\u8bc6\u5206\u5b50\u5f62\u8c61\u662f\nA. \u5218\u5411\u9ad8\nB. \u9ec4\u8ff0\u6cf0\nC. \u65b9\u9e3f\u6e10\nD. \u6f58\u5148\u751f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3641532879451763, "meta-math/MetaMath-Mistral-7B": 0.4482780958461333, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9718364388919392, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4853399635314934, "meta-llama/Meta-Llama-3-8B": 0.2986334267609958, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u524d\u9988\u63a7\u5236\u7684\u53d9\u8ff0\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u5e38\u5728\u5c40\u90e8\u548c\u77ed\u65f6\u95f4\u5185\u53d1\u6325\u4f5c\u7528\nB. \u5177\u6709\u6ede\u540e\u548c\u6ce2\u52a8\u7684\u7f3a\u70b9\nC. \u5177\u6709\u9884\u89c1\u6027\uff0c\u9002\u5e94\u8303\u56f4\u5927\nD. \u524d\u9988\u63a7\u5236\u4e0d\u4f1a\u53d1\u751f\u5931\u8bef\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34956851919766924, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.47407019127391786, "HuggingFaceH4/zephyr-7b-beta": 0.9980251127454096, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5013466531406594, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8158756613574855}}, {"question": "\u5728\u793e\u4f1a\u5b66\u7814\u7a76\u65b9\u6cd5\u4e2d\uff0c\u5c5e\u4e8e\u5b9a\u6027\u65b9\u6cd5\u7684\u662f\nA. \u793e\u4f1a\u5b9e\u9a8c\u65b9\u6cd5\nB. \u95ee\u5377\u8c03\u67e5\nC. \u5b9e\u5730\u7814\u7a76\nD. \u4ecb\u5165\u6027\u7814\u7a76\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6793261239929139, "meta-math/MetaMath-Mistral-7B": 0.583691838248412, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8903939331307297, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7954192105885319, "meta-llama/Meta-Llama-3-8B": 0.6355675743985081, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.680215472421319}}, {"question": "\u90c1\u8fbe\u592b\u6240\u5f00\u521b\u7684\u5c0f\u8bf4\u6837\u5f0f\u662f\nA. \u8bd7\u5316\u5c0f\u8bf4\nB. \u54f2\u7406\u5c0f\u8bf4\nC. \u201c\u81ea\u53d9\u4f20\u201d\u5c0f\u8bf4\nD. \u6563\u6587\u5316\u5c0f\u8bf4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41951873699531794, "meta-math/MetaMath-Mistral-7B": 0.5594256641199414, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.709542369138355, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3963288117843559, "meta-llama/Meta-Llama-3-8B": 0.3088936322941584, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6285467014047758}}, {"question": "\u6c11\u95f4\u6545\u4e8b\u300a\u94e1\u7f8e\u6848\u300b\u4e2d\uff0c\u9a78\u9a6c\u9648\u4e16\u7f8e\u88ab\u4f9d\u6cd5\u5904\u51b3\u3002\u8fd9\u4e00\u6cd5\u5f8b\u73b0\u8c61\u8868\u660e\nA. \u6cd5\u5f8b\u662f\u516c\u5171\u610f\u5fd7\u7684\u53cd\u6620\uff0c\u5177\u6709\u8d85\u9636\u7ea7\u6027\nB. \u6211\u56fd\u5c01\u5efa\u793e\u4f1a\u7684\u6cd5\u5f8b\u4e5f\u8d2f\u5f7b\u516c\u6c11\u5728\u6cd5\u5f8b\u9762\u524d\u4eba\u4eba\u5e73\u7b49\u7684\u539f\u5219\nC. \u6cd5\u5f8b\u662f\u7edf\u6cbb\u9636\u7ea7\u6574\u4f53\u610f\u5fd7\u3001\u5171\u540c\u610f\u5fd7\u7684\u4f53\u73b0\nD. \u6cd5\u5f8b\u6709\u65f6\u5019\u4e5f\u662f\u88ab\u7edf\u6cbb\u9636\u7ea7\u5c06\u610f\u5fd7\u4e0a\u5347\u4e3a\u56fd\u5bb6\u610f\u5fd7\u7684\u7ed3\u679c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2966017332563093, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u300a\u6c11\u6cd5\u603b\u5219\u300b\u5df2\u4e8e2017\u5e7410\u67081\u65e5\u65bd\u884c\uff0c\u4f46\u300a\u6c11\u6cd5\u901a\u5219\u300b\u5e76\u672a\u5e9f\u9664\uff0c\u5728\u4e24\u6cd5\u5e76\u5b58\u5171\u7528\u9636\u6bb5\uff0c\u5bf9\u4e8e\u540c\u4e00\u4e8b\u9879\u4e8c\u8005\u6709\u4e0d\u540c\u7684\u89c4\u5b9a\uff0c\u9002\u7528\u7684\u539f\u5219\u5e94\u8be5\u662f\nA. \u4e0a\u4f4d\u6cd5\u4f18\u4e8e\u4e0b\u4f4d\u6cd5\nB. \u6cd5\u4e0d\u6eaf\u53ca\u65e2\u5f80\nC. \u7279\u522b\u6cd5\u4f18\u4e8e\u4e00\u822c\u6cd5\nD. \u65b0\u6cd5\u4f18\u4e8e\u65e7\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.33544561004293627, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4888713092779481}}, {"question": "\u4ee5\u4e0b\u54ea\u4e2a\u4e0d\u662f\u53d7\u4f17\u7684\u89d2\u8272\nA. \u4f20\u64ad\u5185\u5bb9\u7684\u201c\u7ffb\u8bd1\u8005\u201d\nB. \u4fe1\u606f\u4ea7\u54c1\u7684\u6d88\u8d39\u8005\nC. \u4f20\u64ad\u6548\u679c\u7684\u53cd\u9988\u8005\nD. \u4f20\u64ad\u6d3b\u52a8\u7684\u53c2\u4e0e\u8005\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5845217176513439, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9999850683349586, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9083086312917591, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3764609902132996}}, {"question": "\u6025\u6027\u708e\u75c7\u75c5\u7076\u4e2d\u6e17\u51fa\u7684\u708e\u7ec6\u80de\u4e3b\u8981\u662f\nA. \u5355\u6838\u7ec6\u80de\nB. \u4e2d\u6027\u7c92\u7ec6\u80de\nC. \u6dcb\u5df4\u7ec6\u80de\nD. \u55dc\u9178\u6027\u7c92\u7ec6\u80de\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6853660643779329, "meta-math/MetaMath-Mistral-7B": 0.9001168704224297, "itpossible/Chinese-Mistral-7B-v0.1": 0.6605189590694459, "HuggingFaceH4/zephyr-7b-beta": 0.972430857013456, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7237484829383745, "meta-llama/Meta-Llama-3-8B": 0.7162426900160535, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7715117641785515}}, {"question": "\u201c \u732b\u8033\u6735 \u201d \u662f\u54ea\u91cc\u7684\u7279\u8272\u5c0f\u5403\nA. \u8d35\u5dde\nB. \u6c5f\u82cf\nC. \u5317\u4eac\nD. \u6d59\u6c5f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u501f\u8d37\u8bb0\u8d26\u6cd5\u4e0b\uff0c\u8d26\u6237\u501f\u65b9\u767b\u8bb0\u589e\u52a0\u6570\u6216\u51cf\u5c11\u6570\u53d6\u51b3\u4e8e\nA. \u6838\u7b97\u65b9\u6cd5\nB. \u8d26\u6237\u53cd\u6620\u7684\u7ecf\u6d4e\u5185\u5bb9\u6027\u8d28\nC. \u8bb0\u8d26\u89c4\u5219\nD. \u8d44\u91d1\u7684\u5e73\u8861\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4094914376756169, "meta-math/MetaMath-Mistral-7B": 0.6282513592608069, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9981558194003733, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5052963379994225, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9225871633698336}}, {"question": "\u4e0b\u5217\u6709\u5173\u7ec6\u80de\u751f\u547d\u5386\u7a0b\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u7ec6\u80de\u51cb\u4ea1\u65f6\u76f8\u5173\u57fa\u56e0\u6d3b\u52a8\u52a0\u5f3a\u6709\u5229\u4e8e\u4e2a\u4f53\u53d1\u80b2\nB. \u7ec6\u80de\u764c\u53d8\u540e\uff0c\u819c\u86cb\u767d\u53d1\u751f\u6539\u53d8\u4e14\u6613\u4e8e\u6269\u6563\u8f6c\u79fb\nC. \u7ec6\u80de\u5206\u5316\u65f6\u6838\u57fa\u56e0\u53ca\u5176\u8868\u8fbe\u4ea7\u7269\u5747\u53d1\u751f\u6539\u53d8\nD. \u7ec6\u80de\u5206\u88c2\u65f6\u7ebf\u7c92\u4f53\u7684\u5206\u914d\u662f\u968f\u673a\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u5173\u80f0\u5c9b\u7d20\u7624\u7684\u63cf\u8ff0\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u5355\u53d1\u80bf\u7624\u5360 90%\u4ee5\u4e0a\nB. \u7ec6\u80de\u5f62\u6001\u662f\u51b3\u5b9a\u5176\u826f\u6076\u6027\u7684\u4e3b\u8981\u4f9d\u636e\nC. \u624b\u672f\u662f\u552f\u4e00\u6839\u6cbb\u6027\u6cbb\u7597\u624b\u6bb5\nD. 90%\u4ee5\u4e0a\u4e3a\u826f\u6027\u80bf\u7624\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4028736763775812, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7522567522922925, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e49\u52a1\u6559\u80b2\u7684\u6839\u672c\u6027\u8d28\u662f\nA. \u5168\u6c11\u6027\nB. \u5f3a\u8feb\u6027\nC. \u666e\u53ca\u6027\nD. \u57fa\u7840\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31437545047285026, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.33424000363035195, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u539f\u7801\u52a0\u51cf\u4ea4\u66ff\u9664\u6cd5\u2f1c\u79f0\u4e3a\u4e0d\u6062\u590d\u4f59\u6570\u6cd5\uff0c\u56e0\u6b64\nA. \u5f53\u67d0\u2f00\u6b65\u8fd0\u7b97\u4e0d\u591f\u51cf\u65f6\uff0c\u505a\u6062\u590d\u4f59\u6570\u7684\u64cd\u4f5c\nB. \u4ec5\u5f53\u6700\u540e\u2f00\u6b65\u4f59\u6570\u4e3a\u8d1f\u65f6\uff0c\u505a\u6062\u590d\u4f59\u6570\u7684\u64cd\u4f5c\nC. \u5f53\u67d0\u2f00\u6b65\u4f59\u6570\u4e3a\u8d1f\u65f6\uff0c\u505a\u6062\u590d\u4f59\u6570\u7684\u64cd\u4f5c\nD. \u4e0d\u5b58\u5728\u6062\u590d\u4f59\u6570\u7684\u64cd\u4f5c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.34993229125498293, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65e9\u671f\u6559\u4f1a\u5927\u5b66\u7684\u6388\u8bfe\u5185\u5bb9\u4e3b\u8981\u662f\uff08\u6587\u6cd5\u3001\u4fee\u8f9e\u3001\u903b\u8f91\u3001\u6570\u5b66\u3001\u51e0\u4f55\u3001\u97f3\u4e50\u548c\u5929\u6587\uff09\uff0c\u5408\u79f0\nA. \u827a\u672f\nB. \u6587\u827a\nC. \u6587\u5b66\nD. \u201c\u4e03\u827a\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9484650923233066, "meta-math/MetaMath-Mistral-7B": 0.9960298110181068, "itpossible/Chinese-Mistral-7B-v0.1": 0.9548195144556213, "HuggingFaceH4/zephyr-7b-beta": 0.9999652866444032, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9943262489033052, "meta-llama/Meta-Llama-3-8B": 0.9825299331610562, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9998164847421741}}, {"question": "\u5bf9\u4e8e\u4e0d\u52a8\u4ea7\u7684\u6cd5\u5b9a\u7ee7\u627f\uff0c\u7532\u56fd\u89c4\u5b9a\u5e94\u4f9d\u6b7b\u8005\u672c\u56fd\u6cd5\uff0c\u4e59\u56fd\u89c4\u5b9a\u5e94\u4f9d\u6b7b\u8005\u6700\u540e\u4f4f\u6240\u5730\u6cd5\uff0c\u4e19\u56fd\u89c4\u5b9a\u5e94\u4f9d\u4e0d\u52a8\u4ea7\u6240\u5728\u5730\u6cd5\u3002\u4e14\u7532\u4e59\u4e24\u56fd\u5747\u8ba4\u4e3a\u81ea\u5df1\u7684\u51b2\u7a81\u6cd5\u6307\u5f15\u7684\u5916\u56fd\u6cd5\u5305\u62ec\u5916\u56fd\u51b2\u7a81\u6cd5\u3002\u8bbe\u4e59\u56fd\u516c\u6c11\u6b7b\u4e8e\u4e19\u56fd\u6700\u540e\u4f4f\u6240\uff0c\u5728\u7532\u56fd\u7559\u4e0b\u4e00\u7b14\u4e0d\u52a8\u4ea7\uff0c\u4e3a\u7ee7\u627f\u6b64\u4e0d\u52a8\u4ea7\uff0c\u4e59\u56fd\u516c\u6c11\u7684\u59bb\u5b50\u5728\u7532\u56fd\u63d0\u51fa\u7ee7\u627f\u4e0d\u52a8\u4ea7\u8bf7\u6c42\uff0c\u7532\u56fd\u6cd5\u9662\u6700\u540e\u51b3\u5b9a\u4f9d\u4e19\u56fd\u51b2\u7a81\u6cd5\u6307\u5b9a\u4f5c\u4e3a\u4e0d\u52a8\u4ea7\u6240\u5728\u5730\u7684\u7532\u56fd\u7ee7\u627f\u6cd5\u5224\u51b3\u4e86\u6848\u4ef6\uff0c\u8fd9\u79cd\u60c5\u51b5\u5728\u56fd\u9645\u53f8\u6cd5\u4e0a\u53eb\nA. \u95f4\u63a5\u53cd\u81f4\nB. \u53cd\u81f4\nC. \u8f6c\u81f4\nD. \u53cc\u91cd\u53cd\u81f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5408762698280217}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u4e0d\u5c5e\u4e8e\u6211\u56fd\u884c\u653f\u6267\u6cd5\u57fa\u672c\u539f\u5219\u7684\u662f\nA. \u5408\u7406\u6027\u539f\u5219\nB. \u534f\u5546\u6027\u539f\u5219\nC. \u6b63\u5f53\u7a0b\u5e8f\u539f\u5219\nD. \u5408\u6cd5\u6027\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8364749384270314, "meta-math/MetaMath-Mistral-7B": 0.9366505154674653, "itpossible/Chinese-Mistral-7B-v0.1": 0.5587467740787438, "HuggingFaceH4/zephyr-7b-beta": 0.9411138472601521, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8938959689902338, "meta-llama/Meta-Llama-3-8B": 0.8242956260459393, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6633767650718727}}, {"question": "\u7528\u4e8e\u8f83\u5927\u4f53\u79ef\u7684\u6df7\u51dd\u571f\uff0c\u6700\u4e0d\u9002\u5b9c\u7684\u6c34\u6ce5\u54c1\u79cd\u662f\nA. \u77ff\u6e23\u6c34\u6ce5\nB. \u7845\u9178\u76d0\u6c34\u6ce5\nC. \u7c89\u7164\u7070\u6c34\u6ce5\nD. \u706b\u5c71\u7070\u6c34\u6ce5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.333183235354062, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7594600826124219}}, {"question": "\u571f\u5c42\u7684\u5305\u6c14\u5e26\u662f\u6307[ ]\u3002\nA. \u571f\u58e4\u7684\u8868\u5c42\nB. \u5730\u9762\u5230\u5730\u4e0b\u6f5c\u6c34\u9762\u4e4b\u95f4\u7684\u571f\u5c42\nC. \u5730\u4e0b\u6f5c\u6c34\u9762\u4ee5\u4e0b\u7684\u571f\u5c42\nD. \u5730\u9762\u4ee5\u4e0b\u7684\u6574\u4e2a\u571f\u5c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3817240629201194, "meta-math/MetaMath-Mistral-7B": 0.602015661775207, "itpossible/Chinese-Mistral-7B-v0.1": 0.5694584517274746, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.40101793144159786, "meta-llama/Meta-Llama-3-8B": 0.43059707547350146, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c35\u5c81\uff0c3\u4e2a\u6708\u6765\u4f4e\u70ed\u3001\u76d7\u6c57\u3001\u6d88\u7626\uff0c1\u4e2a\u6708\u6765\u52b3\u7d2f\u540e\u6c14\u77ed\u3002\u67e5\u4f53\uff1aT37.6\u5ea6\uff0c\u53f3\u4e0b\u80ba\u89e6\u89c9\u9707\u98a4\u51cf\u5f31\uff0c\u53e9\u8bca\u5448\u6d4a\u97f3\uff0c\u547c\u5438\u97f3\u6d88\u5931\u3002\u5fc3\u5c16\u640f\u52a8\u5411\u5de6\u79fb\u4f4d\uff0c\u5fc3\u97f3\u6b63\u5e38\uff0c\u5fc3\u738798\u6b21/\u5206\uff0c\u5f8b\u6574\uff0c\u65e0\u6742\u97f3\uff0c\u8d85\u58f0\u793a\u53f3\u4fa7\u80f8\u8154\u4e2d\u7b49\u91cf\u79ef\u6db2\u3002\u8be5\u60a3\u8005\u8fd8\u53ef\u80fd\u51fa\u73b0\u7684\u4f53\u5f81\u662f\nA. \u53cc\u4fa7\u80f8\u5ed3\u808b\u95f4\u9699\u53d8\u7a84\nB. \u6c14\u7ba1\u5411\u5de6\u79fb\u4f4d\nC. \u53f3\u4e0a\u80ba\u53ef\u95fb\u53ca\u7ba1\u72b6\u547c\u5438\u97f3\nD. \u53f3\u4fa7\u80ba\u5e95\u4e0b\u79fb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.25780995786150784, "meta-math/MetaMath-Mistral-7B": 0.40252653040150815, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.35832757891394634, "meta-llama/Meta-Llama-3-8B": 0.308176776288574, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u5c11\u6570\u6c11\u65cf\u67091\u4ebf\u591a\u4eba\u53e3\uff0c\u5360\u5168\u56fd\u603b\u4eba\u53e3\u767e\u5206\u4e4b\uff08\uff09\u4ee5\u4e0a\nA. \u516d\nB. \u4e03\nC. \u516b\nD. \u4e94\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30150657848650314, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.35888231301687173, "HuggingFaceH4/zephyr-7b-beta": 0.42855370093435796, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3220562534414596, "meta-llama/Meta-Llama-3-8B": 0.3160424181481997, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5546\u54c1\u4ea4\u6362\u7684\u4e24\u4e2a\u5fc5\u8981\u6761\u4ef6\uff1a\u4e00\u662f\u751f\u4ea7\u8d44\u6599\u548c\u52b3\u52a8\u4ea7\u54c1\u5f52\u4e0d\u540c\u7684 \u52b3\u52a8\u8005\u5360\u6709\uff0c\u4e8c\u662f\nA. \u79c1\u6709\u5236\nB. \u793e\u4f1a\u5206\u5de5\nC. \u751f\u4ea7\u529b\u53d1\u5c55\nD. \u751f\u4ea7\u5173\u7cfb\u53d1\u5c55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.45574462846632274, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5540053099704256, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8239822893706397}}, {"question": "\u540c\u65f6\u629b\u63b7\u4e24\u679a\u8d28\u5730\u5747\u5300\u7684\u786c\u5e01\uff0c\u5219\u51fa\u73b0\u4e24\u4e2a\u6b63\u9762\u671d\u4e0a\u7684\u6982\u7387\u662f\nA. $\\frac{1}{8}$\nB. $\\frac{1}{4}$\nC. $\\frac{1}{2}$\nD. $\\frac{1}{3}$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31118566467799214, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8797414461174344, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2712019384407753, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.46795638531563977}}, {"question": "\u5df2\u77e5\u5269\u4f59\u5bff\u547d$T(x)$\u548c$T(y)$\u76f8\u4e92\u72ec\u7acb\uff0c\u4e14$E[T(x)]=E[T(y)]=4\uff0c\\operatorname{Cov}[T(x y)\uff0cT$$(\\overline{x y})]=0.09$\uff0c\u5219$E[T(x y)]$\u7b49\u4e8e$()$\u3002\nA. 3.7\nB. 2.0\nC. 4.0\nD. 2.8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.45279118134207075}}, {"question": "\u4fee\u5b66\u4f5b\u6559\u4e94\u4e58\u5171\u6cd5\uff0c\u5176\u4e2d\u201c\u4e58\u201d\u7684\u672c\u610f\u662f\nA. \u4e58\u5750\nB. \u8f66\u5b50\nC. \u4e0a\u4e58\nD. \u9053\u8def\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6d41\u884c\u75c5\u5b66\u7684\u5b9a\u4e49\u53ef\u6982\u62ec\u4e3a\nA. \u7814\u7a76\u5404\u79cd\u75be\u75c5\u7684\u5b66\u79d1\nB. \u7814\u7a76\u75be\u75c5\u7684\u8bca\u65ad\u3001\u6cbb\u7597\u53ca\u9884\u9632\u7684\u79d1\u5b66\nC. \u7814\u7a76\u4f20\u67d3\u75c5\u7684\u53d1\u751f\u3001\u53d1\u5c55\u4ee5\u53ca\u5982\u4f55\u9632\u5236\u7684\u79d1\u5b66\nD. \u7814\u7a76\u4eba\u7fa4\u4e2d\u75be\u75c5\u4e0e\u5065\u5eb7\u72b6\u51b5\u5206\u5e03\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\uff0c\u5e76\u7814\u7a76\u5982\u4f55\u9632\u5236\u75be\u75c5\u53ca\u4fc3\u8fdb\u5065\u5eb7\u7684\u7b56\u7565\u548c\u63aa\u65bd\u7684\u79d1\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9673657243675666, "meta-math/MetaMath-Mistral-7B": 0.9926631380328507, "itpossible/Chinese-Mistral-7B-v0.1": 0.8226340632119198, "HuggingFaceH4/zephyr-7b-beta": 0.9999609361773967, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9424383278229873, "meta-llama/Meta-Llama-3-8B": 0.9508793599495122, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7acb\u6cd5\u7684\u5173\u952e\u9636\u6bb5\u662f\nA. \u6cd5\u5f8b\u6848\u7684\u5ba1\u8bae\nB. \u6cd5\u5f8b\u6848\u7684\u901a\u8fc7\nC. \u6cd5\u5f8b\u6848\u7684\u63d0\u51fa\nD. \u6cd5\u5f8b\u7684\u516c\u5e03\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.42758728815268504, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6525594028509747, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7568440745671059}}, {"question": "\u5bf9\u4e8e\u80c3\u6392\u7a7a\u7684\u53d9\u8ff0\uff0c\u4e0b\u5217\u54ea\u4e00\u9879\u662f\u9519\u8bef\u7684\nA. \u86cb\u767d\u8d28\u98df\u7269\u6bd4\u7cd6\u548c\u8102\u80aa\u98df\u7269\u6392\u7a7a\u6162\nB. \u6df7\u5408\u98df\u7269\u7531\u80c3\u5b8c\u5168\u6392\u7a7a\u97004\uff5e6\u5c0f\u65f6\nC. \u80c3\u5185\u98df\u7269\u7684\u91cf\u4e0e\u80c3\u6392\u7a7a\u7684\u901f\u7387\u6709\u5173\nD. \u98df\u7269\u8fdb\u5165\u80c3\u540e5\u5206\u949f\u5373\u6709\u90e8\u5206\u6392\u7a7a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.38902257709985416, "meta-math/MetaMath-Mistral-7B": 0.5906120724995961, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u7535\u78c1\u73b0\u8c61\u7684\u8bf4\u6cd5\u4e2d\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u95ed\u5408\u7535\u8def\u7684\u90e8\u5206\u5bfc\u4f53\u5728\u78c1\u573a\u4e2d\u8fd0\u52a8\u65f6\uff0c\u4e00\u5b9a\u4ea7\u751f\u611f\u5e94\u7535\u6d41\nB. \u53d1\u7535\u673a\u5de5\u4f5c\u65f6\uff0c\u5c06\u673a\u68b0\u80fd\u8f6c\u5316\u4e3a\u7535\u80fd\nC. \u73a9\u5177\u7535\u52a8\u8f66\u7684\u7535\u52a8\u673a\u662f\u5229\u7528\u7535\u78c1\u611f\u5e94\u73b0\u8c61\u5de5\u4f5c\u7684\nD. \u901a\u7535\u5bfc\u4f53\u5728\u78c1\u573a\u4e2d\u53d7\u529b\u65b9\u5411\u53ea\u4e0e\u7535\u6d41\u7684\u65b9\u5411\u6709\u5173\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u6cfb\u5357\u8865\u5317\u201d\u6cd5\u9002\u7528\u4e8e\nA. \u80be\u9634\u865a\u800c\u809d\u9633\u4ea2\nB. \u80be\u9634\u865a\u800c\u76f8\u706b\u52a8\nC. \u5fc3\u9634\u865a\u800c\u5fc3\u9633\u4ea2\nD. \u80be\u9634\u865a\u800c\u5fc3\u706b\u65fa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679cA\u3001B\u4e24\u4e2a\u5224\u65ad\u4e0d\u80fd\u540c\u5047\uff0c\u4f46\u5374\u53ef\u4ee5\u540c\u771f\uff0c\u5219\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u662f\nA. \u4e0b\u53cd\u5bf9\u5173\u7cfb\nB. \u77db\u76fe\u5173\u7cfb\nC. \u5dee\u7b49\u5173\u7cfb\nD. \u53cd\u5bf9\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2653616406916089, "meta-math/MetaMath-Mistral-7B": 0.47712050901244224, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3802392203343767, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9a7e\u9a76\u673a\u52a8\u8f66\u5728\u9053\u8def\u4e0a\u8ffd\u9010\u7ade\u9a76\uff0c\u60c5\u8282\u6076\u52a3\uff0c\u4f1a\u53d7\u5230\u4ec0\u4e48\u5904\u7f5a\nA. \u5904\u7ba1\u5236\uff0c\u5e76\u5904\u7f5a\u91d1\nB. \u59041\u5e74\u4ee5\u4e0a\u5f92\u5211\nC. \u59046\u4e2a\u6708\u5f92\u5211\nD. \u5904\u62d8\u5f79\uff0c\u5e76\u5904\u7f5a\u91d1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6075634589377894, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1949\u5e746\u6708\uff0c\u6bdb\u6cfd\u4e1c\u5728\u300a\u8bba\u4eba\u6c11\u6c11\u4e3b\u4e13\u653f\u300b\u4e2d\u6307\u51fa\uff1a\u201c\u4e00\u8fb9\u5012\u662f\u5b59\u4e2d\u5c71\u7684\u56db\u5341\u5e74\u7ecf\u9a8c\u548c\u5171\u4ea7\u515a\u7684\u4e8c\u5341\u516b\u5e74\u7ecf\u9a8c\u6559\u7ed9\u6211\u4eec\u7684\uff0c\u6df1\u77e5\u6b32\u8fbe\u5230\u80dc\u5229\u548c\u5de9\u56fa\u80dc\u5229\uff0c\u5fc5\u987b\u4e00\u8fb9\u5012\u2026\u2026\u4e2d\u56fd\u4eba\u6c11\u4e0d\u662f\u5012\u5411\u5e1d\u56fd\u4e3b\u4e49\u4e00\u8fb9\uff0c\u5c31\u662f\u5012\u5411\u793e\u4f1a\u4e3b\u4e49\u4e00\u8fb9\uff0c\u7edd\u65e0\u4f8b\u5916\u3002\u201d\u65b0\u4e2d\u56fd\u5728\u8fd9\u4e00\u5916\u4ea4\u65b9\u9488\u6307\u5bfc\u4e0b\u7684\u5916\u4ea4\u5b9e\u8df5\u6709\uff1aa\u4e0e\u82cf\u8054\u5efa\u7acb\u5916\u4ea4\u5173\u7cfb\uff1bb\u52a0\u5165\u534e\u6c99\u6761\u7ea6\u7ec4\u7ec7\uff1bc\u5728\u4e07\u9686\u4f1a\u8bae\u4e0a\u63d0\u51fa\u201c\u6c42\u540c\u5b58\u5f02\u201d\uff1bd\u79ef\u6781\u53d1\u5c55\u4e0e\u4e0d\u7ed3\u76df\u8fd0\u52a8\u56fd\u5bb6\u7684\u5173\u7cfb\nA. ad\nB. ac\nC. a\nD. ab\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4524320522222725, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.42886487419360814, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f53\u5185\u6700\u91cd\u8981\u7684\u8131\u6c28\u57fa\u65b9\u5f0f\u662f\nA. \u6c28\u57fa\u8f6c\u79fb\u4f5c\u7528\nB. \u8054\u5408\u8131\u6c28\u57fa\u4f5c\u7528\nC. \u6c27\u5316\u8131\u6c28\u57fa\nD. \u8fd8\u539f\u8131\u6c28\u57fa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35757047101120343, "meta-math/MetaMath-Mistral-7B": 0.5757783500641865, "itpossible/Chinese-Mistral-7B-v0.1": 0.47399083139323794, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3468666740605408, "meta-llama/Meta-Llama-3-8B": 0.40848354399971304, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5009868633489192}}, {"question": "\u4e0b\u5217\u4e0d\u5c5e\u4e8e\u7ed3\u7f14\u7ec4\u7ec7\u75c5\u7684\u662f\nA. \u539f\u53d1\u6027\u5e72\u71e5\u7efc\u5408\u5f81\nB. \u6210\u4eba Still \u75c5\nC. \u7c7b\u98ce\u6e7f\u5173\u8282\u708e\nD. Reiter \u7efc\u5408\u5f81\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.570845767610853, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u4e3b\u5f20\u5efa\u7acb\u7684\u56fd\u9645\u653f\u6cbb\u7ecf\u6d4e\u65b0\u79e9\u5e8f\u7684\u57fa\u7840\u662f\nA. \u4e0d\u7ed3\u76df\nB. \u534f\u5546\u89e3\u51b3\nC. \u72ec\u7acb\u81ea\u4e3b\nD. \u548c\u5e73\u5171\u5904\u4e94\u9879\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8481314371851516, "meta-math/MetaMath-Mistral-7B": 0.7713448593186868, "itpossible/Chinese-Mistral-7B-v0.1": 0.5923630607728143, "HuggingFaceH4/zephyr-7b-beta": 0.9994363221629208, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7408106533155492, "meta-llama/Meta-Llama-3-8B": 0.4930919582176738, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.38989534889252425}}, {"question": "\u7b2c\u4e00\u53f0\u7535\u8111ENIAC\u8bde\u751f\u5728\nA. \u4e2d\u56fd\nB. \u5fb7\u56fd \nC. \u7f8e\u56fd\nD. \u82f1\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9657620748263466, "meta-math/MetaMath-Mistral-7B": 0.999294334778414, "itpossible/Chinese-Mistral-7B-v0.1": 0.9445241901238793, "HuggingFaceH4/zephyr-7b-beta": 0.9998503338383714, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9571598374079731, "meta-llama/Meta-Llama-3-8B": 0.9833779151338352, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9994983089788974}}, {"question": "\u4ee5\u4e0b\u5173\u4e8e\u6570\u5b57\u8bc1\u4e66\u7684\u53d9\u8ff0\u4e2d\uff0c\u9519\u8bef\u7684\u662f\nA. \u8bc1\u4e66\u643a\u5e26\u6301\u6709\u8005\u7684\u7b7e\u540d\u7b97\u6cd5\u6807\u8bc6\nB. \u8bc1\u4e66\u901a\u5e38\u7531CA\u5b89\u5168\u8ba4\u8bc1\u4e2d\u5fc3\u53d1\u653e\nC. \u8bc1\u4e66\u7684\u6709\u6548\u6027\u53ef\u4ee5\u901a\u8fc7\u9a8c\u8bc1\u6301\u6709\u8005\u7684\u7b7e\u540d\u9a8c\u8bc1\nD. \u8bc1\u4e66\u901a\u5e38\u643a\u5e26CA\u7684\u516c\u5f00\u5bc6\u94a5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.32846576456294085, "meta-math/MetaMath-Mistral-7B": 0.41093156971367806, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9805776772907484, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5217975788069056, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "20\u4e16\u7eaa90\u5e74\u4ee3\uff0c\u4fc4\u7f57\u65af\u5148\u540e\u8fdb\u884c\u4e86\u51e0\u6b21\u8f83\u5927\u89c4\u6a21\u7684\u519b\u4e8b\u6539\u9769\nA. \u4e8c\u6b21\nB. \u56db\u6b21\nC. \u4e09\u6b21\nD. \u4e94\u6b21\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.47331624679088397, "meta-math/MetaMath-Mistral-7B": 0.5528630849018501, "itpossible/Chinese-Mistral-7B-v0.1": 0.38957469939794354, "HuggingFaceH4/zephyr-7b-beta": 0.938367985073888, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.632924022231153, "meta-llama/Meta-Llama-3-8B": 0.42520436346770346, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u897f\u6e38\u8bb0\u300b\u91cc\u7684\u5b59\u609f\u7a7a\u795e\u901a\u5e7f\u5927\uff0c\u4ed6\u5728\u82b1\u679c\u5c71\u7684\u540d\u5b57\u53eb\nA. \u9f50\u5929\u5927\u5723\nB. \u5b59\u884c\u8005\nC. \u5b59\u609f\u7a7a\nD. \u7f8e\u7334\u738b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6502328306867534, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8368718918903181}}, {"question": "\u4e16\u754c\u65e0\u70df\u65e5\u662f\nA. 4\u670830\u65e5\nB. 6\u670829\u65e5\nC. 5\u670831\u65e5\nD. 7\u670831\u65e5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3984203859679838, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c24\u5c81\u3002\u80cc\u91cd\u7269\u65f6\u7a81\u7136\u6655\u50122\u5c0f\u65f6\u5165\u9662\u3002\u67e5\u4f53\uff1aP120\u6b21/\u5206\uff0cR30\u6b21/\u5206\uff0cBP80/60mmHg\uff0c\u795e\u6e05\uff0c\u9762\u8272\u82cd\u767d\uff0c\u8179\u80c0\uff0c\u5168\u8179\u8f7b\u5ea6\u538b\u75db\u53ca\u53cd\u8df3\u75db\uff0c\u79fb\u52a8\u6027\u6d4a\u97f3\u9633\u6027\uff0c\u80a0\u9e23\u97f3\u6d88\u5931\uff0c\u5de6\u4e0b\u80f8\u6709\u76ae\u80a4\u7600\u6591\u75d5\u8ff9\u30021\u5468\u524d\u56e0\u8f66\u7978\u649e\u51fb\u5de6\u4e0b\u80f8\u90e8\uff0c\u66fe\u5367\u5e8a\u4f11\u606f2\u5929\u3002\u4e3a\u8fdb\u4e00\u6b65\u660e\u786e\u8bca\u65ad\uff0c\u6025\u8bca\u9996\u9009\u7684\u68c0\u67e5\u662f\nA. MRI\nB. CT\nC. X\u7ebf\nD. B\u8d85\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6570a\u5206\u89e3\u8d28\u56e0\u6570\u662fa=2*2*3\uff0c\u6570b\u5206\u89e3\u8d28\u56e0\u6570\u662fb=2*2*5\uff0c\u6570a\u548c\u6570b\u7684\u6700\u5927\u516c\u56e0\u6570\u662f\u89e3\u8d28\u56e0\u6570\u662fa=2*2*3\uff0c\u6570b\u5206\u89e3\u8d28\u56e0\u6570\u662fb=2*2*5\uff0c\u6570a\u548c\u6570b\u7684\u6700\u5927\u516c\u56e0\u6570\u662f\nA. 4\nB. 6\nC. 2\nD. 60\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\uff0c\u6709\u540d\u8bcd\u4f5c\u72b6\u8bed\u7684\u4e00\u53e5\u662f\nA. \u6545\u5929\u4e0b\u76e1\u4ee5\u6241\u9d72\u7232\u80fd\u751f\u6b7b\u4eba\u3002\nB. \u4eca\u5148\u751f\u513c\u7136\u4e0d\u9060\u5343\u91cc\u800c\u5ead\u6559\u4e4b\uff0c\u9858\u4ee5\u7570\u65e5\u3002\nC. \u4eca\u5aaa\u5c0a\u9577\u5b89\u541b\u4e4b\u4f4d\u3002\nD. \u662f\u6545\u660e\u541b\u8cb4\u4e94\u7e20\u800c\u8ce4\u91d1\u7389\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30844852467973854, "meta-math/MetaMath-Mistral-7B": 0.4507223823461397, "itpossible/Chinese-Mistral-7B-v0.1": 0.3233250233605477, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u5bb6\u5ead\u751f\u6d3b\u4e2d\u52aa\u529b\u52b3\u52a8\u3001\u4e0d\u8f9e\u8f9b\u82e6\u3001\u4e0d\u61d2\u60f0\uff0c\u4e14\u7231\u60dc\u8d22\u7269\u3001\u5408\u7406\u652f\u51fa\u3001\u4e0d\u6d6a\u8d39\u3002\u8fd9\u662f\u5bb6\u5ead\u7f8e\u5fb7\u4e2d\nA. \u7537\u5973\u5e73\u7b49\u7684\u8981\u6c42\nB. \u5c0a\u8001\u7231\u5e7c\u7684\u8981\u6c42\nC. \u52e4\u4fed\u6301\u5bb6\u7684\u8981\u6c42\nD. \u592b\u59bb\u548c\u8c10\u7684\u8981\u6c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9833137893237062, "meta-math/MetaMath-Mistral-7B": 0.9982525186625918, "itpossible/Chinese-Mistral-7B-v0.1": 0.9846302694107958, "HuggingFaceH4/zephyr-7b-beta": 0.9981538655236308, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9927404765411725, "meta-llama/Meta-Llama-3-8B": 0.9355776095958295, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9963782118394182}}, {"question": "\u91ca\u8fe6\u725f\u5c3c\u609f\u9053\u5730\u548c\u521d\u8f6c\u6cd5\u8f6e\u5730\u5206\u522b\u662f\nA. \u9e7f\u91ce\u82d1\u3001\u62d8\u5c38\u90a3\u8fe6\u57ce\nB. \u83e9\u63d0\u4f3d\u8036\u3001\u84dd\u6bd7\u5c3c\u82b1\u56ed\nC. \u83e9\u63d0\u4f3d\u8036\u3001\u9e7f\u91ce\u82d1\nD. \u84dd\u6bd7\u5c3c\u82b1\u56ed\u3001\u9e7f\u91ce\u82d1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3626062801770275, "meta-math/MetaMath-Mistral-7B": 0.6483554786599579, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5019227022601327, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6329186678291578}}, {"question": "\u7f8e\u56fd\u5361\u7279\u5229\u666e\u548c\u68ee\u7279\u5728\u5176\u4e13\u8457\u300a\u6709\u6548\u7684\u516c\u5171\u5173\u7cfb\u300b\u4e00\u4e66\u4e2d\nA. \u63d0\u51fa\u4e86\"\u51e1\u5ba3\u4f20\u7686\u597d\u4e8b\"\u7684\u547d\u9898\nB. \u63d0\u51fa\u4e86\"\u516c\u4f17\u5fc5\u987b\u88ab\u544a\u77e5\"\u7684\u547d\u9898\nC. \u63d0\u51fa\u4e86\"\u6295\u516c\u4f17\u6240\u597d\"\u7684\u4e3b\u5f20\nD. \u63d0\u51fa\u4e86\"\u53cc\u5411\u5bf9\u79f0\"\u7684\u516c\u5171\u5173\u7cfb\u6a21\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7742246280499004, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u552f\u4e00\u5b8c\u6574\u6210\u73af\u7684\u5589\u8f6f\u9aa8\u662f\nA. \u4f1a\u538c\u8f6f\u9aa8\nB. \u73af\u72b6\u8f6f\u9aa8\nC. \u52fa\u72b6\u8f6f\u9aa8\nD. \u7532\u72b6\u8f6f\u9aa8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4051419076297877, "meta-math/MetaMath-Mistral-7B": 0.6816779876618669, "itpossible/Chinese-Mistral-7B-v0.1": 0.6495721931564802, "HuggingFaceH4/zephyr-7b-beta": 0.9514063697917655, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3690058809048272, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9568985785197053}}, {"question": "R\u3001L\u3001C\u5e76\u8054\u7535\u8def\u5904\u4e8e\u8c10\u632f\u72b6\u6001\u65f6\uff0c\u7535\u5bb9\u4e24\u7aef\u7684\u7535\u538b\u7b49\u4e8e\nA. \u7535\u5bb9\u5668\u989d\u5b9a\u7535\u538b\nB. \u7535\u6e90\u7535\u538b\u4e0e\u7535\u8def\u54c1\u8d28\u56e0\u7d20Q\u7684\u6bd4\u503c\nC. \u7535\u6e90\u7535\u538b\nD. \u7535\u6e90\u7535\u538b\u4e0e\u7535\u8def\u54c1\u8d28\u56e0\u7d20Q\u7684\u4e58\u79ef\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3381195864214883, "meta-math/MetaMath-Mistral-7B": 0.5374779482178804, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5171612293732416, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.472030522969263, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53e5\u4e2d\u7684\u6210\u8bed\u4f7f\u7528\u4e0d\u6070\u5f53\u7684\u662f\nA. \u4e94\u5c81\u7684\u5c0f\u4f84\u5b50\u6234\u7740\u5927\u6a90\u5e3d\uff0c\u522b\u7740\u73a9\u5177\u67aa\uff0c\u715e\u6709\u4ecb\u4e8b\u5730\u5728\u623f\u95f4\u91cc\u5de1\u89c6\uff0c\u90a3\u6a21\u6837\u771f\u662f\u8ba9\u4eba\u5fcd\u4fca\u4e0d\u7981\u3002\nB. \u4e3a\u4e86\u7b79\u5efa\u5357\u6781\u957f\u57ce\u7ad9\uff0c\u4ed6\u5455\u5fc3\u6ca5\u8840\uff1b\u957f\u57ce\u7ad9\u843d\u6210\u65f6\uff0c\u8fd9\u4f4d\u94a2\u94c1\u822c\u7684\u6c49\u5b50\u4e5f\u6d41\u6cea\u4e86\u3002\nC. \u7b49\u5f97\u4e0d\u8010\u70e6\u7684\u7236\u4eb2\u5bf9\u513f\u5b50\u8bf4\uff1a\u201c\u770b\u4f60\u5988\u51fa\u4e2a\u95e8\u8fd8\u5f97\u6253\u626e\u534a\u5929\uff0c\u771f\u662f\u9ebb\u70e6\u2014\u2014\u8ba9\u5979\u6df1\u5c45\u7b80\u51fa\u8fd8\u633a\u4e0d\u5bb9\u6613\uff01\u201d\nD. \u7af9\u7b4f\u5728\u6e4d\u6025\u7684\u6cb3\u6d41\u4e2d\uff0c\u5c31\u50cf\u4e00\u53ea\u6f02\u6d6e\u4e8e\u6c34\u9762\u7684\u7532\u866b\uff0c\u8239\u5de5\u5c0f\u5fc3\u7ffc\u7ffc\u5730\u6491\u7740\u7b4f\u5b50\uff0c\u60df\u6050\u5b83\u88ab\u5de8\u6d6a\u6253\u7ffb\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30559070232016367, "meta-math/MetaMath-Mistral-7B": 0.3728641892601242, "itpossible/Chinese-Mistral-7B-v0.1": 0.3175234229675448, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.417305741279239, "meta-llama/Meta-Llama-3-8B": 0.39406193324431593, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5740062292160226}}, {"question": "\u5c06\u4e0b\u5217\u5404\u9879\u6309\u6240\u8868\u793a\u5e74\u9f84\u5927\u5c0f\u987a\u5e8f\u6392\u5217\uff0c\u6b63\u786e\u7684\u987a\u5e8f\u5e94\u662f\u3002a\u4e0d\u60d1\uff1bb\u5782\u9aeb\uff1bc\u82b1\u7532\uff1bd\u52a0\u51a0\uff1be\u800c\u7acb\uff1bf\u53e4\u7a00\uff1bg\u534a\u767e\nA. fecbdga\nB. bdagecf\nC. bdeagcf\nD. dbcfage\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.28850952576306876, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.39325342265749486}}, {"question": "\u6211\u56fd\u300a\u5a5a\u59fb\u6cd5\u300b\u7b2c33\u6761\u89c4\u5b9a\uff1a\u73b0\u5f79\u519b\u4eba\u7684\u914d\u5076\u8981\u6c42\u79bb\u5a5a\uff0c\u987b\u5f97\u519b\u4eba\u7684\u540c\u610f\uff0c\u4f46\u519b\u4eba\u4e00\u65b9\u6709\u91cd\u5927\u8fc7\u9519\u7684\u9664\u5916\u3002\u4f9d\u636e\u6cd5\u7406\u5b66\u7684\u6709\u5173\u539f\u7406\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u8be5\u6761\u4e2d\u6240\u89c4\u5b9a\u7684\u519b\u4eba\u7684\u914d\u5076\u5728\u79bb\u5a5a\u65b9\u9762\u6240\u627f\u62c5\u7684\u4e49\u52a1\u6ca1\u6709\u76f8\u5e94\u7684\u6743\u5229\u5b58\u5728\nB. \u8be5\u6cd5\u5f8b\u6761\u6587\u5b8c\u6574\u5730\u8868\u8fbe\u4e86\u4e00\u4e2a\u6cd5\u5f8b\u89c4\u5219\u7684\u6784\u6210\u8981\u7d20\nC. \u73b0\u5f79\u519b\u4eba\u4e0e\u5176\u914d\u5076\u4e4b\u95f4\u7684\u6743\u5229\u4e49\u52a1\u662f\u4e0d\u4e00\u81f4\u7684\nD. \u8be5\u6761\u6240\u89c4\u5b9a\u7684\u6cd5\u5f8b\u4e49\u52a1\u662f\u4e00\u79cd\u5bf9\u4eba\u4e49\u52a1\u6216\u76f8\u5bf9\u4e49\u52a1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3898115540279693, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9544598455172003, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5510\u672b\u4e94\u4ee3\uff0c\u4e2d\u592e\u96c6\u6743\u5927\u4e3a\u524a\u5f31\u3002\u4e3a\u626d\u8f6c\u8fd9\u4e00\u5c40\u9762\uff0c\u5b8b\u521d\u91c7\u53d6\u7684\u91cd\u8981\u63aa\u65bd\u4e4b\u4e00\u662f\nA. \u8bbe\u67a2\u5bc6\u9662\nB. \u8bbe\u4e09\u53f8\u4f7f\nC. \u6587\u5b98\u4efb\u77e5\u5dde\nD. \u65bd\u884c\u5c06\u5175\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u572850\u7c73\u77ed\u8ddd\u79bb\u8dd1\u7684\u6bd4\u8d5b\u4e2d\uff0c\u8fd0\u53d1\u52a8\u7684\u80fd\u91cf\u76f4\u63a5\u4f9b\u80fd\u7684\u662f\u3002\nA. \u6709\u6c27\u6c27\u5316\u7cfb\u7edf\nB. \u78f7\u9178\u539f\u7cfb\u7edf\u548c\u6709\u6c27\u6c27\u5316\u7cfb\u7edf\nC. \u78f7\u9178\u539f\u7cfb\u7edf\u3014\u4e09\u78f7\u9178\u817a\u82f7\uff0d\u78f7\u9178\u808c\u9178ATP\uff0dCP\u3015\nD. \u4e73\u9178\u80fd\u7cfb\u7edf\u3014\u65e0\u6c27\u7cd6\u9175\u89e3\u7cfb\u7edf\u3015\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6238727266960317, "meta-math/MetaMath-Mistral-7B": 0.7712776823274416, "itpossible/Chinese-Mistral-7B-v0.1": 0.7209134884014595, "HuggingFaceH4/zephyr-7b-beta": 0.9258568403250923, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8334758011825951, "meta-llama/Meta-Llama-3-8B": 0.6110465563328047, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9ea6\u514b\u5362\u6c49\u5a92\u4ecb\u7406\u8bba\u7684\u4e2d\u5fc3\u8bba\u70b9\u662f\nA. \u5168\u7403\u6751\nB. \u5a92\u4ecb\u5373\u8baf\u606f\nC. \u5a92\u4ecb\u662f\u4eba\u4f53\u7684\u5ef6\u4f38\nD. \u5a92\u4ecb\u6709\u201c\u51b7\u201d\u201c\u70ed\u201d\u4e4b\u5206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5194829257124444, "meta-llama/Meta-Llama-3-8B": 0.6278600461331116, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8058420074051442}}, {"question": "\u5728\u6559\u80b2\u6559\u5b66\u6d3b\u52a8\u8fc7\u7a0b\u4e2d\uff0c\u6559\u5e08\u53eb\u5b66\u751f\u5230\u529e\u516c\u5ba4\u53d6\u6709\u5173\u6559\u5b66\u5668\u6750\uff0c\u9020\u6210\u5b66\u751f\u4f24\u5bb3\u7684\uff0c\u5b66\u6821\nA. \u8d1f\u95f4\u63a5\u8d23\u4efb\nB. \u65e0\u8d23\u4efb\uff0c\u6559\u5e08\u6709\u8d23\u4efb\nC. \u65e0\u8d23\u4efb\nD. \u8d1f\u76f4\u63a5\u8d23\u4efb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5e02\u6c11\u6cd5\u4ea6\u79f0\u516c\u6c11\u6cd5\uff0c\u662f\u7f57\u9a6c\u56fd\u5bb6\u65e9\u671f\u7684\u6cd5\u5f8b\u2026\u2026\u7f57\u9a6c\u6cd5\u91c7\u7528\u5c5e\u4eba\u4e3b\u4e49\u800c\u975e\u5c5e\u5730\u4e3b\u4e49\uff0c\u5c31\u662f\u8bf4\u51e1\u662f\u7f57\u9a6c\u516c\u6c11\u5747\u53d7\u6cd5\u5f8b\u7684\u4fdd\u62a4\uff0c\u800c\u4e0d\u8bba\u5176\u5c45\u4f4f\u5730\u533a\u5982\u4f55\u3002\u201d\u636e\u6b64\u5224\u65ad\uff0c\u5efa\u56fd\u4e4b\u521d(\u516c\u5143\u524d5\u4e16\u7eaa\u65e9\u671f\u4ee5\u524d)\u7684\u7f57\u9a6c\nA. \u516c\u6c11\u5728\u672c\u90a6\u6d3b\u52a8\u65f6\u53d7\u6210\u6587\u6cd5\u4fdd\u62a4\nB. \u516c\u6c11\u5728\u5916\u90a6\u6d3b\u52a8\u65f6\u53d7\u516c\u6c11\u6cd5\u4fdd\u62a4\nC. \u516c\u6c11\u5728\u5916\u90a6\u6d3b\u52a8\u65f6\u53d7\u4e07\u6c11\u6cd5\u4fdd\u62a4\nD. \u5c45\u6c11\u5728\u672c\u90a6\u6d3b\u52a8\u90fd\u53d7\u516c\u6c11\u6cd5\u4fdd\u62a4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5165225558641927, "meta-math/MetaMath-Mistral-7B": 0.8727730182676733, "itpossible/Chinese-Mistral-7B-v0.1": 0.5593087567095605, "HuggingFaceH4/zephyr-7b-beta": 0.4724254569293215, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7958709736076073, "meta-llama/Meta-Llama-3-8B": 0.2821833983601388, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u5173\u4e8e\u6f14\u66ff\u7684\u8bf4\u6cd5\u54ea\u4e2a\u6b63\u786e?\nA. \u6f14\u66ff\u4ee5\u5148\u950b\u79cd\u4e3a\u53d1\u5c55\u9876\u70b9\nB. \u6f14\u66ff\u5c31\u662f\u4e00\u4e2a\u7269\u79cd\u53d6\u4ee3\u53e6\u4e00\u4e2a\u7269\u79cd\uff0c\u65e0\u89c4\u5f8b\u6027\nC. \u6f14\u66ff\u5c31\u662f\u4e00\u4e2a\u7269\u79cd\u53d6\u4ee3\u53e6\u4e00\u4e2a\u7269\u79cd\uff0c\u65e0\u65b9\u5411\u6027\nD. \u6f14\u66ff\u662f\u6307\u5728\u4e00\u5b9a\u533a\u57df\u5185\u4e00\u4e2a\u7fa4\u843d\u88ab\u53e6\u4e00\u4e2a\u7fa4\u843d\u6240\u66ff\u4ee3\u7684\u8fc7\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6244342256518577, "meta-math/MetaMath-Mistral-7B": 0.956756167502905, "itpossible/Chinese-Mistral-7B-v0.1": 0.6308807579222103, "HuggingFaceH4/zephyr-7b-beta": 0.9999085942584927, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7216635300260705, "meta-llama/Meta-Llama-3-8B": 0.9471089528600162, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9478327135094489}}, {"question": "\u4e00\u5bf9\u540c\u6e90\u67d3\u8272\u4f53\nA. \u5728\u7ec6\u80de\u5206\u88c2\u95f4\u671f\u914d\u5bf9\u5e73\u884c\u6392\u5217\nB. \u603b\u662f\u5e26\u6709\u76f8\u540c\u7684\u57fa\u56e0\nC. \u5f62\u6001\u548c\u7ed3\u6784\u76f8\u540c\nD. \u6765\u6e90\u4e8e\u751f\u7269\u540c\u4e00\u4eb2\u672c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.41270329704319336, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u827e\u68ee\u514b\u7684\u6c14\u8d28\u7406\u8bba\uff0c\u4e00\u4e2a\u4eba\u8868\u73b0\u4e3a\u6e29\u548c\u3001\u9547\u5b9a\u3001\u5b89\u5b81\u3001\u5584\u4e8e\u514b\u5236\u81ea\u5df1\u3002\u8fd9\u79cd\u4eba\u7684\u6c14\u8d28\u5c5e\u4e8e\nA. \u4e0d\u7a33\u5b9a\u5916\u5411\u578b\nB. \u7a33\u5b9a\u5185\u5411\u578b\nC. \u4e0d\u7a33\u5b9a\u5185\u5411\u578b\nD. \u7a33\u5b9a\u5916\u5411\u578b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7358124181979191, "meta-math/MetaMath-Mistral-7B": 0.9767960112445299, "itpossible/Chinese-Mistral-7B-v0.1": 0.5710154803573902, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8782508900500612, "meta-llama/Meta-Llama-3-8B": 0.925429800508721, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9972127282479628}}, {"question": "\u5973\uff0c32\u5c81\u30029\u4e2a\u6708\u56e0\u5148\u5929\u6027\u80c6\u603b\u7ba1\u56ca\u80bf\u884c\u56ca\u80bf\u5207\u9664\u3001\u80c6\u80a0Roux-Y\u543b\u5408\u672f\uff0c\u672f\u4e2d\u66fe\u8f93\u8840400ml\u30022\u4e2a\u6708\u6765\u6613\u201c\u611f\u5192\u201d\uff0c\u81ea\u670d\u6297\u751f\u7d20\u597d\u8f6c\uff0c\u8fd1\u4e00\u5468\u51fa\u73b0\u53d1\u70ed\u3001\u5bd2\u6218\uff0c\u6700\u9ad8\u4f53\u6e29\u8fbe39\u2103\u3002\u67e5\u4f53\uff1aP123\u6b21/\u5206\uff0cR22\u6b21/\u5206\uff0cBP102/80mmHg\uff0c\uff0c\u5de9\u819c\u9ec4\u67d3\uff0c\u53cc\u80ba\u547c\u5438\u97f3\u7c97\uff0c\u4e0a\u8179\u8f7b\u538b\u75db\u3002\u5316\u9a8c\uff1aWBC18\u00d7109/L\uff0c\u4e2d\u6027\u7c92\u7ec6\u80de89%\uff0cTBil121mmol/L\uff0cDBil86mmol/L\uff0cALT203U/L\u3002\u6b64\u75c5\u75c7\u7684\u6839\u672c\u539f\u56e0\u6700\u53ef\u80fd\u662f\nA. \u4f53\u8d28\u865a\u5f31\u6613\u611f\u67d3\nB. \u809d\u95e8\u90e8\u80c6\u7ba1\u764c\nC. \u80c6\u80a0\u543b\u5408\u53e3\u72ed\u7a84\nD. \u672f\u4e2d\u8f93\u8840\u611f\u67d3\u809d\u708e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u53e4\u4ee3\u6709\u5f88\u591a\u8ba1\u91cf\u5355\u4f4d\uff0c\u6bd4\u5982\u8bd7\u53e5\u201c\u9ec4\u6cb3\u8fdc\u4e0a\u767d\u4e91\u95f4\uff0c\u4e00\u7247\u5b64\u57ce\u4e07\u4ede\u5c71\u201d\u4e2d\u7684\u201c\u4ede\u201d\uff0c\u4e00\u4ede\u7ea6\u76f8\u5f53\u4e8e\nA. \u4e00\u4e2a\u6210\u5e74\u4eba\u7684\u9ad8\u5ea6\nB. \u6210\u5e74\u4eba\u4e00\u638c\u7684\u957f\u5ea6\nC. \u6210\u5e74\u4eba\u98df\u6307\u7684\u957f\u5ea6\nD. \u6210\u5e74\u4eba\u4e00\u81c2\u7684\u957f\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.33012266124059403, "itpossible/Chinese-Mistral-7B-v0.1": 0.3331832353540621, "HuggingFaceH4/zephyr-7b-beta": 0.8298264324025714, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3528619482070927, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6731\u81ea\u6e05\u66fe\u5bf9\u4e00\u4f4d\u8bd7\u4eba\u7684\u4f5c\u54c1\u4f5c\u51fa\u8fd9\u6837\u7684\u8bc4\u4ef7\uff1a\u201c\u8fd9\u4e0d\u662f\u2018\u6076\u4e4b\u82b1\u2019\u7684\u8d5e\u9882\uff0c\u800c\u662f\u7d22\u6027\u8ba9\u2018\u4e11\u6076\u2019\u65e9\u4e9b\u6076\u8d2f\u6ee1\u76c8\uff0c\u2018\u7edd\u671b\u2019\u91cc\u624d\u6709\u5e0c\u671b\u201d\u3002\u4ed6\u6240\u8bc4\u4ef7\u7684\u8fd9\u4f4d\u8bd7\u4eba\u662f\nA. \u5f90\u5fd7\u6469\nB. \u827e\u9752\nC. \u95fb\u4e00\u591a\nD. \u81e7\u514b\u5bb6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31283638571410965, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4612836906017622, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u8179\u819c\u7c98\u8fde\u7684\u53d9\u8ff0\u4e2d\uff0c\u9519\u8bef\u7684\u662f\nA. \u7c98\u8fde\u8d8a\u5e7f\uff0c\u80a0\u6897\u963b\u8d8a\u91cd\nB. \u76ee\u524d\u5c1a\u65e0\u6709\u6548\u7684\u9884\u9632\u7c98\u8fde\u7684\u65b9\u6cd5\nC. \u7c98\u8fde\u53ef\u5f15\u8d77\u80a0\u6897\u963b\nD. \u591a\u4e3a\u8179\u8154\u624b\u672f\u6216\u708e\u75c7\u7684\u540e\u679c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4fc4\u7f57\u65af\u753b\u5bb6\u5217\u5bbe\u7684\u4ee3\u8868\u4f5c\u662f\nA. \u300a\u5973\u8d35\u65cf\u83ab\u6d1b\u5353\u5a03\u300b\nB. \u300a\u4f0f\u5c14\u52a0\u6cb3\u7ea4\u592b\u300b\nC. \u300a\u6885\u675c\u8428\u4e4b\u7b4f\u300b\nD. \u300a\u81ea\u7531\u5973\u795e\u9886\u5bfc\u7740\u4eba\u6c11\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6555481075460609, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7386216104467598}}, {"question": "\u7ec4\u6210\u8ba1\u7b97\u673a\u7684CPU\u7684\u4e24\u2f24\u90e8\u4ef6\u662f\nA. \u63a7\u5236\u5668\u548c\u5bc4\u5b58\u5668\nB. \u8fd0\u7b97\u5668\u548c\u63a7\u5236\u5668\nC. \u8fd0\u7b97\u5668\u548c\u5185\u5b58\nD. \u63a7\u5236\u5668\u548c\u5185\u5b58\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5245246670051591, "meta-math/MetaMath-Mistral-7B": 0.901002704454697, "itpossible/Chinese-Mistral-7B-v0.1": 0.6108689907652102, "HuggingFaceH4/zephyr-7b-beta": 0.9992319233719434, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8292639282658535, "meta-llama/Meta-Llama-3-8B": 0.909387343816493, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9942934157854049}}, {"question": "\u4fe1\u606f\u5316\u6218\u4e89\u7684\u5236\u80dc\u7406\u5ff5\u662f()\uff0c\u901a\u8fc7\u7834\u51fb\u654c\u4eba\u4f5c\u6218\u4f53\u7cfb\uff0c\u8fbe\u5230\u5de7\u6218\u800c\u5c48\u4eba\u4e4b\u5175\u7684\u76ee\u7684\u3002\nA. \u6d88\u8017\u654c\u4eba\u3001\u6467\u6bc1\u654c\u4eba\nB. \u4fa6\u5bdf\u654c\u4eba\u3001\u6467\u6bc1\u654c\u4eba\nC. \u76d1\u89c6\u654c\u4eba\u3001\u63a7\u5236\u654c\u4eba\nD. \u63a7\u5236\u654c\u4eba\u3001\u762b\u75ea\u654c\u4eba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3974567345895239, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.474069820068834, "HuggingFaceH4/zephyr-7b-beta": 0.5586619144125526, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.402873681921027, "meta-llama/Meta-Llama-3-8B": 0.38181830457859056, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6592906865699353}}, {"question": "\u5728Excel\u2f2f\u4f5c\u8868\u7684\u5355\u5143\u683c\u4e2d\u8ba1\u7b97\u2f00\u7ec4\u6570\u636e\u540e\u51fa\u73b0########\uff0c\u8fd9\u662f\u7531\u4e8e\uff08 \uff09\u6240\u81f4\nA. \u5355\u5143\u683c\u663e\u793a\u5bbd\u5ea6\u4e0d\u591f\nB. \u8ba1\u7b97\u673a\u516c\u5f0f\u51fa\u9519\nC. \u6570\u636e\u683c\u5f0f\u51fa\u9519\nD. \u8ba1\u7b97\u6570\u636e\u51fa\u9519\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7267073509598792, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u6cd5\u7684\u521b\u5236\u4e0e\u9002\u7528\u4e3b\u4f53\u7684\u4e0d\u540c\uff0c\u6cd5\u53ef\u4ee5\u5206\u4e3a\nA. \u56fd\u5185\u6cd5\u4e0e\u56fd\u9645\u6cd5\nB. \u6839\u672c\u6cd5\u4e0e\u666e\u901a\u6cd5\nC. \u4e00\u822c\u6cd5\u4e0e\u7279\u522b\u6cd5\nD. \u6210\u6587\u6cd5\u4e0e\u4e0d\u6210\u6587\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.46714527813461526, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5224080269652552, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8436087248387826, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.583000389790348}}, {"question": "\u6709\u4e00\u7c7b\u75c5\u6bd2\u7684 RNA \u662f\u5177\u6709\u4fb5\u67d3\u6027\u7684\uff0c\u5b83\u662f\nA. -RNA \u75c5\u6bd2\nB. \u53cd\u8f6c\u5f55\u75c5\u6bd2\nC. +RNA \u75c5\u6bd2\nD. RNA \u75c5\u6bd2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6552169989509361, "meta-math/MetaMath-Mistral-7B": 0.9348738675930018, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.934910709711085, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8554850308800462, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u75c5\u6bd2\u7ed3\u6784\u53d9\u8ff0\u9519\u8bef\u7684\u662f\nA. \u8863\u58f3\u7531\u58f3\u7c92\u6784\u6210\nB. \u75c5\u6bd2\u5305\u819c\u8868\u9762\u53ef\u6709\u523a\u7a81\nC. \u6838\u9178\u548c\u8863\u58f3\u7ec4\u6210\u6838\u8863\u58f3\nD. \u6709\u5305\u819c\u7684\u75c5\u6bd2\u624d\u6709\u611f\u67d3\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5811754723694454, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5567499026621375, "HuggingFaceH4/zephyr-7b-beta": 0.9998989278301887, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8994483569533126, "meta-llama/Meta-Llama-3-8B": 0.7057115367770689, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5371\u9669\u62a5\u8b66\u95ea\u5149\u706f\u53ef\u7528\u4e8e\u4e0b\u5217\u4ec0\u4e48\u573a\u5408\nA. \u5f15\u9886\u540e\u8f66\u884c\u9a76\u65f6\nB. \u673a\u52a8\u8f66\u53d1\u751f\u6545\u969c\u505c\u8f66\u65f6\nC. \u5728\u9053\u8def\u4e0a\u8ddf\u8f66\u884c\u9a76\u65f6\nD. \u9047\u5230\u9053\u8def\u62e5\u5835\u65f6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6597546155179647, "meta-math/MetaMath-Mistral-7B": 0.733312420330307, "itpossible/Chinese-Mistral-7B-v0.1": 0.9415434140697411, "HuggingFaceH4/zephyr-7b-beta": 0.9966122586821394, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5822866226803364, "meta-llama/Meta-Llama-3-8B": 0.7398567899648871, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8eSVM\u6cdb\u5316\u8bef\u5dee\u63cf\u8ff0\u6b63\u786e\u7684\u662f\nA. \u8d85\u5e73\u9762\u4e0e\u652f\u6301\u5411\u91cf\u4e4b\u95f4\u8ddd\u79bb\nB. SVM\u7684\u8bef\u5dee\u9608\u503c\nC. \u4ee5\u4e0a\u90fd\u4e0d\u662f\nD. SVM\u5bf9\u672a\u77e5\u6570\u636e\u7684\u9884\u6d4b\u80fd\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.478664124476461, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.871934579619215, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u901a\u5e38\u53ef\u4ee5\u81ea\u5236\u54ea\u79cd\u6240\u9700\u7ef4\u751f\u7d20\nA. \u7ef4\u751f\u7d20D\nB. \u7ef4\u751f\u7d20B\nC. \u7ef4\u751f\u7d20A\nD. \u7ef4\u751f\u7d20E\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9959115085068376, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53d8\u5f02\u662f\u6307:\nA. \u5404\u89c2\u5bdf\u5355\u4f4d\u67d0\u6d4b\u5b9a\u503c\u5dee\u5f02\u8f83\u5927\nB. \u5404\u89c2\u5bdf\u5355\u4f4d\u4e4b\u95f4\u7684\u5dee\u5f02\nC. \u5404\u89c2\u5bdf\u5355\u4f4d\u6709\u5173\u60c5\u51b5\u4e0d\u540c\nD. \u540c\u8d28\u57fa\u7840\u4e0a\uff0c\u5404\u89c2\u5bdf\u5355\u4f4d\u4e4b\u95f4\u7684\u5dee\u5f02\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.45590670387610305, "meta-math/MetaMath-Mistral-7B": 0.6358412418538766, "itpossible/Chinese-Mistral-7B-v0.1": 0.45169546814576433, "HuggingFaceH4/zephyr-7b-beta": 0.999518848557765, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6371872450378857, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u79ef\u6781\u53c2\u4e0e\u7ecf\u6d4e\u5168\u7403\u5316\uff0c\u4ece\u5e02\u573a\u7ecf\u6d4e\u7279\u5f81\u6765\u770b\uff0c\u662f\u56e0\u4e3a\nA. \u793e\u4f1a\u5316\u5927\u751f\u4ea7\u5360\u4e86\u7edf\u6cbb\u5730\u4f4d\nB. \u5e02\u573a\u7ecf\u6d4e\u53ef\u4ee5\u5b9e\u73b0\u8d44\u6e90\u7684\u4f18\u5316\u914d\u7f6e\nC. \u5e02\u573a\u7ecf\u6d4e\u5177\u6709\u7ade\u4e89\u6027\uff0c\u53c2\u4e0e\u7ecf\u6d4e\u5168\u7403\u5316\u624d\u80fd\u5b58\u5728\u7ade\u4e89\nD. \u5e02\u573a\u7ecf\u6d4e\u5177\u6709\u5f00\u653e\u6027\uff0c\u53ea\u6709\u5bf9\u5916\u5f00\u653e\uff0c\u624d\u80fd\u626c\u957f\u907f\u77ed\uff0c\u53d1\u5c55\u81ea\u5df1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.47098022470269785, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5014158004789578, "HuggingFaceH4/zephyr-7b-beta": 0.9257834534042014, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6371872519275763, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5485244271967402}}, {"question": "\u5b9e\u73b0\u793e\u4f1a\u4e3b\u4e49\u7684\u5fc5\u8981\u653f\u6cbb\u524d\u63d0\u662f\nA. \u65e0\u4ea7\u9636\u7ea7\u7684\u9769\u547d\u6597\u4e89\nB. \u5efa\u7acb\u65e0\u4ea7\u9636\u7ea7\u4e13\u653f\nC. \u5efa\u7acb\u5e7f\u6cdb\u7684\u9769\u547d\u7edf\u4e00\u6218\u7ebf\nD. \u65e0\u4ea7\u9636\u7ea7\u653f\u515a\u7684\u9886\u5bfc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6536\u4e70\u88ab\u62d0\u5356\u7684\u5987\u5973\u3001\u513f\u7ae5\u7f6a\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u6709\nA. \u5bf9\u88ab\u4e70\u513f\u7ae5\u6ca1\u6709\u8650\u5f85\u884c\u4e3a\uff0c\u4e0d\u963b\u788d\u5bf9\u5176\u8fdb\u884c\u89e3\u6551\u7684\uff0c\u53ef\u4ee5\u4ece\u8f7b\u5904\u7f5a\u6216\u8005\u51cf\u8f7b\u5904\u7f5a\nB. \u6309\u7167\u88ab\u4e70\u5987\u5973\u7684\u610f\u613f\uff0c\u4e0d\u963b\u788d\u5176\u8fd4\u56de\u539f\u5c45\u4f4f\u5730\u7684\uff0c\u53ef\u4ee5\u4ece\u8f7b\u3001\u51cf\u8f7b\u6216\u8005\u514d\u9664\u5904\u7f5a\nC. \u5bf9\u88ab\u4e70\u513f\u7ae5\uff0c\u53ea\u8981\u4e0d\u963b\u788d\u5bf9\u5176\u8fdb\u884c\u89e3\u6551\u7684\uff0c\u5c31\u4ece\u8f7b\u5904\u7f5a\nD. \u5bf9\u88ab\u4e70\u513f\u7ae5\u6ca1\u6709\u8650\u5f85\u884c\u4e3a\uff0c\u4e0d\u963b\u788d\u5bf9\u5176\u8fdb\u884c\u89e3\u6551\u7684\uff0c\u53ef\u4ee5\u4ece\u8f7b\u5904\u7f5a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4488254281202955, "meta-math/MetaMath-Mistral-7B": 0.8701500956197081, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9956960958741882, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4550542394314207, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u5973\uff0c26\u5c81\u3002\u65b0\u5a5a\u7b2c3\u5929\uff0c\u5373\u53d1\u70ed\uff0c\u8170\u75db\uff0c\u5c3f\u9891\u5c3f\u6025\uff0c\u5c3f\u9053\u707c\u75db\uff0c\u5c0f\u4fbf\u9ec4\u5c11\uff0c\u820c\u7ea2\u82d4\u9ec4\uff0c\u8109\u6570\u3002\u4e34\u5e8a\u8bca\u65ad\u6700\u53ef\u80fd\u662f\nA. \u5c0f\u80a0\u5b9e\u70ed\u8bc1\nB. \u6e7f\u70ed\u4e0b\u6ce8\u8bc1\nC. \u8180\u80f1\u6e7f\u70ed\u8bc1\nD. \u70ed\u7ed3\u8180\u80f1\u8bc1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.46226550588276905}}, {"question": "\u540c\u8f66\u9053\u884c\u9a76\u7684\u8f66\u8f86\u524d\u65b9\u9047\u5230\u4e0b\u5217\u54ea\u79cd\u8f66\u8f86\u4e0d\u5f97\u8d85\u8f66\nA. \u5c0f\u578b\u8d27\u8f66\nB. \u6267\u884c\u4efb\u52a1\u7684\u6551\u62a4\u8f66\nC. \u8d85\u8f7d\u5927\u578b\u8d27\u8f66\nD. \u5927\u578b\u5ba2\u8f66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3695456302465622, "meta-math/MetaMath-Mistral-7B": 0.74094764627533, "itpossible/Chinese-Mistral-7B-v0.1": 0.5222529994711513, "HuggingFaceH4/zephyr-7b-beta": 0.9592918707307508, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7884979723669581, "meta-llama/Meta-Llama-3-8B": 0.786347195079244, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8269126345760738}}, {"question": "\u52d2\u5e9e\u4ece\u793e\u4f1a\u5fc3\u7406\u5b66\u89c6\u89d2\u5bf9\u96c6\u4f53\u884c\u4e3a\u8fdb\u884c\u89e3\u91ca\u7684\u7406\u8bba\u88ab\u79f0\u4e3a\nA. \u4e4c\u5408\u4e4b\u4f17\u7406\u8bba\nB. \u793e\u4f1a\u8fd0\u52a8\u7684\u8d44\u6e90\u52a8\u5458\u7406\u8bba\nC. \u642d\u4fbf\u8f66\u7406\u8bba\nD. \u4ef7\u503c\u7d2f\u52a0\u7406\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4843866949724652, "meta-math/MetaMath-Mistral-7B": 0.6197193450611695, "itpossible/Chinese-Mistral-7B-v0.1": 0.6567371114705436, "HuggingFaceH4/zephyr-7b-beta": 0.9497059791642364, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5937452345760874, "meta-llama/Meta-Llama-3-8B": 0.739953424862085, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.59646547582118}}, {"question": "\u5bf9\u4e24\u4e2a\u6570\u503c\u53d8\u91cf\u540c\u65f6\u8fdb\u884c\u4e86\u76f8\u5173\u548c\u56de\u5f52\u5206\u6790\uff0c $r$ \u6709\u7edf\u8ba1\u5b66\u610f\u4e49 $(P<0.05)$\uff0c\u5219\nA. \u4e0d\u80fd\u786e\u5b9a $\\mathrm{b}$ \u6709\u65e0\u7edf\u8ba1\u5b66\u610f\u4e49\u3002\nB. b \u65e0\u7edf\u8ba1\u5b66\u610f\u4e49\nC. b \u6709\u7edf\u8ba1\u5b66\u610f\u4e49\nD. $b$ \u6709\u9ad8\u5ea6\u7684\u7edf\u8ba1\u5b66\u610f\u4e49\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5788091380542656, "meta-math/MetaMath-Mistral-7B": 0.6401759613752381, "itpossible/Chinese-Mistral-7B-v0.1": 0.34993229125498293, "HuggingFaceH4/zephyr-7b-beta": 0.9522283543359696, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5846062109875091, "meta-llama/Meta-Llama-3-8B": 0.6804111615938152, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7682676719189269}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6587\u6863\u7a97\u53e3\u7684\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u53ea\u80fd\u6253\u5f00\u4e00\u4e2a\u6587\u6863\u7a97\u53e3\nB. \u53ef\u4ee5\u540c\u65f6\u6253\u5f00\u591a\u4e2a\u6587\u6863\u7a97\u53e3\uff0c\u4f46\u5728\u5c4f\u5e55\u4e0a\u53ea\u80fd\u89c1\u5230\u4e00\u4e2a\u6587\u6863\u7684\u7a97\u53e3\nC. \u53ef\u4ee5\u540c\u65f6\u6253\u5f00\u591a\u4e2a\u6587\u6863\u7a97\u53e3\uff0c\u4f46\u5176\u4e2d\u53ea\u6709\u4e00\u4e2a\u662f\u6d3b\u52a8\u7a97\u53e3\nD. \u53ef\u4ee5\u540c\u65f6\u6253\u5f00\u591a\u4e2a\u6587\u6863\u7a97\u53e3\uff0c\u88ab\u6253\u5f00\u7684\u7a97\u53e3\u90fd\u662f\u6d3b\u52a8\u7a97\u53e3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8427617315332486, "meta-math/MetaMath-Mistral-7B": 0.953718631841795, "itpossible/Chinese-Mistral-7B-v0.1": 0.3277078385444777, "HuggingFaceH4/zephyr-7b-beta": 0.9999233279680361, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9459667892909228, "meta-llama/Meta-Llama-3-8B": 0.8938478298241372, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9514793224336149}}, {"question": "\u4fe1\u606f\u5b89\u5168\u4e2dPDR\u6a21\u578b\u7684\u5173\u952e\u56e0\u7d20\u662f\nA. \u6280\u672f\nB. \u6a21\u578b\nC. \u4eba\nD. \u5ba2\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6898286481329081, "meta-math/MetaMath-Mistral-7B": 0.9604579170684026, "itpossible/Chinese-Mistral-7B-v0.1": 0.7326879605263883, "HuggingFaceH4/zephyr-7b-beta": 0.9897858534297912, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5641403828750953, "meta-llama/Meta-Llama-3-8B": 0.3660090230845428, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6011202127934413}}, {"question": "\u4e0b\u5217\u6709\u5173\u4eba\u4eec\u5bf9\u672a\u6765\u7684\u5411\u5f80\u548c\u8ffd\u6c42\u4e2d\uff0c\u5c5e\u4e8e\u793e\u4f1a\u7406\u60f3\u7684\u662f\nA. \u628a\u6211\u56fd\u5efa\u8bbe\u6210\u4e3a\u5bcc\u5f3a\u3001\u6c11\u4e3b\u3001\u6587\u660e\u7684\u793e\u4f1a\u4e3b\u4e49\u73b0\u4ee3\u5316\u56fd\u5bb6\nB. \u201c\u5bcc\u8d35\u4e0d\u80fd\u6deb\uff0c\u8d2b\u8d31\u4e0d\u80fd\u79fb\uff0c\u5a01\u6b66\u4e0d\u80fd\u5c48\u201d\nC. \u201c\u4e09\u5341\u4ea9\u5730\u4e00\u5934\u725b\uff0c\u8001\u5a46\u5b69\u5b50\u70ed\u7095\u5934\u201d\nD. \u8c0b\u4e00\u4e2a\u9002\u5408\u81ea\u5df1\u7684\u804c\u4f4d\uff0c\u5e72\u4e00\u756a\u76ca\u4e8e\u4eba\u6c11\u7684\u4e8b\u4e1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9802394115613592, "meta-math/MetaMath-Mistral-7B": 0.9992064397538147, "itpossible/Chinese-Mistral-7B-v0.1": 0.8555541738846235, "HuggingFaceH4/zephyr-7b-beta": 0.9992960240910881, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9875658101962725, "meta-llama/Meta-Llama-3-8B": 0.5893012251445238, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.932670965279092}}, {"question": "\u5173\u4e8e\u75c5\u6bd2\u9057\u4f20\u7269\u8d28\u7684\u53d9\u8ff0\uff0c\u4e0b\u5217\u54ea\u4e00\u9879\u6b63\u786e\u7684\nA. \u90fd\u662fRNA\nB. \u540c\u65f6\u5b58\u5728DNA\u548cRNA\nC. \u90fd\u662fDNA\nD. \u6709\u7684\u662fDNA\uff0c\u6709\u7684\u662fRNA\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6524676755093666, "meta-math/MetaMath-Mistral-7B": 0.8734226303796911, "itpossible/Chinese-Mistral-7B-v0.1": 0.6109588665935083, "HuggingFaceH4/zephyr-7b-beta": 0.9993130213029532, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9264341525931806, "meta-llama/Meta-Llama-3-8B": 0.6159436046177763, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u8d1f\u9700\u6c42\u60c5\u51b5\u4e0b\uff0c\u5e02\u573a\u8425\u9500\u7ba1\u7406\u7684\u4efb\u52a1\u662f\nA. \u523a\u6fc0\u5e02\u573a\u8425\u9500\nB. \u5f00\u53d1\u5e02\u573a\u8425\u9500\nC. \u6539\u53d8\u5e02\u573a\u8425\u9500\nD. \u91cd\u632f\u5e02\u573a\u8425\u9500\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5751870025025523, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6565706970317761}}, {"question": "\u9a7e\u9a76\u4eba\u5728\u9a7e\u9a76\u8bc1\u6709\u6548\u671f\u6ee1\u524d\u591a\u957f\u65f6\u95f4\u7533\u8bf7\u6362\u8bc1\nA. 60\u65e5\u5185\nB. 30\u65e5\u5185\nC. 90\u65e5\u5185\nD. 6\u4e2a\u6708\u5185\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u6d41\u57df\u9762\u79ef\u4e3a 1000km\uff0c\u591a\u5e74\u5e73\u5747\u964d\u6c34\u91cf\u4e3a 1050mm\uff0c\u591a\u5e74\u5e73\u5747\u6d41\u91cf\u4e3a 15m^3/s\uff0c\u8be5\u6d41\u57df\u591a\u5e74\u5e73\u5747\u7684\u5f84\u6d41\u7cfb\u6570\u4e3a[ ]\u3002\nA. 0.55\nB. 0.45\nC. 0.68\nD. 0.65\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0a\u6d77\u79d1\u7814\u2f08\u5458\u5c06\u592a\u9633\u80fd\u6280\u672f\u4e0e\u6d77\u2f54\u6de1\u5316\u2f2f\u7a0b\u5de7\u5999\u201c\u5ac1\u63a5\u201d\uff0c\u2f24\u5e45\u5ea6\u964d\u4f4e\u4e86\u6d77\u2f54\u6de1\u5316\u7684\u6210\u672c\u3002\u8fd9\u2f00\u6210\u679c\u5c06\u60e0\u6cfd\u201c\u2f00\u5e26\u2f00\u8def\u201d\u4e0a\u4e25\u91cd\u7f3a\u2f54\u2f7d\u5149\u70ed\u8d44\u6e90\u4e30\u5bcc\u7684\u5730\u533a\u3002\u8fd9\u4e9b\u5730\u533a\u4e3b\u8981\u662f\nA. \u4e1c\u4e9a\u3001\u5357\u4e9a\nB. \u4e2d\u4e9a\u3001\u2ec4\u4e9a\nC. \u2f24\u6d0b\u6d32\u3001\u2ec4\u6b27\nD. \u4e1c\u6b27\u3001\u4e1c\u5357\u4e9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5140409661471672, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5386\u7ecf\u6570\u5343\u5e74\u5927\u98ce\u5927\u6d6a\u548c\u5174\u8870\u53d8\u5316\u7684\u8003\u9a8c\uff0c\u6574\u4e2a\u4e2d\u534e\u6c11\u65cf\u4e00\u76f4\u4fdd\u6301\u7740\u84ec\u52c3\u7684\u751f\u673a\u548c\u65fa\u76db\u7684\u6d3b\u529b\uff0c\u7a33\u56fa\u5730\u51dd\u805a\u5728\u4e00\u8d77\uff0c\u5916\u6765\u52bf\u529b\u51b2\u4e0d\u6563\uff0c\u5e1d\u56fd\u4e3b\u4e49\u6253\u4e0d\u57ae\u3002\u8fd9\u91cc\u56fa\u7136\u6709\u7ecf\u6d4e\u3001\u653f\u6cbb\u3001\u793e\u4f1a\u7b49\u65b9\u9762\u7684\u539f\u56e0\uff0c\u4f46\u5176\u4e2d\u6700\u6839\u672c\u7684\u662f\uff0c\u6211\u4eec\u56fd\u5bb6\u548c\u6c11\u65cf\u5177\u6709\u60a0\u4e45\u7684\nA. \u7231\u56fd\u4e3b\u4e49\u4f20\u7edf\nB. \u4e2a\u4eba\u4e3b\u4e49\u4f20\u7edf\nC. \u81ea\u7531\u4e3b\u4e49\u4f20\u7edf\nD. \u793e\u4f1a\u4e3b\u4e49\u4f20\u7edf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9390432065023842, "meta-math/MetaMath-Mistral-7B": 0.9616119617053057, "itpossible/Chinese-Mistral-7B-v0.1": 0.9861616377485785, "HuggingFaceH4/zephyr-7b-beta": 0.9999531256270601, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.981584404126713, "meta-llama/Meta-Llama-3-8B": 0.9757559736792561, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9773519297087703}}, {"question": "\u6709\u5229\u4e8e\u975e\u8840\u7ea2\u7d20\u94c1\u5438\u6536\u7684\u662f\nA. \u7ef4\u751f\u7d20C\nB. \u8349\u9178\nC. \u81b3\u98df\u7ea4\u7ef4\nD. \u9499\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7088297005418055, "meta-math/MetaMath-Mistral-7B": 0.9419143355606613, "itpossible/Chinese-Mistral-7B-v0.1": 0.9098677016340895, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.73217033894344, "meta-llama/Meta-Llama-3-8B": 0.9301188278480838, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6309\u7167\u9009\u62e9\u6027\u56e0\u7d20\u7406\u8bba\u7684\u89c2\u70b9\uff0c\u5927\u4f17\u4f20\u5a92\u53d1\u51fa\u7684\u4fe1\u606f\u4f20\u5411\u53d7\u4f17\u65f6\u9047\u5230\u7684\u7b2c\u4e00\u5173\u662f\u53d7\u4f17\u7684\nA. \u9009\u62e9\u6027\u6ce8\u610f\nB. \u9009\u62e9\u6027\u7406\u89e3\nC. \u9009\u62e9\u6027\u8bb0\u5fc6\nD. \u9009\u62e9\u6027\u66b4\u9732\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34412869855849515, "meta-math/MetaMath-Mistral-7B": 0.5533142653531633, "itpossible/Chinese-Mistral-7B-v0.1": 0.41829520677068827, "HuggingFaceH4/zephyr-7b-beta": 0.9041388804538298, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9153194054207678, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8576279274854776}}, {"question": "\u86cb\u767d\u8d28\u662f\u51b3\u5b9a\u751f\u7269\u4f53\u7ed3\u6784\u548c\u529f\u80fd\u7684\u91cd\u8981\u7269\u8d28\u3002\u4e0b\u5217\u76f8\u5173\u53d9\u8ff0\u9519\u8bef\u7684\u662f\nA. \u7ec6\u80de\u5185\u86cb\u767d\u8d28\u53d1\u751f\u6c34\u89e3\u65f6\uff0c\u901a\u5e38\u9700\u8981\u53e6\u4e00\u79cd\u86cb\u767d\u8d28\u7684\u53c2\nB. \u86cb\u767d\u8d28\u7684\u57fa\u672c\u6027\u8d28\u4e0d\u4ec5\u4e0e\u78b3\u9aa8\u67b6\u6709\u5173\uff0c\u800c\u4e14\u4e5f\u4e0e\u529f\u80fd\u57fa\u56e2\u6709\u5173\nC. \u7ec6\u80de\u819c\u3001\u7ec6\u80de\u8d28\u57fa\u8d28\u4e2d\u8d1f\u8d23\u8f6c\u8fd0\u6c28\u57fa\u9178\u7684\u8f7d\u4f53\u90fd\u662f\u86cb\u767d\u8d28\nD. \u6c28\u57fa\u9178\u4e4b\u95f4\u8131\u6c34\u7f29\u5408\u751f\u6210\u7684H2O\u4e2d\uff0c\u6c22\u6765\u81ea\u4e8e\u6c28\u57fa\u548c\u7fa7\u57fa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u91c7\u7528\u7535\u6297\u5668\u964d\u538b\u542f\u52a8\uff0c\u4ee5\u4e0b\u53d9\u8ff0\u4e0d\u6b63\u786e\u7684\u662f\nA. \u7535\u6297\u5668\u964d\u538b\u542f\u52a8\u4e00\u822c\u5e94\u7528\u4e8e\u4f4e\u538b\u5c0f\u5bb9\u91cf\u7535\u52a8\u673a\nB. \u7535\u6297\u5668\u964d\u538b\u542f\u52a8\u4e00\u822c\u5e94\u7528\u4e8e\u9ad8\u538b\u7535\u52a8\u673a\nC. \u91c7\u7528\u7535\u6297\u5668\u964d\u538b\u542f\u52a8\uff0c\u7535\u52a8\u673a\u7684\u8f6c\u77e9\u6309\u5176\u7aef\u7535\u538b\u4e8c\u6b21\u65b9\u7684\u6bd4\u4f8b\u964d\u4f4e\nD. \u91c7\u7528\u7535\u6297\u5668\u964d\u538b\u542f\u52a8\uff0c\u7535\u52a8\u673a\u7684\u7535\u6d41\u6309\u5176\u7aef\u7535\u538b\u7684\u6bd4\u4f8b\u964d\u4f4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u56fd\u5bb6\u6d77\u5173\u5f81\u6536\u7684\u5173\u7a0e\u662f\nA. \u8d22\u653f\u6536\u5165\u7684\u4e3b\u8981\u6765\u6e90\nB. \u4e13\u95e8\u5bf9\u51fa\u53e3\u5546\u54c1\u5f81\u6536\u7684\u7a0e\u79cd\nC. \u5e02\u573a\u8c03\u63a7\u7684\u4e3b\u8981\u6760\u6746\nD. \u6211\u56fd\u7a0e\u6536\u7684\u91cd\u8981\u7a0e\u79cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6276979415386279, "meta-math/MetaMath-Mistral-7B": 0.7926424074427059, "itpossible/Chinese-Mistral-7B-v0.1": 0.60464964357264, "HuggingFaceH4/zephyr-7b-beta": 0.9769165960005517, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7522657911029285, "meta-llama/Meta-Llama-3-8B": 0.45961515612572623, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u2f0f\u5185\u4e9a\u94dd\u2f1f\u8d44\u6e90\u4e30\u5bcc\uff0c\u6240\u4ea7\u94dd\u2f1f\u77ff\u2f0f\u4e4e\u5168\u90e8\u4f9b\u51fa\u2f1d\u30022014\u5e74\uff0c\u4e2d\u56fd\u67d0\u4f01\u4e1a\u6295\u8d442\u4ebf\u7f8e\u5143\uff0c\u5f00\u91c7\u2f0f\u5185\u4e9a\u535a\u51ef\u5730\u533a\u7684\u94dd\u2f1f\u77ff\u30022015\u5e749\u2f4929\u2f47\uff0c\u2fb8\u6761\u6ee1\u8f7d\u94dd\u2f1f\u77ff\u768418\u4e07\u5428\u6563\u88c5\u8239\u4ece\u2f0f\u5185\u4e9a\u542f\u7a0b\u5230\u8fbe\u6211\u56fd\u70df\u53f0\u6e2f\u3002\u2f6c\u524d\uff0c\u8be5\u4f01\u4e1a\u6b63\u8ba1\u5212\u52a0\u2f24\u5728\u2f0f\u5185\u4e9a\u6295\u8d44\uff0c\u5efa\u8bbe\u7535\u89e3\u94dd\u57fa\u5730\u3002\u4e2d\u56fd\u4f01\u4e1a\u5728\u2f0f\u5185\u4e9a\u6295\u8d44\u5f00\u91c7\u94dd\u2f1f\u77ff\u7684\u5f71\u54cd\u4e3b\u8981\u662f\nA. \u6269\u2f24\u2f0f\u5185\u4e9a\u5916\u6c47\u6536\u2f0a\u6765\u6e90\nB. \u964d\u4f4e\u6211\u56fd\u94dd\u4ea7\u4e1a\u2f63\u4ea7\u6210\u672c\nC. \u63d0\u2fbc\u2f0f\u5185\u4e9a\u94dd\u2f1f\u77ff\u9644\u52a0\u503c\nD. \u63d0\u2fbc\u6211\u56fd\u94dd\u2f1f\u77ff\u7684\u2f83\u7ed9\u7387\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5404\u7ec4\u6210\u8bed\u4e2d\"\u5dee\"\u7684\u610f\u4e49\u4e0e\u8bfb\u97f3\u76f8\u540c\u7684\u4e00\u7ec4\u662f\nA. \u4e00\u5ff5\u4e4b\u5dee \u4e00\u5dee\u4e8c\u9519 \u6beb\u65e0\u5dee\u522b\nB. \u5dee\u4e4b\u6beb\u5398 \u53c2\u5dee\u4e0d\u9f50 \u5dee\u4e09\u9519\u56db\nC. \u5c61\u51fa\u5dee\u9519 \u6beb\u65e0\u5dee\u522b \u5dee\u5f3a\u4eba\u610f\nD. \u9b3c\u4f7f\u795e\u5dee \u94a6\u5dee\u5927\u81e3 \u5e94\u4ed8\u5dee\u4e8b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5\u968f\u673a\u53d8\u91cf $X$ \u4e0e $Y$ \u5747\u670d\u4ece\u5206\u5e03 $B\\left(1\uff0c \\frac{3}{4}\\right)$\uff0c \u4e14 $\\rho_{X Y}=\\frac{1}{3}$\uff0c \u5219 $P\\{X+Y \\leq 1\\}=$\nA. $\\frac{1}{2}$\nB. $\\frac{1}{4}$\nC. $\\frac{3}{8}$\nD. $\\frac{5}{8}$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3588823130168717, "meta-math/MetaMath-Mistral-7B": 0.4784810179550741, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.581712675115079, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45504753125853586, "meta-llama/Meta-Llama-3-8B": 0.3306562312783846, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7317673895782687}}, {"question": "\u6982\u7387P\u7684\u8303\u56f4\nA. O\u2264P\u22641\nB. O<P<1\nC. -1\u2264P\u22641\nD. P>1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6056904704615578, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6441997464556699, "HuggingFaceH4/zephyr-7b-beta": 0.9984630738039404, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5416364666227671, "meta-llama/Meta-Llama-3-8B": 0.554426385364205, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d44\u672c\u4e3b\u4e49\u751f\u4ea7\u7684\u5fc5\u8981\u6761\u4ef6\u662f\nA. \u8d27\u5e01\u51fa\u73b0\nB. \u793e\u4f1a\u5206\u5de5\nC. \u5bf9\u5916\u8d38\u6613\u7684\u53d1\u5c55\nD. \u6280\u672f\u8fdb\u6b65\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u300a\u7ee7\u627f\u6cd5\u300b\u7684\u6709\u5173\u89c4\u5b9a\uff0c\u4e0b\u5217\u6709\u5173\u7ee7\u627f\u7684\u8868\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u8f6c\u7ee7\u627f\u53ea\u80fd\u9002\u7528\u4e8e\u6cd5\u5b9a\u7ee7\u627f\uff0c\u800c\u4e0d\u80fd\u9002\u7528\u4e8e\u9057\u5631\u7ee7\u627f\nB. \u53d7\u9057\u8d60\u4eba\u653e\u5f03\u53d7\u9057\u8d60\u53ea\u80fd\u91c7\u53d6\u660e\u793a\u65b9\u5f0f\uff0c\u800c\u4e0d\u80fd\u91c7\u53d6\u9ed8\u793a\u65b9\u5f0f\nC. \u7ee7\u627f\u4eba\u653e\u5f03\u7ee7\u627f\u7684\u65b9\u5f0f\u53ef\u4ee5\u91c7\u53d6\u660e\u793a\u548c\u9ed8\u793a\u4e24\u79cd\nD. \u4ee3\u4f4d\u7ee7\u627f\u53ea\u80fd\u9002\u7528\u4e8e\u6cd5\u5b9a\u7ee7\u627f\uff0c\u800c\u4e0d\u80fd\u9002\u7528\u4e8e\u9057\u5631\u7ee7\u627f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u4e0d\u5c5e\u4e8e\u5f71\u54cd\u4e16\u754c\u5e02\u573a\u53d1\u8fbe\u7a0b\u5ea6\u7684\u56e0\u7d20\u7684\u662f\nA. \u8d44\u672c\u4e3b\u4e49\u53d1\u5c55\u7a0b\u5ea6\nB. \u56fd\u9645\u5206\u5de5\u7684\u5e7f\u5ea6\nC. \u53c2\u4e0e\u7684\u56fd\u5bb6\u6570\u91cf\nD. \u5546\u54c1\u603b\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6709\u5173\u6cd5\u5f8b\u89c4\u8303\u7684\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u6cd5\u5f8b\u89c4\u8303\u5305\u62ec\u89c4\u8303\u6027\u6587\u4ef6\nB. \u6cd5\u5f8b\u89c4\u8303\u4e0e\u5176\u4ed6\u793e\u4f1a\u89c4\u8303\u5728\u8bbe\u5b9a\u4eba\u4eec\u884c\u4e3a\u7684\u8303\u56f4\u548c\u7a0b\u5ea6\u4e0a\u5b8c\u5168\u76f8\u540c\nC. \u6cd5\u5f8b\u89c4\u8303\u4e0e\u6cd5\u5f8b\u6761\u6587\u7684\u5185\u6db5\u53ca\u5916\u5728\u8868\u73b0\u5f62\u5f0f\u5b8c\u5168\u76f8\u540c\nD. \u6cd5\u5f8b\u89c4\u8303\u4e0e\u975e\u89c4\u8303\u6027\u6587\u4ef6\u4e0d\u540c\uff0c\u5177\u6709\u666e\u904d\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6314394502656754, "meta-math/MetaMath-Mistral-7B": 0.8947681072318797, "itpossible/Chinese-Mistral-7B-v0.1": 0.657155010764507, "HuggingFaceH4/zephyr-7b-beta": 0.9977373146043879, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6731278560869272, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5957\u8def\u8d37\u201d\u662f\u5047\u501f\u6c11\u95f4\u501f\u8d37\u540d\u4e49\uff0c\u975e\u6cd5\u5360\u6709\u88ab\u5bb3\u4eba\u8d22\u7269\u7684\u8fdd\u6cd5\u72af\u7f6a\u884c\u4e3a\uff0c\u662f\u626b\u9ed1\u9664\u6076\u4e13\u9879\u6597\u4e89\u7684\u91cd\u70b9\u6253\u51fb\u5bf9\u8c61\u3002\u4e0b\u5217\u5173\u4e8e\u201c\u5957\u8def\u8d37\u201d\u7684\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u56e0\u6cd5\u5f8b\u89c4\u5b9a\u6c11\u95f4\u501f\u8d37\u6700\u9ad8\u5e74\u5229\u7387\u4e3a36%\uff0c\u6545\u8d85\u8fc736%\u7684\u53ef\u8ba4\u5b9a\u4e3a\u201c\u5957\u8def\u8d37\u201d\nB. \u201c\u5957\u8def\u8d37\u201d\u72af\u7f6a\u6848\u4ef6\u53ef\u7531\u72af\u7f6a\u884c\u4e3a\u53d1\u751f\u5730\u3001\u7ed3\u679c\u53d1\u751f\u5730\u53ca\u72af\u7f6a\u5acc\u7591\u4eba\u5c45\u4f4f\u5730\u516c\u5b89\u673a\u5173\u4fa6\u67e5\nC. \u56e0\u201c\u5957\u8def\u8d37\u201d\u5c5e\u4e8e\u975e\u6cd5\u884c\u4e3a\uff0c\u8ffd\u503a\u65f6\u4e00\u822c\u4e0d\u4f1a\u91c7\u7528\u4ef2\u88c1\u3001\u8bc9\u8bbc\u65b9\u5f0f\nD. \u6211\u56fd\u5211\u6cd5\u8bbe\u7f6e\u6709\u4e13\u95e8\u9488\u5bf9\u201c\u5957\u8def\u8d37\u201d\u7684\u7f6a\u540d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.31110415356189847, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5443943063547284, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5586619201441899}}, {"question": "\u884c\u8f66\u4e2d\u9047\u5230\u5bf9\u5411\u6765\u8f66\u5360\u9053\u884c\u9a76\uff0c\u5e94\u600e\u6837\u505a\nA. \u7d27\u9760\u9053\u8def\u4e2d\u5fc3\u884c\u9a76\nB. \u7528\u5927\u706f\u8b66\u793a\u5bf9\u65b9\nC. \u4e3b\u52a8\u7ed9\u5bf9\u65b9\u8ba9\u884c\nD. \u903c\u5bf9\u65b9\u9760\u53f3\u884c\u9a76\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6565702804998902, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7433686032716543, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9813988585953352}}, {"question": "\u4ece\u5341\u4e5d\u4e16\u7eaa\u672b\u53f6\u8d77\uff0c\u5728\u56fd\u9645\u53f8\u6cd5\u9886\u57df\u4e2d\uff0c\u5f00\u59cb\u51fa\u73b0\u4e86\u4e00\u4e9b\u4ece\u4e8b\u7edf\u4e00\u56fd\u9645\u53f8\u6cd5\u5de5\u4f5c\u7684\u6709\u5f71\u54cd\u7684\u56fd\u9645\u7ec4\u7ec7\u3002\u5176\u4e2d\uff0c\u5c31\u7edf\u4e00\u51b2\u7a81\u6cd5\u4e0e\u7a0b\u5e8f\u6cd5\u800c\u8a00\uff0c\u6700\u6709\u6210\u6548\u3001\u6700\u6709\u5f71\u54cd\u7684\u5f53\u9996\u63a8\nA. \u7f57\u9a6c\u56fd\u9645\u7edf\u4e00\u53f8\u6cd5\u5b66\u4f1a\nB. \u8054\u5408\u56fd\u56fd\u9645\u8d38\u6613\u6cd5\u59d4\u5458\u4f1a\nC. \u56fd\u9645\u5546\u4f1a\nD. \u6d77\u7259\u56fd\u9645\u53f8\u6cd5\u4f1a\u8bae\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5993773984734513, "meta-math/MetaMath-Mistral-7B": 0.8142747434175046, "itpossible/Chinese-Mistral-7B-v0.1": 0.5963069382596721, "HuggingFaceH4/zephyr-7b-beta": 0.9987824304200623, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8588641481386814, "meta-llama/Meta-Llama-3-8B": 0.7283124283578065, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9866249631110158}}, {"question": "\u82e5\u4e00\u4e2a\u7ba1\u7406\u673a\u6784\u5bf9\u4e00\u4e2a\u5784\u65ad\u4f01\u4e1a\u7684\u9650\u4ef7\u6b63\u597d\u4f7f\u7ecf\u6d4e\u5229\u6da6\u6d88\u5931\uff0c\u5219\u4ef7\u683c\u8981\u7b49\u4e8e\nA. \u5e73\u5747\u6210\u672c\nB. \u5e73\u5747\u53ef\u53d8\u6210\u672c\nC. \u8fb9\u9645\u6536\u76ca\nD. \u8fb9\u9645\u6210\u672c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.43932249108141946, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.36694471738720585}}, {"question": "\u5207\u5272\u73bb\u7483\u7684\u58f0\u97f3\u4f1a\u4f7f\u4eba\u4ea7\u751f\u5bd2\u51b7\u7684\u611f\u89c9\uff0c\u8fd9\u79cd\u73b0\u8c61\u5c5e\u4e8e\nA. \u611f\u89c9\u9002\u5e94\nB. \u611f\u89c9\u4ee3\u507f\nC. \u8054\u89c9\nD. \u611f\u89c9\u5bf9\u6bd4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.46454998140218834, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.561170469631435}}, {"question": "\uff08\uff09\u662f\u4ee5\u63d0\u9ad8\u7ec4\u7ec7\u77e5\u540d\u5ea6\uff0c\u6811\u7acb\u7ec4\u7ec7\u6574\u4f53\u5f62\u8c61\u4e3a\u76ee\u6807\u7684\u516c\u5173\u5e7f\u544a\u3002\nA. \u5546\u54c1\u5e7f\u544a\nB. \u5e7f\u64ad\u5e7f\u544a\nC. \u5f62\u8c61\u5e7f\u544a\nD. \u7535\u89c6\u5e7f\u544a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9419802476644265, "meta-math/MetaMath-Mistral-7B": 0.990256589311847, "itpossible/Chinese-Mistral-7B-v0.1": 0.9419160007257743, "HuggingFaceH4/zephyr-7b-beta": 0.9998279678730669, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9626167086983772, "meta-llama/Meta-Llama-3-8B": 0.9388596724570754, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9253828814219812}}, {"question": "\u4e00\u4e2a\u5386\u53f2\u4e8b\u5b9e\u53ef\u4ee5\u7531\u82e5\u5e72\u4e8b\u4ef6\u6784\u6210\uff0c\u4e8b\u4ef6\u53c8\u53ef\u4ee5\u7531\u82e5\u5e72\u5c0f\u4e8b\u4ef6\u6784\u6210\uff0c\u7531\u6b64\u53ef\u4ee5\u6392\u51fa\u4e00\u7cfb\u5217\u4e8b\u4ef6\u7684\u7b49\u7ea7\u6216\u5c42\u6b21\u3002\u4e0b\u5217\u9009\u9879\u7b26\u5408\u4e0a\u8ff0\u8bf4\u6cd5\u7684\u662f\nA. \u7f57\u65af\u798f\u65b0\u653f\u2014\u2014\u300a\u5168\u56fd\u5de5\u4e1a\u590d\u5174\u6cd5\u300b\u2014\u2014\u300a\u793e\u4f1a\u4fdd\u9669\u6cd5>>\nB. \u79d1\u5b66\u793e\u4f1a\u4e3b\u4e49\u8bde\u751f\u2014\u2014\u5341\u6708\u9769\u547d\u2014\u2014\u793e\u4f1a\u4e3b\u4e49\u601d\u6f6e\nC. \u51e1\u5c14\u8d5b\u2014\u534e\u76db\u987f\u4f53\u7cfb\u2014\u2014\u534e\u76db\u987f\u4f1a\u8bae\u2014\u2014\u300a\u56db\u56fd\u6761\u7ea6>>\nD. \u7b2c\u4e8c\u6b21\u4e16\u754c\u5927\u6218\u2014\u2014\u5fb7\u56fd\u7a81\u88ad\u6ce2\u5170\u2014\u2014\u5fb7\u56fd\u6295\u964d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4482780958461333, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.37238013739438847, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8070846787174939}}, {"question": "\u6211\u56fd\u56db\u5927\u540d\u4ead\u4e2d\u54ea\u4e00\u5ea7\u4ead\u540d\u662f\u6bdb\u6cfd\u4e1c\u9898\u7684\nA. \u6e56\u5fc3\u4ead\nB. \u9676\u7136\u4ead\nC. \u9189\u7fc1\u4ead\nD. \u7231\u665a\u4ead\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u4f46\u613f\u4eba\u957f\u4e45\uff0c\u5343\u91cc\u5171\u5a75\u5a1f\u201d\uff0c\u5176\u4e2d\u5a75\u5a1f\u6307\u7684\u662f\u4ec0\u4e48\nA. \u5a5a\u59fb\nB. \u59fb\u7f18\nC. \u4e00\u79cd\u52a8\u7269\nD. \u6708\u4eae\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.9495373215987021, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9797801182737037, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9785142093570663}}, {"question": "\u793e\u4f1a\u4e3b\u4e49\u9053\u5fb7\u5efa\u8bbe\u7684\u6838\u5fc3\u662f\nA. \u4e3a\u4eba\u6c11\u670d\u52a1\nB. \u5efa\u8bbe\u548c\u8c10\u793e\u4f1a\nC. \u4e3a\u4ed6\u4eba\u7740\u60f3\nD. \u7ef4\u62a4\u793e\u4f1a\u7a33\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.45751603121289286, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u75c5\u4eba\u5728\u884c\u8fdc\u7aef\u56de\u80a0\u5207\u9664\u672f\u540e\u53ef\u53d1\u751f\u7684\u8d2b\u8840\u662f\nA. \u6eb6\u8840\u6027\u8d2b\u8840\nB. \u518d\u751f\u969c\u788d\u6027\u8d2b\u8840\nC. \u6076\u6027\u8d2b\u8840\nD. \u5de8\u5e7c\u7ec6\u80de\u8d2b\u8840\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u627e\u89c4\u5f8b\u586b\u6570\u5b57\u662f\u4e00\u9879\u5f88\u6709\u8da3\u7684\u6e38\u620f\uff0c\u7279\u522b\u953b\u70bc\u89c2\u5bdf\u548c\u601d\u8003\u80fd\u529b\uff0c\u6309\u7167\u201c2+5+7\u2192144935\u201d\u201c3+5+6\u2192184830\u201d\u201c4+4+9\u2192367236\u201d\u7684\u89c4\u5f8b\uff0c\u4ee5\u4e0b\u9009\u9879\u4e2d\u6b63\u786e\u7684\u9009\u9879\u662f\nA. 7+6+4\u2192285224\nB. 7+6+4\u2192422452\nC. 7+6+4\u2192422824\nD. 7+6+4\u2192284270\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3287145937103622, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5280592543921383}}, {"question": "\u4e0b\u5217\u4f1a\u8ba1\u51ed\u8bc1\u4e2d\uff0c\u5c5e\u4e8e\u539f\u59cb\u51ed\u8bc1\u7684\u662f\nA. \u6536\u6b3e\u51ed\u8bc1\nB. \u94f6\u884c\u5b58\u6b3e\u4f59\u989d\u8c03\u8282\u8868\nC. \u6536\u6599\u5355\nD. \u79d1\u76ee\u6c47\u603b\u8868\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.579383913280746, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c17\u5c81\u3002\u534a\u5c0f\u65f6\u524d\u56e0\u8df3\u9a6c\u6bd4\u8d5b\u4e0d\u614e\u9888\u90e8\u53d7\u4f24\uff0c\u521d\u6b65\u68c0\u67e5\uff1a\u60a3\u8005\u53ef\u4e3b\u52a8\u4f5c\u80a9\u524d\u5c48\u3001\u8098\u5c48\u8fd0\u52a8\uff0c\u4f46\u4e0d\u80fd\u4e3b\u52a8\u4f5c\u8098\u4f38\u8fd0\u52a8\uff0c\u53cc\u4e0b\u80a2\u8f6f\u762b\u3002\u8be5\u60a3\u8005\u635f\u4f24\u90e8\u4f4d\u6700\u53ef\u80fd\u662f\u4f4d\u4e8e\nA. \u98886\u30017\nB. \u98883\u30014\nC. \u98884\u30015\nD. \u98885\u30016\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.2741610822679311, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u56db\u7ec4\u5b57\u4e2d\uff0c\u5168\u662f\u4f1a\u610f\u5b57\u7684\u4e00\u7ec4\u662f\nA. \u8349\u614e\u6b66\u4fe1\nB. \u73b0\u8d23\u81f3\u79c9\nC. \u4ece\u6218\u5929\u8d70\nD. \u96c6\u6cea\u627f\u83ab\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9218241805185844, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.47432233369628546, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u53e4\u88c5\u5267\u4e2d\uff0c\u5973\u5b50\u53ef\u540c\u7537\u5b50--\u6837\uff0c\u4eba\u79c1\u587e\u5ff5\u4e66\u3002\u8fd9\u4e0e\u897f\u65b9()\u7684\u6559\u80b2\u6709\u76f8\u4f3c\u4e4b\u5904\u3002\nA. \u53e4\u57c3\u53ca\nB. \u53e4\u5370\u5ea6\nC. \u53e4\u4ee3\u65af\u5df4\u8fbe\nD. \u53e4\u4ee3\u96c5\u5178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5110391347635618, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u7406\u6027\u3001\u4f1a\u7b97\u8ba1\u3001\u8ffd\u6c42\u81ea\u5229\u6700\u5927\u5316\u7684\u4eba\uff0c\u88ab\u4e9a\u5f53\u00b7\u65af\u5bc6\u79f0\u4e3a\nA. \u793e\u4f1a\u4eba\nB. \u7ecf\u6d4e\u4eba\nC. \u9053\u5fb7\u4eba\nD. \u81ea\u7136\u4eba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9675013149146552, "meta-math/MetaMath-Mistral-7B": 0.99913211015995, "itpossible/Chinese-Mistral-7B-v0.1": 0.9673540733931741, "HuggingFaceH4/zephyr-7b-beta": 0.9998258144271995, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9921724706384939, "meta-llama/Meta-Llama-3-8B": 0.9245377929828565, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9986491279682206}}, {"question": "\u4e0b\u5217\u54ea\u4e9b\u72af\u7f6a\u5206\u5b50\u4e0d\u6210\u7acb\u7279\u522b\u7d2f\u72af\uff1f\nA. \u6bd2\u54c1\u72af\u7f6a\u7684\u72af\u7f6a\u5206\u5b50\nB. \u5371\u5bb3\u56fd\u5bb6\u5b89\u5168\u72af\u7f6a\u7684\u72af\u7f6a\u5206\u5b50\nC. \u9ed1\u793e\u4f1a\u6027\u8d28\u7ec4\u7ec7\u72af\u7f6a\u7684\u72af\u7f6a\u5206\u5b50\nD. \u6050\u6016\u6d3b\u52a8\u72af\u7f6a\u7684\u72af\u7f6a\u5206\u5b50\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3486508148517961, "meta-math/MetaMath-Mistral-7B": 0.333183235354062, "itpossible/Chinese-Mistral-7B-v0.1": 0.36199713893516505, "HuggingFaceH4/zephyr-7b-beta": 0.7741803721952168, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7237436393092385, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u5c5e\u4e8e\u4f01\u4e1a\u7684\u57fa\u672c\u793e\u4f1a\u8d23\u4efb\u7684\u662f\nA. \u4fdd\u62a4\u6d88\u8d39\u8005\u6743\u76ca\nB. \u4fdd\u62a4\u751f\u6001\u5e73\u8861\nC. \u83b7\u53d6\u5229\u6da6\nD. \u51cf\u5c11\u73af\u5883\u6c61\u67d3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6211\u56fd\u98ce\u4fd7\u4e2d\uff0c\u5e38\u5e38\u907f\u8bb373\u548c84\u8fd9\u4e24\u4e2a\u5c81\u6570\uff0c\u56e0\u4e3a\u8fd9\u662f\u4e24\u4f4d\u5386\u53f2\u4eba\u7269\u53bb\u4e16\u7684\u5e74\u9f84\uff0c\u4ed6\u4eec\u662f\nA. \u5468\u6b66\u738b\u548c\u5468\u6587\u738b\nB. \u5b54\u5b50\u548c\u5b5f\u5b50\nC. \u6c49\u9ad8\u7956\u548c\u6c49\u6b66\u5e1d\nD. \u8001\u5b50\u548c\u5e84\u5b50\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3532370534784464, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3418241299464301, "HuggingFaceH4/zephyr-7b-beta": 0.3571996106219397, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u76f8\u540c\u767d\u7531\u5ea6$(v_1\uff0cv_2)$\u53ca$F$\u503c\u65f6\uff0c\u65b9\u5dee\u9f50\u6027\u68c0\u9a8c\u4e0e\u65b9\u5dee\u5206\u6790\u6240\u5f97\u7684$P$\u503c\nA. \u524d\u8005\u5927\nB. \u524d\u8005\u662f\u540e\u8005\u7684\u4e24\u500d\nC. \u524d\u8005\u5c0f\nD. \u4e24\u8005\u76f8\u7b49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2575662067712908, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53\u5355\u4e00\u53ef\u53d8\u6295\u5165\u8981\u7d20\u7684\u4ef7\u683c\u7b49\u4e8e\u5b83\u7684\u8fb9\u9645\u4ea7\u91cf\u6536\u5165\u65f6\uff0c\u53ef\u53d8\u6295\u5165\u8981\u7d20\u7684\u6295\u5165\u91cf\u53ef\u4f7f\nA. \u4f01\u4e1a\u7684\u603b\u5229\u6da6\u6700\u5927\nB. \u4f01\u4e1a\u7684\u5e73\u5747\u4ea7\u91cf\u6700\u5927\nC. \u4f01\u4e1a\u7684\u603b\u4ea7\u91cf\u6700\u5927\nD. \u4f01\u4e1a\u7684\u751f\u4ea7\u6210\u672c\u6700\u4f4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.643538568548376, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6813830977844975, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4396762240620707, "meta-llama/Meta-Llama-3-8B": 0.38822591501510517, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5162696200841004}}, {"question": "\u80be\u6162\u6027\u6392\u65a5\u53cd\u5e94\u7684\u7a81\u51fa\u75c5\u53d8\u662f\nA. \u8840\u7ba1\u58c1\u7ea4\u7ef4\u7d20\u6837\u574f\u6b7b\nB. \u8840\u7ba1\u5185\u819c\u589e\u539a\nC. \u8840\u7ba1\u5185\u819c\u7ea4\u7ef4\u5316\nD. \u8840\u7ba1\u58c1\u73bb\u7483\u6837\u53d8\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3354456100429363, "meta-math/MetaMath-Mistral-7B": 0.3499320087587726, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8238089271090528, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.39845851868824184, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.47972253249523755}}, {"question": "\u4e0b\u5217\u6bcf\u4e2a\u9009\u9879\u4e2d\uff0c\u7532\u3001\u4e59\u4e24\u4e2a\u53cd\u5e94\u5c5e\u4e8e\u540c\u4e00\u79cd\u53cd\u5e94\u7c7b\u578b\u7684\u662f\nA. \u7532:\u4e59\u7532\u70f7\u4e0e\u6c2f\u6c14\u53cd\u5e94\u5236\u5907\u56db\u6c2f\u5316\u78b3\u3002\u4e59\uff1a\u4e59\u70ef\u901a\u5165\u9178\u6027KMnO4\u6eb6\u6db2\u4e2d\u3002\nB. \u7532:\u8461\u8404\u7cd6\u4e0e\u65b0\u5236Cu(OH)2\u60ac\u6d4a\u6db2\u4f5c\u7528\u8f6c\u5316\u4e3a\u8461\u8404\u7cd6\u9178\u3002\u4e59\uff1a\u82ef\u4e0e\u6db2\u6eb4\u53cd\u5e94\u5236\u5907\u6eb4\u82ef\u3002\nC. \u7532:\u4e59\u9187\u8f6c\u5316\u4e3a\u4e59\u3002\u4e59\uff1a\u4e59\u70ef\u4e0eHCl\u4f5c\u7528\u5236\u5907\u6c2f\u4e59\u70f7\u3002\nD. \u7532:\u4e59\u9178\u4e59\u916f\u5728\u7a00\u786b\u9178\u5b58\u5728\u4e0b\u4e0e\u6c34\u53cd\u5e94\u3002\u4e59\uff1a\u4e59\u9187\u4e0e\u51b0\u918b\u9178\u53cd\u5e94\u751f\u6210\u4e59\u9178\u4e59\u916f\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.33065623127838456, "meta-llama/Meta-Llama-3-8B": 0.3615085256056106, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u4e0d\u5c5e\u4e8e\u975e\u8bed\u8a00\u7b26\u53f7\u7684\u662f\nA. \u5f00\u5fc3\u7684\u7b11\u5bb9\nB. \u4e0e\u522b\u4eba\u63e1\u624b\nC. \u7a7f\u7740\u6253\u626e\nD. \u6587\u7ae0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8714534004711995, "meta-math/MetaMath-Mistral-7B": 0.951079134327229, "itpossible/Chinese-Mistral-7B-v0.1": 0.4038289903987044, "HuggingFaceH4/zephyr-7b-beta": 0.9996337192239338, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5610476759258113, "meta-llama/Meta-Llama-3-8B": 0.9014204429085682, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.46589231983090074}}, {"question": "\u7537\u6027\uff0c60 \u5c81\u3002\u81ea\u8ff0 1 \u5e74\u534a\u524d\u56e0\u76f4\u80a0\u80bf\u7624\u5728\u5916\u9662\u624b\u672f\u6cbb\u7597\uff0c\u5177\u4f53\u672f\u5f0f\u4e0d\u6e05\u3002\u68c0\u67e5\u89c1\u5de6\u4e0b\u8179\u90e8\u65c1\u6b63\u4e2d 12cm\u76f4\u5207\u53e3\u7622\u75d5\uff0c\u5176\u5916\u4fa7\u6709\u4e00\u80a0\u9020\u56d7\uff0c\u809b\u95e8\u5df2\u4e0d\u5b58\u5728\u3002\u6700\u4f73\u6cbb\u7597\u65b9\u6cd5\u662f\nA. \u80bf\u7624\u5c04\u9891\u6d88\u878d\nB. \u53f3\u534a\u809d\u5207\u9664\u672f\nC. \u809d\u90e8\u5206\u5207\u9664\u672f\nD. \u5168\u8eab\u5316\u7597\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.506410041019293, "meta-math/MetaMath-Mistral-7B": 0.4393224910814194, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7721947427182156, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5731662960932854, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c34\u91cf\u5e73\u8861\u65b9\u7a0b\u5f0f$P-R-E=\\Delta v$ (\u5176\u4e2d $P$ \u3001 $R$ \u3001 $E$ \u3001 $\\Delta v$ \u5206\u522b\u4e3a\u67d0\u4e00\u65f6\u6bb5\u7684\u6d41\u57df\u964d\u6c34\u91cf\u3001\u5206\u522b\u4e3a\u67d0\u4e00\u65f6\u6bb5\u7684\u6d41\u57df\u964d\u6c34\u91cf\u3001\u5f84\u6d41\u91cf\u84b8\u53d1\u91cf\u548c\u84c4\u6c34\u53d8\u91cf)\uff0c\u9002\u7528\u4e8e[ ]\u3002\nA. \u95ed\u5408\u6d41\u57df\u4efb\u610f\u65f6\u6bb5\u60c5\u51b5\nB. \u975e\u95ed\u5408\u6d41\u57df\u4efb\u610f\u65f6\u6bb5\u60c5\u51b5\nC. \u975e\u95ed\u5408\u6d41\u57df\u591a\u5e74\u5e73\u5747\u60c5\u51b5\nD. \u95ed\u5408\u6d41\u57df\u591a\u5e74\u5e73\u5747\u60c5\u51b5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.308176776288574, "HuggingFaceH4/zephyr-7b-beta": 0.9561735317187086, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5939475280115325, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u690e\u95f4\u5b54\u662f\nA. \u690e\u4f53\u95f4\u7684\u5b54\nB. \u76f8\u90bb\u690e\u9aa8\u7684\u4e0a\u3001\u4e0b\u5207\u8ff9\u56f4\u6210\u7684\u5b54\nC. \u690e\u4f53\u4e0e\u690e\u5f13\u56f4\u6210\u7684\u5b54\nD. \u6a2a\u7a81\u95f4\u7684\u5b54\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5499664010318035, "meta-math/MetaMath-Mistral-7B": 0.7729451195793301, "itpossible/Chinese-Mistral-7B-v0.1": 0.5438618904727415, "HuggingFaceH4/zephyr-7b-beta": 0.9982423522739691, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8966918093905552, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9424444515256196}}, {"question": "\u5173\u4e8e\u98df\u7269\u7279\u522b\u52a8\u529b\u4f5c\u7528\u7684\u4ea7\u751f\u4e0b\u5217\u9009\u9879\u6b63\u786e\u7684\u662f\nA. \u6444\u98df\u541e\u54bd\u52a8\u4f5c\u4ea7\u751f\u7684\nB. \u662f\u673a\u4f53\u7531\u4e8e\u6444\u53d6\u98df\u7269\u800c\u5f15\u8d77\u4f53\u5185\u80fd\u91cf\u6d88\u8017\u589e\u52a0\u7684\u73b0\u8c61\u3002\nC. \u98df\u7269\u672c\u8eab\u6240\u5177\u6709\u7684\u70ed\u80fd\u548c\u6e29\u5ea6\nD. \u5480\u56bc\u52a8\u4f5c\u5f15\u8d77\u5480\u56bc\u808c\u8fd0\u52a8\u4ea7\u70ed\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4156323428146416, "meta-math/MetaMath-Mistral-7B": 0.9293146894196768, "itpossible/Chinese-Mistral-7B-v0.1": 0.34520661566200816, "HuggingFaceH4/zephyr-7b-beta": 0.9991835239807919, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5967470129336251, "meta-llama/Meta-Llama-3-8B": 0.8575125069300241, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5102876641018756}}, {"question": "\u5df2\u77e5\u5706\u7684\u534a\u5f84\u4e3a$\\sqrt{2}cm$\uff0c\u5706\u5fc3\u5230\u76f4\u7ebfl\u7684\u8ddd\u79bb\u4e3a1.4cm\uff0c\u5219\u76f4\u7ebf\u4e0e\u5706\u7684\u516c\u5171\u70b9\u6570\u4e3a\nA. 2\u4e2a\nB. 1\u4e2a \nC. 0\u4e2a\nD. \u4e0d\u786e\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3177952409322493, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u5bf9\u4e8e\u201c\u751f\u6daf\u201d\u4e00\u8bcd\u7684\u7406\u89e3\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u751f\u6daf\u4e2d\u7684\u7ecf\u9a8c\u5851\u9020\u4e86\u72ec\u7279\u7684\u884c\u52a8\u65b9\u6848\nB. \u5305\u62ec\u4e86\u4e2a\u4eba\u5728\u5bb6\u5ead\u3001\u5b66\u6821\u548c\u793e\u4f1a\u4e0e\u5de5\u4f5c\u6709\u5173\u6d3b\u52a8\u7684\u7ecf\u9a8c\nC. \u751f\u6daf\u5177\u6709\u95f4\u65ad\u6027\u7684\u7279\u5f81\nD. \u751f\u6daf\u7684\u53d1\u5c55\u662f\u4eba\u7684\u4e00\u751f\u4e2d\u8fde\u7eed\u4e0d\u65ad\u7684\u8fc7\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6684858101044147, "meta-math/MetaMath-Mistral-7B": 0.9463322468406735, "itpossible/Chinese-Mistral-7B-v0.1": 0.8485824491583289, "HuggingFaceH4/zephyr-7b-beta": 0.9997397500037674, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9746831328086124, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5676326514486097}}, {"question": "\u7537\u6027\uff0c50 \u5c81\u30021 \u4e2a\u6708\u6765\u6709\u8fc7 2 \u6b21\u65e0\u75db\u6027\u8840\u5c3f\uff0c\u8fd1\u65e5\u89c9\u9634\u56ca\u5760\u80c0\u611f\uff0c\u5367\u4f4d\u5760\u80c0\u65e0\u51cf\u8f7b\uff0c\u4f53\u68c0\u53d1\u73b0\u53f3\u4fa7\u9634\u56ca\u5185\u7cbe\u7d22\u9759\u8109\u66f2\u5f20\u3002\u6700\u53ef\u80fd\u7684\u8bca\u65ad\u662f\nA. \u80be\u7ed3\u77f3\nB. \u80be\u764c\nC. \u539f\u53d1\u6027\u7cbe\u7d22\u9759\u8109\u66f2\u5f20\nD. \u8180\u80f1\u80bf\u7624\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e16\u754c\u5404\u56fd\u7684\u5b66\u5236\u5b58\u5728\u7740\u5dee\u5f02\uff0c\u4f46\u5728\u5165\u5b66\u5e74\u9f84\uff0c\u4e2d\u5c0f\u5b66\u5206\u6bb5\u7b49\u65b9\u9762\u5374\u53c8\u8f83\u9ad8\u7684\u4e00\u81f4\u6027\u3002\u8fd9\u8bf4\u660e\u5b66\u5236\u7684\u5efa\u7acb\u4e3b\u8981\u4f9d\u636e\nA. \u540d\u65cf\u548c\u6587\u5316\u4f20\u7edf\nB. \u793e\u4f1a\u653f\u6cbb\u7ecf\u6d4e\u5236\u5ea6\nC. \u9752\u5c11\u5e74\u8eab\u5fc3\u53d1\u5c55\u89c4\u5f8b\nD. \u751f\u4ea7\u529b\u53d1\u5c55\u6c34\u5e73\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8900090326963234, "meta-math/MetaMath-Mistral-7B": 0.9777306892633388, "itpossible/Chinese-Mistral-7B-v0.1": 0.9016943961155705, "HuggingFaceH4/zephyr-7b-beta": 0.966644005125772, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9353546339030587, "meta-llama/Meta-Llama-3-8B": 0.7715117484177691, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9091564885450475}}, {"question": "\u5728\u6d89\u5916\u4ee3\u7406\u4e2d\uff0c\u5173\u4e8e\u672c\u4eba\u548c\u4ee3\u7406\u4eba\u4e4b\u95f4\u6743\u5229\u4e49\u52a1\u5173\u7cfb\uff0c\u4e00\u822c\u5e94\u9002\u7528\u7684\u51c6\u636e\u6cd5\u662f\nA. \u4ee3\u7406\u5173\u7cfb\u6210\u7acb\u5730\u6cd5\nB. \u5f53\u4e8b\u4eba\u81ea\u4e3b\u9009\u62e9\u7684\u6cd5\u5f8b\nC. \u4ee3\u7406\u4eba\u4e3a\u4ee3\u7406\u884c\u4e3a\u5730\u6cd5\nD. \u4ee3\u7406\u4eba\u8425\u4e1a\u5730\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3697259344211468, "meta-math/MetaMath-Mistral-7B": 0.4435043247787303, "itpossible/Chinese-Mistral-7B-v0.1": 0.3454901351726419, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.42284688246515895, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c37\u5c81\u3002\u56e0\u5de6\u53f6\u7532\u72b6\u817a\u4e73\u5934\u72b6\u764c\u884c\u7532\u72b6\u817a\u5de6\u53f6\u5168\u5207\u3001\u5ce1\u90e8\u53ca\u53f3\u53f6\u5927\u90e8\u5207\u9664\u672f\u3002\u672f\u540e\u7b2c\u4e00\u5929\u53d1\u751f\u547c\u5438\u6025\u4fc3\uff0c\u53e3\u5468\u9ebb\u6728\uff0c\u624b\u8db3\u6301\u7eed\u6027\u75c9\u631b\u3002\u9996\u9009\u5904\u7406\u63aa\u65bd\nA. \u62c6\u9664\u9888\u90e8\u4f24\u53e3\u7f1d\u7ebf\nB. \u6c14\u7ba1\u5207\u5f00\nC. \u68c0\u67e5\u5f15\u6d41\u7ba1\nD. \u9759\u8109\u6ce8\u5c04\u9499\u5242\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.30601362565976303, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u91d1\u5c5e\u7f50\u5706\u7f50\u7f16\u53f7\u6309\nA. \u5916\u5f84\u5916\u9ad8\nB. \u5185\u5f84\u5916\u9ad8\nC. \u5185\u5f84\u5185\u9ad8\nD. \u5916\u5f84\u5185\u9ad8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.333183235354062, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u516c\u5171\u5173\u7cfb\u4f5c\u4e3a\u4e00\u79cd\u804c\u4e1a\u548c\u4e00\u95e8\u5b66\u79d1\uff0c\u6700\u65e9\u4ea7\u751f\u4e8e\nA. \u82f1\u56fd\nB. \u7f8e\u56fd\nC. \u6cd5\u56fd\nD. \u5965\u5730\u5229\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5735725572864181, "meta-math/MetaMath-Mistral-7B": 0.7927784024267464, "itpossible/Chinese-Mistral-7B-v0.1": 0.911880568736168, "HuggingFaceH4/zephyr-7b-beta": 0.88165535324335, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7951893388160126, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.78536273772984}}, {"question": "$f(x)=x \\mathrm{e}^x$ \u7684 $n$ \u9636\u9ea6\u514b\u52b3\u6797\u516c\u5f0f\u4e3a ( ) .\nA. $x+x^2+\\frac{x^3}{2 !}+\\cdots+\\frac{x^n}{(n-1) !}+\\frac{\\mathrm{e}^{\\theta x}(n+\\theta x)}{(n+1) !} x^{n+1}\uff0c 0<\\theta<1$\nB. $1+x+\\frac{x^2}{21}+\\ldots+\\frac{x^n}{n !}+\\frac{\\mathrm{e}^{\\theta x}(n+1+\\theta x)}{(n+1) !} x^{n+1}\uff0c 0<\\theta<1$\nC. $x+x^2+\\frac{x^3}{2 !}+\\cdots+\\frac{x^n}{(n-1) !}+\\frac{\\mathrm{e}^{\\theta x}(n+1+\\theta x)}{(n+1) !} x^{n+1}\uff0c 0<\\theta<1$\nD. $1+x+\\frac{x^2}{2 !}+\\ldots+\\frac{x^{n-1}}{(n-1) !}+\\frac{\\mathrm{e}^{\\theta x}(n+\\theta x)}{n !} x^{n+1}\uff0c 0<\\theta<1$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6385491330800508, "itpossible/Chinese-Mistral-7B-v0.1": 0.2848664895071881, "HuggingFaceH4/zephyr-7b-beta": 0.8461702190787836, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5511358903021977, "meta-llama/Meta-Llama-3-8B": 0.48533996353149345, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c49\u6b66\u5e1d\u65f6\u671f\uff0c\u4e3b\u7236\u5043\u7684\u201c\u63a8\u6069\u4ee4\u201d\u5e2e\u52a9\u7687\u5e1d\u89e3\u51b3\u4e86\u56fd\u5bb6\u5206\u88c2\u7684\u95ee\u9898\uff0c\u800c\u53e6\u4e00\u4f4d\u5927\u81e3\u8463\u4ef2\u8212\u5219\u63d0\u8bae\u5728\u4e2d\u592e\u8bbe\u7f6e()\u4f20\u6388\u5112\u5bb6\u7ecf\u5178\uff0c\u4ece\u800c\u6709\u5229\u4e8e\u601d\u60f3\u7684\u7edf\u4e00\uff0c\u8fd9\u662f\u5f53\u65f6\u6700\u9ad8\u7684\u6559\u80b2\u673a\u6784\u3002\nA. \u5d07\u6587\u9986\nB. \u7a37\u4e0b\u5b66\u5bab\nC. \u592a\u5b66\nD. \u56fd\u5b50\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.469903862992708, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.43242334555747225, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6941733553585822, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.45961512651782666}}, {"question": "\u4e00\u8d28\u70b9\u5728\u67d0\u6bb5\u65f6\u95f4\u5185\u505a\u66f2\u7ebf\u8fd0\u52a8\uff0c\u90a3\u4e48\u5728\u8fd9\u6bb5\u65f6\u95f4\u5185\nA. \u901f\u5ea6\u53ef\u4ee5\u4e0d\u53d8\uff0c\u52a0\u901f\u5ea6\u4e00\u5b9a\u5728\u4e0d\u65ad\u5730\u6539\u53d8\nB. \u901f\u5ea6\u4e00\u5b9a\u5728\u4e0d\u65ad\u5730\u6539\u53d8\uff0c\u52a0\u901f\u5ea6\u4e5f\u4e00\u5b9a\u5728\u4e0d\u65ad\u5730\u6539\u53d8\nC. \u901f\u5ea6\u53ef\u4ee5\u4e0d\u53d8\uff0c\u52a0\u901f\u5ea6\u4e5f\u53ef\u4ee5\u4e0d\u53d8\nD. \u901f\u5ea6\u4e00\u5b9a\u5728\u4e0d\u65ad\u5730\u6539\u53d8\uff0c\u52a0\u901f\u5ea6\u53ef\u4ee5\u4e0d\u53d8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5057620150133189, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5e03\u5362\u59c6\u5c06\u8ba4\u77e5\u76ee\u6807\u7531\u4f4e\u5230\u9ad8\u5206\u4e3a\u54ea\u516d\u4e2a\u5c42\u6b21\nA. \u77e5\u8bc6\u3001\u9886\u4f1a\u3001\u8fd0\u7528\u3001\u5206\u6790\u3001 \u7efc\u5408\u3001\u8bc4\u4ef7\nB. \u77e5\u8bc6\u3001\u8fd0\u7528\u3001\u9886\u4f1a\u3001\u5206\u6790\u3001\u7efc\u5408\u3001\u8bc4\u4ef7\nC. \u77e5\u8bc6\u3001\u9886\u4f1a\u3001\u8fd0\u7528\u3001\u7efc\u5408\u3001\u5206\u6790\u3001 \u8bc4\u4ef7\nD. \u77e5\u8bc6\u3001\u9886\u4f1a\u3001\u5206\u6790\u3001\u8fd0\u7528\u3001\u7efc\u5408\u3001\u8bc4\u4ef7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3895746993979434, "meta-llama/Meta-Llama-3-8B": 0.3956423519781027, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9284786331964525}}, {"question": "\u5728\u8170\u690e\u8fd0\u52a8\u4e2d\uff0c\u54ea\u9879\u53ef\u88ab\u770b\u4f5c\u521a\u4f53\nA. \u5173\u8282\u56ca\nB. \u690e\u4f53\nC. \u690e\u95f4\u76d8\nD. \u97e7\u5e26\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39105853750106623, "meta-math/MetaMath-Mistral-7B": 0.8011531592579348, "itpossible/Chinese-Mistral-7B-v0.1": 0.3932534368794713, "HuggingFaceH4/zephyr-7b-beta": 0.8317571988069785, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.474069820068834, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8403542246668078}}, {"question": "\u5173\u4e8e\u751f\u7269\u7ec4\u7ec7\u4e2d\u8fd8\u539f\u7cd6\u3001\u8102\u80aa\u3001\u86cb\u767d\u8d28\u548cDNA\u7684\u9274\u5b9a\u5b9e\u9a8c\uff0c\u4e0b\u5217\u53d9\u8ff0\u6b63\u786e\u7684\u662f\nA. \u4e8c\u82ef\u80fa\u8bd5\u5242\u548c\u7528\u4e8e\u914d\u5236\u6590\u6797\u8bd5\u5242\u7684NaOH\u6eb6\u6db2\u90fd\u5448\u65e0\u8272\nB. \u9274\u5b9a\u8fd8\u539f\u7cd6\u3001\u86cb\u767d\u8d28\u548cDNA\u90fd\u9700\u8981\u8fdb\u884c\u6c34\u6d74\u52a0\u70ed\nC. \u8fd8\u539f\u7cd6\u3001DNA\u7684\u9274\u5b9a\u901a\u5e38\u5206\u522b\u4f7f\u7528\u53cc\u7f29\u8132\u8bd5\u5242\u3001\u4e8c\u82ef\u80fa\u8bd5\u5242\nD. \u8102\u80aa\u3001\u86cb\u767d\u8d28\u9274\u5b9a\u65f6\u5206\u522b\u53ef\u89c1\u6a58\u9ec4\u8272\u9897\u7c92\u3001\u7816\u7ea2\u8272\u6c89\u6dc0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u54ea\u4e2a\u6545\u4e8b\u4e0d\u662f\u4e09\u56fd\u6545\u4e8b\nA. \u8d1f\u8346\u8bf7\u7f6a\nB. \u4e09\u987e\u8305\u5e90\nC. \u8349\u8239\u501f\u7bad\nD. \u8d64\u58c1\u5927\u6218\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe $n$ \u7ef4\u5411\u91cf\u7ec4 ${\\alpha}_1\uff0c{\\alpha}_2\uff0c\\cdots\uff0c{\\alpha}_m(m<n)$ \u7ebf\u6027\u65e0\u5173\uff0c \u5219 $n$ \u7ef4\u5217\u5411\u91cf\u7ec4 ${\\beta}_1\uff0c{\\beta}_2\uff0c\\cdots\uff0c{\\beta}_m$ \u7ebf\u6027\u65e0\u5173\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\u4e3a( )\nA. \u5411\u91cf\u7ec4 ${\\beta}_1\uff0c{\\beta}_2\uff0c\\cdots\uff0c{\\beta}_m$ \u53ef\u7531\u5411\u91cf\u7ec4 ${\\alpha}_1\uff0c{\\alpha}_2\uff0c\\cdots\uff0c{\\alpha}_m$ \u7ebf\u6027\u8868\u793a.\nB. \u5411\u91cf\u7ec4 ${\\alpha}_1\uff0c{\\alpha}_2\uff0c\\cdots\uff0c{\\alpha}_m$ \u4e0e\u5411\u91cf\u7ec4 ${\\beta}_1\uff0c{\\beta}_2\uff0c \\cdots\uff0c {\\beta}_m$ \u7b49\u4ef7\nC. \u77e9\u9635 $A=({\\alpha}_1\uff0c\\cdots\uff0c{\\alpha}_m)$ \u4e0e\u77e9\u9635 $B=({\\beta}_1\uff0c\\cdots\uff0c{\\beta}_m)$ \u7b49\u4ef7\nD. \u5411\u91cf\u7ec4 ${\\alpha}_1\uff0c{\\alpha}_2\uff0c\\cdots\uff0c{\\alpha}_m$ \u53ef\u7531\u5411\u91cf\u7ec4 ${\\beta}_1\uff0c{\\beta}_2\uff0c\\cdots\uff0c{\\beta}_m$ \u7ebf\u6027\u8868\u793a.\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5737569552832984}}, {"question": "\u201c\u6240\u6709\u7684\u903b\u8f91\u5b66\u5bb6\u90fd\u662f\u54f2\u5b66\u5bb6\uff0c\u6240\u6709\u7f8e\u5b66\u5bb6\u90fd\u662f\u903b\u8f91\u5b66\u5bb6\uff0c\u56e0\u6b64\uff0c\u6240\u6709\u7f8e\u5b66\u5bb6\u90fd\u662f\u54f2\u5b66\u5bb6\u201d\uff0c\u8fd9\u4e2a\u4e09\u6bb5\u8bba\u4e2d\uff0c\u7ed3\u8bba\u6807\u8bc6\u8bcd\u662f\nA. \u90fd\u662f\nB. \u54f2\u5b66\u5bb6\nC. \u6240\u6709\nD. \u56e0\u6b64\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5321864498118799, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5905829471208284}}, {"question": "\u4e0b\u5217\u5404\u53e5\u4e2d\uff0c\u6ca1\u6709\u8bed\u75c5\u7684\u2f00\u53e5\u662f\nA. \u2f24\u5bb6\u90fd\u77e5\u9053\uff0c\u9664\u4e86960\u4e07\u5e73\u2f45\u516c\u2fa5\u7684\u9646\u5730\u56fd\u2f1f\uff0c\u6211\u56fd\u8fd8\u62e5\u6709\u8fd1300\u4e07\u5e73\u2f45\u516c\u2fa5\u7684\u201c\u84dd\u2f8a\u56fd\u2f1f\u201d\u3002\nB. \u56db\u5ddd\u8354\u679d\u4eab\u6709\u76db\u540d\uff0c\u5510\u2f08\u5df2\u591a\u6709\u8bb0\u8ff0\uff1b\u8521\u8944\u300a\u8354\u679d\u8c31\u300b\u2f83\u4ece\u6210\u4e66\u540e\uff0c\u798f\u5efa\u8354\u679d\u624d\u5f00\u59cb\u4e3a\u2f08\u91cd\u89c6\u3002\nC. \u80fd\u4e0d\u80fd\u5229\u2f64\u201c\u6210\u7ef5\u201d\u57ce\u9645\u2fbc\u94c1\u7684\u5efa\u6210\u901a\u2ecb\u8fdb\u2f00\u6b65\u63a8\u8fdb\u7ef5\u9633\u7ecf\u6d4e\u53d1\u5c55\uff0c\u53d6\u51b3\u4e8e\u5e02\u653f\u5e9c\nD. \u8fd1\u2f00\u4e24\u4e2a\u2f49\u6765\uff0c\u5317\u4eac\u3001\u676d\u5dde\u3001\u6210\u90fd\u7b49\u57ce\u5e02\u7eb7\u7eb7\u51fa\u53f0\u653f\u7b56\uff0c\u5f00\u59cb\u4e25\u60e9\u2f46\u89c6\u7ea2\u7eff\u706f\u4fe1\u53f7\u4e71\u7a7f\u2ee2\u8def\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0e\u9ecf\u819c\u76f8\u5173\u6dcb\u5df4\u7ec4\u7ec7\u6dcb\u5df4\u7624\u53d1\u751f\u5173\u7cfb\u5bc6\u5207\u7684\u7ec6\u83cc\u662f\nA. \u5927\u80a0\u57c3\u5e0c\u83cc\nB. \u8349\u7eff\u8272\u94fe\u7403\u83cc\nC. \u5e7d\u95e8\u87ba\u6746\u83cc\nD. \u91d1\u9ec4\u8272\u8461\u8404\u7403\u83cc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33110093121618506, "meta-math/MetaMath-Mistral-7B": 0.5881915604793101, "itpossible/Chinese-Mistral-7B-v0.1": 0.6656445495444429, "HuggingFaceH4/zephyr-7b-beta": 0.8071935372481529, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5364816052401277, "meta-llama/Meta-Llama-3-8B": 0.5174255073248947, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9159883643116795}}, {"question": "\u82e5\u5b58\u50a8\u5468\u671f100ns\uff0c\u6bcf\u6b21\u8bfb\u51fa\u2f00\u4e2a\u5b57\u8282\uff0c\u5219\u8be5\u5b58\u50a8\u5668\u7684\u6570\u636e\u4f20\u8f93\u7387\u4e3a\nA. 8\u00d7106\u4f4d/\u79d2\nB. 32\u00d7106\u4f4d/\u79d2\nC. 80\u00d7106\u4f4d/\u79d2\nD. 80Mb/\u79d2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.371068906204966, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.42265147385286184, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3148300531811561, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "$n$ \u7ef4\u5217\u5411\u91cf $\\boldsymbol{\\alpha}_1\uff0c \\boldsymbol{\\alpha}_2\uff0c \\cdots\uff0c \\boldsymbol{\\alpha}_s$ \u7ebf\u6027\u65e0\u5173\u7684\u5145\u8981\u6761\u4ef6\u662f ( )\nA. \u5b58\u5728\u4e0d\u5168\u4e3a\u96f6\u7684\u6570 $k_1\uff0c k_2\uff0c \\cdots\uff0c k_s$\uff0c \u4f7f\u5f97 $k_1 \\boldsymbol{\\alpha}_1+k_2 \\boldsymbol{\\alpha}_2+\\cdots+k_s \\boldsymbol{\\alpha}_s \\neq \\mathbf{0}$\nB. $\\boldsymbol{\\alpha}_1\uff0c \\boldsymbol{\\alpha}_2-\\boldsymbol{\\alpha}_1\uff0c \\boldsymbol{\\alpha}_3-\\boldsymbol{\\alpha}_1\uff0c \\cdots\uff0c \\boldsymbol{\\alpha}_s-\\boldsymbol{\\alpha}_1$ \u7ebf\u6027\u65e0\u5173\nC. \u53bb\u6389\u4efb\u4e00\u5411\u91cf $\\boldsymbol{\\alpha}_i$ \u540e\uff0c $\\boldsymbol{\\alpha}_1\uff0c \\cdots\uff0c \\boldsymbol{\\alpha}_{i-1}\uff0c \\boldsymbol{\\alpha}_{i+1}\uff0c \\cdots\uff0c \\boldsymbol{\\alpha}_s$ \u7ebf\u6027\u65e0\u5173\nD. \u6dfb\u52a0\u5411\u91cf $\\boldsymbol{\\beta}$ \u540e\uff0c $\\boldsymbol{\\alpha}_1\uff0c \\boldsymbol{\\alpha}_2\uff0c \\cdots\uff0c \\boldsymbol{\\alpha}_s\uff0c \\boldsymbol{\\beta}$ \u7ebf\u6027\u65e0\u5173\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u4ee5\u884c\u4e3a\u540e\u679c\u6765\u5224\u65ad\u884c\u4e3a\u7684\u9053\u5fb7\u5408\u7406\u6027\u201d\u4f53\u73b0\u7684\u9053\u5fb7\u89c2\u662f\nA. \u9053\u4e49\u8bba\nB. \u529f\u5229\u8bba\nC. \u663e\u8981\u4e49\u52a1\u8bba\nD. \u76f8\u5bf9\u4e3b\u4e49\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5998024652774493, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3301226564612724, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e2d\u56fd\uff0c\u201c\u6559\u80b2\u201d\u4e8c\u5b57\u5408\u7528\u6700\u65e9\u51fa\u73b0\u5728\nA. \u300a\u4e2d\u5eb8\u300b\nB. \u300a\u5b66\u8bb0\u300b\nC. \u300a\u8bf4\u6587\u89e3\u5b57\u300b\nD. \u300a\u5b5f\u5b50\u00b7\u5c3d\u5fc3\u4e0a\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3527809287490976, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.37480846975763354, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.47139482437594743}}, {"question": "\u6ca1\u6709\u62bd\u8c61\u7684\u771f\u7406\uff0c\u771f\u7406\u603b\u662f\u5177\u4f53\u7684\u3002\u8fd9\u4e00\u547d\u9898\u5f3a\u8c03\nA. \u771f\u7406\u7684\u5185\u5bb9\u662f\u5ba2\u89c2\u7684\uff0c\u5f62\u5f0f\u662f\u4e3b\u89c2\u7684\nB. \u771f\u7406\u662f\u611f\u6027\u7684\u76f4\u89c2\uff0c\u4e0d\u662f\u7406\u6027\u7684\u62bd\u8c61\nC. \u540c\u4e00\u5bf9\u8c61\u771f\u7406\u6027\u7684\u8ba4\u8bc6\u53ea\u6709\u4e00\u4e2a\nD. \u771f\u7406\u662f\u4e0e\u4eba\u7684\u5177\u4f53\u5229\u76ca\u76f8\u8054\u7cfb\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31507373199033, "meta-math/MetaMath-Mistral-7B": 0.3411591690619688, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.49803980968801514, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d34\u73b0\u7387\u7684\u5927\u5c0f\u5bf9\u73b0\u91d1\u6d41\u7684\u73b0\u503c\u7684\u5f71\u54cd\uff0c\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u8d34\u73b0\u7387\u8d8a\u5c0f\uff0c\u73b0\u503c\u8d8a\u5c0f\nB. \u8d34\u73b0\u7387\u7684\u5927\u5c0f\u4f1a\u600e\u6837\u5f71\u54cd\u73b0\u503c\uff0c\u8981\u89c6\u5177\u4f53\u6570\u636e\u800c\u5b9a\nC. \u8d34\u73b0\u7387\u8d8a\u5927\uff0c\u73b0\u503c\u8d8a\u5c0f\nD. \u8d34\u73b0\u7387\u7684\u5927\u5c0f\u4f1a\u5f71\u54cd\u73b0\u91d1\u6d41\u672a\u6765\u7684\u4ef7\u503c\uff0c\u4f46\u4e0d\u5f71\u54cd\u5176\u73b0\u503c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6579939113445775, "meta-math/MetaMath-Mistral-7B": 0.5984948945164525, "itpossible/Chinese-Mistral-7B-v0.1": 0.453609445271565, "HuggingFaceH4/zephyr-7b-beta": 0.8277310657753784, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6411639116165614, "meta-llama/Meta-Llama-3-8B": 0.5486567472042243, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c14\u8d28\u7c7b\u578b\u5728\u793e\u4f1a\u8bc4\u4ef7\u4e0a\uff08\uff09\u3002\nA. \u90fd\u662f\u574f\u7684\nB. \u65e0\u597d\u574f\u4e4b\u5206\nC. \u6709\u597d\u6709\u574f\nD. \u90fd\u662f\u597d\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bd7\u96c6\u300a\u7ea2\u70db\u300b\u7684\u4f5c\u8005\u662f\nA. \u95fb\u4e00\u591a\nB. \u90ed\u6cab\u82e5\nC. \u6731\u6e58\nD. \u5f90\u5fd7\u6469\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3723801373943884, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6539\u9769\u5f00\u653e\u4ee5\u6765\uff0c\u6211\u56fd\u57ce\u9547\u9636\u5c42\u7ed3\u6784\u5927\u4f53\u5206\u4e3a\nA. 18 \u4e2a\u9636\u5c42\nB. 20 \u4e2a\u9636\u5c42\nC. 16 \u4e2a\u9636\u5c42\nD. 17 \u4e2a\u9636\u5c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.309493990256606, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.28966338381871215, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.576965421664809}}, {"question": "\u6839\u636e\u7acb\u6cd5\u673a\u5173\u7684\u6027\u8d28\u4e0d\u540c\uff0c\u7acb\u6cd5\u53ef\u5206\u4e3a\nA. \u4e00\u9662\u5236\u7acb\u6cd5\u548c\u4e24\u9662\u5236\u7acb\u6cd5\nB. \u541b\u4e3b\u7acb\u6cd5\u3001\u8bae\u4f1a\u7acb\u6cd5\u3001\u6c11\u4e3b\u7acb\u6cd5\nC. \u4e2d\u592e\u7acb\u6cd5\u548c\u5730\u65b9\u7acb\u6cd5\nD. \u56fd\u5bb6\u7acb\u6cd5\u673a\u5173\u7acb\u6cd5\u3001\u56fd\u5bb6\u884c\u653f\u673a\u5173\u7acb\u6cd5\u548c\u6388\u6743\u7acb\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.450193376197193, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.44617922098161655, "HuggingFaceH4/zephyr-7b-beta": 0.9992393653001255, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.567840840022051, "meta-llama/Meta-Llama-3-8B": 0.530306221043056, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4fe1\u606f\u9690\u85cf\u4e3b\u8981\u7814\u7a76\u5982\u4f55\u5c06\u673a\u5bc6\u4fe1\u606f\u79d8\u5bc6\u9690\u85cf\u4e8e\u53e6\u4e00\u516c\u5f00\u7684\u4fe1\u606f\u4e2d\u3002\u4ee5\u4e0b\u5173\u4e8e\u5229\u7528\u591a\u5a92\u4f53\u6570\u636e\u6765\u9690\u85cf\u673a\u5bc6\u4fe1\u606f\u7684\u53d9\u8ff0\u4e2d\uff0c\u9519\u8bef\u7684\u662f\nA. \u4eba\u773c\u6216\u4eba\u8033\u5bf9\u67d0\u4e9b\u4fe1\u606f\u6709\u4e00\u5b9a\u7684\u63a9\u853d\u6548\u5e94\nB. \u591a\u5a92\u4f53\u4fe1\u606f\u672c\u8eab\u6709\u5f88\u5927\u7684\u5197\u4f59\u6027\nC. \u591a\u5a92\u4f53\u4fe1\u606f\u672c\u8eab\u7f16\u7801\u6548\u7387\u5f88\u9ad8\nD. \u4fe1\u606f\u5d4c\u5165\u5230\u591a\u5a92\u4f53\u4fe1\u606f\u4e2d\u4e0d\u5f71\u54cd\u591a\u5a92\u4f53\u672c\u8eab\u7684\u4f20\u9001\u548c\u4f7f\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7053077711204347, "meta-math/MetaMath-Mistral-7B": 0.8885018099324125, "itpossible/Chinese-Mistral-7B-v0.1": 0.5029677726154719, "HuggingFaceH4/zephyr-7b-beta": 0.9943424877271846, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9229763067758561, "meta-llama/Meta-Llama-3-8B": 0.6536630792000145, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0e\u793e\u4f1a\u4e3b\u6d41\u6216\u4e3b\u5bfc\u6587\u5316\u6240\u4e0d\u540c\u7684\u6587\u5316\u8868\u73b0\u51fa\u6765\u7684\u504f\u5dee\u884c\u4e3a\uff0c\u88ab\u79f0\u4e3a\nA. \u504f\u5dee\u6587\u5316\nB. \u504f\u5dee\u884c\u52a8\nC. \u504f\u5dee\u4e60\u60ef\nD. \u504f\u5dee\u5fc3\u7406\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53e0\u52a0\u5b9a\u7406\u53ea\u9002\u7528\u4e8e\nA. \u4ea4\u6d41\u7535\u8def\nB. \u7ebf\u6027\u7535\u8def\nC. \u6b63\u5f26\u7535\u8def\nD. \u76f4\u6d41\u7535\u8def\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6643600348012851, "meta-math/MetaMath-Mistral-7B": 0.9759046172284447, "itpossible/Chinese-Mistral-7B-v0.1": 0.7419596068595311, "HuggingFaceH4/zephyr-7b-beta": 0.7721188704497608, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6183350947779933, "meta-llama/Meta-Llama-3-8B": 0.5111934837525189, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u661f\u5ea7\u4e2d\u4e0d\u76f8\u90bb\u7684\u662f\nA. \u5ba4\u5973\u5ea7\u4e0e\u540e\u53d1\u5ea7\nB. \u5b9d\u74f6\u5ea7\u4e0e\u5357\u9c7c\u5ea7\nC. \u82f1\u4ed9\u5ea7\u4e0e\u98de\u9a6c\u5ea7\nD. \u5929\u9f99\u5ea7\u4e0e\u6b66\u4ed9\u5ea7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3512536987011173, "HuggingFaceH4/zephyr-7b-beta": 0.6184632772853015, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.32659802886351824, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3423962532986189}}, {"question": "\u673a\u68b0\u5b66\u4e60\u4e0e\u6709\u610f\u4e49\u5b66\u4e60\u5212\u5206\u7684\u4e3b\u8981\u4f9d\u636e\u662f\u3002\nA. \u5b66\u751f\u662f\u5426\u7406\u89e3\u5b66\u4e60\u6750\u6599\nB. \u5b66\u4e60\u76ee\u7684\u662f\u89e3\u51b3\u95ee\u9898\u6216\u662f\u83b7\u5f97\u77e5\u8bc6\nC. \u8981\u5b66\u4e60\u7684\u4e3b\u8981\u5185\u5bb9\u662f\u5448\u73b0\u6216\u7531\u5b66\u751f\u53d1\u73b0\nD. \u5b66\u751f\u662f\u5426\u4e3b\u52a8\u5b66\u4e60\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u540c\u65f6\u629b\u63b7\u4e24\u679a\u8d28\u5730\u5747\u5300\u7684\u786c\u5e01\uff0c\u5219\u51fa\u73b0\u4e24\u4e2a\u6b63\u2faf\u671d\u4e0a\u7684\u6982\u7387\u662f\nA. 1/8\nB. 1/3\nC. 1/2\nD. 1/4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7445456512188147, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5 $0<P(B)<1$\uff0c \u4e14 $P[(A_1+A_2) \\mid B]=P(A_1 \\mid B)+P(A_2 \\mid B)$\uff0c \u5219\u4e0b\u5217\u9009\u9879\u6210\u7acb\u7684\u662f ( )\nA. $P(A_1 B+A_2 B)=P(A_1 B)+P\\left(A_2 B)$\nB. $P[(A_1+A_2) \\mid \\bar{B}]=P(A_1 \\mid \\bar{B})+P(A_2 \\mid \\bar{B})$\nC. $P(A_1+A_2)=P(A_1 \\mid B)+P(A_2 \\mid B)$\nD. $P(B)=P(A_1) P(B \\mid A_1)+P(A_2) P(B \\mid A_2)$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2538449215797171, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.30702389681474196, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53c2\u4e0e\u6784\u6210\u8c37\u80f1\u7518\u80bd\u8fc7\u6c27\u5316\u7269\u9176\u7684\u8425\u517b\u7d20\u662f\nA. \u950c\nB. \u94c1\nC. \u7852\nD. \u786b\u80fa\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4082636630131841, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8748709033801557, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b0\u95fb\u4ef7\u503c\u662f\u4f20\u64ad\u8005\u9009\u62e9\u4e8b\u5b9e\u548c\u63a5\u53d7\u8005\u9009\u62e9\u65b0\u95fb\u7684\nA. \u5fc3\u7406\u9700\u6c42\nB. \u5ba2\u89c2\u6807\u51c6\nC. \u4e3b\u89c2\u610f\u5411\nD. \u653f\u6cbb\u6807\u51c6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5777518135484442, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8585193248628898}}, {"question": "\u8bd7\u53e5\u201c\u6d77\u5185\u5b58\u77e5\u5df1\uff0c\u5929\u6daf\u82e5\u6bd4\u9886\u201d\u7684\u4f5c\u8005\u662f\nA. \u674e\u767d\nB. \u738b\u52c3\nC. \u738b\u7ef4\nD. \u675c\u57d4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.36891085543308744, "itpossible/Chinese-Mistral-7B-v0.1": 0.3361950722028681, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e0b\u5217\u6211\u56fd\u73b0\u884c\u6cd5\u5f8b\u6761\u6b3e\u4e2d\uff0c\u54ea\u9879\u4e0d\u5c5e\u4e8e\u6cd5\u5f8b\u539f\u5219\uff1f\nA. \u7968\u636e\u6d3b\u52a8\u5e94\u5f53\u9075\u5b88\u6cd5\u5f8b\u3001\u884c\u653f\u6cd5\u89c4\uff0c\u4e0d\u5f97\u635f\u5bb3\u793e\u4f1a\u516c\u5171\u5229\u76ca\nB. \u515a\u5fc5\u987b\u5728\u5baa\u6cd5\u548c\u6cd5\u5f8b\u7684\u8303\u56f4\u5185\u6d3b\u52a8\nC. \u7ecf\u6279\u51c6\u7684\u4e0a\u5e02\u516c\u53f8\u7684\u80a1\u4efd\uff0c\u4f9d\u7167\u6709\u5173\u6cd5\u5f8b\u3001\u884c\u653f\u6cd5\u89c4\u4e0a\u5e02\u4ea4\u6613\nD. \u516c\u53f8\u5fc5\u987b\u4fdd\u62a4\u804c\u5de5\u7684\u5408\u6cd5\u6743\u76ca\uff0c\u52a0\u5f3a\u52b3\u52a8\u4fdd\u62a4\uff0c\u5b9e\u884c\u5b89\u5168\u751f\u4ea7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7664818405709635, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8742315030381357}}, {"question": "\u300a\u7406\u60f3\u56fd\u300b\u662f\u8c01\u7684\u4f5c\u54c1\uff1f\nA. \u67cf\u62c9\u56fe\nB. \u829d\u8bfa\nC. \u82cf\u683c\u62c9\u5e95\nD. \u4e9a\u529b\u58eb\u591a\u5fb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9445667728312238, "meta-math/MetaMath-Mistral-7B": 0.9954699582519122, "itpossible/Chinese-Mistral-7B-v0.1": 0.9555279829297787, "HuggingFaceH4/zephyr-7b-beta": 0.9998959582325165, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8814296466332106, "meta-llama/Meta-Llama-3-8B": 0.9857524225257601, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9795992306148911}}, {"question": "DRAM\u5b58\u50a8\u5668\u7684\u4e2d\u2f42\u542b\u4e49\u662f\nA. \u52a8\u6001\u968f\u673a\u5b58\u50a8\u5668\nB. \u52a8\u6001\u53ea\u8bfb\u5b58\u50a8\u5668\nC. \u9759\u6001\u968f\u673a\u5b58\u50a8\u5668\nD. \u9759\u6001\u53ea\u8bfb\u5b58\u50a8\u5668\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7306809097713806, "meta-math/MetaMath-Mistral-7B": 0.9637760857017058, "itpossible/Chinese-Mistral-7B-v0.1": 0.9168762075828429, "HuggingFaceH4/zephyr-7b-beta": 0.988338000318723, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9223806489808292, "meta-llama/Meta-Llama-3-8B": 0.9018006011498728, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9912631414262718}}, {"question": "\u4ee5\u4e0b\u5173\u4e8eVPN\u7684\u53d9\u8ff0\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. VPN \u6307\u7528\u6237\u81ea\u5df1\u79df\u7528\u7ebf\u8def\uff0c\u548c\u516c\u5171\u7f51\u7edc\u7269\u7406\u4e0a\u5b8c\u5168\u9694\u79bb\u7684\u3001\u5b89\u5168\u7684\u7ebf\u8def\nB. VPN\u4e0d\u80fd\u540c\u65f6\u5b9e\u73b0\u5bf9\u6d88\u606f\u7684\u8ba4\u8bc1\u548c\u5bf9\u8eab\u4efd\u7684\u8ba4\u8bc1\nC. VPN\u901a\u8fc7\u52a0\u5bc6\u6570\u636e\u4fdd\u8bc1\u901a\u8fc7\u516c\u7f51\u4f20\u8f93\u7684\u4fe1\u606f\u5373\u4f7f\u88ab\u4ed6\u4eba\u622a\u83b7\u4e5f\u4e0d\u4f1a\u6cc4\u9732\nD. VPN \u901a\u8fc7\u8eab\u4efd\u8ba4\u8bc1\u5b9e\u73b0\u5b89\u5168\u76ee\u6807\uff0c\u4e0d\u5177\u5907\u6570\u636e\u52a0\u5bc6\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4285537155312629, "meta-math/MetaMath-Mistral-7B": 0.7920556813744897, "itpossible/Chinese-Mistral-7B-v0.1": 0.35686367501298644, "HuggingFaceH4/zephyr-7b-beta": 0.9788140804278629, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8657884334662301, "meta-llama/Meta-Llama-3-8B": 0.9003701908339252, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9651776831324483}}, {"question": "\u5047\u5b9a\u4f60\u4f7f\u7528SVM\u5b66\u4e60\u6570\u636eX\uff0c\u6570\u636eX\u91cc\u9762\u6709\u4e9b\u70b9\u5b58\u5728\u9519\u8bef\u3002\u73b0\u5728\u5982\u679c\u4f60\u4f7f\u7528\u4e00\u4e2a\u4e8c\u6b21\u6838\u51fd\u6570\uff0c\u591a\u9879\u5f0f\u9636\u6570\u4e3a2\uff0c\u4f7f\u7528\u677e\u5f1b\u53d8\u91cfC\u4f5c\u4e3a\u8d85\u53c2\u4e4b\u4e00\u3002 \u5982\u679c\u4f7f\u7528\u8f83\u5c0f\u7684C\uff08C\u8d8b\u4e8e0\uff09\uff0c\u5219\uff1a\nA. \u4e0d\u786e\u5b9a\nB. \u8bef\u5206\u7c7b\nC. \u6b63\u786e\u5206\u7c7b\nD. \u4ee5\u4e0a\u5747\u4e0d\u6b63\u786e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4775911480746774, "meta-math/MetaMath-Mistral-7B": 0.850350784327483, "itpossible/Chinese-Mistral-7B-v0.1": 0.34478590299221606, "HuggingFaceH4/zephyr-7b-beta": 0.9949012880376998, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7885726480040391, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8703555519782287}}, {"question": "\u4e2d\u56fd\u516c\u6c11\u7532\u4e0e\u4e2d\u56fd\u516c\u6c11\u4e59\u4e8e1993\u5e74\u5728\u4e2d\u56fd\u7ed3\u5a5a\uff0c\u540e\u7532\u7559\u5b66\u52a0\u62ff\u5927\u5e76\u4e8e\u52a0\u62ff\u5927\u67d0\u516c\u53f8\u5de5\u4f5c\u30021997\u5e74\uff0c\u7532\u5728\u52a0\u62ff\u5927\u5b89\u5927\u7565\u7701\u63d0\u8d77\u79bb\u5a5a\u8bc9\u8bbc\uff0c\u4e59\u5f97\u77e5\u540e\u4e5f\u59d4\u6258\u5f8b\u5e08\u5728\u4e2d\u56fd\u5176\u4f4f\u6240\u6240\u5728\u5730\u6cd5\u9662\u63d0\u8d77\u79bb\u5a5a\u8bc9\u8bbc\u3002\u6839\u636e\u6211\u56fd\u6709\u5173\u89c4\u5b9a\uff0c\u4e0b\u5217\u54ea\u79cd\u8bf4\u6cd5\u662f\u6b63\u786e\u7684\uff1f\nA. \u7531\u4e8e\u52a0\u62ff\u5927\u6cd5\u9662\u5148\u884c\u53d7\u7406\uff0c\u56e0\u6b64\u4e2d\u56fd\u6cd5\u9662\u6ca1\u6709\u7ba1\u8f96\u6743\u3002\nB. \u5982\u679c\u4e2d\u56fd\u6cd5\u9662\u5148\u884c\u53d7\u7406\uff0c\u53ef\u4ee5\u884c\u4f7f\u7ba1\u8f96\u6743\uff0c\u4f46\u6709\u5173\u79bb\u5a5a\u5f15\u8d77\u7684\u8d22\u4ea7\u5206\u5272\u95ee\u9898\uff0c\u53ea\u80fd\u9002\u7528\u8d22\u4ea7\u6240\u5728\u5730\u6cd5\u3002\nC. \u4e2d\u56fd\u6cd5\u9662\u6709\u7ba1\u8f96\u6743\u3002\u5173\u4e8e\u79bb\u5a5a\u7684\u6761\u4ef6\u53ca\u8d22\u4ea7\u5206\u5272\u95ee\u9898\uff0c\u90fd\u5e94\u9002\u7528\u4e2d\u56fd\u6cd5\u3002\nD. \u4e2d\u56fd\u6cd5\u9662\u6709\u7ba1\u8f96\u6743\u3002\u5173\u4e8e\u79bb\u5a5a\u7684\u6761\u4ef6\u95ee\u9898\u5e94\u5f53\u9002\u7528\u4e2d\u56fd\u6cd5\uff0c\u4f46\u6709\u5173\u8d22\u4ea7\u5206\u5272\u95ee\u9898\uff0c\u5e94\u5f53\u9002\u7528\u8d22\u4ea7\u6240\u5728\u5730\u6cd5\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.636970583901616, "meta-math/MetaMath-Mistral-7B": 0.801758761149346, "itpossible/Chinese-Mistral-7B-v0.1": 0.6573769985993989, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3938024230069351, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.589217934001767}}, {"question": "\u897f\u6c49\u521d\u5e74\uff0c\u5218\u90a6\u5c01\u7f6e\u8bf8\u4faf\u738b\u56fd\uff0e\u5c01\u5b50\u5f1f\u4e3a\u738b\u3002\u529f\u81e3\u4e3a\u4faf\uff0c\u5e76\u89c4\u5b9a\u201c\u975e\u5218\u6c0f\u4e0d\u738b\uff0e\u975e\u6709\u529f\u4e0d\u4faf\u201d\u3002\u8fd9\u8868\u660e\u5218\u90a6\u63a8\u884c\u5206\u5c01\u5236\u610f\u5728\nA. \u786e\u4fdd\u7687\u4f4d\u4f20\u627f\u7a33\u56fa\nB. \u52a0\u5f3a\u5bf9\u5730\u65b9\u7684\u63a7\u5236\nC. \u7ee7\u627f\u897f\u5468\u7684\u653f\u6cbb\u4f53\u5236\nD. \u5426\u5b9a\u79e6\u671d\u7684\u653f\u6cbb\u4f53\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.927029653457034, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3661763986529049, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u79d1\u6280\u521b\u65b0\u5728\u5f15\u9886\u53d1\u5c55\u3001\u5efa\u8bbe\u521b\u65b0\u578b\u56fd\u5bb6\u4e2d\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\u3002\u4e0b\u5217\u4e2d\u56fd\u91cd\u5927\u79d1\u6280\u6210\u5c31\u7684\u53d6\u5f97\uff0c\u662f\u5728\u793e\u4f1a\u4e3b\u4e49\u73b0\u4ee3\u5316\u5efa\u8bbe\u65b0\u65f6\u671f\u7684\u662f\nA. \u7b2c\u4e00\u9897\u6c22\u5f39\u7206\u70b8\u6210\u529f\nB. \u7b2c\u4e00\u9897\u539f\u5b50\u5f39\u7206\u70b8\u6210\u529f\nC. \u201c\u86df\u9f99\u53f7\u201d\u8f7d\u4eba\u6df1\u6f5c\u5668\u7814\u5236\u6210\u529f\nD. \u7b2c\u4e00\u9897\u4eba\u9020\u5730\u7403\u536b\u661f\u53d1\u5c04\u6210\u529f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.45971693593633167, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3938024429486752, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8111\u819c\u4e2d\u52a8\u8109\u76f4\u63a5\u8d77\u4e8e\nA. \u5927\u8111\u4e2d\u52a8\u8109\nB. \u9888\u5185\u52a8\u8109\nC. \u9888\u5916\u52a8\u8109\nD. \u4e0a\u988c\u52a8\u8109\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0d\u5c5e\u4e8e\u517b\u751f\u57fa\u672c\u539f\u5219\u7684\u662f\nA. \u8fa8\u8bc1\u8bba\u6cbb\nB. \u201c\u8282\u9634\u9633\u800c\u8c03\u521a\u67d4\u201c\u7684\u5b88\u4e2d\u601d\u60f3\nC. \u201c\u6cbb\u672a\u75c5\u201c\u7684\u9884\u9632\u601d\u60f3\nD. \u7a81\u51fa\u7cbe\u795e\u5fc3\u7406\u5065\u5eb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5859786989973701, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6cbb\u7406\u901a\u8d27\u81a8\u80c0\u7684\u5b8f\u89c2\u7ecf\u6d4e\u653f\u7b56\u4e2d\uff0c\u5c5e\u4e8e\u7d27\u7f29\u6027\u8d27\u5e01\u653f\u7b56\u7684\u662f\nA. \u653f\u5e9c\u524a\u51cf\u8d22\u653f\u652f\u51fa\nB. \u4e2d\u592e\u94f6\u884c\u5728\u516c\u5f00\u5e02\u573a\u4e0a\u51fa\u552e\u653f\u5e9c\u503a\u5238\nC. \u63a7\u5236\u96c6\u56e2\u8d2d\u4e70\u529b\nD. \u653f\u5e9c\u786e\u5b9a\u5de5\u8d44\u2014\u7269\u4ef7\u6307\u5bfc\u7ebf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8888684084293911, "meta-math/MetaMath-Mistral-7B": 0.9944867548463519, "itpossible/Chinese-Mistral-7B-v0.1": 0.8910416279883404, "HuggingFaceH4/zephyr-7b-beta": 0.9998689203831604, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9854734200385884, "meta-llama/Meta-Llama-3-8B": 0.602463810722174, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8064721042232873}}, {"question": "\u4e00\u4e2a\u884c\u4e1a\u6709\u5f88\u591a\u4f01\u4e1a\uff0c\u6bcf\u4e2a\u4f01\u4e1a\u9500\u552e\u7684\u4ea7\u54c1\u4e0e\u5176\u4ed6\u4f01\u4e1a\u7684\u4ea7\u54c1\u7565\u6709\u5dee\u522b\uff0c\u8fd9\u6837\u7684\u5e02\u573a\u7ed3\u6784\u88ab\u79f0\u4e3a\nA. \u5784\u65ad\u7ade\u4e89\nB. \u5b8c\u5168\u7ade\u4e89\nC. \u5be1\u5934\nD. \u5784\u65ad\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8687670272051393, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9115818792511509}}, {"question": "\u4ee5\u4e0b\u54ea\u4e24\u4e2a\u661f\u5ea7\u4e0d\u662f\u76f8\u90bb\u7684\nA. \u7267\u592b\u5ea7\u548c\u730e\u72ac\u5ea7\nB. \u730e\u6237\u5ea7\u548c\u6ce2\u6c5f\u5ea7\nC. \u5929\u7434\u5ea7\u548c\u5929\u9e70\u5ea7\nD. \u5357\u9c7c\u5ea7\u548c\u5b9d\u74f6\u5ea7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.33424000363035195, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.26560468668687814, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4619154283166679}}, {"question": "\u4ece\u8111\u5e72\u80cc\u4fa7\u51fa\u8111\u7684\u8111\u795e\u7ecf\u662f\nA. \u526f\u795e\u7ecf\nB. \u4e09\u53c9\u795e\u7ecf\nC. \u820c\u4e0b\u795e\u7ecf\nD. \u6ed1\u8f66\u795e\u7ecf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9a6c\u514b\u601d\u6307\u51fa\uff1a\u201c\u642c\u8fd0\u592b\u548c\u54f2\u5b66\u5bb6\u4e4b\u95f4\u7684\u539f\u59cb\u5dee\u522b\u8981\u6bd4\u5bb6\u72ac\u548c\u730e\u72ac\u4e4b\u95f4\u7684\u5dee\u522b\u5c0f\u5f97\u591a\uff0c\u5b83\u4eec\u4e4b\u95f4\u7684\u9e3f\u6c9f\u662f\u5206\u5de5\u6398\u6210\u7684\u3002\u201d\u8fd9\u8868\u660e\nA. \u4eba\u7684\u806a\u660e\u624d\u667a\u7684\u5927\u5c0f\u4e3b\u8981\u53d6\u51b3\u4e8e\u4e3b\u89c2\u52aa\u529b\u7684\u7a0b\u5ea6\nB. \u4eba\u7684\u806a\u660e\u624d\u667a\u4e3b\u8981\u6765\u6e90\u4e8e\u540e\u5929\u5b9e\u8df5\nC. \u4eba\u7684\u806a\u660e\u624d\u667a\u65e0\u5148\u5929\u533a\u522b\nD. \u4eba\u7684\u806a\u660e\u624d\u667a\u7531\u4eba\u7684\u793e\u4f1a\u653f\u6cbb\u5730\u4f4d\u51b3\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.49900728340059436, "meta-math/MetaMath-Mistral-7B": 0.41121679732018557, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6359533361780554, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8124663037227565, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6709\u5173\u7269\u8d28\u7528\u9014\u7684\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f\nA. \u8fc7\u6c27\u5316\u94a0\u53ef\u7528\u4e8e\u6f5c\u6c34\u8247\u7684\u6c27\u6c14\u6765\u6e90\nB. \u94a0\u53ef\u7528\u4e8e\u51b6\u70bc\u91d1\u5c5e\u949b\nC. \u9ad8\u7eaf\u5ea6\u7684\u7845\u5355\u8d28\u5e7f\u6cdb\u7528\u4e8e\u5236\u9020\u5149\u5bfc\u7ea4\u7ef4\nD. \u5c0f\u82cf\u6253\u53ef\u7528\u4e8e\u6cbb\u7597\u80c3\u9178\u8fc7\u591a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6709\u5173\u5b9e\u9a8c\u7684\u53d9\u8ff0\u6b63\u786e\u7684\u662f\nA. \u9178\u5f0f\u6ef4\u5b9a\u7ba1\u6ce8\u5165Na2CO3\u6eb6\u6db2\u4e4b\u524d\u5e94\u68c0\u67e5\u662f\u5426\u6f0f\u6db2\nB. \u5236\u5907\u4e59\u9178\u4e59\u916f\u65f6\uff0c\u5c06\u4e59\u9187\u548c\u4e59\u9178\u4f9d\u6b21\u52a0\u5165\u5230\u6d53\u786b\u9178\u4e2d\nC. \u7528\u94c2\u4e1d\u8638\u53d6\u5c11\u91cf\u67d0\u6eb6\u6db2\u8fdb\u884c\u7130\u8272\u53cd\u5e94\uff0c\u706b\u7130\u5448\u9ec4\u8272\uff0c\u8be5\u6eb6\u6db2\u4e00\u5b9a\u662f\u94a0\u76d0\u6eb6\u6db2\nD. \u9664\u53bbFe(OH)3\u56fa\u4f53\u4e2d\u5c11\u91cf\u7684Mg(OH)2\uff0c\u52a0\u5165\u8db3\u91cf\u9971\u548cFeCl3\u6eb6\u6db2\uff0c\u5145\u5206\u6405\u62cc\u540e\u8fc7\u6ee4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4792568976060359, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4189649620196225}}, {"question": "\u5bf9\u8fdd\u7ea6\u91d1\u7684\u6027\u8d28\u5404\u56fd\u6709\u4e0d\u540c\u89c4\u5b9a\uff0c\u8ba4\u4e3a\u8fdd\u7ea6\u91d1\u5177\u6709\u60e9\u7f5a\u6027\u7684\u56fd\u5bb6\u662f\nA. \u7f8e\u56fd\nB. \u5fb7\u56fd\nC. \u6cd5\u56fd\nD. \u82f1\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2616492824408659, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3060136256597631, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u83f1\u5f62ABCD\u4e2d\uff0c\u5bf9\u89d2\u7ebfAC=4\uff0c$\\angle BAD=120^{\\circ }$\uff0c\u5219\u83f1\u5f62ABCD\u7684\u5468\u957f\u4e3a\nA. 16\nB. 20\nC. 15\nD. 18\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2801288226217134, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9053\u5fb7\u7684\u529f\u80fd\u662f\u6307\u9053\u5fb7\u4f5c\u4e3a\u793e\u4f1a\u610f\u8bc6\u7684\u7279\u6b8a\u5f62\u6001\u5bf9\u4e8e\u793e\u4f1a\u53d1\u5c55\u6240\u5177\u6709\u7684\u529f\u6548\u548c\u80fd\u529b\u3002\u9053\u5fb7\u5177\u6709\u591a\u65b9\u9762\u7684\u529f\u80fd\uff0c\u5176\u4e2d\u53cd\u6620\u793e\u4f1a\u73b0\u5b9e\u7279\u522b\u662f\u53cd\u6620\u793e\u4f1a\u7ecf\u6d4e\u5173\u7cfb\u7684\u529f\u6548\u548c\u80fd\u529b\u662f\u9053\u5fb7\u7684\nA. \u8c03\u8282\u529f\u80fd\nB. \u8ba4\u8bc6\u529f\u80fd\nC. \u8bc4\u4ef7\u529f\u80fd\nD. \u6559\u80b2\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.33490177288810025, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bb6\u755c\u7684\u9752\u5e74\u671f\u6307\nA. \u4ece\u6027\u6210\u719f\u5230\u5f00\u59cb\u8870\u8001\nB. \u4ece\u6027\u6210\u719f\u5230\u4f53\u6210\u719f\nC. \u4ece\u65ad\u5976\u5230\u4f53\u6210\u719f\nD. \u4ece\u65ad\u5976\u5230\u6027\u6210\u719f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4124582239315712, "meta-math/MetaMath-Mistral-7B": 0.6444289172367038, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9825153406712301, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8024040121369996, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d2d\u4e70\u98df\u54c1\u65f6\uff0c\u4f60\u8981\u6ce8\u610f\nA. A\u4e0eB\nB. \u751f\u4ea7\u5382\u5bb6\nC. \u751f\u4ea7\u65e5\u671f\nD. \u751f\u4ea7\u5382\u5bb6\u3001\u751f\u4ea7\u65e5\u671f\u3001\u4fdd\u8d28\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8910363455891799, "meta-math/MetaMath-Mistral-7B": 0.9559227635247268, "itpossible/Chinese-Mistral-7B-v0.1": 0.7445456285455141, "HuggingFaceH4/zephyr-7b-beta": 0.9997196384223747, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.974100489441063, "meta-llama/Meta-Llama-3-8B": 0.9641526544742024, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9969731343778268}}, {"question": "\u65e0\u5dee\u5f02\u66f2\u7ebf\u662f\u4e00\u6761\u51f8\u5411\u539f\u70b9\u7684\u66f2\u7ebf\uff0c\u8fd9\u662f\u7531\uff08\uff09\u51b3\u5b9a\u7684\u3002\nA. \u8fb9\u9645\u6210\u672c\u9012\u51cf\nB. \u603b\u6548\u7528\u9012\u51cf\nC. \u8fb9\u9645\u66ff\u4ee3\u7387\u9012\u51cf\nD. \u8fb9\u9645\u6548\u7528\u9012\u51cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c45\u5c81\u3002\u81ea\u89c9\u53cc\u624b\u6307\u5173\u8282\u75bc\u75db2\u5e74\uff0c\u8fd1\u534a\u5e74\u611f\u89c9\u5173\u8282\u50f5\u786c\u3002\u8840\u6e05\u5b66\u68c0\u67e5\uff1a\u7c7b\u98ce\u6e7f\u56e0\u5b50\u9633\u6027\u3002\u8be5\u60a3\u8005\u5173\u8282\u5185\u7684\u4e3b\u8981\u75c5\u7406\u6539\u53d8\u662f\nA. \u8089\u82bd\u80bf\u6027\u708e\nB. \u5927\u91cf\u55dc\u9178\u6027\u7c92\u7ec6\u80de\u6d78\u6da6\nC. \u6d46\u6db2\u6e17\u51fa\u6027\u708e\nD. \u975e\u5316\u8113\u6027\u589e\u751f\u6027\u6ed1\u819c\u708e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3351436884814033, "meta-math/MetaMath-Mistral-7B": 0.3423962339378881, "itpossible/Chinese-Mistral-7B-v0.1": 0.5476903452800108, "HuggingFaceH4/zephyr-7b-beta": 0.8555541736041355, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38453152591357337, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5de7\u514b\u529b\u201d\u4e00\u8bcd\u4ece\u6784\u8bcd\u4e0a\u8bf4\u662f\nA. \u4e00\u4e2a\u4e09\u97f3\u8282\u7684\u8bcd\nB. \u8bcd\u6839+\u8bcd\u7f00\u201d\u6784\u6210\u7684\u8bcd\nC. \u4e09\u4e2a\u8bcd\u6839\u6784\u6210\u7684\u8bcd\nD. \u4e24\u4e2a\u8bcd\u6839\u6784\u6210\u7684\u8bcd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u60c5\u51b5\u4e2d\uff0c\u80fd\u591f\u4f7f\u80ba\u901a\u6c14/\u8840\u6d41\u6bd4\u503c\u589e\u9ad8\u6700\u660e\u663e\u7684\u662f\nA. \u652f\u6c14\u7ba1\u54ee\u5598\u53d1\u4f5c\nB. \u80ba\u6c34\u80bf\nC. \u80ba\u7ea4\u7ef4\u5316\u5f62\u6210\nD. \u80ba\u6813\u585e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7012778626202343, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d75\u67d0\u4e0e\u738b\u67d0\u4e3a\u90bb\u5c45\uff0c\u5e38\u6709\u53e3\u89d2\u3002\u4e00\u5929\uff0c\u7531\u4e8e\u738b\u67d0\u517b\u7684\u7f8a\u5403\u4e86\u8d75\u67d0\u83dc\u5730\u91cc\u7684\u83dc\uff0c\u8d75\u67d0\u5927\u6012\uff0c\u9042\u780d\u4f10\u738b\u67d0\u5c4b\u524d\u7684\u679c\u681110\u68f5\u3002\u738b\u67d0\u5411\u4e61\u6d3e\u51fa\u6240\u63a7\u544a\uff0c\u4e61\u6d3e\u51fa\u6240\u63a5\u5230\u63a7\u544a\uff0c\u7ecf\u8c03\u67e5\u540e\uff0c\u4ee5\u53bf\u516c\u5b89\u5c40\u7684\u540d\u4e49\u5bf9\u8d75\u67d0\u4f5c\u51fa5\u5929\u7684\u62d8\u7559\u51b3\u5b9a\uff0c\u5e76\u8d23\u4ee4\u8d75\u67d0\u8d54\u507f\u738b\u67d0200\u5143\u3002\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u8d75\u67d0\u56e0\u780d\u4f10\u738b\u67d0\u7684\u679c\u6811\u800c\u5f62\u6210\u7684\u635f\u5bb3\u8d54\u507f\u5173\u7cfb\u5c5e\u4e8e\u8c03\u6574\u6027\u6cd5\u5f8b\u5173\u7cfb\nB. \u8d75\u67d0\u56e0\u780d\u4f10\u738b\u67d0\u7684\u679c\u6811\u800c\u5f62\u6210\u7684\u635f\u5bb3\u8d54\u507f\u5173\u7cfb\u5c5e\u4e8e\u76f8\u5bf9\u6cd5\u5f8b\u5173\u7cfb\nC. \u738b\u67d0\u56e0\u5bf9\u679c\u6811\u4eab\u6709\u6240\u6709\u6743\u800c\u5f62\u6210\u7684\u6cd5\u5f8b\u5173\u7cfb\u5c5e\u4e8e\u76f8\u5bf9\u6cd5\u5f8b\u5173\u7cfb\nD. \u53bf\u516c\u5b89\u5c40\u4e0e\u8d75\u67d0\u5f62\u6210\u4e86\u5e73\u6743\u7684\u6cd5\u5f8b\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3614425540026495, "meta-math/MetaMath-Mistral-7B": 0.7358119974550248, "itpossible/Chinese-Mistral-7B-v0.1": 0.4671452929714373, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6663471510085567}}, {"question": "\u4f5b\u6559\u6700\u77ed\u7684\u7ecf\u662f\nA. \u666e\u95e8\u54c1\nB. \u5f25\u9640\u7ecf\nC. \u91d1\u521a\u7ecf\nD. \u5fc3\u7ecf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3669421044268479, "itpossible/Chinese-Mistral-7B-v0.1": 0.43932247639970834, "HuggingFaceH4/zephyr-7b-beta": 0.9299213330303697, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45256531407760214, "meta-llama/Meta-Llama-3-8B": 0.9188816177998524, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u53e4\u4ee3\u6700\u5927\u7684\u4e00\u90e8\u767e\u79d1\u5168\u4e66\u662f\nA. \u300a\u897f\u6e38\u8bb0\u300b\nB. \u300a\u56db\u5e93\u5168\u4e66\u300b\nC. \u300a\u6c38\u4e50\u5927\u5178\u300b\nD. \u300a\u4e09\u56fd\u6f14\u4e49\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7245421346096511, "meta-math/MetaMath-Mistral-7B": 0.9816144882626519, "itpossible/Chinese-Mistral-7B-v0.1": 0.9034476156657179, "HuggingFaceH4/zephyr-7b-beta": 0.9366758765246538, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9238892858024147, "meta-llama/Meta-Llama-3-8B": 0.9635513641176033, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u96e8\u540e\u8def\u9762\u6e7f\u6ed1\uff0c\u8f66\u8f86\u884c\u9a76\u4e2d\u7d27\u6025\u5236\u52a8\u65f6\uff0c\u5bb9\u6613\u5bfc\u81f4\u4ec0\u4e48\nA. \u53d1\u751f\u4fa7\u6ed1\u3001\u5f15\u53d1\u4ea4\u901a\u4e8b\u6545\nB. \u5f15\u8d77\u53d1\u52a8\u673a\u7184\u706b\nC. \u4e0d\u88ab\u5176\u4ed6\u8f66\u8f86\u9a7e\u9a76\u4eba\u53d1\u73b0\nD. \u56e0\u89c6\u7ebf\u6a21\u7cca\u800c\u649e\u8f66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.955560247390188, "meta-math/MetaMath-Mistral-7B": 0.9902346029787147, "itpossible/Chinese-Mistral-7B-v0.1": 0.9848363934217722, "HuggingFaceH4/zephyr-7b-beta": 0.9996917414721223, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9967601442323746, "meta-llama/Meta-Llama-3-8B": 0.9772500614788344, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9955081456847213}}, {"question": "\u6309\u7167\u9009\u62e9\u6027\u5206\u7c7b\u53ef\u4ee5\u628a\u8bfe\u7a0b\u5206\u4e3a\nA. \u5fb7\u80b2\u8bfe\u7a0b\u3001\u667a\u80b2\u8bfe\u7a0b\u3001\u4f53\u80b2\u8bfe\u7a0b\u3001\u7f8e\u80b2\u8bfe\u7a0b\u548c\u52b3\u52a8\u6280\u672f\u8bfe\u7a0b\u7b49\nB. \u5730\u65b9\u8bfe\u7a0b\u548c\u6821\u672c\u8bfe\u7a0b\nC. \u5fc5\u4fee\u8bfe\u7a0b\u548c\u9009\u4fee\u8bfe\u7a0b\nD. \u5206\u79d1\u8bfe\u7a0b\u4e0e\u7efc\u5408\u8bfe\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5287058513771482, "meta-math/MetaMath-Mistral-7B": 0.6692518243164326, "itpossible/Chinese-Mistral-7B-v0.1": 0.669865625722377, "HuggingFaceH4/zephyr-7b-beta": 0.6219564602323083, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.769862413029975, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e09\u4f4d\u4e8c\u8fdb\u5236\u6570\u53ef\u4ee5\u8868\u793a\u7684\u6700\u5927\u7684\u5341\u8fdb\u5236\u6570\u662f\nA. 5\nB. 6\nC. 7\nD. 4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5591242931674764, "meta-math/MetaMath-Mistral-7B": 0.5536827191734115, "itpossible/Chinese-Mistral-7B-v0.1": 0.31283638571410965, "HuggingFaceH4/zephyr-7b-beta": 0.6402044601577757, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7103735292782345, "meta-llama/Meta-Llama-3-8B": 0.33620111756034343, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4734378070313683}}, {"question": "\u4eba\u662f\u7ec4\u7ec7\u4e2d\u6700\u91cd\u8981\u7684\u56e0\u7d20\uff0c\u4eba\u4e0d\u662f\u7b80\u5355\u7684\u751f\u4ea7\u5de5\u5177\uff0c\u800c\u662f\u5177\u6709\u590d\u6742\u7684\u4e2a\u6027\u548c\u591a\u65b9\u9762\u7684\u9700\u6c42\uff0c\u8fd9\u79cd\u89c2\u70b9\u6765\u81ea\u4e8e\nA. \u7ec4\u7ec7\u884c\u4e3a\u7406\u8bba\nB. \u6743\u53d8\u7406\u8bba\nC. \u79d1\u5b66\u7ba1\u7406\u7406\u8bba\nD. \u4eba\u9645\u5173\u7cfb\u7406\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6457454053342816, "meta-math/MetaMath-Mistral-7B": 0.5523332604731227, "itpossible/Chinese-Mistral-7B-v0.1": 0.6650151042217025, "HuggingFaceH4/zephyr-7b-beta": 0.9419143384140999, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.49214353592654353, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1928\u5e74\u5728\u7b2c\u516d\u5c4a\u7f8e\u6d32\u56fd\u5bb6\u4f1a\u8bae\u901a\u8fc7\u7684\u300a\u5e03\u65af\u5854\u66fc\u7279\u6cd5\u5178\u300b\u5bf9\u7ee7\u627f\u51c6\u636e\u6cd5\u91c7\u7528\nA. \u517c\u91c7\u533a\u522b\u5236\u548c\u540c\u4e00\u5236\nB. \u540c\u4e00\u5236\nC. \u6cd5\u9662\u5730\u6cd5\nD. \u533a\u522b\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u4e2a\u79bb\u6563\u6982\u7387\u5206\u5e03\u6709\u5982\u4e0b\u6027\u8d28:$(1) p_k=c(1+1 / k) p_{k-1}\uff0ck=1\uff0c2\uff0c\\cdots ;(2) p_0=0.5$\uff0c\u5219$c=( )$\u3002\nA. 0.35\nB. 0.25\nC. 0.29\nD. 0.42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31740786589267667, "meta-math/MetaMath-Mistral-7B": 0.3607860911977634, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.29972404264597124, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d28\u5730\u6c89\u91cd\u7684\u77ff\u77f3\u3001\u5316\u77f3\u3001\u8d1d\u58f3\u7c7b\u836f\u7269\uff0c\u591a\u653e\u5728\u6597\u67b6\u7684\nA. \u8f83\u4e0b\u5c42\nB. \u4e2d\u4e0a\u5c42\nC. \u4e2d\u4e0b\u5c42\nD. \u9ad8\u5c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3789724008412032, "HuggingFaceH4/zephyr-7b-beta": 0.7019512325996174, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.29660173325630934, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.411811260039643}}, {"question": "\u4e0b\u5217\u73b0\u8c61\u4e2d\uff0c\u4e0d\u5c5e\u4e8e\u6559\u80b2\u7684\u662f ()\nA. \u65b0\u751f\u513f\u542e\u5438\u6bcd\u4e73\nB. \u611f\u53d7\u7f8e\u597d\u7684\u81ea\u7136\u73af\u5883\nC. \u5bb6\u957f\u6279\u8bc4\u5b69\u5b50\nD. \u53c2\u52a0\u753b\u5c55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6f0f\u7535\u4fdd\u62a4\u5668\u7684\u4f7f\u7528\u5c31\u662f\u9632\u6b62\nA. \u7535\u7ebf\u77ed\u8def\u4e8b\u6545\nB. \u89e6\u7535\u4e8b\u6545\nC. \u7535\u8377\u8d85\u8d1f\u8377\nD. \u7535\u538b\u6ce2\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8546055020640395, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5418889370632913, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8363307620850272}}, {"question": "\u4f01\u4e1a\u81ea\u5f8b\u884c\u4e3a\u7684\u9053\u5fb7\u89c4\u6807\u51c6\u4e0e\u6cd5\u5f8b\uff08\u4ed6\u5f8b\uff09\u89c4\u7684\u884c\u4e3a\u6807\u51c6\u76f8\u6bd4\nA. \u524d\u8005\u9ad8\u4e8e\u540e\u8005\nB. \u524d\u8005\u4f4e\u4e8e\u540e\u8005\nC. \u524d\u8005\u548c\u540e\u8005\u76f8\u5f53\nD. \u4e8c\u8005\u6ca1\u6709\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4285480493619188, "meta-math/MetaMath-Mistral-7B": 0.5142932921063458, "itpossible/Chinese-Mistral-7B-v0.1": 0.3499320087587727, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5399674125904516, "meta-llama/Meta-Llama-3-8B": 0.30765668163964977, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e09\u7ea7\u8d1f\u8377\u5bf9\u4f9b\u7535\u7684\u8981\u6c42\u662f\nA. \u65e0\u7279\u6b8a\u8981\u6c42\nB. \u7531\u4e00\u56de6kV\u53ca\u4ee5\u4e0a\u4e13\u7528\u67b6\u7a7a\u7ebf\u4f9b\u7535\nC. \u7531\u4e24\u8def\u9ad8\u538b\u7535\u6e90\u4f9b\u7535\nD. \u7531\u4e24\u8def\u4f4e\u538b\u7535\u6e90\u4f9b\u7535\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c82 \u5c81\u3002\u524d\u5929\u996e\u9152\u540e\u51fa\u73b0\u8179\u90e8\u80c0\u75db\uff0c\u5c0f\u4fbf\u9891\uff0c\u91cf\u5c11\uff0c\u9010\u6e10\u52a0\u91cd\u3002\u5e73\u65f6\u5927\u4fbf\u5e72\u71e5\uff0c2~3\u59291\u6b21\u3002\u67e5\u4f53:\u8179\u90e8\u81a8\u9686\uff0c\u4e0b\u8179\u4e3a\u8457\uff0c\u5168\u8179\u538b\u75db\uff0c\u4e0b\u8179\u66f4\u91cd\uff0c\u808c\u7d27\u5f20\u4e0d\u660e\u663e\uff0c\u80a0\u9e23\u97f3\u6d3b\u8dc3\u3002\u7ecf\u80a5\u7682\u6c34\u704c\u80a0\u540e\uff0c\u6392\u51fa\u8f83\u591a\u7c7b\u5757\uff0c\u8179\u75db\u65e0\u660e\u663e\u7f13\u89e3\u3002\u6b64\u60a3\u8005\u6700\u53ef\u80fd\u7684\u8bca\u65ad\u662f\nA. \u4e60\u60ef\u6027\u4fbf\u79d8\nB. \u6025\u6027\u8180\u80f1\u708e\nC. \u6025\u6027\u5c3f\u6f74\u7559\nD. \u4e59\u72b6\u7ed3\u80a0\u626d\u8f6c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u98df\u4e0d\u538c\u7cbe\uff0c\u810d\u4e0d\u538c\u7ec6\u201d\u662f\u54ea\u4e00\u4f4d\u5723\u4eba\u6240\u8bf4\nA. \u5b54\u5b50\nB. \u8340\u5b50\nC. \u5e84\u5b50\nD. \u8001\u5b50\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.38745561900026004, "itpossible/Chinese-Mistral-7B-v0.1": 0.48201929285216283, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5208125011242347, "meta-llama/Meta-Llama-3-8B": 0.6237060342711439, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6082587423139688}}, {"question": "\u201c\u4eba\u8fd8\u8981\u6709\u70b9\u513f\u4e1c\u897f\uff0c\u624d\u53eb\u6d3b\u7740\u3002\u201d\u8fd9\u53e5\u8bdd\u51fa\u81ea\u5c0f\u8bf4\nA. \u300a\u53d7\u6212\u300b\nB. \u300a\u9676\u6e0a\u660e\u5199\u633d\u6b4c\u300b\nC. \u300a\u68cb\u738b\u300b\nD. \u300a\u7ec4\u7ec7\u90e8\u6765\u4e86\u4e2a\u5e74\u8f7b\u4eba\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7466987770090817}}, {"question": "\u8f93\u5c3f\u7ba1\nA. \u8d77\u59cb\u4e8e\u80be\u76c2\nB. \u53ef\u5206\u4e3a\u8179\u76c6\u4e24\u6bb5\nC. \u5f00\u53e3\u4e8e\u8180\u80f1\u9888\nD. \u4e3a\u8179\u819c\u5185\u4f4d\u5668\u5b98\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8ba1\u7b97\u673a\u6267\u884c a=8, b=5, a=a+b, b=a-b, print a,b \u7a0b\u5e8f\u6bb5\u540e\uff0c\u8f93\u51fa\u7684\u7ed3\u679c\u4e3a\nA. 8,5\nB. 13, 8\nC. 3, 13\nD. 13, 3 \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.30601362565976303, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4527911443093546, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3402618538483215}}, {"question": "\u5728\u8def\u7531\u5668\u4e0a\u914d\u7f6e\u9ed8\u8ba4\u2f79\u5173\u6b63\u786e\u7684\u5730\u5740\u4e3a\nA. 0.0.0.0 255.255.255.255\nB. 255.255.255.255 0.0.0.0\nC. 0.0.0.0 255.255.255.0\nD. 0.0.0.0 0.0.0.0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u516c\u5e73\u7406\u8bba\u6ce8\u91cd\u7684\u662f\nA. \u8bc4\u4f30\u516c\u5e73\nB. \u5206\u914d\u516c\u5e73\nC. \u8fc7\u7a0b\u516c\u5e73\nD. \u7a0b\u5e8f\u516c\u5e73\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3318363651291732, "meta-math/MetaMath-Mistral-7B": 0.8245083977577683, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9910744805019887, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6484778499969632, "meta-llama/Meta-Llama-3-8B": 0.5878419260854046, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5929386395157618}}, {"question": "\u8fd1\u4e9b\u5e74\u63a2\u660e\uff0c\u6d77\u5e95\u201c\u53ef\u71c3\u51b0\u201d\uff08\u5929\u7136\u2f53\u2f54\u5408\u7269\uff09\u50a8\u91cf\u6781\u4e3a\u4e30\u5bcc\uff0c\u5176\u5f00\u53d1\u6280\u672f\u4ea6\u2f47\u8d8b\u6210\u719f\u3002\u5f00\u53d1\u5229\u2f64\u201c\u53ef\u71c3\u51b0\u201d\u5c06\u4ea7\u2f63\u7684\u73af\u5883\u6548\u76ca\u6709\nA. \u66ff\u4ee3\u7164\u548c\u2f6f\u6cb9\uff0c\u51cf\u8f7b\u5bf9\u2f24\u2f53\u7684\u6c61\u67d3\nB. \u53ef\u53d6\u4ee3\u2f00\u4e9b\u6838\u7535\u7ad9\uff0c\u51cf\u5c11\u6838\u5e9f\u6599\u7684\u6c61\u67d3\nC. \u53ef\u53d6\u4ee3\u2f54\u7535\u7ad9\uff0c\u6539\u5584\u2f24\u2f53\u8d28\u91cf\nD. \u2f46CO2\u6392\u653e\uff0c\u51cf\u8f7b\u201c\u6e29\u5ba4\u6548\u5e94\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f71\u7247\u300a\u6218\u8230\u6ce2\u5c06\u91d1\u53f7\u300b\u662f20\u4e16\u7eaa20\u5e74\u4ee3\u54ea\u4e00\u4e2a\u56fd\u5bb6\u7684\u91cd\u8981\u4f5c\u54c1\nA. \u6cd5\u56fd\nB. \u7f8e\u56fd\nC. \u610f\u5927\u5229\nD. \u82cf\u8054\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4907780289409721, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.632363492188097, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2009\u5e74\u6df1\u5316\u7ecf\u6d4e\u4f53\u5236\u6539\u9769\u5de5\u4f5c\u63d0\u51fa\uff0c\u201c\u8fdb\u4e00\u6b65\u653e\u5bbd\u670d\u52a1\u4e1a\u5e02\u573a\u51c6\u5165\uff0c\u9f13\u52b1\u975e\u516c\u6709\u5236\u4f01\u4e1a\u53c2\u4e0e\u56fd\u6709\u670d\u52a1\u4f01\u4e1a\u6539\u9769\u201d\u3002\u8fd9\u662f\u56e0\u4e3a\nA. \u975e\u516c\u6709\u5236\u7ecf\u6d4e\u7684\u7f3a\u9677\u5df2\u7ecf\u88ab\u5f25\u8865\nB. \u6211\u56fd\u79ef\u6781\u4fc3\u8fdb\u975e\u516c\u6709\u5236\u7ecf\u6d4e\u7684\u53d1\u5c55\nC. \u975e\u516c\u6709\u5236\u7ecf\u6d4e\u662f\u793e\u4f1a\u4e3b\u4e49\u7ecf\u6d4e\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\nD. \u975e\u516c\u6709\u5236\u7ecf\u6d4e\u662f\u6211\u56fd\u7ecf\u6d4e\u7684\u4e3b\u4f53\u90e8\u5206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.7908838249940497, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9122453135644654}}, {"question": "\u73b0\u5728\uff0c\u6c11\u8425\u4f01\u4e1a\u5360\u4e2d\u56fd\u6cd5\u4eba\u4f01\u4e1a\u768460%\u4ee5\u4e0a\uff0c\u521b\u9020\u4e86\u4e2d\u56fdGDP\u7684\u7ea640%\uff0c\u5e76\u4e14\u5448\u73b0\u51fa\u91cd\u578b\u5316\u3001\u7531\u52b3\u52a8\u5bc6\u96c6\u578b\u4ea7\u4e1a\u5411\u8d44\u672c\u5bc6\u96c6\u578b\u4ea7\u4e1a\u52a0\u901f\u5347\u7ea7\u7684\u8d8b\u52bf\u3002\u8fd9\u8868\u660e\nA. \u6211\u56fd\u73b0\u9636\u6bb5\u5e76\u4e0d\u9700\u8981\u4fdd\u6301\u516c\u6709\u5236\u7684\u4f18\u52bf\u5730\u4f4d\nB. \u6211\u56fd\u7684\u6df7\u5408\u6240\u6709\u5236\u4f01\u4e1a\u7684\u6bd4\u4f8b\u65e5\u76ca\u63d0\u9ad8\nC. \u4e0e\u516c\u6709\u5236\u76f8\u6bd4\u8f83\uff0c\u6c11\u8425\u7ecf\u6d4e\u66f4\u52a0\u9002\u5e94\u6211\u56fd\u7684\u56fd\u60c5\nD. \u6c11\u8425\u4f01\u4e1a\u7684\u7d20\u8d28\u548c\u7ade\u4e89\u529b\u4e0d\u65ad\u63d0\u5347\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4515194856341806, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5700425167059974, "HuggingFaceH4/zephyr-7b-beta": 0.6219680457365437, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7282693659304215, "meta-llama/Meta-Llama-3-8B": 0.9316183246294382, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7222037561965203}}, {"question": "\u5f20\u4e09\u5c06\u4e00\u6279\u8d27\u7269\u4ea4\u7ed9\u67d0\u94c1\u8def\u516c\u53f8\uff0c\u8ba9\u5176\u8fd0\u5230\u5317\u4eac\u897f\u7ad9\uff0c\u5f20\u4e09\u4e0e\u67d0\u94c1\u8def\u516c\u53f8\u4e4b\u95f4\u5f62\u6210\u7684\u6cd5\u5f8b\u5173\u7cfb\u7684\u5ba2\u4f53\u662f\nA. \u706b\u8f66\nB. \u94c1\u8def\nC. \u8d27\u7269\nD. \u8fd0\u8f93\u884c\u4e3a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44590024984016846, "meta-math/MetaMath-Mistral-7B": 0.9029685120028991, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8169813459419005, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6084147301602645, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5fae\u578b\u8ba1\u7b97\u673a\u952e\u76d8\u4e0a\u7684Shift\u952e\u79f0\u4e3a\nA. \u6362\u6863\u952e\nB. \u56de\u8f66\u6362\u884c\u952e\nC. \u7a7a\u683c\u952e\nD. \u9000\u683c\u952e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6421560287919718, "meta-math/MetaMath-Mistral-7B": 0.8660523902245084, "itpossible/Chinese-Mistral-7B-v0.1": 0.704055476618215, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8335609180173797, "meta-llama/Meta-Llama-3-8B": 0.4305119068018152, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9983643623241604}}, {"question": "\u5f53\u81ea\u7531\u5ea6\u4e0d\u53d8\u65f6\uff0c\u5173\u4e8e$x^2$\u503c\u4e0e$P$\u503c\u7684\u5173\u7cfb\uff0c\u4e0b\u5217\u54ea\u4e00\u9879\u662f\u6b63\u786e\u7684\nA. $x^2$\u503c\u53d8\u5316\u65f6\uff0cP\u503c\u53d8\u5927\u6216\u53d8\u5c0f\u3002\nB. $x^2$\u503c\u8d8a\u5927\uff0cP\u503c\u8d8a\u5c0f\u3002\nC. $x^2$\u503c\u53d8\u5316\u65f6\uff0cP\u503c\u4e0d\u53d8\u3002\nD. $x^2$\u503c\u8d8a\u5927\uff0cP\u503c\u8d8a\u5927\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7416373611801309, "meta-math/MetaMath-Mistral-7B": 0.8202197808244356, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9971168779995195, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9424754402998593, "meta-llama/Meta-Llama-3-8B": 0.5878419260854046, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9617912744751009}}, {"question": "\u5c0f\u738b\u6a21\u4eff\u53e4\u4eba\u586b\u4e86\u4e00\u9996\u8bcd\uff0c\u5199\u5b8c\u540e\u53d1\u73b0\u6b63\u6587\u521a\u597d100\u5b57\uff0c\u8bf7\u95ee\u4ed6\u7528\u7684\u662f\u54ea\u79cd\u8bcd\u724c\nA. \u5ff5\u5974\u5a07\nB. \u6ee1\u5ead\u82b3\nC. \u96e8\u9716\u94c3\nD. \u6c34\u9f99\u541f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2712019384407753, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u80c3\u80a0\u5e73\u6ed1\u808c\u6536\u7f29\u8282\u5f8b\u8d77\u51b3\u5b9a\u6027\u4f5c\u7528\u7684\u662f\nA. \u5e73\u6ed1\u808c\u672c\u8eab\u7684\u8282\u5f8b\nB. \u6162\u6ce2\u7684\u9891\u7387\nC. \u52a8\u4f5c\u7535\u4f4d\u7684\u9891\u7387\nD. \u52a8\u4f5c\u7535\u4f4d\u7684\u6570\u503c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4509200727518753, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5327801800314095}}, {"question": "\u67d0\u4eba\u4ee5\u4e00\u5b9a\u901f\u7387\u5782\u76f4\u6cb3\u5cb8\uff08\u9759\u6c34\u4e2d\u7684\u901f\u7387\u5782\u76f4\u6cb3\u5cb8\uff09\u5411\u5bf9\u5cb8\u6e38\u53bb\uff0c\u5f53\u6c34\u6d41\u7684\u8fd0\u52a8\u662f\u5300\u901f\u65f6\uff0c\u4ed6\u6240\u6e38\u8fc7\u7684\u8def\u7a0b\u3001\u8fc7\u6cb3\u6240\u7528\u7684\u65f6\u95f4\u4e0e\u6c34\u901f\u7684\u5173\u7cfb\u662f\nA. \u6c34\u901f\u5927\u65f6\uff0c\u8def\u7a0b\u957f\uff0c\u65f6\u95f4\u957f\nB. \u8def\u7a0b\u3001\u65f6\u95f4\u4e0e\u6c34\u901f\u65e0\u5173\nC. \u6c34\u901f\u5927\u65f6\uff0c\u8def\u7a0b\u957f\uff0c\u65f6\u95f4\u4e0d\u53d8\nD. \u6c34\u901f\u5927\u65f6\uff0c\u8def\u7a0b\u957f\uff0c\u65f6\u95f4\u77ed\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.39995069855480087, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5103194218428023, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3661158989262944, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6a21\u4eff\u8bf4\u7684\u5408\u7406\u6027\u5728\u4e8e\nA. \u59cb\u7ec8\u6ce8\u91cd\u827a\u672f\u7684\u60c5\u611f\u6027\u56e0\u7d20\nB. \u59cb\u7ec8\u6293\u4f4f\u827a\u672f\u4e0e\u73b0\u5b9e\u4e16\u754c\u7684\u5173\u7cfb\nC. \u59cb\u7ec8\u80af\u5b9a\u4eba\u7684\u4e3b\u4f53\u56e0\u7d20\nD. \u59cb\u7ec8\u5f3a\u8c03\u827a\u672f\u7684\u5f62\u5f0f\u56e0\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3297050302960631, "meta-math/MetaMath-Mistral-7B": 0.5727180452425591, "itpossible/Chinese-Mistral-7B-v0.1": 0.30702389681474196, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3362011175603434, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5481730093869999}}, {"question": "\u4e0b\u5217\u54ea\u4e2a\u5929\u4f53\u4eca\u5929\u4e0e\u592a\u9633\u7684\u89d2\u8ddd\u79bb\u6700\u5c0f\nA. \u5317\u843d\u5e08\u95e8\nB. \u6597\u5bbf\u56db\nC. \u5a04\u5bbf\u4e09\nD. \u5fc3\u5bbf\u4e8c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8ePH\u9176\u4fc3\u53cd\u5e94\u901f\u5ea6\u5f71\u54cd\u7684\u8bba\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. PH\u8fc7\u9ad8\u6216\u8fc7\u4f4e\u53ef\u4f7f\u9176\u53d1\u751f\u53d8\u6027\nB. \u6700\u9002PH\u662f\u9176\u7684\u7279\u6027\u5e38\u6570\nC. \u6700\u9002PH\u4e0d\u662f\u9176\u7684\u7279\u6027\u5e38\u6570\nD. PH\u5f71\u54cd\u9176\u3001\u5e95\u7269\u6216\u8f85\u52a9\u56e0\u5b50\u7684\u89e3\u79bb\u5ea6\uff0c\u4ece\u800c\u5f71\u54cd\u9176\u4fc3\u53cd\u5e94\u901f\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u6b64\u4ea6\u4e00\u662f\u975e\uff0c\u5f7c\u4ea6\u4e00\u662f\u975e\u201d\u7684\u547d\u9898\uff0c\u5176\u542b\u4e49\u662f\nA. \u5f3a\u8c03\u771f\u7406\u7684\u5ba2\u89c2\u6027\nB. \u5426\u8ba4\u771f\u7406\u5177\u6709\u5ba2\u89c2\u6807\u51c6\nC. \u5f3a\u8c03\u771f\u7406\u5177\u6709\u5ba2\u89c2\u6807\u51c6\nD. \u5426\u8ba4\u771f\u7406\u7684\u5ba2\u89c2\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6709\u5173\u5b9e\u9a8c\u8bd5\u5242\u6216\u5b9e\u9a8c\u65b9\u6cd5\u7684\u53d9\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u4f7f\u7528\u9002\u5b9c\u6d53\u5ea6\u7684\u785d\u9178\u94be\u6eb6\u6db2\u53ef\u4ee5\u8fde\u7eed\u89c2\u5bdf\u5230\u6d0b\u8471\u8868\u76ae\u7ec6\u80de\u7684\u8d28\u58c1\u5206\u79bb\u590d\u539f\u73b0\u8c61\nB. \u5361\u8bfa\u6c0f\u6db2\u56fa\u5b9a\u7ec6\u80de\u5f62\u6001\u540e\u9700\u7528\u6e05\u6c34\u51b2\u6d172\u6b21\u518d\u5236\u7247\nC. \u690d\u7269\u7684\u751f\u957f\u7d20\u548c\u4eba\u7684\u80f0\u5c9b\u7d20\u5747\u80fd\u4e0e\u53cc\u7f29\u8132\u8bd5\u5242\u53d1\u751f\u4f5c\u7528\u4ea7\u751f\u7d2b\u8272\u53cd\u5e94\nD. \u7814\u7a76\u571f\u58e4\u4e2d\u5c0f\u52a8\u7269\u7c7b\u7fa4\u7684\u4e30\u5bcc\u5ea6\u65f6\uff0c\u91c7\u7528\u6807\u5fd7\u91cd\u6355\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.378744010542104, "meta-math/MetaMath-Mistral-7B": 0.7772385851589834, "itpossible/Chinese-Mistral-7B-v0.1": 0.5055494961770451, "HuggingFaceH4/zephyr-7b-beta": 0.7965635825594826, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.594032755698128, "meta-llama/Meta-Llama-3-8B": 0.3300364758848943, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4487218497629704}}, {"question": "\u4e0b\u5217\u56db\u79cd\u9009\u9879\u4e2d\uff0c\u54ea\u4e00\u9879\u4e0d\u5c5e\u4e8e\u7ade\u6280\u4f53\u80b2\u793e\u4f1a\u529f\u80fd\u7684\u9009\u9879\u3002\nA. \u4fc3\u8fdb\u7ecf\u6d4e\u7684\u53d1\u5c55\u548c\u7e41\u8363\nB. \u632f\u594b\u6c11\u65cf\u7cbe\u795e\nC. \u6700\u5927\u9650\u5ea6\u5730\u6316\u6398\u4eba\u7684\u751f\u7406\u6f5c\u529b\nD. \u4e30\u5bcc\u4eba\u4eec\u7684\u6587\u5316\u548c\u7cbe\u795e\u751f\u6d3b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.29863342676099575, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7631054981871113, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e9a\u91cc\u58eb\u591a\u5fb7\u7684\u903b\u8f91\u4e2d\uff0c\u88ab\u8ba4\u4e3a\u662f\u80af\u5b9a\u6216\u5426\u5b9a\u4e3b\u9879\u5177\u6709\u8c13\u9879\u6027\u8d28\u7684\u8bed\u53e5\u7684\u662f\nA. \u63a8\u8bba\nB. \u8bba\u8bc1\nC. \u547d\u9898\nD. \u4fee\u8f9e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.553084570188329, "meta-math/MetaMath-Mistral-7B": 0.7758033780440873, "itpossible/Chinese-Mistral-7B-v0.1": 0.3689108554330874, "HuggingFaceH4/zephyr-7b-beta": 0.9887462956033687, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.865687768853871, "meta-llama/Meta-Llama-3-8B": 0.7567935875193733, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.694072587172701}}, {"question": "\u5728\u5c0f\u8bf4\u300a\u54e6\uff0c\u9999\u96ea\u300b\u4e2d\uff0c\u8d28\u6734\u7f8e\u5316\u8eab\u7684\u662f\nA. \u51e4\u5a07\nB. \u77ff\u51b6\u5b66\u9662\u7684\u5b66\u751f\nC. \u201c\u5317\u4eac\u8bdd\u201d\nD. \u9999\u96ea\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31049698480066085, "meta-math/MetaMath-Mistral-7B": 0.47305862883333805, "itpossible/Chinese-Mistral-7B-v0.1": 0.6184469595494585, "HuggingFaceH4/zephyr-7b-beta": 0.7557949840531825, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u201c\u6240\u6709A\u90fd\u4e0d\u662fB\uff0c\u6240\u6709C\u662fB\u3002\u201d\u4e3a\u524d\u63d0\u8fdb\u884c\u4e09\u6bb5\u8bba\u63a8\u7406\uff0c\u5176\u7ed3\u8bba\u4e3a\nA. \u6240\u6709A\u4e0d\u662fC\nB. \u6709\u4e9bA\u662fC\nC. \u6709\u7684C\u662fA\nD. \u6709\u7684B\u662fA\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.557670998484387, "meta-math/MetaMath-Mistral-7B": 0.6422323806610163, "itpossible/Chinese-Mistral-7B-v0.1": 0.3265980288635183, "HuggingFaceH4/zephyr-7b-beta": 0.999910347054547, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6013265700796366, "meta-llama/Meta-Llama-3-8B": 0.4392175912406816, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.41896497847570785}}, {"question": "\u5c0f\u674e\u53bb\u5916\u5730\u51fa\u5dee\uff0c\u51fa\u884c\u5de5\u5177\u53ef\u4ee5\u5728\u98de\u673a\u3001\u706b\u8f66\u3001\u957f\u9014\u6c7d\u8f66\u4e4b\u95f4\u8fdb\u884c\u9009\u62e9\uff0c\u4e09\u79cd\u8fd0\u8f93\u65b9\u5f0f\u7ecf\u8425\u4f01\u4e1a\u4e4b\u95f4\u7684\u7ade\u4e89\u5173\u7cfb\u662f\nA. \u4e00\u822c\u7ade\u4e89\u8005\nB. \u613f\u671b\u7ade\u4e89\u8005\nC. \u54c1\u724c\u7ade\u4e89\u8005\nD. \u4ea7\u54c1\u5f62\u5f0f\u7ade\u4e89\u8005\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3306562312783846, "itpossible/Chinese-Mistral-7B-v0.1": 0.2731272040287072, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.325455072595945, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9410987838914409}}, {"question": "\u5728$\\bigtriangleup ABC$ \u4e2d\uff0c$a=2\\sqrt{2},B=\\frac{\\pi}{4},\\angle A=\\frac{\\pi}{3}$\uff0c\u5219b=\nA. $\\frac{5\\sqrt{3}}{4}$\nB. $\\frac{4\\sqrt{3}}{3}$\nC. $\\frac{5\\sqrt{3}}{3}$\nD. $\\frac{3\\sqrt{3}}{2}$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.28677228251758863, "HuggingFaceH4/zephyr-7b-beta": 0.30702389681474196, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c55\u5c81\u3002\u4e0a\u8179\u90e8\u88ab\u6c7d\u8f66\u649e\u4f242\u5c0f\u65f6\uff0c\u5267\u70c8\u8179\u75db\uff0c\u4f34\u6076\u5fc3\u5455\u5410\uff0c\u795e\u5fd7\u6de1\u6f20\u3002\u67e5\u4f53\uff1aP135\u6b21/\u5206\uff0cBP75/45mmHg\uff0c\u5168\u8179\u6709\u538b\u75db\u3001\u53cd\u8df3\u75db\u53ca\u808c\u7d27\u5f20\uff0c\u79fb\u52a8\u6027\u6d4a\u97f3\u53ef\u7591\u9633\u6027\uff0c\u80a0\u9e23\u97f3\u51cf\u5f31\u3002\u6700\u53ef\u80fd\u7684\u8bca\u65ad\u662f\nA. \u80c3\u7a7f\u5b54\nB. \u5c0f\u80a0\u7834\u88c2\nC. \u8179\u819c\u540e\u8840\u80bf\nD. \u809d\u813e\u7834\u88c2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u793e\u4f1a\u4e3b\u4e49\u65f6\u671f\uff0c\u6c11\u65cf\u95ee\u9898\u957f\u671f\u5b58\u5728\u7684\u4e3b\u8981\u539f\u56e0\u662f\nA. \u5404\u6c11\u65cf\u7684\u6587\u5316\u5dee\u5f02\u4ecd\u7136\u5b58\u5728\nB. \u6c11\u65cf\u95f4\u7684\u5dee\u5f02\u548c\u53d1\u5c55\u5dee\u8ddd\u4e0d\u53ef\u80fd\u5728\u77ed\u65f6\u671f\u5185\u6d88\u706d\nC. \u5404\u6c11\u65cf\u95f4\u5728\u7ecf\u6d4e\u751f\u6d3b\u4e2d\u8fd8\u672a\u5b9e\u73b0\u5171\u540c\u5bcc\u88d5\nD. \u793e\u4f1a\u4e3b\u4e49\u65f6\u671f\u9636\u7ea7\u6597\u4e89\u5728\u4e00\u5b9a\u65f6\u671f\u4e00\u5b9a\u8303\u56f4\u5185\u4ecd\u7136\u5b58\u5728\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4288648741936082, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4e8e\u590d\u7b49\u4f4d\u57fa\u56e0\u7684\u4efb\u4f55\u57fa\u56e0\uff0c\u53cc\u500d\u4f53\u751f\u7269\u5728\u4e00\u4e2a\u57fa\u56e0\u5ea7\u4f4d\u4e0a\u6700\u591a\u6709\u591a\u5c11\u4e2a\u7b49\u4f4d\u57fa\u56e0\nA. 10\nB. 6\nC. 2\nD. 1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3728641892601242, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.40059490765870587}}, {"question": "\u5931\u6c34\u65f6\u8868\u73b0\u4e3a\u795e\u5fd7\u604d\u60da\u3001\u5c3f\u5c11\u3001\u5c3f\u6bd4\u91cd\u964d\u4f4e\uff0c\u5c5e\u4e8e\nA. \u4f4e\u6e17\u6027\u8131\u6c34\nB. \u7b49\u6e17\u6027\u8131\u6c34\nC. \u9ad8\u6e17\u6027\u8131\u6c34\nD. \u8131\u6c34\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8579709922285524}}, {"question": "\u2f64PowerPoint\u5236\u4f5c\u7684\u6f14\u793a\u2f42\u7a3f\u9ed8\u8ba4\u7684\u6269\u5c55\u540d\u662f\nA. .pwp\nB. .ppt\nC. .pop\nD. .ppn\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8810232194886057, "meta-math/MetaMath-Mistral-7B": 0.9963215059547761, "itpossible/Chinese-Mistral-7B-v0.1": 0.926580027443938, "HuggingFaceH4/zephyr-7b-beta": 0.9987799655797507, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9040042391600485, "meta-llama/Meta-Llama-3-8B": 0.9894827310028563, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.999974128360695}}, {"question": "\u53e4\u5e0c\u814a\u54f2\u5b66\u5bb6\u82cf\u683c\u62c9\u5e95\u521b\u7acb\u4e86\u201c\u4ea7\u5a46\u672f\u201d\u3002\u5b83\u8868\u8fbe\u7684\u4e3b\u8981\u6559\u5b66\u65b9\u6cd5\u662f\nA. \u8c08\u8bdd\u6cd5\nB. \u8bb2\u6388\u6cd5\nC. \u8ba8\u8bba\u6cd5\nD. \u6f14\u793a\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6908286501997145, "meta-math/MetaMath-Mistral-7B": 0.976456616868324, "itpossible/Chinese-Mistral-7B-v0.1": 0.7876687434550607, "HuggingFaceH4/zephyr-7b-beta": 0.9569834138505224, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.611640794009883, "meta-llama/Meta-Llama-3-8B": 0.7100999178934575, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2\u4f4d\u7537\u2f63\u548c3\u4f4d\u2f25\u2f63\u51715\u4f4d\u540c\u5b66\u7ad9\u6210\u2f00\u6392\uff0c\u82e5\u7537\u2f63\u7532\u4e0d\u7ad9\u5728\u4e24\u7aef\uff0c3\u4f4d\u2f25\u2f63\u4e2d\u6709\u4e14\u53ea\u6709\u4e24\u4f4d\u2f25\u2f63\u76f8\u90bb\uff0c\u5219\u4e0d\u540c\u7684\u6392\u6cd5\u603b\u6570\u5171\u6709\nA. 42\nB. 36\nC. 60\nD. 48\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5404\u79cd\u751f\u4ea7\u529b\u8981\u7d20\u4e2d\u6700\u5177\u6709\u6d3b\u529b\u548c\u5f39\u6027\u7684\u90e8\u5206\u7684\u662f\nA. \u4eba\u529b\u8d44\u6e90\nB. \u77e5\u8bc6\u8d44\u6e90\nC. \u8d22\u52a1\u8d44\u6e90\nD. \u5b9e\u7269\u8d44\u6e90\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5695217387616521, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9984249979498544, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6567074016218906, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u7535\u963b\u7535\u8def\u4e2d\uff0c\u82e5\u661f\u578b\u8fde\u63a5\u4e2d3\u4e2a\u7535\u963b\u76f8\u540c\uff0c\u5747\u4e3a6\u6b27\u59c6\uff0c\u5219\u7b49\u6548\u4e09\u89d2\u578b\u8fde\u63a5\u4e2d3\u4e2a\u7535\u963b\u4e5f\u76f8\u540c\uff0c\u5b83\u4eec\u7b49\u4e8e\nA. 2\nB. 6\nC. 3\nD. 18\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u793e\u4f1a\u95ee\u9898\u7814\u7a76\u7684\u7406\u8bba\u4e2d\uff0c\u8ba4\u4e3a\u4e0d\u5b58\u5728\u5ba2\u89c2\u610f\u4e49\u4e0a\u7684\u793e\u4f1a\u95ee\u9898\u7684\u7406\u8bba\u662f\nA. \u793e\u4f1a\u5efa\u6784\u7406\u8bba\nB. \u793e\u4f1a\u89e3\u7ec4\u7406\u8bba\nC. \u884c\u4e3a\u504f\u5dee\u7406\u8bba\nD. \u6587\u5316\u51b2\u7a81\u7406\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8920982386008558}}, {"question": "1953\u5e746\u670824\u65e5\u300a\u4eba\u6c11\u65e5\u62a5\u300b\u62a5\u9053\uff1a\u5f53\u9009\u7684\u4ee3\u8868\u674e\u957f\u51e4\u8001\u5927\u5a18\u7b11\u7740\u8bf4\u8bdd\u4e86\uff0c\u201c\u65e7\u793e\u4f1a\u5987\u5973\u5728\u8857\u4e0a\u8bf4\u8bdd\u90fd\u4e0d\u884c\uff0c\u54ea\u6709\u5987\u5973\u7684\u9009\u4e3e\u6743\u3002\u2026\u2026\u4eca\u540e\u5927\u5bb6\u6709\u4ec0\u4e48\u610f\u89c1\u544a\u8bc9\u6211\uff0c\u6211\u4e00\u5b9a\u7ed9\u5927\u5bb6\u5e26\u4e0a\u53bb\u201d\u3002\u8be5\u65b0\u95fb\u62a5\u9053\u8bf4\u660e\u5f53\u65f6\nA. \u4eba\u6c11\u6c11\u4e3b\u539f\u5219\u5f97\u5230\u843d\u5b9e\nB. \u6c11\u4e3b\u653f\u6cbb\u5efa\u8bbe\u6cd5\u5236\u5316\nC. \u653f\u6cbb\u534f\u5546\u5236\u5ea6\u8986\u76d6\u9762\u5e7f\nD. \u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u5236\u5efa\u7acb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6723627636357823, "meta-math/MetaMath-Mistral-7B": 0.9495943509717067, "itpossible/Chinese-Mistral-7B-v0.1": 0.38253298924946455, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7970365908813789, "meta-llama/Meta-Llama-3-8B": 0.5414438265588815, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5efa\u8bbe\u5de5\u7a0b\u5b9e\u65bd\u65bd\u5de5\u603b\u627f\u5305\u7684\uff0c\u5bf9\u65bd\u5de5\u73b0\u573a\u5b89\u5168\u751f\u4ea7\u8d1f\u603b\u8d23\u7684\u5355\u4f4d\u662f\nA. \u603b\u627f\u5305\u5355\u4f4d\nB. \u54a8\u8be2\u5355\u4f4d\nC. \u76d1\u7406\u5355\u4f4d\nD. \u5efa\u8bbe\u5355\u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5264809888990991, "meta-math/MetaMath-Mistral-7B": 0.9276963758935172, "itpossible/Chinese-Mistral-7B-v0.1": 0.4659805019161052, "HuggingFaceH4/zephyr-7b-beta": 0.9878262493205257, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6612239008836178, "meta-llama/Meta-Llama-3-8B": 0.9493662973280704, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9989449658333123}}, {"question": "600\u662f\u6570\u52171*2,2*3,3*4\uff0c...\u7684\u7b2c\nA. 25\u9879\nB. 24\u9879 \nC. 30\u9879\nD. 20\u9879 \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.25780995786150784, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2848664895071882, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.28966338381871215, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5015219291087172}}, {"question": "\u5c71\u533a\u9053\u8def\u5bf9\u5b89\u5168\u884c\u8f66\u7684\u4e3b\u8981\u5f71\u54cd\u662f\u4ec0\u4e48\nA. \u4ea4\u901a\u60c5\u51b5\u5355\u4e00\nB. \u5761\u957f\u5f2f\u6025\uff0c\u89c6\u8ddd\u4e0d\u8db3\nC. \u9053\u8def\u6807\u5fd7\u5c11\nD. \u8f66\u6d41\u5bc6\u5ea6\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7778874554392566, "meta-math/MetaMath-Mistral-7B": 0.8302282167600246, "itpossible/Chinese-Mistral-7B-v0.1": 0.9802041077615982, "HuggingFaceH4/zephyr-7b-beta": 0.9990475642936597, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9893223094678337, "meta-llama/Meta-Llama-3-8B": 0.9779866264141355, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9961207065132532}}, {"question": "\u4f01\u4e1a\u5b58\u5165\u8bc1\u5238\u516c\u53f8\u5c1a\u672a\u6295\u8d44\u7684\u8d44\u91d1\uff0c\u5e94\u501f\u8bb0\u7684\u79d1\u76ee\u662f\nA. \u201c\u5176\u4ed6\u8d27\u5e01\u8d44\u91d1\u201d\nB. \u201c\u94f6\u884c\u5b58\u6b3e\u201d\nC. \u201c\u5176\u4ed6\u5e94\u6536\u6b3e\u201d\nD. \u201c\u4ea4\u6613\u6027\u91d1\u878d\u8d44\u4ea7\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3190122270038758, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3575234844587316, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9099624540872623}}, {"question": "\u884c\u4e3a\u4eba\u4f2a\u9020\u8d27\u5e01\u5e76\u51fa\u552e\u6216\u8005\u8fd0\u8f93\u7684\uff0c\uff08\uff09\u3002\nA. \u6bd4\u8f83\u4f2a\u9020\u8d27\u5e01\u7f6a\u548c\u51fa\u552e\u3001\u8fd0\u8f93\u5047\u5e01\u7f6a\u7684\u5211\u7f5a\u8f7b\u91cd\uff0c\u62e9\u4e00\u91cd\u7f6a\u4ece\u91cd\u5904\u7f5a\nB. \u4ee5\u4f2a\u9020\u8d27\u5e01\u7f6a\u4e00\u7f6a\u5b9a\u7f6a\u4ece\u91cd\u5904\u7f5a\nC. \u4ee5\u51fa\u552e\u3001\u8fd0\u8f93\u5047\u5e01\u7f6a\u4e00\u7f6a\u5b9a\u7f6a\u4ece\u91cd\u5904\u7f5a\nD. \u4ee5\u4f2a\u9020\u8d27\u5e01\u7f6a\u548c\u51fa\u552e\u3001\u8fd0\u8f93\u5047\u5e01\u7f6a\u5b9e\u884c\u6570\u7f6a\u5e76\u7f5a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5574087329634703, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bba\u8bc1\u7684\u529f\u80fd\u5305\u62ec\u8bc1\u6210\u548c\nA. \u53cd\u9a73\nB. \u8fa9\u8bba\nC. \u63a8\u8bba\nD. \u5f52\u8c2c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3993599168490195, "HuggingFaceH4/zephyr-7b-beta": 0.3723804572973971, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4588123714082308, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1954\u5e74\uff0c\u5f53\u9009\u7b2c\u4e00\u5c4a\u5168\u56fd\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u76841226\u540d\u4ee3\u8868\uff0c\u662f\u4e2d\u56fd\u6709\u53f2\u4ee5\u6765\u7b2c\u4e00\u6b21\u7ecf\u8fc7\u666e\u9009\u4ea7\u751f\u7684\uff0c\u6545\u6b64\u6b21\u5927\u4f1a\u53c8\u88ab\u79f0\u4e3a\u201c\u7b2c\u4e00\u6b21\u771f\u6b63\u7684\u4eba\u6c11\u5927\u4f1a\u201d\u3002\u8fd9\u6b21\u201c\u771f\u6b63\u7684\u4eba\u6c11\u5927\u4f1a\u201d\u7684\u4e3b\u8981\u529f\u7ee9\u662f\nA. \u5236\u5b9a\u4e86\u6211\u56fd\u7b2c\u4e00\u90e8\u793e\u4f1a\u4e3b\u4e49\u7c7b\u578b\u7684\u5baa\u6cd5\nB. \u4eba\u6c11\u6709\u4e86\u884c\u4f7f\u653f\u6cbb\u6743\u529b\u7684\u552f\u4e00\u673a\u5173\nC. \u89c4\u5b9a\u6743\u529b\u5c5e\u4e8e\u4eba\u6c11\uff0c\u4eba\u6c11\u5f00\u59cb\u6210\u4e3a\u56fd\u5bb6\u4e3b\u4eba\nD. \u4eba\u6c11\u4ee3\u8868\u7531\u6d77\u9009\u4ea7\u751f\uff0c\u4f53\u73b0\u4e86\u6c11\u4e3b\u7684\u5e7f\u6cdb\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2731272040287072, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u7ba1\u7406\u8fc7\u7a0b\u4e2d\u5f15\u5bfc\u7ec4\u7ec7\u4e4b\u95f4\u3001\u4eba\u5458\u4e4b\u95f4\u5efa\u7acb\u76f8\u4e92\u534f\u4f5c\u548c\u4e3b\u52a8\u914d\u5408\u7684\u826f\u597d\u5173\u7cfb\uff0c\u6709\u6548\u5229\u7528\u5404\u79cd\u8d44\u6e90\uff0c\u4ee5\u5b9e\u73b0\u5171\u540c\u9884\u671f\u76ee\u6807\u7684\u6d3b\u52a8\u662f\nA. \u63a7\u5236\nB. \u6307\u6325\nC. \u534f\u8c03\nD. \u51b3\u7b56\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9745980432674769, "meta-math/MetaMath-Mistral-7B": 0.9946784403322461, "itpossible/Chinese-Mistral-7B-v0.1": 0.9614835905623325, "HuggingFaceH4/zephyr-7b-beta": 0.9997904687069353, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.802353254172477, "meta-llama/Meta-Llama-3-8B": 0.9803723911115487, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9991067019949388}}, {"question": "\u901a\u8fc7\u6539\u53d8\u547d\u9898\u7684\u8d28\u6216\u91cf\u4ece\u800c\u63a8\u5bfc\u51fa\u65b0\u547d\u9898\u7684\u8bba\u8bc1\u7684\u662f\nA. \u8d28\u91cf\u8bba\u8bc1\nB. \u5bf9\u5f53\u5173\u7cfb\u8bba\u8bc1\nC. \u4e09\u6bb5\u8bba\u8bba\u8bc1\nD. \u76f4\u8a00\u547d\u9898\u8fd0\u7b97\u8bba\u8bc1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.32205625344145955, "HuggingFaceH4/zephyr-7b-beta": 0.9668444525363006, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.44204420384636295, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u501f\u8d37\u8bb0\u8d26\u6cd5\u4e0b\uff0c\u8d26\u6237\u7684\u501f\u65b9\u8868\u793a\nA. \u5229\u6da6\u7684\u589e\u52a0\u548c\u8d39\u7528\u7684\u51cf\u5c11\nB. \u6536\u5165\u7684\u589e\u52a0\u548c\u8d44\u4ea7\u7684\u51cf\u5c11\nC. \u5229\u6da6\u7684\u589e\u52a0\u548c\u8d1f\u503a\u7684\u51cf\u5c11\nD. \u8d39\u7528\u7684\u589e\u52a0\u548c\u6536\u5165\u7684\u51cf\u5c11\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6709\u5173\u8102\u8d28\u7684\u53d9\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u8102\u8d28\u4e2d\u7684\u78f7\u8102\u662f\u7ec6\u80de\u819c\u7684\u7ec4\u6210\u6210\u5206\nB. \u7ef4\u751f\u7d20D\u548c\u6027\u6fc0\u7d20\u4e0d\u5c5e\u4e8e\u56fa\u9187\u7c7b\u7269\u8d28\nC. \u8102\u8d28\u5728\u6838\u7cd6\u4f53\u3001\u5185\u8d28\u7f51\u53ca\u9ad8\u5c14\u57fa\u4f53\u4e0a\u5408\u6210\nD. \u8102\u80aa\u6bd4\u76f8\u540c\u8d28\u91cf\u7684\u591a\u7cd6\u5f7b\u5e95\u6c27\u5316\u4ea7\u80fd\u5c11\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6412198179586335, "HuggingFaceH4/zephyr-7b-beta": 0.7299378829309167, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6307190573232292, "meta-llama/Meta-Llama-3-8B": 0.577751801343928, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8253532542034072}}, {"question": "\u4e0b\u5217\u56db\u7ec4\u5b57\u4e2d\uff0c\u5168\u662f\u4f1a\u610f\u5b57\u7684\u4e00\u7ec4\u662f\nA. \u9e23\u6b65\u6b66\u76ca\nB. \u4ea6\u8857\u708e\u4f10\nC. \u5b89\u5373\u548c\u7267\nD. \u6d89\u5d14\u83ab\u79c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.32136375586403143, "meta-math/MetaMath-Mistral-7B": 0.7262607153812723, "itpossible/Chinese-Mistral-7B-v0.1": 0.32479161563275305, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.46382608841969236, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5731663289931384}}, {"question": "\u5973\u6027\uff0c13 \u5c81\u30021 \u5e74\u524d\u65e0\u610f\u4e2d\u53d1\u73b0\u53cc\u80a9\u80cc\u90e8\u4e0d\u7b49\u9ad8\uff0c\u540e\u9010\u6e10\u660e\u663e\u3002X \u7ebf\u663e\u793a\u80f8\u690e\u4fa7\u51f8\u7578\u5f62\uff0cCobb \u89d2 25 \u5ea6\uff0c\u4e34\u5e8a\u8bca\u65ad\u4e3a\u7279\u53d1\u6027\u810a\u67f1\u4fa7\u51f8\u3002\u5bf9\u8be5\u60a3\u8005\u7684\u6700\u4f73\u6cbb\u7597\u65b9\u6848\u662f\nA. \u7275\u5f15\u6309\u6469\u6cbb\u7597\uff0c\u6bcf\u6708\u968f\u8bca\u4e00\u6b21\nB. \u6539\u53d8\u5750\u59ff\nC. \u4f69\u6234\u652f\u5177\uff0c\u6bcf\u534a\u5e74\u968f\u8bca\u4e00\u6b21\nD. \u7acb\u5373\u624b\u672f\u6cbb\u7597\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.35096048266444385, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6721283590673176}}, {"question": "\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u63d0\u7eaf\u6df7\u6709\u5c11\u91cf\u785d\u9178\u94be\u7684\u6c2f\u5316\u94a0\uff0c\u5e94\u91c7\u7528\u5728\u8f83\u9ad8\u6e29\u5ea6\u4e0b\u5236\u5f97\u6d53\u6eb6\u6db2\u518d\u51b7\u5374\u7ed3\u6676\u3001\u8fc7\u6ee4\u3001\u5e72\u71e5\u7684\u65b9\u6cd5\nB. \u91d1\u5c5e\u94a0\u3001\u7535\u77f3(\u78b3\u5316\u9499CaC2)\u7b49\u7740\u706b\u65f6\uff0c\u7acb\u5373\u7528\u9ad8\u538b\u6c34\u67aa\u55b7\u6c34\u6216\u6ce1\u6cab\u706d\u706b\u5668\u706d\u706b\uff0c\u9632\u6b62\u8513\u5ef6\nC. \u5df2\u77e5Ksp[Al(OH)3]\u226aKsp[Mg(OH)2]\u3002\u5728\u542b\u7b49\u7269\u8d28\u7684\u91cf\u6d53\u5ea6\u7684Al(NO3)3\u3001Mg(NO3)2\u7684\u6df7\u5408\u6eb6\u6db2\u4e2d\u6ef4\u52a0NaOH\u6eb6\u6db2\uff0c\u5219Al^{3\uff0b}\uff0b2Mg2\uff0b\uff0b7OH\uff0d===Al(OH)3\uff0b2Mg(OH)2\nD. \u505a\u84b8\u998f\u5b9e\u9a8c\u65f6\uff0c\u5728\u84b8\u998f\u70e7\u74f6\u4e2d\u5e94\u52a0\u5165\u6cb8\u77f3\uff0c\u4ee5\u9632\u66b4\u6cb8\u3002\u5982\u679c\u5728\u6cb8\u817e\u524d\u53d1\u73b0\u5fd8\u8bb0\u52a0\u6cb8\u77f3\uff0c\u5e94\u7acb\u5373\u505c\u6b62\u52a0\u70ed\uff0c\u51b7\u5374\u540e\u8865\u52a0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3499320087587727, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ece\u672c\u4f01\u4e1a\u8d2d\u4e70\u67d0\u4ea7\u54c1\u7684\u987e\u5ba2\u5360\u8be5\u4ea7\u54c1\u6240\u6709\u987e\u5ba2\u7684\u767e\u5206\u6bd4\u6307\u7684\u662f\nA. \u4ef7\u683c\u9009\u62e9\u6027\nB. \u987e\u5ba2\u6e17\u900f\u7387\nC. \u987e\u5ba2\u9009\u62e9\u6027\nD. \u987e\u5ba2\u5fe0\u8bda\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.38812073416459414, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.46418504027829066, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9726155784838714}}, {"question": "\u516c\u5b89\u673a\u5173\u4ea4\u901a\u7ba1\u7406\u90e8\u95e8\u5bf9\u7d2f\u79ef\u8bb0\u5206\u8fbe\u5230\u89c4\u5b9a\u5206\u503c\u7684\u9a7e\u9a76\u4eba\u600e\u6837\u5904\u7406\nA. \u590415\u65e5\u4ee5\u4e0b\u62d8\u7559\nB. \u4f9d\u6cd5\u8ffd\u7a76\u5211\u4e8b\u8d23\u4efb\nC. \u7ec8\u751f\u7981\u9a7e\nD. \u8fdb\u884c\u6cd5\u5f8b\u6cd5\u89c4\u6559\u80b2\uff0c\u91cd\u65b0\u8003\u8bd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.569939583941847, "meta-math/MetaMath-Mistral-7B": 0.8070846706096847, "itpossible/Chinese-Mistral-7B-v0.1": 0.8956868615960111, "HuggingFaceH4/zephyr-7b-beta": 0.9923038176593308, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8778650322885962, "meta-llama/Meta-Llama-3-8B": 0.5175034446957364, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6280\u672f\u6587\u5316\u7684\u6838\u5fc3\u4e0e\u7075\u9b42\u662f\nA. \u6280\u672f\u4ea7\u54c1\nB. \u6280\u672f\u4ef7\u503c\u89c2\nC. \u6280\u672f\u77e5\u8bc6\nD. \u6280\u672f\u7cbe\u795e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7540656981762253, "meta-math/MetaMath-Mistral-7B": 0.693888835774802, "itpossible/Chinese-Mistral-7B-v0.1": 0.5867781071071037, "HuggingFaceH4/zephyr-7b-beta": 0.9983531237381169, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7569721715522559, "meta-llama/Meta-Llama-3-8B": 0.5110802833895126, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8ba1\u7b97\u673a\u8f6f\u4ef6\u7cfb\u7edf\u7ec4\u6210\u662f\nA. \u7cfb\u7edf\u8f6f\u4ef6\u548c\u5e94\u7528\u8f6f\u4ef6\nB. Windows\u7cfb\u7edf\u548cOffice\u7cfb\u7edf\nC. DOS\u548cWPS\nD. Windows\u7cfb\u7edf\u548c\u5e94\u7528\u8f6f\u4ef6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7011197206525208, "meta-math/MetaMath-Mistral-7B": 0.7098060766245713, "itpossible/Chinese-Mistral-7B-v0.1": 0.9194383896284942, "HuggingFaceH4/zephyr-7b-beta": 0.9462291487286897, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7309927598316847, "meta-llama/Meta-Llama-3-8B": 0.8090183536892488, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5\u683d\u57f9\u5927\u9ea6\u7684\u4f53\u7ec6\u80de\u67097\u5bf9\u540c\u6e90\u67d3\u8272\u4f53\uff0c\u5219\u6709\u4e1d\u5206\u88c2\u4e2d\u671f\u7ec6\u80de\u5185\u6709\u67d3\u8272\u4f53\nA. 14\u6761\nB. 42\u6761\nC. 28\u6761\nD. 7\u6761\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.345490130553087, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7861953432076848}}, {"question": "2010 \u5e74\uff0c\u7532\u7acb\u81ea\u4e66\u9057\u5631\u4e00\u4efd\uff0c\u8868\u793a\u81ea\u5df1\u7684\u623f\u5c4b\u7531\u513f\u5b50\u4e59\u7ee7\u627f\uff0c\u5c4b\u5185\u7684\u7d2b\u6a80\u5bb6\u5177\u7531\u5b59\u5b50\u4e19\u7ee7\u627f\u30022018 \u5e74\u7532\u5c06\u623f\u5c4b\u5356\u7ed9\u4efb\u67d0\uff0c\u5f97\u6b3e 120 \u4e07\u5143\uff0c\u5e76\u529e\u7406\u4e86\u8fc7\u6237\u767b\u8bb0\u624b\u7eed\uff0c\u540e\u7532\u75c5\u6545\u3002\u5bf9\u6b64\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u7532\u7acb\u9057\u5631\u540e\u4e0d\u5f97\u51fa\u5356\u9057\u5631\u5904\u5206\u7684\u8d22\u4ea7\nB. \u4e19\u6709\u6743\u57fa\u4e8e\u9057\u8d60\u53d6\u5f97\u7d2b\u6a80\u5bb6\u5177\nC. \u7532\u6240\u7acb\u81ea\u4e66\u9057\u5631\u7684\u5185\u5bb9\u5168\u90e8\u88ab\u64a4\u56de\nD. \u4e59\u6709\u6743\u57fa\u4e8e\u9057\u5631\u7ee7\u627f\u6743\u53d6\u5f97\u5356\u623f\u6b3e 120 \u4e07\u5143\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6901161456354293}}, {"question": "\u67d0\u4fdd\u9669\u4eba\u627f\u4fdd\u7684\u98ce\u9669\u7ec4\u5408\u5177\u6709\u5982\u4e0b\u7279\u5f81:(1) \u7406\u8d54\u53d1\u751f\u6982\u7387\u4e3a 0.05 ;(2) \u7406\u8d54\u53d1\u751f\u65f6\uff0c\u7406\u8d54\u989dB\u670d\u4ece(0\uff0c400)\u4e0a\u7684\u5747\u5300\u5206\u5e03\u3002\u5df2\u77e5\u8be5\u4fdd\u9669\u4eba\u7684\u5b89\u5168\u9644\u52a0\u7cfb\u6570\u4e3a0.5\uff0c\u5219\u4fdd\u9669\u4eba\u81f3\u5c11\u8981\u627f\u4fdd___\u4efd\u4fdd\u5355\uff0c\u624d\u80fd\u4f7f\u603b\u8d54\u4ed8\u8d85\u8fc7\u603b\u4fdd\u8d39\u7684\u6982\u7387\u4e3a0.05 \u3002\nA. 249\nB. 278\nC. 252\nD. 263\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3393227209307047, "meta-math/MetaMath-Mistral-7B": 0.46454998140218834, "itpossible/Chinese-Mistral-7B-v0.1": 0.3160424181481997, "HuggingFaceH4/zephyr-7b-beta": 0.42078951285102983, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3265980288635183, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6784\u6210\u4eba\u751f\u5e78\u798f\u7684\u57fa\u672c\u8981\u7d20\u4e0d\u5305\u62ec\nA. \u4eba\u7684\u5168\u9762\u53d1\u5c55\nB. \u4eba\u7684\u53cb\u7231\u60c5\u8c0a\nC. \u4eba\u7684\u8eab\u5fc3\u5065\u5eb7\nD. \u4eba\u7684\u521b\u9020\u6d3b\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "WI-FI\u7f51\u7edc\u5b89\u5168\u63a5\u5165\u662f\u4e00\u79cd\u4fdd\u62a4\u65e0\u7ebf\u7f51\u7edc\u5b89\u5168\u7684\u7cfb\u7edf\uff0cWPA\u52a0\u5bc6\u7684\u8ba4\u8bc1\u65b9\u5f0f\u4e0d\u5305\u62ec\nA. WEP\nB. WPA\u548cWPA2\nC. WPA-PSK\nD. WPA2-PSK\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.42089300850283357, "meta-math/MetaMath-Mistral-7B": 0.8107135601424191, "itpossible/Chinese-Mistral-7B-v0.1": 0.5629233884425658, "HuggingFaceH4/zephyr-7b-beta": 0.7354479479950679, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6939483006253783, "meta-llama/Meta-Llama-3-8B": 0.6274250707593, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9146336152655846}}, {"question": "65*4+4*35=(65+35)*4\uff0c\u5e94\u7528\u7684\u662f\nA. \u52a0\u6cd5\u7ed3\u5408\u7387\nB. \u4e58\u6cd5\u5206\u914d\u5f8b\nC. \u52a0\u6cd5\u4ea4\u6362\u5f8b\nD. \u4e58\u6cd5\u4ea4\u6362\u5f8b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43782998460853517, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4785320257918234, "HuggingFaceH4/zephyr-7b-beta": 0.7105482350481406, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.624848662243112, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7556402984307765}}, {"question": "\u5982\u679c\u6709\u7684a\u662fb\uff1b\u6709\u7684a\u4e0d\u662fb\uff0c\u5e76\u4e14\u6709\u7684b\u662fa \uff0c\u6709\u7684b\u4e0d\u662fa\uff1b\u90a3\u4e48\uff0ca\u4e0eb\u8fd9\u4e24\u4e2a\u6982\u5ff5\u5177\u6709()\u5173\u7cfb\u3002\nA. \u4ea4\u53c9\nB. \u771f\u5305\u542b\u4e8e\nC. \u5168\u5f02\nD. \u5168\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.38181856650850615, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4653251025019098, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5528630915008341}}, {"question": "\u5173\u4e8e\u67cf\u62c9\u56fe\u7684\u201c\u7406\u5ff5\u8bf4\u201d\uff0c\u4e0b\u5217\u8bf4\u6cd5\u9519\u8bef\u7684\u662f\uff1f\nA. \u7406\u5ff5\u662f\u4e8b\u7269\u7684\u5171\u76f8\nB. \u7406\u5ff5\u662f\u4e8b\u7269\u5728\u901a\u8fc7\u6a21\u4eff\u522b\u7684\u4e8b\u7269\u800c\u5f97\u5230\u7684\nC. \u7406\u5ff5\u662f\u4e8b\u7269\u5b58\u5728\u7684\u6839\u636e\nD. \u7406\u5ff5\u662f\u4e8b\u7269\u8ffd\u6c42\u7684\u76ee\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7077153357626375, "meta-math/MetaMath-Mistral-7B": 0.8761690276362772, "itpossible/Chinese-Mistral-7B-v0.1": 0.6379189305357377, "HuggingFaceH4/zephyr-7b-beta": 0.9992816946998081, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9184584629912802, "meta-llama/Meta-Llama-3-8B": 0.37354501646787847, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6558958219836356}}, {"question": "\u8054\u5408\u56fd\u63d0\u51fa\u9884\u9632\u6027\u75c5\u7684\u201cABC\u201d\u539f\u5219\u4e0d\u5305\u62ec\nA. \u5b89\u5168\u5957\uff1a\u5b89\u5168\u5957\u5bf9\u4e8e\u9632\u6cbb\u6027\u75c5\u548c\u827e\u6ecb\u75c5\uff0c\u662f\u975e\u5e38\u6709\u6548\u7684\u65b9\u6cd5\nB. \u63a5\u53d7\u6027\u6559\u80b2\uff1a\u79d1\u5b66\u9632\u8303\u6027\u75c5\u3002\nC. \u8282\u5236\u6b32\u671b\uff1a\u9752\u5c11\u5e74\u6700\u597d\u907f\u514d\u8fc7\u65e9\u6027\u884c\u4e3a\nD. \u5fe0\u8bda\uff1a\u56fa\u5b9a\u6027\u4f34\u4fa3\uff0c\u4e0d\u4e0e\u964c\u751f\u4eba\u53d1\u751f\u6027\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u52a8\u7269\u5c24\u5176\u662f\u7565\u5fae\u9ad8\u7b49\u7684\u52a8\u7269\uff0c\u5b8c\u5168\u540c\u4eba\u4e00\u6837\uff0c\u751f\u6765\u5c31\u6709\u4e00\u79cd\u7531\u9057\u4f20\u800c\u5f97\u5230\u7684\u6f5c\u5728\u7684\u6559\u80b2\uff0c\u5176\u6548\u679c\u89c1\u8bf8\u4e2a\u4f53\u7684\u53d1\u5c55\u8fc7\u7a0b\u3002\u201d\u8fd9\u53e5\u8bdd\u662f\u8bf4\u6559\u80b2\u7684\u4ea7\u751f\u4e0e () \u76f8\u5173\u3002\nA. \u751f\u7269\u8d77\u6e90\u8bf4\nB. \u6df7\u5408\u8d77\u6e90\u8bf4\nC. \u52b3\u52a8\u8d77\u6e90\u8bf4\nD. \u5fc3\u7406\u8d77\u6e90\u8bf4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6190372837643353, "meta-math/MetaMath-Mistral-7B": 0.7108084406275819, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6396850146781693, "meta-llama/Meta-Llama-3-8B": 0.32205625344145955, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u8fdb\u884c\u6210\u7ec4\u8bbe\u8ba1\u4e24\u6837\u672c\u79e9\u548c\u68c0\u9a8c\u65f6\uff0c\u4ee5\u4e0b\u54ea\u79cd\u68c0\u9a8c\u5047\u8bbe\u662f\u6b63\u786e\u7684\nA. $\\mathrm{H}_0$:\u4e24\u603b\u4f53\u5206\u5e03\u76f8\u540c$\\mathrm{H}_1$:\u4e24\u603b\u4f53\u5206\u5e03\u4e0d\u540c\nB. $\\mathrm{H}_0$:\u4e24\u6837\u672c\u5206\u5e03\u76f8\u540c$\\mathrm{H}_1$:\u4e24\u6837\u672c\u5206\u5e03\u4e0d\u540c\nC. $\\mathrm{H}_0$:\u4e24\u6837\u672c\u5747\u6570\u76f8\u7b49$\\mathrm{H}_1$:\u4e24\u6837\u672c\u5747\u6570\u4e0d\u7b49\nD. $\\mathrm{H}_0$:\u4e24\u603b\u4f53\u5747\u6570\u76f8\u7b49$\\mathrm{H}_1$:\u4e24\u603b\u4f53\u5747\u6570\u4e0d\u7b49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.39267122066595683}}, {"question": "\u7537\u6027\uff0c75\u5c81\uff0c1\u5468\u524d\u56e0\u80c6\u56ca\u591a\u53d1\u5c0f\u7ed3\u77f3\u884c\u8179\u8154\u955c\u80c6\u56ca\u5207\u9664\u672f\uff0c\u8fd12\u5929\u53d1\u73b0\u5de9\u819c\u9ec4\u67d3\uff0c\u60a3\u8005\u51fa\u73b0\u9ec4\u75b8\u6700\u53ef\u80fd\u7684\u539f\u56e0\u662f\nA. \u80c6\u56ca\u5185\u7ed3\u77f3\u843d\u5165\u80c6\u7ba1\nB. \u80c6\u7ba1\u6c34\u80bf\u72ed\u7a84\nC. \u672f\u4e2d\u635f\u4f24\u80c6\u603b\u7ba1\nD. \u7532\u578b\u809d\u708e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3346258625113097, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3313635536266756, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u79cd\u5546\u54c1\u4ef7\u683c\u4e0b\u964d\u5bf9\u5176\u4e92\u8865\u54c1\u6700\u76f4\u63a5\u7684\u5f71\u54cd\u662f\nA. \u4e92\u8865\u54c1\u7684\u9700\u6c42\u66f2\u7ebf\u5411\u53f3\u79fb\u52a8\nB. \u4e92\u8865\u54c1\u7684\u4ef7\u683c\u4e0b\u964d\nC. \u4e92\u8865\u54c1\u7684\u4f9b\u7ed9\u66f2\u7ebf\u5411\u53f3\u79fb\u52a8\nD. \u4e92\u8865\u54c1\u7684\u9700\u6c42\u66f2\u7ebf\u5411\u5de6\u79fb\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4288786973386337, "meta-math/MetaMath-Mistral-7B": 0.873510291719205, "itpossible/Chinese-Mistral-7B-v0.1": 0.41750598569287845, "HuggingFaceH4/zephyr-7b-beta": 0.836137039747201, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.42570984569007086, "meta-llama/Meta-Llama-3-8B": 0.46589234207604846, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9649026873616696}}, {"question": "\u5c06\u8bb8\u591a\u8fc7\u4e8e\u72ed\u5c0f\u7684\u5b50\u5e02\u573a\u7ec4\u5408\u8d77\u6765\u201c\u5f02\u4e2d\u6c42\u540c\u201d\uff0c\u4ee5\u4fbf\u80fd\u4ee5\u8f83\u4f4e\u7684\u6210\u672c\u548c\u4ef7\u683c\u53bb\u6ee1\u8db3\u8fd9\u4e00\u5e02\u573a\u7684\u9700\u6c42\uff0c\u8fd9\u79cd\u6218\u7565\u662f\nA. \u96c6\u4e2d\u6027\u8425\u9500\u6218\u7565\nB. \u5e02\u573a\u5b9a\u4f4d\u6218\u7565\nC. \u5e02\u573a\u7ec6\u5206\u6218\u7565\nD. \u53cd\u5e02\u573a\u7ec6\u5206\u6218\u7565\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ca4\u83dc\u4e3b\u8981\u8986\u76d6\u5728\nA. \u957f\u6c5f\u4e0b\u6e38\u5730\u533a\nB. \u73e0\u6c5f\u6d41\u57df\nC. \u957f\u6c5f\u4e2d\u6e38\u5730\u533a\nD. \u9ec4\u6cb3\u6d41\u57df\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7890273715672634, "meta-math/MetaMath-Mistral-7B": 0.9510599991725324, "itpossible/Chinese-Mistral-7B-v0.1": 0.9437115568098717, "HuggingFaceH4/zephyr-7b-beta": 0.9881961724414444, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7730708534005056, "meta-llama/Meta-Llama-3-8B": 0.9553639710814654, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2010\u5e744\u670828\u65e5\u53d1\u5e03\u5b9e\u65bd\u7684\u300a\u5e7f\u4e1c\u7701\u59d4\u3001\u7701\u653f\u5e9c\u5173\u4e8e\u52a0\u5feb\u5916\u7ecf\u8d38\u6218\u7565\u8f6c\u578b\u63d0\u5347\u56fd\u9645\u7ade\u4e89\u529b\u7684\u51b3\u5b9a\u300b\u63d0\u51fa\uff0c\u5e7f\u4e1c\u5916\u8d38\u8981\u4ece\u201c\u5f15\u8fdb\u6765\u201d\u4e3a\u4e3b\uff0c\u5411\u201c\u5f15\u8fdb\u6765\u201d\u3001\u201c\u8d70\u51fa\u53bb\u201d\u5e76\u91cd\u8f6c\u578b\u3002\u8fd9\u610f\u5473\u7740\u5e7f\u4e1c\u7701\nA. \u5bf9\u5916\u5f00\u653e\u5c06\u53d1\u5c55\u5230\u4e00\u4e2a\u65b0\u7684\u5c42\u6b21\nB. \u628a\u6269\u5927\u5185\u9700\u4f5c\u4e3a\u53d1\u5c55\u7ecf\u6d4e\u7684\u57fa\u672c\u6218\u7565\nC. \u628a\u72ec\u7acb\u81ea\u4e3b\u4f5c\u4e3a\u5bf9\u5916\u5f00\u653e\u7684\u7acb\u8db3\u70b9\nD. \u901a\u8fc7\u5229\u7528\u5916\u8d44\u63a8\u52a8\u81ea\u4e3b\u521b\u65b0\u548c\u4ea7\u4e1a\u5347\u7ea7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3154169752260313, "meta-math/MetaMath-Mistral-7B": 0.5174255222079569, "itpossible/Chinese-Mistral-7B-v0.1": 0.6030414802880227, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5740062263707733, "meta-llama/Meta-Llama-3-8B": 0.6837774311650492, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5286436818441229}}, {"question": "\u5272\u793c\u5236\u5ea6\u662f\u4ee5\u4e0b\u54ea\u4e2a\u5b97\u6559\u7684\u7956\u4f20\u793c\u4eea\u3002\nA. \u4e07\u795e\u6559\nB. \u4f0a\u65af\u5170\u6559\nC. \u72b9\u592a\u6559\nD. \u592a\u9633\u795e\u6559\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7453295810184943, "meta-math/MetaMath-Mistral-7B": 0.7637257450951053, "itpossible/Chinese-Mistral-7B-v0.1": 0.5746624561183111, "HuggingFaceH4/zephyr-7b-beta": 0.9985414177819883, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7864209161839198, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9524676901412733}}, {"question": "M42\u4e0e\u4ee5\u4e0b\u54ea\u4e2a\u5929\u4f53\u5149\u8c31\u6700\u63a5\u8fd1\nA. M57\nB. M31\nC. \u592a\u9633\nD. \u53cc\u5b50\u5ea7\u6d41\u661f\u96e8\u7fa4\u5185\u6d41\u661f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u8d44\u4ea7\u9636\u7ea7\u65b0\u95fb\u4e8b\u4e1a\u5728\u5904\u7406\u4e0e\u53d7\u4f17\u5173\u7cfb\u65f6\nA. \u5728\u4e00\u4e9b\u95ee\u9898\u548c\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u8003\u8651\u5e76\u6ee1\u8db3\u7fa4\u4f17\u7684\u67d0\u4e9b\u4fe1\u606f\u9700\u8981\nB. \u5728\u6574\u4e2a\u8fc7\u7a0b\u4e2d\u90fd\u80fd\u6ee1\u8db3\u7fa4\u4f17\u7684\u9700\u8981\nC. \u5728\u6240\u6709\u95ee\u9898\u4e0a\u90fd\u80fd\u5b8c\u5168\u6ee1\u8db3\u7fa4\u4f17\u7684\u9700\u8981\nD. \u6839\u672c\u4e0d\u8003\u8651\u6ee1\u8db3\u7fa4\u4f17\u9700\u8981\u7684\u95ee\u9898\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7002578568884864, "meta-math/MetaMath-Mistral-7B": 0.7974471956357128, "itpossible/Chinese-Mistral-7B-v0.1": 0.7373662402759436, "HuggingFaceH4/zephyr-7b-beta": 0.9870889185716091, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6370072625985967, "meta-llama/Meta-Llama-3-8B": 0.8542731821733099, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.839582204216552}}, {"question": "\u56fd\u5bb6\u5784\u65ad\u8d44\u672c\u4e3b\u4e49\u7684\u5f62\u5f0f\u4e2d\uff0c\u6700\u4e3b\u8981\u7684\u3001\u6700\u91cd\u8981\u7684\u5f62\u5f0f\u662f\nA. \u56fd\u5bb6\u5e02\u573a\u5784\u65ad\u7ecf\u6d4e\nB. \u56fd\u5bb6\u8c03\u8282\u7ecf\u6d4e\nC. \u56fd\u5bb6\u81ea\u7136\u5784\u65ad\u7ecf\u6d4e\nD. \u516c\u79c1\u5408\u8425\u7ecf\u6d4e \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u98df\u54c1\u76d0\u6e0d\u548c\u7cd6\u6e0d\u5728\u4fdd\u85cf\u539f\u7406\u4e0a\u5c5e\u4e8e\nA. \u6709\u751f\u673a\u539f\u7406\nB. \u65e0\u83cc\u539f\u7406\nC. \u5047\u6b7b\u539f\u7406\nD. \u6709\u83cc\u539f\u7406\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4344001499587066, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.36150852560561064, "HuggingFaceH4/zephyr-7b-beta": 0.6711779987507978, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.47020082125287926, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3466892061295826}}, {"question": "\u96c6\u4e2d\u6536\u8d2e\u6838\u6280\u672f\u7684\u5e94\u7528\u5355\u4f4d\u7532\u5c06\u4ea7\u751f\u7684\u653e\u5c04\u6027\u5e9f\u7269\u4ea4\u7ed9\u4e0d\u5177\u6709\u8d2e\u5b58\u6838\u5e9f\u7269\u8d44\u683c\u7684\u4e59\u50a8\u8fd0\u516c\u53f8\u4fdd\u7ba1\uff0c\u67d0\u65e5\uff0c\u4e59\u516c\u53f8\u56e0\u4fdd\u7ba1\u4e0d\u614e\u4f7f\u653e\u5c04\u6027\u5e9f\u7269\u5916\u6cc4\uff0c\u9020\u6210\u591a\u4eba\u53d7\u4f24\u548c\u516c\u79c1\u8d22\u4ea7\u8f83\u5927\u635f\u5931\uff0c\u5219\nA. \u7532\u3001\u4e59\u5e94\u5f53\u5bf9\u653e\u5c04\u6027\u5e9f\u7269\u5916\u6cc4\u9020\u6210\u7684\u4ed6\u4eba\u635f\u5bb3\u627f\u62c5\u8fde\u5e26\u8d23\u4efb\nB. \u7532\u3001\u4e59\u7b7e\u8ba2\u7684\u4ed3\u50a8\u5408\u540c\u5c5e\u4e8e\u53ef\u64a4\u9500\u5408\u540c\nC. \u52a0\u5bb3\u4eba\u627f\u62c5\u7684\u8d23\u4efb\u4ece\u6027\u8d28\u4e0a\u770b\u5e94\u5f53\u5c5e\u4e8e\u73af\u5883\u6c61\u67d3\u8d23\u4efb\nD. \u53d7\u5bb3\u4eba\u4e0d\u80fd\u76f4\u63a5\u8981\u6c42\u4ed3\u50a8\u8005\u4e59\u627f\u62c5\u8d23\u4efb\uff0c\u53ea\u80fd\u8981\u6c42\u751f\u4ea7\u8005\u7532\u627f\u62c5\u8d23\u4efb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.49171755411425316, "meta-math/MetaMath-Mistral-7B": 0.7369101847860214, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9966954740652951, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4195983921193397, "meta-llama/Meta-Llama-3-8B": 0.4257098456900709, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5756480945512373}}, {"question": "\u897f\u6c49\u521d\u671f\uff0c\u9053\u5bb6\u5b66\u8bf4\u517c\u91c7\u9634\u9633\u3001\u5112\u3001\u58a8\u3001\u540d\u3001\u6cd5\u5404\u5bb6\u5b66\u8bf4\u7684\u7cbe\u9ad3\uff1b\u540e\u6765\u8463\u4ef2\u8212\u7684\u5112\u5bb6\u5b66\u8bf4\u4e5f\u5438\u6536\u9634\u9633\u4e94\u884c\u3001\u6cd5\u3001\u9053\u7b49\u5404\u79cd\u601d\u60f3\u3002\u4fc3\u6210\u5f53\u65f6\u5b66\u672f\u601d\u60f3\u4e0a\u5448\u73b0\u8fd9\u79cd\u7279\u5f81\u7684\u4e3b\u8981\u56e0\u7d20\u662f\nA. \u517c\u6536\u5e76\u84c4\u7684\u6587\u5316\u653f\u7b56\nB. \u73b0\u5b9e\u7edf\u6cbb\u9700\u8981\nC. \u767e\u5bb6\u4e89\u9e23\u5c40\u9762\u7684\u5ef6\u7eed\nD. \u738b\u56fd\u52bf\u529b\u5f3a\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6805037503740065, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6d88\u8d39\u8005\u8fbe\u5230\u5747\u8861\u65f6\uff0c\u6700\u540e\u4e00\u5355\u4f4d\u5546\u54c1\u7ed9\u6d88\u8d39\u8005\u5e26\u6765\u7684\u6548\u7528\u53d6\u51b3\u4e8e\nA. \u6d88\u8d39\u8005\u7684\u6536\u5165\u548c\u4ef7\u683c\nB. \u5546\u54c1\u7684\u4ef7\u683c\nC. \u5546\u54c1\u7684\u8d28\u91cf\nD. \u6d88\u8d39\u8005\u7684\u6536\u5165\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9206070197825923, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4618304558084146, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9ad8\u8840\u538b\u60a3\u8005\u4e0e\u6b63\u5e38\u4eba\u76f8\u6bd4\uff0c\u660e\u663e\u5347\u9ad8\u7684\u6307\u6807\u662f\nA. \u5fc3\u8f93\u51fa\u91cf\nB. \u5fc3\u810f\u505a\u529f\u91cf\nC. \u5fc3\u6307\u6570\nD. \u5c04\u8840\u5206\u6570\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3512536987011174, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bfc\u81f4\u2f54\u8d44\u6e90\u4e0d\u2f9c\u548c\u2f64\u2f54\u7d27\u5f20\u7684\u539f\u56e0\u662f\u591a\u2f45\u2faf\u7684\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u4e0e\u6b64\u2f46\u5173\u7684\u662f\nA. \u5174\u4fee\u2f54\u5229\u2f2f\u7a0b\nB. \u2f54\u6c61\u67d3\u3001\u2f54\u6d6a\u8d39\u4e25\u91cd\nC. \u2f54\u8d44\u6e90\u65f6\u7a7a\u5206\u5e03\u4e0d\u5747\nD. \u2f08\u7c7b\u5bf9\u6de1\u2f54\u7684\u9700\u6c42\u91cf\u589e\u2f24\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9934410228013463, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9032437505934889, "meta-llama/Meta-Llama-3-8B": 0.6036570805949657, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4403307015169765}}, {"question": "$x^2\\neq y^2$\u662f$x\\neq y$\u4e14$x\\neq -y$\u7684\nA. \u65e2\u4e0d\u5145\u5206\u4e5f\u4e0d\u5fc5\u8981\u6761\u4ef6\nB. \u5145\u5206\u4e0d\u5fc5\u8981\u6761\u4ef6 \nC. \u5145\u8981\u6761\u4ef6\nD. \u5fc5\u8981\u4e0d\u5145\u5206\u6761\u4ef6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6308874625299957, "meta-math/MetaMath-Mistral-7B": 0.8549135256786501, "itpossible/Chinese-Mistral-7B-v0.1": 0.42678911359993355, "HuggingFaceH4/zephyr-7b-beta": 0.9760752756985809, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.703460536803934, "meta-llama/Meta-Llama-3-8B": 0.30601362565976303, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3687101202574093}}, {"question": "\u5728\u73b0\u4ee3\u5e02\u573a\u7ecf\u6d4e\u6761\u4ef6\u4e0b\uff0c\u5e02\u573a\u8425\u9500\u7ba1\u7406\u8fc7\u7a0b\u7684\u9996\u8981\u6b65\u9aa4\u662f\nA. \u5e02\u573a\u8425\u9500\u7ec4\u5408\nB. \u7ba1\u7406\u5e02\u573a\u8425\u9500\u6d3b\u52a8\nC. \u9009\u62e9\u76ee\u6807\u5e02\u573a\nD. \u5206\u6790\u5e02\u573a\u673a\u4f1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.438995984168298, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.40415511943589777, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8691712553559372, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9930241335483272}}, {"question": "\u793e\u4f1a\u5b66\u5728\u4e2d\u56fd\u7684\u4f20\u64ad\u548c\u53d1\u5c55\u8fc7\u7a0b\u4e2d\uff0c1891\u5e74\uff0c\u5eb7\u6709\u4e3a\u5728\u6559\u5b66\u5927\u7eb2\u5206\u7c7b\u5b66\u79d1\u4e2d\u63d0\u51fa\u4e86\nA. \u7fa4\u5b66\nB. \u4eba\u5b66\nC. \u793e\u4f1a\u5b66\nD. \u4ec1\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u80fd\u591f\u9006\u5411\u8f6c\u8fd0\u80c6\u56fa\u9187\u5230\u809d\u7684\u8102\u86cb\u767d\u662f\nA. CM\nB. LDL\nC. VLDL\nD. HDL\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9373576512856854, "meta-math/MetaMath-Mistral-7B": 0.997946681880964, "itpossible/Chinese-Mistral-7B-v0.1": 0.6701108071370289, "HuggingFaceH4/zephyr-7b-beta": 0.999966671731283, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9127642625896916, "meta-llama/Meta-Llama-3-8B": 0.5733974774828536, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u884c\u4e3a\u4e3b\u4e49\u5fc3\u7406\u5b66\u6d3e\u7684\u4ee3\u8868\u4eba\u7269\u662f\nA. \u9b4f\u7279\u66fc\u4e0e\u82db\u52d2\nB. \u534e\u751f\u4e0e\u65af\u91d1\u7eb3\nC. \u8a79\u59c6\u65af\u4e0e\u675c\u5a01\nD. \u51af\u7279\u4e0e\u94c1\u94a6\u7eb3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.9041037589779881, "HuggingFaceH4/zephyr-7b-beta": 0.40165481700483674, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9561056269863968, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9980219802894357}}, {"question": "\u88ab\u6709\u4f4d\u4f20\u8bb0\u4f5c\u5bb6\u5f62\u5bb9\u4e3a\u201c\u72b9\u5982\u884c\u4e3a\u79d1\u5b66\u7684\u8fbe\u5c14\u6587\u201d\u7684\u662f\nA. \u52d2\u6e29\nB. \u970d\u592b\u5170\nC. \u62c9\u65af\u97e6\u5c14\nD. \u674e\u666e\u66fc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6307\u6807\u5347\u9ad8\u5e38\u63d0\u793a\u7cfb\u7edf\u6027\u7ea2\u6591\u72fc\u75ae\u75c5\u60c5\u6d3b\u52a8\uff0c\u4f46\u9664\u5916\nA. \u6297dsDNA \u6297\u4f53\nB. C\u53cd\u5e94\u86cb\u767d\nC. \u8840\u6c89\nD. \u8865\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5617903994571193, "meta-math/MetaMath-Mistral-7B": 0.5982184082446659, "itpossible/Chinese-Mistral-7B-v0.1": 0.7254723215969149, "HuggingFaceH4/zephyr-7b-beta": 0.9946519559806768, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7333071429388314, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u5728\u53d1\u5c55\u5bf9\u5916\u8d38\u6613\uff0c\u5438\u6536\u5229\u7528\u5916\u8d44\uff0c\u5bf9\u5916\u6280\u672f\u4ea4\u6d41\u5de5\u4f5c\u4e2d\uff0c\u90fd\u5fc5\u987b\u59cb\u7ec8\u575a\u6301\u7684\u4e00\u6761\u539f\u5219\u662f\nA. \u81ea\u529b\u66f4\u751f\u539f\u5219\nB. \u548c\u5e73\u5171\u5904\u539f\u5219\nC. \u56db\u9879\u57fa\u672c\u539f\u5219\nD. \u5e73\u7b49\u4e92\u5229\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.958202806404152, "meta-math/MetaMath-Mistral-7B": 0.944018665068321, "itpossible/Chinese-Mistral-7B-v0.1": 0.8541985663401849, "HuggingFaceH4/zephyr-7b-beta": 0.9999706601230997, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8585597439678797, "meta-llama/Meta-Llama-3-8B": 0.8985464469022794, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9846453930091779}}, {"question": "\u5728\u52a0\u70ed\u60c5\u51b5\u4e0b\u6700\u5bb9\u6613\u53d1\u751f\u6c27\u5316\u805a\u5408\u7684\u6cb9\u8102\u79cd\u7c7b\u662f\nA. \u5927\u8c46\u6cb9\nB. \u82b1\u751f\u6cb9\nC. \u6a44\u6984\u6cb9\nD. \u4e9a\u9ebb\u6cb9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.44229883850970647, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u5f52\u7eb3\u903b\u8f91\u4e2d\uff0c\u6240\u6709\u524d\u63d0\u771f\u800c\u7ed3\u8bba\nA. \u53ef\u80fd\u771f\nB. \u5fc5\u7136\u5047\nC. \u5fc5\u7136\u771f\nD. \u53ef\u80fd\u5047\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6b66\u672f\u7684\u4e3b\u8981\u5185\u5bb9\u662f\nA. \u6280\u51fb\u52a8\u4f5c\nB. \u8868\u6f14\u52a8\u4f5c\nC. \u821e\u8e48\u52a8\u4f5c\nD. \u6742\u6280\u52a8\u4f5c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7928832484164758, "meta-math/MetaMath-Mistral-7B": 0.9858638287627542, "itpossible/Chinese-Mistral-7B-v0.1": 0.6039639520781223, "HuggingFaceH4/zephyr-7b-beta": 0.9714781626499446, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.95817039817978, "meta-llama/Meta-Llama-3-8B": 0.7188069120378247, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8972807907991294}}, {"question": "\u60a3\u8005\uff0c\u7537\uff0c48\u5c81\uff0c\u80c3\u8118\u80c0\u75db2\u5468\uff0c\u8fde\u53ca\u80c1\u808b\uff0c\u55f3\u6c14\u540e\u75bc\u75db\u51cf\u8f7b\uff0c\u6bcf\u4e8e\u60c5\u5fd7\u523a\u6fc0\u65f6\u80c3\u75db\u52a0\u91cd\uff0c\u98df\u6b32\u4e0d\u632f\uff0c\u5608\u6742\u541e\u9178\uff0c\u820c\u7ea2\uff0c\u82d4\u8584\u767d\uff0c\u8109\u5f26\u3002\u8be5\u60a3\u8005\u8fa8\u8bc1\u4e3a\nA. \u809d\u80c3\u4e0d\u548c\nB. \u5bd2\u51dd\u6c14\u6ede\nC. \u809d\u80c3\u90c1\u70ed\nD. \u996e\u98df\u505c\u6ede\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3109999396265533, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5636743380160091, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4575160312128929, "meta-llama/Meta-Llama-3-8B": 0.5496802597378747, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.619478666869461}}, {"question": "\u4e0b\u5217\u7968\u636e\u53ef\u4ee5\u8d34\u73b0\u7684\u662f\nA. \u5546\u4e1a\u6c47\u7968\nB. \u94f6\u884c\u672c\u7968\nC. \u652f\u7968\nD. \u94f6\u884c\u6c47\u7968\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43650345671381396, "meta-math/MetaMath-Mistral-7B": 0.3829719541229078, "itpossible/Chinese-Mistral-7B-v0.1": 0.5708526788478793, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6135467451056755, "meta-llama/Meta-Llama-3-8B": 0.6471407236939258, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6137512045498528}}, {"question": "\u7537\u6027\uff0c55\u5c81\uff0c\u4e59\u809d\u75c5\u53f215\u5e74\uff0cCT\u63d0\u793a\u53f3\u809d\u76f4\u5f848cm\u80bf\u7269\uff0c\u9760\u8fd1\u7b2c\u4e00\u809d\u95e8\uff0cChild\u5206\u7ea7C\u7ea7\uff0cAFP890ng/ml\uff0c\u8003\u8651\u809d\u764c\u3002\u7ecf\u8fc73\u4e2a\u6708\u7684\u6cbb\u7597\uff0c\u80bf\u7624\u7f29\u5c0f\u81f35\u00d76cm\uff0c\u809d\u529f\u80fd\u597d\u8f6c\uff0c\u65e0\u8fdc\u5904\u8f6c\u79fb\uff0c\u6b64\u65f6\u6700\u9002\u5408\u7684\u6cbb\u7597\u662f\uff1a\nA. \u4ecb\u5165\u6cbb\u7597\nB. \u5316\u7597\nC. \u624b\u672f\u6cbb\u7597\nD. \u6d88\u878d\u6cbb\u7597\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5792519279198733, "meta-math/MetaMath-Mistral-7B": 0.7731335944482038, "itpossible/Chinese-Mistral-7B-v0.1": 0.34993229125498293, "HuggingFaceH4/zephyr-7b-beta": 0.9685574299276922, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8018738316758651, "meta-llama/Meta-Llama-3-8B": 0.39497563408366365, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5045554214104194}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u7b26\u5408\u6211\u56fd\u5f53\u524d\u5904\u4e8e\u5de5\u4e1a\u5316\u52a0\u901f\u65f6\u671f\u7684\u4ea7\u4e1a\u7ed3\u6784\u7279\u70b9\u7684\u662f\nA. \u6280\u672f\u77e5\u8bc6\u5bc6\u96c6\u578b\u4ea7\u4e1a\u5360\u7b2c\u4e8c\u4ea7\u4e1a\u6bd4\u91cd\u8fc5\u901f\u4e0a\u5347\nB. \u57ce\u5e02\u5316\u57fa\u672c\u7ed3\u675f\nC. \u5236\u9020\u4e1a\u5904\u4e8e\u6210\u957f\u671f\nD. \u5236\u9020\u4e1a\u5904\u4e8e\u7ed3\u6784\u9ad8\u53d8\u6362\u7387\u65f6\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u4e2a\u623f\u5730\u4ea7\u4ee3\u7406\u5546\u77e5\u9053\u4ed6\u4ee3\u7406\u7684\u623f\u5b50\u7684\u90bb\u5c45\u7ecf\u5e38\u4e3e\u884c\u55a7\u95f9\u7684\u5bb4\u4f1a\uff0c\u4f46\u4ed6\u4ece\u4e0d\u628a\u8fd9\u4e00\u70b9\u544a\u8bc9\u6709\u610f\u5411\u7684\u987e\u5ba2\uff0c\u8be5\u79cd\u884c\u4e3a\nA. \u4e0d\u5c5e\u4e8e\u6b3a\u9a97\nB. \u4ee5\u4e0a\u7b54\u6848\u90fd\u4e0d\u5bf9\nC. \u82e5\u987e\u5ba2\u8868\u793a\u5e0c\u671b\u5c45\u4f4f\u5728\u4e00\u4e2a\u5b89\u9759\u7684\u73af\u5883\u4e2d\uff0c\u5374\u6ca1\u6709\u88ab\u544a\u77e5\u5219\u4e3a\u6b3a\u9a97\nD. \u5c5e\u4e8e\u6b3a\u9a97\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e16\u754c\u6700\u5927\u7684\u4f5b\u5b66\u9662\u4e94\u660e\u4f5b\u5b66\u9662\u4f4d\u4e8e\u54ea\u91cc\uff1f\nA. \u9053\u5b5a\nB. \u5587\u8363\nC. \u7089\u970d\nD. \u5fb7\u683c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.29660173325630934, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5162696214801247}}, {"question": "\u6e29\u5ea6\u5bf9\u9176\u4fc3\u53cd\u5e94\u901f\u5ea6\u5f71\u54cd\u7684\u8bba\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u6700\u9002\u6e29\u5ea6\u662f\u9176\u7684\u7279\u6027\u5e38\u6570\uff0c\u4e0e\u53cd\u5e94\u65f6\u95f4\u65e0\u5173\nB. \u6e29\u5ea6\u5347\u9ad8\u53cd\u5e94\u901f\u5ea6\u52a0\u5feb\uff0c\u4e0e\u4e00\u822c\u50ac\u5316\u5242\u5b8c\u5168\u76f8\u540c\nC. \u4f4e\u6e29\u53ef\u4f7f\u5927\u591a\u6570\u9176\u53d1\u751f\u53d8\u6027\u800c\u4f7f\u9176\u6d3b\u6027\u964d\u4f4e\nD. \u6700\u9002\u6e29\u5ea6\u4e0d\u662f\u9176\u7684\u7279\u6027\u5e38\u6570\uff0c\u5ef6\u957f\u53cd\u5e94\u65f6\u95f4\uff0c\u5176\u6700\u9002\u6e29\u5ea6\u964d\u4f4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u884c\u4e1a\u5438\u5f15\u529b\u548c\u4e1a\u52a1\u529b\u91cf\u90fd\u5904\u5728\u4e2d\u7b49\u6c34\u5e73\u7684\u6218\u7565\u4e1a\u52a1\u5355\u4f4d\u5728\u591a\u56e0\u7d20\u6295\u8d44\u7ec4\u5408\u4e2d\u6240\u5904\u7684\u4f4d\u7f6e\u662f\nA. \u7ea2\u8272\u5730\u5e26\nB. \u767d\u8272\u5730\u5e26\nC. \u7eff\u8272\u5730\u5e26\nD. \u9ec4\u8272\u5730\u5e26\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u54ea\u4e2a/\u4e9b\u9009\u9879\u5bf9 K \u6298\u4ea4\u53c9\u9a8c\u8bc1\u7684\u63cf\u8ff0\u662f\u6b63\u786e\u7684\nA. \u5982\u679c K=N\uff0c\u90a3\u4e48\u5176\u79f0\u4e3a\u7559\u4e00\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u5176\u4e2d N \u4e3a\u9a8c\u8bc1\u96c6\u4e2d\u7684\u6837\u672c\u6570\u91cf\nB. \u66f4\u5927\u7684 K \u503c\u76f8\u6bd4\u4e8e\u5c0f K \u503c\u5c06\u5bf9\u4ea4\u53c9\u9a8c\u8bc1\u7ed3\u6784\u6709\u66f4\u9ad8\u7684\u4fe1\u5fc3\nC. \u4ee5\u4e0a\u90fd\u662f\nD. \u589e\u5927 K \u5c06\u5bfc\u81f4\u4ea4\u53c9\u9a8c\u8bc1\u7ed3\u679c\u65f6\u9700\u8981\u66f4\u591a\u7684\u65f6\u95f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5422810457261552, "meta-math/MetaMath-Mistral-7B": 0.8587178339356211, "itpossible/Chinese-Mistral-7B-v0.1": 0.5399970676962301, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8184174084681447, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9375559644956921}}, {"question": "\u5386\u53f2\u4e0a\u6770\u51fa\u4eba\u7269\u7684\u4ea7\u751f\nA. \u5076\u7136\u4e0e\u5fc5\u7136\u7684\u7edf\u4e00\nB. \u6709\u7684\u662f\u5076\u7136\u6709\u7684\u662f\u5fc5\u7136\nC. \u7eaf\u7cb9\u5076\u7136\u7684\nD. \u7eaf\u7cb9\u5fc5\u7136\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5896157047068116, "meta-math/MetaMath-Mistral-7B": 0.907568109649337, "itpossible/Chinese-Mistral-7B-v0.1": 0.8975028763068977, "HuggingFaceH4/zephyr-7b-beta": 0.8873448786869732, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.692786019697154, "meta-llama/Meta-Llama-3-8B": 0.9475250382676229, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u72b6\u817a\u7d20\u5728\u751f\u7406\u6d53\u5ea6\u8303\u56f4\u5185\u5bf9\u7269\u8d28\u4ee3\u8c22\u7684\u5f71\u54cd\u662f\nA. \u52a0\u5f3a\u86cb\u767d\u8d28\u5206\u89e3\uff0c\u51fa\u73b0\u8d1f\u6c2e\u5e73\u8861\nB. \u4fc3\u8fdb\u8102\u80aa\u9178\u5408\u6210\uff0c\u5e76\u6291\u5236\u5176\u964d\u89e3\nC. \u6291\u5236\u7ec4\u7ec7\u5229\u7528\u7cd6\uff0c\u4f7f\u8840\u7cd6\u6d53\u5ea6\u5347\u9ad8\nD. \u4fc3\u8fdb\u80c6\u56fa\u9187\u5408\u6210\uff0c\u66f4\u52a0\u901f\u5176\u8f6c\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ece\u4e1a\u4eba\u5458\u65e0\u8bba\u4ece\u4e8b\u4ec0\u4e48\u804c\u4e1a\uff0c\u90fd\u5e94\u8be5\u5e72\u4e00\u884c\u7231\u4e00\u884c\uff0c\u7231\u4e00\u884c\u94bb\u4e00\u884c\uff0c\u7cbe\u76ca\u6c42\u7cbe\uff0c\u5c3d\u804c\u5c3d\u8d23\u3002\u8fd9\u662f\u804c\u4e1a\u9053\u5fb7\u57fa\u672c\u8981\u6c42\u4e2d\nA. \u8bda\u5b9e\u5b88\u4fe1\u7684\u8981\u6c42\nB. \u529e\u4e8b\u516c\u9053\u7684\u8981\u6c42\nC. \u5949\u732e\u793e\u4f1a\u7684\u8981\u6c42\nD. \u7231\u5c97\u656c\u4e1a\u7684\u8981\u6c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7928832484164758, "meta-math/MetaMath-Mistral-7B": 0.9689960055426761, "itpossible/Chinese-Mistral-7B-v0.1": 0.9741338670665302, "HuggingFaceH4/zephyr-7b-beta": 0.9991943864886548, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9452552956789914, "meta-llama/Meta-Llama-3-8B": 0.6251503074581314, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9688878672202591}}, {"question": "\u6cd5\u6cbb\u7684\u7ec8\u6781\u6027\u7684\u76ee\u6807\u548c\u4ef7\u503c\u662f\nA. \u5145\u5206\u5c0a\u91cd\u548c\u6269\u5c55\u4eba\u6743\nB. \u9075\u5faa\u6743\u529b\u5236\u7ea6\u539f\u5219\nC. \u575a\u6301\u6743\u5229\u4e49\u52a1\u4e00\u81f4\u6027\nD. \u575a\u6301\u6cd5\u5f8b\u9762\u524d\u4eba\u4eba\u5e73\u7b49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5738897703498776, "meta-math/MetaMath-Mistral-7B": 0.7812639486098482, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6213222294338056, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5026135546683439, "meta-llama/Meta-Llama-3-8B": 0.8091861129222124, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u852c\u83dc\uff0c\u54ea\u4e9b\u4f1a\u5f15\u8d77\u98df\u7269\u4e2d\u6bd2\nA. AC\nB. \u5df2\u53d1\u82bd\u7684\u571f\u8c46\nC. \u672a\u716e\u719f\u7684\u6241\u8c46\nD. \u751f\u97ed\u83dc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.48074794862901915, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3560660444072497, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d28\u91cf\u4e3am\u7684\u94a2\u7403\u2f83\u2fbc\u5904\u843d\u4e0b\uff0c\u4ee5\u901f\u5ea6v1\u78b0\u5730\uff0c\u78b0\u540e\u7ad6\u76f4\u5411\u4e0a\u5f39\u56de\uff0c\u5f39\u56de\u65f6\u7684\u901f\u5ea6\u4e3av2\uff0c\u78b0\u649e\u65f6\u95f4\u5f88\u77ed\uff0c\u5219\u5728\u78b0\u649e\u8fc7\u7a0b\u4e2d\uff0c\u5730\u2faf\u5bf9\u94a2\u7403\u7684\u51b2\u91cf\u7684\u2f45\u5411\u548c\u2f24\u2f29\u662f\nA. \u5411\u4e0a\uff0cm(vl+v2)\nB. \u5411\u4e0a\uff0cm(v1-v2)\nC. \u5411\u4e0b\uff0cm(v1-v2)\nD. \u5411\u4e0b\uff0cm(vl+v2)\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.349326402199841, "meta-math/MetaMath-Mistral-7B": 0.6119408604901369, "itpossible/Chinese-Mistral-7B-v0.1": 0.29863342676099575, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.34031470637689565, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u5c60\u683c\u6d85\u592b\u300a\u2ed4\u69db\u300b\u4e2d\uff0c\u201c\u2ed4\u69db\u201d\u7684\u8c61\u5f81\u610f\u4e49\u662f\nA. \u2fb0\u547d\u5f81\u9014\u4e0a\u7684\u8270\u96be\u9669\u963b\nB. \u2fb0\u547d\u8005\u7684\u732e\u8eab\u7cbe\u795e\nC. \u53c2\u52a0\u2fb0\u547d\u7684\u6761\u4ef6\nD. \u2fb0\u547d\u4e8b\u4e1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.407015121755923, "meta-math/MetaMath-Mistral-7B": 0.7494318678286804, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6451285286103056, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8523001702776573, "meta-llama/Meta-Llama-3-8B": 0.3845315400200278, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7734649421131145}}, {"question": "\u4e0b\u5217\u9879\u76ee\u4e2d\uff0c\u5c5e\u4e8e\u4e8b\u4e1a\u5355\u4f4d\u56fa\u5b9a\u8d44\u4ea7\u5bf9\u5916\u6295\u8d44\u7684\u5165\u8d26\u4ef7\u503c\u662f\nA. \u8d26\u9762\u4ef7\u503c\nB. \u8d26\u9762\u539f\u503c\nC. \u8bc4\u4f30\u4ef7\u503c\nD. \u91cd\u7f6e\u4ef7\u503c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36354477947811104, "meta-math/MetaMath-Mistral-7B": 0.3542800456958427, "itpossible/Chinese-Mistral-7B-v0.1": 0.307023896814742, "HuggingFaceH4/zephyr-7b-beta": 0.8146712101165919, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5276865898921733}}, {"question": "\u4e0b\u8ff0\u5404\u529f\u80fd\u4e2d\uff0c\u5c5e\u4e8e\u914d\u7f6e\u7ba1\u7406\u7684\u8303\u7574\u7684\u529f\u80fd\u662f\nA. \u6d4b\u8bd5\u7ba1\u7406\u529f\u80fd\nB. \u5b9a\u4e49\u548c\u4fee\u6539\u2f79\u7edc\u5143\u7d20\u95f4\u7684\u4e92\u8054\u5173\u7cfb\nC. \u2f2f\u4f5c\u8d1f\u8f7d\u76d1\u89c6\u529f\u80fd\nD. \u6570\u636e\u6536\u96c6\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7426410200575829, "meta-math/MetaMath-Mistral-7B": 0.9960439425549472, "itpossible/Chinese-Mistral-7B-v0.1": 0.8334447711032568, "HuggingFaceH4/zephyr-7b-beta": 0.9999145659863384, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9602097651069382, "meta-llama/Meta-Llama-3-8B": 0.8741111838437092, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9960819696594381}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u5c71\u4e1c\u98ce\u5473\u8bf4\u6cd5\u9519\u8bef\u7684\u662f\nA. \u5c71\u4e1c\u540d\u83dc\u6709\u7cd6\u918b\u9ec4\u6cb3\u9ca4\u9c7c\u3001\u6cb9\u7206\u53cc\u8106\u3001\u4e5d\u8f6c\u5927\u80a0\u7b49\nB. \u5c71\u4e1c\u83dc\u7684\u53e3\u5473\u6781\u91cd\u7eaf\u6b63\u9187\u6d53\uff0c\u54b8\u3001\u9c9c\u3001\u9178\u3001\u751c\u3001\u8fa3\u5404\u5473\u7686\u6709\uff0c\u5f88\u5c11\u6709\u590d\u5408\u5473\nC. \u5c71\u4e1c\u83dc\u5584\u4e8e\u5236\u6c64\u3001\u7528\u6c64\uff0c\u5e38\u7528\u5236\u597d\u7684\u6e05\u6c64\u3001\u5976\u6c64\u52a0\u5165\u5c71\u73cd\u6d77\u5473\u9c9c\u852c\u6742\u8d27\u4e2d\nD. \u5c71\u4e1c\u53d6\u6599\u4ee5\u7f8a\u8089\u4e3a\u4e3b\u800c\u4ee5\u7f8a\u3001\u732a\u5408\u70f9\u4e3a\u5e38\u89c1\uff0c\u83dc\u80b4\u4ee5\u70ed\u70eb\u7099\u53e3\u3001\u9165\u900f\u4eba\u5473\u800c\u8457\u79f0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u4eec\u5b9e\u9645\u4e0a\u6309\u7167\u6cd5\u5f8b\u89c4\u5b9a\u7684\u884c\u4e3a\u6a21\u5f0f\u53bb\u884c\u4e3a\uff0c\u6cd5\u5f8b\u88ab\u4eba\u4eec\u5b9e\u9645\u9075\u5b88\u3001\u6267\u884c\u6216\u9002\u7528\uff0c\u5728\u6cd5\u5b66\u4e0a\u79f0\u4e3a\nA. \u6cd5\u5f8b\u5b9e\u6548\nB. \u6cd5\u5f8b\u6548\u76ca\nC. \u6cd5\u5f8b\u9002\u7528\nD. \u6cd5\u5f8b\u6548\u679c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3774574824058608, "HuggingFaceH4/zephyr-7b-beta": 0.7733709284376001, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8862550588796403, "meta-llama/Meta-Llama-3-8B": 0.429814792569935, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5548269004106684}}, {"question": "\u5bfc\u81f4\u4eba\u4f53\u751f\u547d\u6b7b\u4ea1\u7684\u6839\u672c\u75c5\u673a\u662f\nA. \u9634\u9633\u5931\u8c03\nB. \u9634\u9633\u504f\u76db\nC. \u9634\u9633\u79bb\u51b3\nD. \u9634\u9633\u504f\u8870\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5594256649813023, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.43495988922323, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u7fa4\u843d\u4e2d\u51b3\u5b9a\u7740\u6574\u4e2a\u7fa4\u843d\u7684\u5185\u90e8\u7ed3\u6784\u548c\u7279\u6b8a\u73af\u5883\uff0c\u5f80\u5f80\u662f\u4e3b\u8981\u5c42\u7684\u4f18\u52bf\u79cd\u7684\u690d\u7269\u79cd\u662f\nA. \u4e9a\u4f18\u52bf\u79cd\nB. \u9644\u5c5e\u79cd\nC. \u5efa\u7fa4\u79cd\nD. \u4f18\u52bf\u79cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8bf4\u6cd5\u6709\u9519\u8bef\u7684\u2f00\u9879\u662f\nA. \u201c\u2ee9\u53d1\u5782\u9aeb\uff0c\u5e76\u6021\u7136\u2f83\u4e50\u201d\u4e2d\u7684\u201c\u2ee9\u53d1\u201d\u6307\u2f7c\u2f08\uff0c\u201c\u5782\u9aeb\u201d\u6307\u2f29\u5b69\u3002 \nB. \u6b27\u9633\u4fee\uff0c\u5510\u5b8b\u2f0b\u2f24\u5bb6\u4e4b\u2f00\uff0c\u201c\u9189\u7fc1\u4e4b\u610f\u4e0d\u5728\u9152\u201d\u51fa\u2f83\u4ed6\u7684\u4f5c\u54c1\u300a\u9189\u7fc1\u4ead\u8bb0\u300b\u3002\nC. \u300a\u6355\u86c7\u8005\u8bf4\u300b\u7684\u201c\u8bf4\u201d\uff0c\u662f\u53e4\u4ee3\u2f00\u79cd\u53d9\u4e8b\u517c\u8bae\u8bba\u7684\u2f42\u4f53\u3002\nD. \u6cd5\u56fd\u4f5c\u5bb6\u5951\u79d1\u592b\u7684\u2f29\u8bf4\u300a\u53d8\u2f8a\u2ef0\u300b\uff0c\u5851\u9020\u4e86\u590f\u6d1b\u514b\u8fd9\u4e2a\u5178\u578b\u7684\u2f42\u5b66\u5f62\u8c61\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6d88\u8d39\u51fd\u6570\u7684\u659c\u7387\u53d6\u51b3\u4e8e\nA. \u7531\u4e8e\u6536\u5165\u53d8\u5316\u5f15\u8d77\u7684\u6295\u8d44\u91cf\nB. \u4e0e\u53ef\u652f\u914d\u6536\u5165\u65e0\u5173\u7684\u6d88\u8d39\u91cf\nC. \u8fb9\u9645\u6d88\u8d39\u503e\u5411\nD. \u5e73\u5747\u6d88\u8d39\u503e\u5411\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7307497819028164, "meta-math/MetaMath-Mistral-7B": 0.9638160382238995, "itpossible/Chinese-Mistral-7B-v0.1": 0.6308803839816646, "HuggingFaceH4/zephyr-7b-beta": 0.9991996868694334, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9424172120032727, "meta-llama/Meta-Llama-3-8B": 0.7823888575661865, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9938387693758571}}, {"question": "\u6709\u2f00\u7535\u5b66\u5143\u4ef6\uff0c\u4e0a\u2faf\u6807\u6709\u201c35V 2200\u03bcF\u201d\u5b57\u6837\uff0c\u7531\u6b64\u53ef\u77e5\u8be5\u7535\u5b66\u5143\u4ef6\u662f\nA. \u7535\u611f\u5668\nB. \u7535\u963b\u5668\nC. \u7535\u5bb9\u5668\nD. \u7535\u6e90\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9635824648505846, "meta-math/MetaMath-Mistral-7B": 0.9964210342477144, "itpossible/Chinese-Mistral-7B-v0.1": 0.9467223191358647, "HuggingFaceH4/zephyr-7b-beta": 0.9997536212226686, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9855074599084556, "meta-llama/Meta-Llama-3-8B": 0.9680464133852525, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9967211620479647}}, {"question": "\u6839\u636e\u300a\u8054\u5408\u56fd\u56fd\u9645\u8d27\u7269\u9500\u552e\u5408\u540c\u516c\u7ea6\u300b\uff0c\u5356\u65b9\u6309\u4e70\u65b9\u7684\u53d1\u4ef7\u8981\u6c42\u53d1\u8fd0\u8d27\u7269\u65f6\nA. \u5356\u65b9\u7684\u53d1\u8d27\u884c\u4e3a\u4e0d\u6784\u6210\u63a5\u53d7\nB. \u5356\u65b9\u7684\u53d1\u8d27\u884c\u4e3a\u6784\u6210\u63a5\u53d7\nC. \u5356\u65b9\u7684\u53d1\u8d27\u884c\u4e3a\u6784\u6210\u53d1\u4ef7\nD. \u5356\u65b9\u5fc5\u987b\u4ee5\u4e66\u9762\u5f62\u5f0f\u8868\u793a\u63a5\u53d7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6864735687274994, "meta-math/MetaMath-Mistral-7B": 0.810374784715238, "itpossible/Chinese-Mistral-7B-v0.1": 0.648186553496865, "HuggingFaceH4/zephyr-7b-beta": 0.9963848539156049, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8520494320075328, "meta-llama/Meta-Llama-3-8B": 0.5083706432920275, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7068017120392797}}, {"question": "\u300a\u7ebd\u7ea6\u65f6\u62a5\u300b\u67d0\u9a7b\u534e\u8bb0\u8005\u5728\u56de\u5fc6\u5f55\u4e2d\u8ff0\u53ca\u201c\u5e7f\u5dde\u8058\u8bf7\u82cf\u8054\u987e\u95ee\u201d\u201c\u6b66\u6c49\u7fa4\u4f17\u96c6\u4f1a\u5e86\u795d\u80dc\u5229\u201d\u201c\u4e0a\u6d77\u7684\u82f1\u3001\u7f8e\u3001\u65e5\u79df\u754c\u5fd9\u7740\u589e\u5175\u52a9\u9632\u201d\u201c\u653b\u514b\u5357\u4eac\u201d\u201c\u5317\u4eac\u5468\u8fb9\u6218\u4e8b\u8fde\u8fde\u201d\u7b49\u3002\u8fd9\u4e9b\u60c5\u5f62\u51fa\u73b0\u4e8e\nA. \u4e94\u56db\u8fd0\u52a8\u65f6\u671f\nB. \u56fd\u6c11\u9769\u547d\u65f6\u671f\nC. \u8f9b\u4ea5\u9769\u547d\u65f6\u671f\nD. \u5168\u9762\u6297\u6218\u65f6\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7599191685088602}}, {"question": "\u4ee5\u4e0b\u6076\u610f\u4ee3\u7801\u4e2d\uff0c\u5c5e\u4e8e\u5b8f\u75c5\u6bd2\u7684\u662f\nA. Macro.Melissa\nB. Worm.Blaster.g\nC. Backdoor.Agobot.frt\nD. Trojan.huigezi.a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8883645709137903, "meta-math/MetaMath-Mistral-7B": 0.9989052225707753, "itpossible/Chinese-Mistral-7B-v0.1": 0.5899675946343427, "HuggingFaceH4/zephyr-7b-beta": 0.916816815199181, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9634133246036505, "meta-llama/Meta-Llama-3-8B": 0.9780270784435742, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.993140878484804}}, {"question": "\u5728\u4ea7\u4e1a\u5e02\u573a\u4e0a\uff0c\u4ea7\u4e1a\u8d2d\u4e70\u8005\u5bf9\u4ea7\u4e1a\u7528\u54c1\u7684\u9700\u6c42\u53d7\u4ef7\u683c\u6ce2\u52a8\u7684\u5f71\u54cd\u4e0d\u5927\uff0c\u8fd9\u8bf4\u660e\u4ea7\u4e1a\u5e02\u573a\u7684\u9700\u6c42\nA. \u5b8c\u5168\u5f39\u6027\nB. \u7f3a\u4e4f\u5f39\u6027\nC. \u5b8c\u5168\u65e0\u5f39\u6027\nD. \u5bcc\u6709\u5f39\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4968388227465335, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5669816732096951, "meta-llama/Meta-Llama-3-8B": 0.6563789424065777, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9830489353650731}}, {"question": "\u5728\u8ba1\u7b97\u673a\u5185\u90e8\u2f64\u673a\u5185\u7801\u2f7d\u4e0d\u2f64\u56fd\u6807\u7801\u8868\u793a\u6c49\u5b57\u7684\u539f\u56e0\u662f\nA. \u6709\u4e9b\u6c49\u5b57\u7684\u56fd\u6807\u7801\u4e0d\u552f\u2f00\uff0c\u2f7d\u673a\u5185\u7801\u552f\u2f00\nB. \u5728\u6709\u4e9b\u60c5\u51b5\u4e0b\uff0c\u56fd\u6807\u7801\u6709\u53ef\u80fd\u9020\u6210\u8bef\u89e3\nC. \u56fd\u6807\u7801\u662f\u56fd\u5bb6\u6807\u51c6\uff0c\u2f7d\u673a\u5185\u7801\u662f\u56fd\u9645\u6807\u51c6\nD. \u673a\u5185\u7801\u2f50\u56fd\u6807\u7801\u5bb9\u6613\u8868\u793a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5982186221592487, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6584250075399712}}, {"question": "\u9996\u6b21\u9610\u660e\u4eba\u4eec\u7684\u9006\u53cd\u5fc3\u7406\u5f62\u6210\u7684\u4e3b\u89c2\u539f\u56e0\u7684\u7f8e\u56fd\u5fc3\u7406\u5b66\u5bb6\u662f\nA. \u963f\u4ec0\nB. \u970d\u592b\u5170\nC. \u5e03\u6797\nD. \u7eb3\u666e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.32465461698188525, "meta-math/MetaMath-Mistral-7B": 0.2801288226217134, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5186714363452677, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u95f4\u6b47\u8bad\u7ec3\u6cd5\u662f\u6307\u5bf9\u591a\u6b21\u7ec3\u4e60\u65f6\u7684()\u4f5c\u51fa\u4e25\u683c\u89c4\u5b9a\uff0c\u53cd\u590d\u8fdb\u884c\u7ec3\u4e60\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002\nA. \u7ec3\u4e60\u65b9\u5f0f\nB. \u95f4\u6b47\u65b9\u5f0f\nC. \u7ec3\u4e60\u6b21\u6570\nD. \u95f4\u6b47\u65f6\u95f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5398223319786618, "meta-math/MetaMath-Mistral-7B": 0.5906171549085212, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8915208475908969, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8101879070444938, "meta-llama/Meta-Llama-3-8B": 0.6283613729621246, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.799865551681013}}, {"question": "\u63d0\u51fa\u201c\u5730\u4e2d\u6d77\u8054\u76df\u201d\u7684\u662f\nA. \u5e03\u4ec0\nB. \u8428\u79d1\u9f50\nC. \u5e03\u83b1\u5c14\nD. \u5b89\u5357\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3984585186882418, "HuggingFaceH4/zephyr-7b-beta": 0.47781935470496195, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4296858733195488, "meta-llama/Meta-Llama-3-8B": 0.31911523504877376, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4918492058142746}}, {"question": "\u708e\u75c7\u4ecb\u8d28\u7684\u4e3b\u8981\u4f5c\u7528\u662f\u4f7f\nA. \u7ec4\u7ec7\u95f4\u6db2\u6e17\u900f\u538b\u589e\u9ad8\nB. \u7ec4\u7ec7\u5206\u89e3\u4ee3\u8c22\u589e\u5f3a\nC. \u5c40\u90e8\u6c22\u79bb\u5b50\u6d53\u5ea6\u589e\u9ad8\nD. \u8840\u7ba1\u6269\u5f20\u901a\u900f\u6027\u589e\u52a0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4084835335145503, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7050725142202846}}, {"question": "\u80fd\u964d\u4f4e\u4f53\u91cd\u3001\u8840\u538b\u548c\u5c3f\u9178\uff0c\u5e76\u80fd\u4fdd\u62a4\u80be\u810f\u7684\u964d\u7cd6\u836f\u7269\u662f\nA. \u745e\u683c\u5217\u5948\nB. \u897f\u683c\u5217\u6c40\nC. \u963f\u5361\u6ce2\u7cd6\nD. \u8fbe\u683c\u5217\u51c0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5443942915710394}}, {"question": "\u4ee5\u4e0b\u54ea\u4e00\u7c7b\u578b\u7684\u7ec6\u83cc\u9057\u4f20\u4ea4\u6362\uff0c\u5fc5\u9700\u4ee5\u76f4\u63a5\u7684\u7ec6\u80de\u4e0e\u7ec6\u80de\u7684\u63a5\u89e6\u4e3a\u524d\u63d0\nA. \u8f6c\u5316\nB. \u4ee5\u4e0a\u6240\u6709\nC. \u63a5\u5408\nD. \u8f6c\u5bfc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5230818743677376, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.43903016993053146, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4956219856186119, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.986795386246383}}, {"question": "\u67cf\u62c9\u56fe\u9ad8\u5ea6\u5f3a\u8c03\u56fd\u5bb6\u5bf9\u6559\u80b2\u7684\u91cd\u89c6\uff0c\u9ad8\u5ea6\u8bc4\u4ef7\u6559\u80b2\u5728\u4eba\u7684\u5851\u9020\u4e2d\u7684\u4f5c\u7528\uff0c\u5c06\u7b97\u672f\u3001\u51e0\u4f55\u3001\u5929\u6587\u3001\u97f3\u4e50\u5217\u5165\u6559\u5b66\u79d1\u76ee\uff0c\u8fd9\u56db\u95e8\u8bfe\u7a0b\u662f\u897f\u6b27\u4e2d\u4e16\u7eaa()\u7684\u6559\u80b2\u5185\u5bb9\u4e4b\u4e00..\nA. \u6559\u4f1a\u6559\u80b2\nB. \u9a91\u571f\u6559\u80b2\nC. \u96c5\u5178\nD. \u53e4\u57c3\u53ca\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6811217701824062, "meta-math/MetaMath-Mistral-7B": 0.917245169396798, "itpossible/Chinese-Mistral-7B-v0.1": 0.4557446136818995, "HuggingFaceH4/zephyr-7b-beta": 0.5510672902220434, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.69217933013342, "meta-llama/Meta-Llama-3-8B": 0.6717095786945153, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u3001\u4e59\u4e24\u6d41\u57df\u9664\u6d41\u57df\u690d\u88ab\u7387\u7532\u5927\u4e8e\u4e59\u5916\uff0c\u5176\u5b83\u6d41\u57df\u4e0b\u57ab\u9762\u56e0\u7d20\u548c\u6c14\u8c61\u56e0\u7d20\u5747\u76f8\u540c\uff0c\u5bf9\u76f8\u540c\u964d\u96e8\u6240\u5f62\u6210\u7684\u6d41\u91cf\u8fc7\u7a0b\uff0c\u7532\u6d41\u57df\u7684\u6d2a\u5cf0\u6d41\u91cf\u6bd4\u4e59\u6d41\u57df\u7684[ ]\u3002\nA. \u5cf0\u73b0\u65f6\u95f4\u665a\u3001\u6d2a\u5cf0\u6d41\u91cf\u5927\nB. \u5cf0\u73b0\u65f6\u95f4\u665a\u3001\u6d2a\u5cf0\u6d41\u91cf\u5c0f\nC. \u5cf0\u73b0\u65f6\u95f4\u65e9\u3001\u6d2a\u5cf0\u6d41\u91cf\u5927\nD. \u5cf0\u73b0\u65f6\u95f4\u65e9\u3001\u6d2a\u5cf0\u6d41\u91cf\u5c0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u6574\u5408\u8425\u9500\u4e3a\u624b\u6bb5\uff0c\u901a\u8fc7\u5ba2\u6237\u6ee1\u610f\u5b9e\u73b0\u5229\u6da6\u589e\u957f\u7684\u5e02\u573a\u8425\u9500\u7ba1\u7406\u54f2\u5b66\u6307\u7684\u662f\nA. \u63a8\u9500\u89c2\u5ff5\nB. \u5e02\u573a\u8425\u9500\u89c2\u5ff5\nC. \u4ea7\u54c1\u89c2\u5ff5\nD. \u751f\u4ea7\u89c2\u5ff5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9606801671095628, "meta-math/MetaMath-Mistral-7B": 0.9901301622338357, "itpossible/Chinese-Mistral-7B-v0.1": 0.9691880295257586, "HuggingFaceH4/zephyr-7b-beta": 0.9997878837022993, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9857162485233418, "meta-llama/Meta-Llama-3-8B": 0.9685453250130329, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9911248075634633}}, {"question": "\u6c34\u4fe3\u75c5\u662f\u4ec0\u4e48\u4e2d\u6bd2\u5f15\u8d77\u7684\nA. \u7837\nB. \u6c30\nC. \u82ef\nD. \u6c5e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8997728419961772, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8655087615386322, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7969207771335275}}, {"question": "\u6b63\u5e38\u6708\u7ecf\u671f\u65f6\uff0c\u5e38\u51fa\u73b0\u7684\u4e73\u5934\u6ea2\u6db2\u662f\nA. \u6d46\u6db2\u6027\u65e0\u8272\u6ea2\u6db2\nB. \u9ec4\u8272\u6216\u9ec4\u7eff\u8272\u6ea2\u6db2\nC. \u68d5\u8910\u8272\u6ea2\u6db2\nD. \u8840\u6027\u6ea2\u6db2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.40637542112739916, "meta-math/MetaMath-Mistral-7B": 0.8883973382176914, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9647632440755086, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9100219115109697, "meta-llama/Meta-Llama-3-8B": 0.6255106909075493, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u975e\u5355\u8c03\u903b\u8f91\u7684\u63d0\u51fa\u8005\u662f\nA. \u7ebd\u7279\nB. \u7ea6\u7ff0\u900a\nC. \u5e03\u83b1\u5c14\nD. \u56fe\u5c14\u654f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u56fd\u5bb6\u4e2d\u957f\u671f\u6559\u80b2\u6539\u9769\u548c\u53d1\u5c55\u89c4\u5212\u8981\uff082010-2020\uff09\u300b\u63d0\u51fa\uff0c\u6559\u80b2\u516c\u5e73\u662f\u793e\u4f1a\u516c\u5e73\u7684\u91cd\u8981\u57fa\u7840\u3002\u6559\u80b2\u516c\u5e73\u7684\u5173\u952e\u662f\nA. \u8fc7\u7a0b\u516c\u5e73\nB. \u65f6\u673a\u516c\u5e73\nC. \u8d77\u70b9\u516c\u5e73\nD. \u7ed3\u679c\u516c\u5e73\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u5409\u82ac\u5546\u54c1\u9700\u6c42\u66f2\u7ebf\u7684\u8868\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u5e73\u884c\u4e8e\u6a2a\u5750\u6807\nB. \u5411\u53f3\u4e0b\u65b9\u503e\u659c\nC. \u5411\u53f3\u4e0a\u65b9\u503e\u659c\nD. \u5782\u76f4\u4e8e\u6a2a\u5750\u6807\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.45089242388111916, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.41829522724991014, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u62bd\u67e5\u4ea7\u54c1\u7684\u5c3a\u2f28\u8fc7\u7a0b\u4e2d\uff0c\u5c06\u5176\u5c3a\u2f28\u5206\u6210\u82e5\u2f32\u7ec4\uff0c[3.5\uff0c6)\u662f\u5176\u4e2d\u7684\u2f00\u7ec4\uff0c\u62bd\u67e5\u51fa\u7684\u4e2a\u4f53\u5728\u8be5\u7ec4\u4e0a\u7684\u9891\u7387\u4e3a0.2\uff0c\u8be5\u7ec4\u4e0a\u7684\u76f4\u2f45\u56fe\u7684\u2fbc\u4e3ah\uff0c\u5219h\u4e3a\nA. 0.1 \nB. 0.2\nC. 0.05\nD. 0.08\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.507856715399924, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u9176\u4fc3\u53cd\u5e94\u4e2d\uff0c\u51b3\u5b9a\u53cd\u5e94\u7279\u5f02\u6027\u7684\u662f\nA. \u8f85\u9176\nB. \u9176\u86cb\u767d\nC. \u65e0\u673a\u79bb\u5b50\nD. \u6eb6\u6db2PH\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5835155124651066, "meta-math/MetaMath-Mistral-7B": 0.8146712885964567, "itpossible/Chinese-Mistral-7B-v0.1": 0.8142675774424317, "HuggingFaceH4/zephyr-7b-beta": 0.9228423699376186, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9312244531521592, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.905945149861078}}, {"question": "\u8840\u6d46\u4e2d\u7ef4\u751f\u7d20E\u7531\u4e0b\u5217\u54ea\u4e2a\u86cb\u767d\u8d28\u643a\u5e26\u81f3\u5404\u4e2a\u7ec4\u7ec7\nA. \u03b1-\u8102\u86cb\u767d\nB. \u03b2-\u8102\u86cb\u767d\nC. \u524d\u03b2-\u8102\u86cb\u767d\nD. \u8102\u86cb\u767d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.29863342676099575, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u660e\u786e\u5730\u63d0\u51fa\u201c\u5168\u515a\u529e\u62a5\uff0c\u7fa4\u4f17\u529e\u62a5\u201d\u4e3b\u5f20\u7684\u662f\nA. \u5217\u5b81\nB. \u9a6c\u514b\u601d\nC. \u6069\u683c\u65af\nD. \u6bdb\u6cfd\u4e1c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8021400434506374, "meta-math/MetaMath-Mistral-7B": 0.9651100420016513, "itpossible/Chinese-Mistral-7B-v0.1": 0.7050607448252848, "HuggingFaceH4/zephyr-7b-beta": 0.9995327834069703, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7945669746728165, "meta-llama/Meta-Llama-3-8B": 0.9168420577414912, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7783900942077124}}, {"question": "\u5173\u4e8e\u901a\u7535\u76f4\u5bfc\u7ebf\u5728\u5300\u5f3a\u78c1\u573a\u4e2d\u6240\u53d7\u7684\u5b89\u57f9\u529b\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u5b89\u57f9\u529b\u7684\u65b9\u5411\u603b\u662f\u5782\u76f4\u4e8e\u78c1\u573a\u7684\u65b9\u5411\nB. \u5b89\u57f9\u529b\u7684\u5927\u5c0f\u4e0e\u901a\u7535\u76f4\u5bfc\u7ebf\u548c\u78c1\u573a\u65b9\u5411\u7684\u5939\u89d2\u65e0\u5173\nC. \u5b89\u57f9\u529b\u7684\u65b9\u5411\u53ef\u4ee5\u4e0d\u5782\u76f4\u4e8e\u76f4\u5bfc\u7ebf\nD. \u5c06\u76f4\u5bfc\u7ebf\u4ece\u4e2d\u70b9\u6298\u6210\u76f4\u89d2\uff0c\u5b89\u57f9\u529b\u7684\u5927\u5c0f\u4e00\u5b9a\u53d8\u4e3a\u539f\u6765\u7684\u4e00\u534a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31110415356189847, "meta-math/MetaMath-Mistral-7B": 0.7201578783369469, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.429814807177486, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9427971578399703}}, {"question": "\u67d0\u6559\u6388\u8ba4\u4e3a\uff1a\u5143\u671d\u884c\u7701\u5236\u4e2d\u592e\u96c6\u6743\u662f\u79e6\u6c49\u4ee5\u6765\u90e1\u53bf\u5236\u4e2d\u592e\u96c6\u6743\u6a21\u5f0f\u7684\u8f83\u9ad8\u7ea7\u6f14\u5316\u5f62\u6001\uff0c\u4e5f\u662f\u4e24\u5b8b\u5426\u5b9a\u5510\u540e\u671f\u85e9\u9547\u5206\u6743\u7684\u7ee7\u7eed\uff0c\u76f8\u5f53\u4e8e\u81ea\u968b\u671d\u59cb\u7b2c\u4e09\u4e2a\u201c\u6b63\u4e00\u53cd\u4e00\u5408\u201d\u9636\u6bb5\u7684\u201c\u5408\u201d\u3002\u4f5c\u8005\u8ba4\u4e3a\nA. \u884c\u7701\u5236\u5b9e\u73b0\u4e86\u4e2d\u592e\u96c6\u6743\u548c\u5730\u65b9\u5206\u6743\u7684\u6709\u673a\u7ed3\u5408\nB. \u884c\u7701\u5236\u5de9\u56fa\u4e86\u5143\u671d\u7684\u4e2d\u592e\u7edf\u6cbb\u548c\u56fd\u5bb6\u7684\u7edf\u4e00\nC. \u884c\u7701\u5236\u4e3b\u8981\u7740\u773c\u4e8e\u653f\u6cbb\u4e0a\u7684\u7edf\u6cbb\u548c\u519b\u4e8b\u4e0a\u7684\u63a7\u5236\nD. \u884c\u7701\u5236\u501f\u9274\u4e86\u90e1\u53bf\u5236\u7684\u4f18\u70b9\uff0c\u6448\u5f03\u4e86\u4e24\u5b8b\u653f\u6cbb\u5236\u5ea6\u7684\u7f3a\u70b9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4701322766120167, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.543195932355548, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5927\u8dc3\u6b65\u524d\u7a7f\u8981\u6c42\u524d\u8df3\u8ddd\u79bb\u987b\uff08\uff09\u5f13\u6b65\u3002\nA. \u5927\u4e8e\nB. \u7b49\u4e8e\nC. \u4e0d\u505a\u8981\u6c42\nD. \u5c0f\u4e8e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3808476994002367, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4118112744772438, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4357316726014201}}, {"question": "\u5173\u4e8e\u56fd\u9645\u6050\u6016\u4e3b\u4e49\uff0c\u4e0b\u5217\u9519\u8bef\u7684\u63cf\u8ff0\u662f\u54ea\u4e00\u4e2a\nA. \u662f\u975e\u56fd\u5bb6\u884c\u4e3a\u4f53\u5a01\u80c1\u4e16\u754c\u5b89\u5168\u7684\u5178\u578b\u6837\u5f0f\nB. \u5df2\u7ecf\u6210\u4e3a\u5c11\u6570\u5927\u56fd\u63a8\u884c\u9738\u6743\u4e3b\u4e49\u548c\u5f3a\u6743\u653f\u6cbb\u7684\u501f\u53e3\nC. \u9700\u8981\u4e16\u754c\u5404\u56fd\u76f8\u4e92\u5408\u4f5c\uff0c\u5171\u540c\u5e94\u5bf9\nD. \u662f\u4e16\u754c\u5404\u56fd\u9762\u4e34\u7684\u6700\u4e3b\u8981\u5a01\u80c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.597771010754106, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ea4\u901a\u4fe1\u53f7\u5206\u4e3a\u54ea\u51e0\u79cd\nA. \u4eba\u884c\u6a2a\u9053\u706f\u4fe1\u53f7\nB. \u4ea4\u901a\u6307\u6325\u68d2\u4fe1\u53f7\u3001\u624b\u52bf\u4fe1\u53f7\nC. \u5176\u4ed6\u9009\u9879\u5747\u53ef\nD. \u6307\u6325\u706f\u4fe1\u53f7\u3001\u8f66\u9053\u706f\u4fe1\u53f7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c34\u4f4d\u6d41\u91cf\u5173\u7cfb\u66f2\u7ebf\u4f4e\u6c34\u5ef6\u957f\u65b9\u6cd5\u4e2d\u7684\u65ad\u6d41\u6c34\u4f4d\u4e3a()\nA. \u65ad\u9762\u4e2d\u6b7b\u6c34\u533a\u7684\u6c34\u4f4d\nB. \u6cb3\u5e8a\u6700\u4f4e\u70b9\nC. \u6c34\u4f4d\u4e3a\u96f6\nD. \u6d41\u91cf\u7b49\u4e8e\u96f6\u7684\u6c34\u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37325341258520345, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5257224213109671, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8638175445000765, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6649267334943828}}, {"question": "\u5173\u4e8e\u6cd5\u5f8b\u5236\u5b9a\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u6709\nA. \u793e\u4f1a\u7269\u8d28\u751f\u6d3b\u6761\u4ef6\u5bf9\u7edf\u6cbb\u9636\u7ea7\u610f\u5fd7\u7684\u5185\u5bb9\u5177\u6709\u51b3\u5b9a\u4f5c\u7528\uff0c\u56e0\u6b64\uff0c\u53cd\u6620\u7edf\u6cbb\u9636\u7ea7\u7684\u610f\u5fd7\u7684\u7acb\u6cd5\u90fd\u662f\u793e\u4f1a\u7269\u8d28\u751f\u6d3b\u6761\u4ef6\u7684\u53cd\u6620\nB. \u6cd5\u5f8b\u5236\u5b9a\u8fc7\u7a0b\u8d8a\u6c11\u4e3b\uff0c\u88ab\u7edf\u6cbb\u9636\u7ea7\u7684\u610f\u5fd7\u5c31\u4f1a\u53cd\u6620\u5f97\u8d8a\u5145\u5206\nC. \u6cd5\u5f8b\u5236\u5b9a\u7684\u76ee\u6807\u5728\u4e8e\u4ea7\u751f\u5177\u6709\u666e\u904d\u6027\u3001\u89c4\u8303\u6027\u3001\u5f3a\u5236\u6027\u7684\u6cd5\u5f8b\u89c4\u8303\uff0c\u5c06\u7edf\u6cbb\u9636\u7ea7\u7684\u610f\u5fd7\u4e0a\u5347\u4e3a\u56fd\u5bb6\u610f\u5fd7\nD. \u6cd5\u5f8b\u5236\u5b9a\u7684\u4e3b\u4f53\u662f\u7279\u5b9a\u7684\u56fd\u5bb6\u673a\u5173\uff0c\u5728\u6211\u56fd\uff0c\u7acb\u6cd5\u6743\u4e13\u5c5e\u4e8e\u6743\u529b\u673a\u5173\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5162696200841003}}, {"question": "\u7ebf\u6027\u89c4\u5212\u65b9\u6cd5\u662f\u786e\u5b9a\u591a\u79cd\u4ea7\u54c1\u4ea7\u91cf\u6700\u4f18\u7ec4\u5408\u51b3\u7b56\u7684\u6709\u6548\u65b9\u6cd5\u4e4b\u4e00\u3002\u800c\u6c42\u89e3\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u7684\u65b9\u6cd5\u5305\u62ec\nA. \u56fe\u89e3\u6cd5\u3001\u4ee3\u6570\u6cd5\u3001\u5355\u7eaf\u5f62\u6cd5\nB. \u4ee3\u6570\u6cd5\u3001\u5355\u7eaf\u5f62\u6cd5\u3001\u5fae\u5206\u6cd5\nC. \u56fe\u89e3\u6cd5\u3001\u4ee3\u6570\u6cd5\u3001\u5fae\u5206\u6cd5\nD. \u56fe\u89e3\u6cd5\u3001\u5355\u7eaf\u5f62\u6cd5\u3001\u5fae\u5206\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3998452016318697, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9931221309949517, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45961514132177633, "meta-llama/Meta-Llama-3-8B": 0.5917908630659813, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u988c\u795e\u7ecf\u4ece\u4e0b\u5217\u4f55\u7ed3\u6784\u51fa\u9885\u8154\nA. \u7834\u88c2\u5b54\nB. \u5706\u5b54\nC. \u830e\u4e73\u5b54\nD. \u5375\u5706\u5b54\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6709\u5173\u7ec6\u80de\u6838\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u6709\u4e1d\u5206\u88c2\u8fc7\u7a0b\u4e2d\u5b58\u5728\u6838\u819c\u6d88\u5931\u548c\u91cd\u65b0\u5f62\u6210\u7684\u73b0\u8c61\nB. \u5c0f\u5206\u5b50\u7269\u8d28\u53ef\u4ee5\u901a\u8fc7\u6838\u5b54\uff0c\u5927\u5206\u5b50\u7269\u8d28\u4e0d\u80fd\nC. \u86cb\u767d\u8d28\u662f\u7ec6\u80de\u6838\u4e2d\u67d3\u8272\u8d28\u7684\u7ec4\u6210\u6210\u5206\nD. \u7ec6\u80de\u6838\u4e2d\u53ef\u8fdb\u884c\u9057\u4f20\u7269\u8d28\u7684\u590d\u5236\u548c\u8f6c\u5f55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5293487359075433, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5495239649998085}}, {"question": "\u4ee5\u5584\u6076\u8bc4\u4ef7\u7684\u65b9\u5f0f\u6765\u8bc4\u4ef7\u548c\u8c03\u8282\u4eba\u4eec\u884c\u4e3a\u7684\u89c2\u5ff5\u548c\u89c4\u8303\uff0c\u662f\u4eba\u7c7b\u81ea\u6211\u5b8c\u5584\u7684\u4e00\u79cd\u4ef7\u503c\u6807\u51c6\uff0c\u8fd9\u6307\u7684\u662f\nA. \u4e60\u4fd7\nB. \u5b97\u6559\nC. \u6cd5\u5f8b\nD. \u9053\u5fb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8025464691428514, "meta-math/MetaMath-Mistral-7B": 0.7955372836768867, "itpossible/Chinese-Mistral-7B-v0.1": 0.854151017560034, "HuggingFaceH4/zephyr-7b-beta": 0.979809867902682, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8436088194833461, "meta-llama/Meta-Llama-3-8B": 0.9272221423702263, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9789867226632382}}, {"question": "\u4e0b\u5217\u56db\u7ec4\u5b57\u4e2d\uff0c\u5168\u662f\u5f62\u58f0\u5b57\u7684\u4e00\u7ec4\u662f\nA. \u540e\u7f8e\u78a7\u9014\nB. \u52c9\u6d77\u8c10\u76d7\nC. \u817e\u8d3c\u7ae0\u6c41\nD. \u5f92\u95ee\u8fa9\u80cc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5\u5411\u91cfa\uff0cb\u6ee1\u8db3$|a|=1,a\\cdot b=-1$\uff0c\u5219$a\\cdot(2a-b)$ \u7b49\u4e8e\nA. 3\nB. 4\nC. 2\nD. 0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.29539205153207015, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.325455072595945, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3512536987011174}}, {"question": "\u4ee5\u4e0b\u54ea\u9879\u4e0d\u662f\u516c\u5f00\u53d1\u884c\u7684\u4ee5\u5929\u6587\u7231\u597d\u8005\u4e3a\u4e3b\u8981\u8bfb\u8005\u7fa4\u7684\u6742\u5fd7\nA. Astronomer\nB. AstronomyNow\nC. Sky&Telescope\nD. \u5929\u6587\u7231\u597d\u8005\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e0b\u5217\u60e9\u6212\u63aa\u65bd\u4e2d\uff0c\u884c\u653f\u5904\u5206\u548c\u884c\u653f\u5904\u7f5a\u5171\u540c\u9002\u7528\u7684\u6709\nA. \u5f00\u9664\nB. \u8b66\u544a\nC. \u62d8\u7559\nD. \u7f5a\u6b3e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5036304519274085, "meta-math/MetaMath-Mistral-7B": 0.8733476548518889, "itpossible/Chinese-Mistral-7B-v0.1": 0.43686817683787627, "HuggingFaceH4/zephyr-7b-beta": 0.8242956476276396, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5804687337547755, "meta-llama/Meta-Llama-3-8B": 0.31712010892822357, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8001\u5e74\u4eba\u7528\u836f\u65f6\uff0c\u4e0d\u5b9c\u4e0e\u9e9d\u9999\u4fdd\u5fc3\u4e38\u540c\u65f6\u670d\u7528\u7684\u836f\u7269\u662f\nA. \u963f\u5361\u6ce2\u7cd6\nB. \u5730\u9ad8\u8f9b\nC. \u590d\u65b9\u7ef4\u751f\u7d20\nD. \u963f\u53f8\u5339\u6797\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.333183235354062, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4978740434545173}}, {"question": "\u76f8\u5bf9\u539f\u6709\u7684\u53d1\u5c55\uff0c\u8d44\u6e90\u578b\u57ce\u5e02\u65b0\u2f63\u671f\u7684\u53d1\u5c55\u4e3b\u8981\u7740\u773c\u4e8e\nA. \u5ef6\u2ed3\u539f\u6709\u4ea7\u4e1a\u94fe\nB. \u62d3\u5bbd\u8d44\u6e90\u8fdb\u2f1d\u901a\u9053\nC. \u5173\u95ed\u91cd\u6c61\u67d3\u4f01\u4e1a\nD. \u57f9\u80b2\u65b0\u7ecf\u6d4e\u589e\u2ed3\u70b9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5754010196870603, "HuggingFaceH4/zephyr-7b-beta": 0.9257431778078375, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6473869190161163, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8a00\u8bba\u4e2d\u6d89\u53ca\u5230\u7684\u4eba\u624d\u9009\u62d4\u5236\u5ea6\uff0c\u6309\u51fa\u73b0\u987a\u5e8f\u5148\u540e\u6392\u5217\u6b63\u786e\u7684\u662f\uff1aa\u5b66\u901a\u884c\u4fee\uff0c\u7ecf\u4e2d\u535a\u58eb\uff1bb\u5b97\u5e08\u975e\u6709\u519b\u529f\u8bba\uff0c\u4e0d\u5f97\u4e3a\u5c5e\u7c4d\uff1bc\u4e5d\u54c1\u8bbf\u4eba\uff0c\u552f\u95ee\u4e2d\u6b63\uff1bd\u98ce\u5439\u91d1\u699c\u843d\u51e1\u4e16\uff0c\u4e09\u5341\u4e09\u4eba\u540d\u5b57\u9999\nA. cbda\nB. bacd\nC. bdac\nD. acbd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3881207341645942, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8f66\u8f86\u56e0\u6545\u969c\u5fc5\u987b\u5728\u9ad8\u901f\u516c\u8def\u505c\u8f66\u65f6\uff0c\u5e94\u5728\u8f66\u540e\u65b9\u81f3\u5c11\u591a\u5c11\u7c73\u5904\u8bbe\u7f6e\u6545\u969c\u8b66\u544a\u6807\u5fd7\uff0c\u591c\u95f4\u8fd8\u9700\u5f00\u542f\u793a\u5ed3\u706f\u548c\u540e\u4f4d\u706f\nA. 50\nB. 200\nC. 100\nD. 150\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36010898878387526, "meta-math/MetaMath-Mistral-7B": 0.4527235818153333, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5316\u5b66\u6bd2\u5242\u7684\u4f24\u5bb3\u7279\u70b9\u6709\uff1a\u6740\u4f24\u8303\u56f4\u5927\u3001\u4f24\u5bb3\u9014\u5f84\u591a\u3001\uff08\uff09\u7b49\u3002\nA. \u6301\u7eed\u65f6\u95f4\u957f\nB. \u6b7b\u4ea1\u7387\u9ad8\nC. \u81f4\u75c5\u529b\u5f3a\nD. \u5177\u6709\u4f20\u67d3\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.45019295532128123, "meta-math/MetaMath-Mistral-7B": 0.8449126628462016, "itpossible/Chinese-Mistral-7B-v0.1": 0.36987570828713495, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4703895934361933, "meta-llama/Meta-Llama-3-8B": 0.5236882854811112, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.35823796793174434}}, {"question": "\u751f\u6001\u7cfb\u7edf\u53cd\u670d\u52a1\u662f\u6307\u968f\u7740\u53d7\u635f\u7684\u81ea\u7136\u751f\u6001\u7cfb\u7edf\u7ed3\u6784\u548c\u529f\u80fd\u5728\u4eba\u7c7b\u4e3b\u52a8\u5e72\u9884\u4fdd\u62a4\u63aa\u65bd\u5b9e\u65bd\u4e0b\uff0c\u9010\u6b65\u5f97\u5230\u6062\u590d\u7684\u540c\u65f6\uff0c\u751f\u6001\u7cfb\u7edf\u5bf9\u4eba\u7c7b\u65e5\u5e38\u751f\u6d3b\u548c\u751f\u4ea7\u6d3b\u52a8\u4ea7\u751f\u7684\u8d1f\u9762\u5f71\u54cd\u3002\u6839\u636e\u4e0a\u8ff0\u5b9a\u4e49\uff0c\u4e0b\u5217\u4e0d\u5c5e\u4e8e\u751f\u6001\u7cfb\u7edf\u53cd\u670d\u52a1\u7684\u662f\nA. \u56e0\u7981\u6b62\u4f7f\u7528\u9664\u8349\u5242\uff0c\u519c\u6c11\u4e0d\u5f97\u4e0d\u6295\u5165\u66f4\u5927\u7684\u4eba\u529b\u6210\u672c\u6765\u62d4\u6389\u6742\u8349\nB. \u67d0\u5730\u52a8\u7269\u4fdd\u62a4\u5de5\u4f5c\u5f00\u5c55\u4ee5\u6765\uff0c\u7315\u7334\u6570\u91cf\u5267\u589e\uff0c\u5b83\u4eec\u5e38\u9a9a\u6270\u5f53\u5730\u5c45\u6c11\nC. \u67d0\u5730\u4fee\u5efa\u6c34\u575d\u6539\u5584\u7ecf\u6d4e\u72b6\u51b5\u7684\u540c\u65f6\uff0c\u4e5f\u4f7f\u4e00\u90e8\u5206\u5386\u53f2\u9057\u8ff9\u906d\u5230\u7834\u574f\nD. \u57ce\u5e02\u5e72\u9053\u79cd\u690d\u7684\u5927\u91cf\u6cd5\u56fd\u68a7\u6850\uff0c\u662f\u8bf1\u53d1\u5e02\u6c11\u8fc7\u654f\u6027\u9f3b\u708e\u7684\u91cd\u8981\u56e0\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.34412869855849515, "HuggingFaceH4/zephyr-7b-beta": 0.9883723994856674, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5264099920580938, "meta-llama/Meta-Llama-3-8B": 0.3756998283262494, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6cd5\u7406\u5b66\uff0c\u4e0b\u5217\u8868\u8ff0\u9519\u8bef\u7684\u662f\nA. \u6cd5\u7406\u5b66\u662f\u4e00\u95e8\u4ee5\u6cd5\uff08\u6216\u6cd5\u5f8b\uff09\u8fd9\u4e00\u793e\u4f1a\u73b0\u8c61\u53ca\u5176\u89c4\u5f8b\u4e3a\u7814\u7a76\u5bf9\u8c61\u7684\u793e\u4f1a\u79d1\u5b66\nB. \u6cd5\u7406\u5b66\u5728\u6cd5\u5b66\u4f53\u7cfb\u4e2d\u5360\u6709\u7279\u6b8a\u7684\u5730\u4f4d\uff0c\u5b83\u662f\u6cd5\u5b66\u7684\u4e00\u822c\u7406\u8bba\u3001\u57fa\u7840\u7406\u8bba\u548c\u65b9\u6cd5\u8bba\nC. \u6cd5\u7406\u5b66\u662f\u4ece\u603b\u4f53\u4e0a\u7814\u7a76\u6cd5\u548c\u6cd5\u5f8b\u73b0\u8c61\u7684\u4e00\u822c\u89c4\u5f8b\uff0c\u7814\u7a76\u6cd5\u7684\u4ea7\u751f\u3001\u672c\u8d28\u3001\u4f5c\u7528\u3001\u53d1\u5c55\u7b49\u57fa\u672c\u95ee\u9898\nD. \u6cd5\u7406\u5b66\u4e0e\u90e8\u95e8\u6cd5\u5b66\u4e4b\u95f4\u662f\u201c\u4e00\u822c\u201d\u4e0e\u201c\u7279\u6b8a\u201d\u7684\u5173\u7cfb\u3002\u6cd5\u7406\u5b66\u662f\u901a\u8fc7\u5bf9\u6240\u6709\u90e8\u95e8\u6cd5\u6750\u6599\u8fdb\u884c\u9ad8\u5ea6\u62bd\u8c61\u6982\u62ec\u800c\u83b7\u5f97\u6750\u6599\u6765\u6e90\u7684\uff0c\u6240\u4ee5\u6cd5\u7406\u5b66\u65e2\u63d0\u4f9b\u4e86\u7814\u7a76\u90e8\u95e8\u6cd5\u5b66\u7684\u7acb\u573a\u3001\u89c2\u70b9\u548c\u65b9\u6cd5\uff0c\u540c\u65f6\u5b83\u6240\u9610\u8ff0\u7684\u57fa\u672c\u6982\u5ff5\u3001\u57fa\u672c\u539f\u7406\u548c\u57fa\u672c\u77e5\u8bc6\uff0c\u5bf9\u90e8\u95e8\u6cd5\u5b66\u7684\u7814\u7a76\u53c8\u5177\u6709\u6307\u5bfc\u610f\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u201c\u6d3b\u5b50\u9f99\u201c\u548c\u201c\u6d3b\u6b66\u677e\u201d\u4e4b\u79f0\u7684\u662f\u6f14\u5458\nA. \u5c1a\u548c\u7389\u4e0e\u76d6\u53eb\u5929\nB. \u76d6\u53eb\u5929\u4e0e\u9a6c\u8fde\u826f\nC. \u76d6\u53eb\u5929\u4e0e\u5468\u4fe1\u82b3\nD. \u6768\u5c0f\u697c\u4e0e\u76d6\u53eb\u5929\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2986334267609957, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u7535\u6c14\u6bcd\u7ebf\u9001\u7535\u65f6\u5e94\u5148\u5408\nA. \u51fa\u7ebf\u5f00\u5173\nB. \u7535\u538b\u4e92\u611f\u5668\nC. \u6bcd\u8054\u5f00\u5173\nD. \u7535\u6e90\u5f00\u5173\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "SSL \u534f\u8bae\u662f\u5bf9\u79f0\u5bc6\u7801\u6280\u672f\u548c\u516c\u94a5\u5bc6\u7801\u6280\u672f\u76f8\u7ed3\u5408\u7684\u534f\u8bae\uff0c\u8be5\u534f\u8bae\u4e0d\u80fd\u63d0\u4f9b\u7684\u5b89\u5168\u670d\u52a1\u662f\nA. \u5b8c\u6574\u6027\nB. \u53ef\u7528\u6027\nC. \u53ef\u8ba4\u8bc1\u6027\nD. \u4fdd\u5bc6\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44805845996875854, "meta-math/MetaMath-Mistral-7B": 0.7444697016914468, "itpossible/Chinese-Mistral-7B-v0.1": 0.5784327657967581, "HuggingFaceH4/zephyr-7b-beta": 0.9112851842601816, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5963071863888576, "meta-llama/Meta-Llama-3-8B": 0.3827714968312363, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8eRoBERTa\u7684\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f\nA. \u4e0d\u505aNSP\u4efb\u52a1\nB. \u91c7\u7528\u9759\u6001\u63a9\u7801\u673a\u5236\nC. \u91c7\u7528\u66f4\u591a\u8bad\u7ec3\u6570\u636e\nD. \u8bad\u7ec3\u91c7\u7528\u66f4\u5927batch size\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3220562534414596, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u4e13\u4e1a\u6237\u53bb\u5e74\u6bcf\u516c\u9877\u4ea7\u7cae\u98df9400\u5343\u514b\uff0c\u6bd4\u524d\u5e74\u589e\u4ea7\u4e8c\u6210\uff0c\u524d\u5e74\u6bcf\u516c\u9877\u4ea7\u7cae\u98df()\u5343\u514b\nA. 9400*(1-20%)\nB. 9400/(1-20%)\nC. 9400*(1+20%)\nD. 9400/(1+20%)\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u6cd5\u5f8b\u6240\u4f53\u73b0\u7684\u56fd\u5bb6\u610f\u5fd7\u8d77\u6700\u7ec8\u51b3\u5b9a\u4f5c\u7528\u7684\u56e0\u7d20\u662f\nA. \u4e00\u5b9a\u7684\u793e\u4f1a\u7269\u8d28\u751f\u6d3b\u6761\u4ef6\nB. \u56fd\u5bb6\u7684\u5386\u53f2\u4f20\u7edf\nC. \u56fd\u5bb6\u653f\u6743\u7684\u7ec4\u7ec7\u5f62\u5f0f\nD. \u56fd\u5bb6\u7684\u9636\u7ea7\u7ed3\u6784\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5825616614053231, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5168\u5fc3\u5168\u610f\u4e3a\u4eba\u6c11\u670d\u52a1\u662f\u653f\u5e9c\u516c\u5171\u5173\u7cfb\u7684\nA. \u5b97\u65e8\nB. \u76ee\u6807\nC. \u8ffd\u6c42\nD. \u65b9\u5411\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4167997488575099, "meta-math/MetaMath-Mistral-7B": 0.5401621603820909, "itpossible/Chinese-Mistral-7B-v0.1": 0.6034309437228907, "HuggingFaceH4/zephyr-7b-beta": 0.9750125196829529, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6609139288245792, "meta-llama/Meta-Llama-3-8B": 0.6468110805314542, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6268009995616401}}, {"question": "\u300a\u84dd\u8272\u591a\u7459\u6cb3\u5706\u821e\u66f2\u300b\u7684\u4f5c\u8005\u662f\u662f\u5965\u5730\u5229\u4f5c\u66f2\u5bb6\nA. \u674e\u65af\u7279\nB. \u8d1d\u591a\u82ac\nC. \u7ea6\u7ff0\u2022\u65bd\u7279\u52b3\u65af\nD. \u5df4\u8d6b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.32465461698188525, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5486567472042243, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9430087239219372, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9972959662791459}}, {"question": "\u5b66\u751f\u6216\u8fd0\u53d1\u52a8\u53ca\u6559\u7ec3\u5458\u5728\u6bd4\u8d5b\u4e2d\u5177\u6709\u89c4\u610f\u8bc6\u53ca\u7eaa\u5f8b\u4fee\u517b\uff0c\u8fd9\u662f\u6587\u660e\u7684\u8868\u73b0\uff0c\u4e5f\u662f\u5bf9\u4f53\u80b2\u4e8b\u4e1a\u5177\u6709\u9ad8\u5ea6\u8d23\u4efb\u611f\u7684\u8868\u8fbe\uff0c\u662f\u4e00\u79cd\nA. \u9053\u5fb7\u4e49\u52a1\nB. \u9053\u5fb7\u884c\u4e3a\nC. \u9053\u5fb7\u8868\u8fbe\nD. \u9053\u5fb7\u4fee\u517b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u75c5\u6bd2\u5728\u5bbf\u4e3b\u7ec6\u80de\u5185\u7684\u590d\u5236\u5468\u671f\u8fc7\u7a0b\uff0c\u6b63\u786e\u7684\u63cf\u8ff0\u662f\nA. \u5438\u9644\u3001\u8131\u58f3\u3001\u751f\u7269\u5408\u6210\u3001\u6210\u719f\u53ca\u91ca\u653e\nB. \u5438\u9644\u3001\u7ed3\u5408\u3001\u7a7f\u5165\u3001\u751f\u7269\u5408\u6210\u3001\u6210\u719f\u53ca\u91ca\u653e\nC. \u7279\u5f02\u6027\u7ed3\u5408\u3001\u8131\u58f3\u3001\u590d\u5236\u3001\u7ec4\u88c5\u53ca\u91ca\u653e\nD. \u5438\u9644\u3001\u7a7f\u5165\u3001\u8131\u58f3\u3001\u751f\u7269\u5408\u6210\u3001\u7ec4\u88c5\u6210\u719f\u4e0e\u91ca\u653e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5321685538972363}}, {"question": "1989\u5e74\u7f8e\u56fd\u5e03\u4ec0\u653f\u5e9c\u63d0\u51fa\u4e86\u8d85\u8d8a\u904f\u5236\u6218\u7565\uff0c\u5176\u6838\u5fc3\u5185\u5bb9\u662f\u4ec0\u4e48\nA. \u653e\u5f03\u5bf9\u82cf\u8054\u7684\u519b\u4e8b\u904f\u5236\uff0c\u505c\u6b62\u6838\u519b\u5907\u7ade\u8d5b\nB. \u6269\u5927\u81ea\u7531\u56fd\u5bb6\u5927\u5bb6\u5ead\uff0c\u628a\u82cf\u8054\u52bf\u529b\u63a8\u56de\u672c\u571f\nC. \u628a\u82cf\u8054\u548c\u4e1c\u6b27\u56fd\u5bb6\u7eb3\u5165\u897f\u65b9\u793e\u4f1a\u4f53\u7cfb\nD. \u4ee5\u5b9e\u529b\u4e3a\u540e\u76fe\uff0c\u52a0\u5f3a\u5bf9\u7b2c\u4e09\u4e16\u754c\u7684\u6e17\u900f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3191152350487737, "HuggingFaceH4/zephyr-7b-beta": 0.9939841980427967, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.36144170536635384, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u70e7\u4f24\u60a3\u8005\u7684\u4e3b\u8981\u8fd0\u52a8\u6b63\u786e\u7684\u662f\nA. \u82e5\u65e0\u7981\u5fcc\uff0c\u5e94\u5c3d\u65e9\u8fdb\u884c\nB. \u53ea\u505a\u60a3\u4fa7\uff0c\u8d8a\u591a\u8d8a\u597d\nC. \u6bcf\u6b21\u90fd\u8981\u5728\u6cbb\u7597\u5e08\u6307\u5bfc\u4e0b\u8fdb\u884c\nD. \u63d0\u9ad8\u60a3\u8005\u7684\u81ea\u4fe1\u5fc3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5305\u62ec\u4e00\u4e2a\u6216\u4e00\u4e2a\u4ee5\u4e0a\u8f83\u77ed\u547d\u9898\u4f5c\u4e3a\u5176\u81ea\u8eab\u7684\u4e00\u90e8\u5206\u7684\u547d\u9898\u79f0\u4e3a\nA. \u5426\u5b9a\u547d\u9898\nB. \u5206\u6790\u547d\u9898\nC. \u590d\u5408\u547d\u9898\nD. \u5168\u79f0\u547d\u9898\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.944861735880653, "meta-math/MetaMath-Mistral-7B": 0.9918198696697161, "itpossible/Chinese-Mistral-7B-v0.1": 0.6805758991011608, "HuggingFaceH4/zephyr-7b-beta": 0.9817375756009502, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.965321640494172, "meta-llama/Meta-Llama-3-8B": 0.8582584253077743, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9357187284063828}}, {"question": "\u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u201c\u9e21\u201d\u901a\u6307\u7684\u4e00\u9879\u662f\nA. \u5c0f\u660e\u4ece\u5c0f\u5c31\u559c\u6b22\u9e21 \nB. \u517b\u9e21\u573a\u91cc\u6709\u8bb8\u591a\u53ea\u9e21\nC. \u738b\u5976\u5976\u517b\u4e86\u4e8c\u5341\u53ea\u9e21\nD. \u5f20\u4e09\u4e00\u4e2a\u4eba\u5403\u4e86\u4e00\u53ea\u9e21\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4420834850852256, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.29660173325630934, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.44149038699937615}}, {"question": "\u4ee5\u4e0b\u6709\u5173\u81ea\u7136\u4eba\u4eba\u683c\u6743\u4e0e\u8eab\u4efd\u6743\u5f02\u540c\u7684\u8868\u8ff0\uff0c\u4e0d\u6b63\u786e\u7684\u9009\u9879\u662f\nA. \u4eba\u683c\u6743\u59cb\u4e8e\u51fa\u751f\uff0c\u8eab\u4efd\u6743\u90a3\u4e48\u4ee5\u53d6\u5f97\u4e00\u5b9a\u8eab\u4efd\u4e3a\u524d\u63d0\nB. \u4eba\u683c\u6743\u53d7\u5230\u4fb5\u5bb3\u540e\u6743\u5229\u4eba\u53ef\u4ee5\u8bf7\u6c42\u7cbe\u795e\u635f\u5bb3\u8d54\u507f\uff0c\u8eab\u4efd\u6743\u90a3\u4e48\u4e0d\u53ef\u4ee5\nC. \u4eba\u683c\u6743\u4e0e\u8eab\u4efd\u6743\u5747\u6ca1\u6709\u76f4\u63a5\u8d22\u4ea7\u5185\u5bb9\nD. \u4eba\u683c\u6743\u4e0e\u8eab\u4efd\u6743\u5747\u5c5e\u4e8e\u652f\u914d\u6743\u548c\u7edd\u5bf9\u6743\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3904690953095985, "meta-math/MetaMath-Mistral-7B": 0.8020749577024483, "itpossible/Chinese-Mistral-7B-v0.1": 0.511193498646212, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5514773161455352, "meta-llama/Meta-Llama-3-8B": 0.5052963230999333, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9325518916190451}}, {"question": "\u82e5\u51fd\u6570$f(x)=log_{2}(1+x)-log_{2}(1-x)$\u5728[a,b]\u4e0a\u7684\u6700\u5927\u503c\u4e0e\u6700\u5c0f\u503c\u4e4b\u548c\u6070\u4e3a0\uff0c\u5219\u5b9e\u6570a,b\u6ee1\u8db3\nA. -1<=a<b<=1\u4e14a+b=0\nB. -1<a<b<1\u4e14a+b=0\nC. a<b\u4e14a+b=0\nD. a=b=0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e45\u75c5\u754f\u5bd2\u7684\u4e3b\u8981\u75c5\u56e0\u662f\nA. \u98ce\u90aa\u88ad\u8868\nB. \u5bd2\u90aa\u5185\u4fb5\nC. \u9633\u6c14\u865a\u8870\nD. \u6e7f\u90aa\u5916\u88ad\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f3a\u5236\u8bbf\u95ee\u63a7\u5236\uff08MAC\uff09\u662f\u4e00\u79cd\u4e0d\u5141\u8bb8\u4e3b\u4f53\u5e72\u6d89\u7684\u8bbf\u95ee\u63a7\u5236\u7c7b\u578b\u3002\u6839\u636e MAC \u7684\u5b89\u5168\u7ea7\u522b\uff0c\u7528\u6237\u4e0e\u8bbf\u95ee\u7684\u4fe1\u606f\u7684\u8bfb\u5199\u5173\u7cfb\u6709\u56db\u79cd\u7c7b\u578b\uff0c\u5176\u4e2d\u80fd\u4fdd\u8bc1\u6570\u636e\u5b8c\u6574\u6027\u7684\u8bfb\u5199\u7ec4\u5408\u65b9\u5f0f\u662f\nA. \u4e0a\u8bfb-\u4e0b\u5199\nB. \u4e0b\u8bfb-\u4e0a\u5199\nC. \u4e0b\u8bfb-\u4e0b\u5199\nD. \u4e0a\u8bfb-\u4e0a\u5199\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u51ef\u672b\u5c14\u8bf4\uff1a\u201c\u6211\u4eec\u5e0c\u671b\u6210\u4e3a\u4e00\u4e2a\u73b0\u4ee3\u5316\u56fd\u5bb6\uff0c\u6211\u4eec\u7684\u5934\u8111\u613f\u610f\u63a5\u53d7\u73b0\u65f6\u4ee3\u601d\u60f3\uff0c\u4f46\u6211\u4eec\u4ecd\u5f97\u4fdd\u6301\u81ea\u8eab\u4e0d\u53d8\u3002\u201d\u4e3a\u6b64\u4ed6\u63a8\u884c\u7684\u6539\u9769\u6709\uff1aa\u5e9f\u9664\u653f\u6559\u5408\u4e00\uff1bb\u7528\u62c9\u4e01\u5b57\u6bcd\u62fc\u5199\u571f\u8033\u5176\u8bed\uff1bc\u8d70\u82cf\u4fc4\u53d1\u5c55\u9053\u8def\uff1bd\u5927\u529b\u53d1\u5c55\u519c\u4e1a\nA. bc\nB. bd\nC. ab\nD. ac\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2938890773609541, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.532807709857952, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.333183235354062, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4633112104759485}}, {"question": "\u6559\u80b2\u7684\u672c\u8d28\u7279\u5f81\u662f ()\nA. \u7cfb\u7edf\u6027\nB. \u77e5\u8bc6\u6027\nC. \u79d1\u5b66\u6027\nD. \u80b2\u4eba\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8135883552998274, "meta-math/MetaMath-Mistral-7B": 0.9604145159823108, "itpossible/Chinese-Mistral-7B-v0.1": 0.8252384048559013, "HuggingFaceH4/zephyr-7b-beta": 0.9954949782698559, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8882240483469878, "meta-llama/Meta-Llama-3-8B": 0.9201752338528308, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.994194703546759}}, {"question": "\u4ece\u6cd5\u5f8b\u7684\u90e8\u95e8\u8fdb\u884c\u5206\u7c7b\uff0c\u6cd5\u5b66\u53ef\u5206\u4e3a\nA. \u5baa\u6cd5\u5b66\u3001\u6c11\u6cd5\u5b66\u3001\u5211\u6cd5\u5b66\u7b49\nB. \u7406\u8bba\u6cd5\u5b66\u548c\u5e94\u7528\u6cd5\u5b66\nC. \u6cd5\u5b66\u672c\u79d1\u548c\u6cd5\u5b66\u8fb9\u7f18\u5b66\u79d1\nD. \u56fd\u5185\u6cd5\u5b66\u3001\u56fd\u9645\u6cd5\u5b66\u3001\u6cd5\u5f8b\u53f2\u5b66\u3001\u6bd4\u8f83\u6cd5\u5b66\u548c\u5916\u56fd\u6cd5\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7497655048615822}}, {"question": "\u663e\u793a\u5668\u7684\u4e3b\u8981\u6280\u672f\u6307\u6807\u4e4b\u4e00\u662f\nA. \u8017\u7535\u91cf \nB. \u91cd\u91cf\nC. \u5206\u8fa8\u7387\nD. \u4eae\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9744702900727378, "meta-math/MetaMath-Mistral-7B": 0.9990954819286846, "itpossible/Chinese-Mistral-7B-v0.1": 0.9607042213358736, "HuggingFaceH4/zephyr-7b-beta": 0.9997697528872082, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9936794408018186, "meta-llama/Meta-Llama-3-8B": 0.9533770035973239, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9992328106377004}}, {"question": "1879\u5e74\uff0c\u4e0a\u6d77\u79df\u754c\u7ba1\u7406\u5f53\u5c40\u4e3a\u4e25\u5b88\u7537\u5973\u4e4b\u5927\u9632\uff0c\u89c4\u5b9a\u4e00\u8f86\u4eba\u529b\u8f66\u53ea\u80fd\u8f7d\u5ba2\u4e00\u540d\uff0c\u5982\u4e0d\u9075\u884c\uff0c\u4fbf\u8981\u7f5a\u94b1\u3002\u8fd9\u4e00\u89c4\u5b9a\nA. \u4e3a\u6c7d\u8f66\u62c9\u5ba2\u63d0\u4f9b\u65b9\u4fbf\nB. \u4e25\u91cd\u8fdd\u80cc\u4e86\u793e\u60c5\u6c11\u610f\nC. \u987a\u5e94\u4e86\u5386\u53f2\u53d1\u5c55\u8d8b\u52bf\nD. \u4f53\u73b0\u65b0\u65e7\u6742\u9648\u7684\u73b0\u5b9e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679c\u5176\u4ed6\u56e0\u7d20\u4fdd\u6301\u4e0d\u53d8\uff0c\u4e0b\u5217\u56db\u79cd\u60c5\u51b5\u4e00\u5b9a\u4f1a\u4f7f\u9700\u6c42\u51cf\u5c11\u7684\u662f\nA. \u6536\u5165\u589e\u52a0\uff0c\u540c\u65f6\u66ff\u4ee3\u54c1\u7684\u4ef7\u683c\u4e0b\u964d\nB. \u6536\u5165\u51cf\u5c11\uff0c\u540c\u65f6\u4e92\u8865\u54c1\u7684\u4ef7\u683c\u4e0b\u964d\nC. \u6536\u5165\u589e\u52a0\uff0c\u540c\u65f6\u66ff\u4ee3\u54c1\u7684\u4ef7\u683c\u4e0a\u6da8\nD. \u6536\u5165\u51cf\u5c11\uff0c\u540c\u65f6\u4e92\u8865\u54c1\u7684\u4ef7\u683c\u4e0a\u6da8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe\u51fd\u6570 $f(x)=\\frac{1}{e^{\\frac{x}{x-1}}-1}$\uff0c \u5219\nA. $x=0$ \u662f $f(x)$ \u7684\u7b2c\u4e8c\u7c7b\u95f4\u65ad\u70b9\uff0c $x=1$ \u662f $f(x)$ \u7684\u7b2c\u4e00\u7c7b\u95f4\u65ad\u70b9.\nB. $x=0$ \u662f $f(x)$ \u7684\u7b2c\u4e00\u7c7b\u95f4\u65ad\u70b9\uff0c $x=1$ \u662f $f(x)$ \u7684\u7b2c\u4e8c\u7c7b\u95f4\u65ad\u70b9;\nC. $x=0\uff0c x=1$ \u90fd\u662f $f(x)$ \u7684\u7b2c\u4e8c\u7c7b\u95f4\u65ad\u70b9;\nD. $x=0\uff0c x=1$ \u90fd\u662f $f(x)$ \u7684\u7b2c\u4e00\u7c7b\u95f4\u65ad\u70b9;\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3756998283262494, "meta-math/MetaMath-Mistral-7B": 0.5959422475042095, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4934439662247107, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u91cd\u590d\u6742\u5408\u4f53\u5728\u51cf\u6570\u5206\u88c2\u914d\u5bf9\u65f6\u5f62\u6210\u7684\u91cd\u590d\u5708\u4e2d\u5305\u542b\nA. \u4e24\u6761\u91cd\u590d\u67d3\u8272\u4f53\nB. \u4e00\u6761\u91cd\u590d\u67d3\u8272\u4f53\nC. \u4e24\u6761\u6b63\u5e38\u67d3\u8272\u4f53\nD. \u4e00\u6761\u6b63\u5e38\u67d3\u8272\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.557248599059922, "meta-math/MetaMath-Mistral-7B": 0.7237095186777898, "itpossible/Chinese-Mistral-7B-v0.1": 0.4925631496981296, "HuggingFaceH4/zephyr-7b-beta": 0.6761204303858225, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6981802211571649, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.500242836942423}}, {"question": "\u82e5 $\\lim _{x \\rightarrow 0}\\left(\\mathrm{e}^x+a x^2+b x\\right)^{\\frac{1}{x^2}}=1$\uff0c\u5219\nA. $a=\\frac{1}{2}\uff0c b=-1$.\nB. $a=-\\frac{1}{2}\uff0c b=-1$.\nC. $a=\\frac{1}{2}\uff0c b=1$\nD. $a=-\\frac{1}{2}\uff0c b=1$.\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.28850952576306876, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5408\u91d1\u7ed3\u6784\u94a2\u7684\u5f3a\u5ea6\u7b49\u7ea7\u662f\u6839\u636e\u4ec0\u4e48\u5212\u5206\u7684\nA. \u5c48\u670d\u5f3a\u5ea6\nB. \u6297\u62c9\u5f3a\u5ea6\nC. \u6297\u538b\u5f3a\u5ea6\nD. \u6297\u5f2f\u5f3a\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8087012436153567, "HuggingFaceH4/zephyr-7b-beta": 0.5859089482807428, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6603553072172473, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6d88\u8d39\u8005\u4e0d\u53ef\u80fd\u5728\u771f\u7a7a\u91cc\u505a\u51fa\u81ea\u5df1\u7684\u8d2d\u4e70\u51b3\u7b56\uff0c\u5176\u8d2d\u4e70\u51b3\u7b56\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d7\u5230\u6587\u5316\u3001\u793e\u4f1a\u3001\u4e2a\u4eba\u548c\u5fc3\u7406\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002\u5176\u4e2d\uff0c\u793e\u4f1a\u89d2\u8272\u4e0e\u5730\u4f4d\u5c5e\u4e8e\nA. \u793e\u4f1a\u56e0\u7d20\nB. \u5fc3\u7406\u56e0\u7d20\nC. \u6587\u5316\u56e0\u7d20\nD. \u4e2a\u4eba\u56e0\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5490452057727833, "meta-math/MetaMath-Mistral-7B": 0.9765705014732046, "itpossible/Chinese-Mistral-7B-v0.1": 0.7340173824588783, "HuggingFaceH4/zephyr-7b-beta": 0.8525156980407845, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9097287757130513, "meta-llama/Meta-Llama-3-8B": 0.9713961599502268, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9944986679180288}}, {"question": "\u201c\u6240\u6709\u7684\u903b\u8f91\u5b66\u5bb6\u90fd\u662f\u54f2\u5b66\u5bb6\uff0c\u6240\u6709\u7f8e\u5b66\u5bb6\u90fd\u662f\u903b\u8f91\u5b66\u5bb6\uff0c\u56e0\u6b64\uff0c\u6240\u6709\u7f8e\u5b66\u5bb6\u90fd\u662f\u54f2\u5b66\u5bb6\u201d\uff0c\u8fd9\u4e2a\u4e09\u6bb5\u8bba\u4e2d\uff0c\u5927\u9879\u662f\nA. \u4ee5\u4e0a\u90fd\u4e0d\u5bf9\nB. \u903b\u8f91\u5b66\u5bb6\nC. \u7f8e\u5b66\u5bb6\nD. \u54f2\u5b66\u5bb6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.28966338381871215, "HuggingFaceH4/zephyr-7b-beta": 0.4900269567193398, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5731662960932854}}, {"question": "PING \u547d\u4ee4\u4f7f\u2f64\u4e86\u54ea\u79cdICMP\nA. Echo reply\nB. Redirect\nC. Source quench\nD. Destination unreachable\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9728358309775096, "meta-math/MetaMath-Mistral-7B": 0.9992847476655735, "itpossible/Chinese-Mistral-7B-v0.1": 0.927222139856354, "HuggingFaceH4/zephyr-7b-beta": 0.9999867360136439, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9940541428160066, "meta-llama/Meta-Llama-3-8B": 0.7811188997442978, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u54ea\u4e00\u9879\u8868\u8ff0\u6700\u63a5\u8fd1\u4eba\u7c7b\u76ee\u524d\u5bf9\u7cfb\u5916\u884c\u661f\u536b\u661f\u7684\u63a2\u6d4b\u60c5\u51b5\nA. \u7cfb\u5916\u884c\u661f\u536b\u661f\u4eae\u5ea6\u592a\u4f4e\uff0c\u4eba\u7c7b\u4e0d\u53ef\u80fd\u63a2\u6d4b\u5230\nB. \u6700\u65b0\u53d1\u5c04\u7684\u7a7a\u95f4\u63a2\u6d4b\u5668\u5c06\u6709\u80fd\u529b\u63a2\u6d4b\u7cfb\u5916\u884c\u661f\u536b\u661f\nC. \u5df2\u7ecf\u63a2\u6d4b\u5230\u8d85\u8fc71\u4e07\u9897\u7cfb\u5916\u884c\u661f\u536b\u661f\nD. \u7cfb\u5916\u884c\u661f\u536b\u661f\u4e0d\u5b58\u5728\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.7830425345931723, "itpossible/Chinese-Mistral-7B-v0.1": 0.8082301810658774, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f01\u4e1a\u4ef7\u503c\u89c2\u7684\u4e2d\u5fc3\u5185\u5bb9\u662f\nA. \u4f01\u4e1a\u4f26\u7406\u89c2\nB. \u9053\u5fb7\u4f26\u7406\u89c2\nC. \u9053\u5fb7\u5584\u6076\u89c2\nD. \u4f01\u4e1a\u4eba\u6587\u89c2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5730277845462262, "meta-math/MetaMath-Mistral-7B": 0.8449126726090886, "itpossible/Chinese-Mistral-7B-v0.1": 0.5081293129570026, "HuggingFaceH4/zephyr-7b-beta": 0.9167347858055672, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8388534317718398, "meta-llama/Meta-Llama-3-8B": 0.3534716292209113, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7572163529183729}}, {"question": "\u4ee5\u4e0b\u54ea\u9879\u4e0d\u5c5e\u4e8e\u54f2\u5b66\u4ea7\u751f\u7684\u6761\u4ef6\nA. \u4eba\u4eec\u5f00\u59cb\u5173\u6ce8\u7ec8\u6781\u5173\u6000\u7684\u95ee\u9898\nB. \u4eba\u4eec\u6709\u65f6\u95f4\u53bb\u601d\u8003\u8fd9\u4e9b\u95ee\u9898\nC. \u751f\u4ea7\u529b\u5fc5\u987b\u5f97\u5230\u53d1\u5c55\nD. \u5fc5\u987b\u6709\u601d\u60f3\u7684\u81ea\u7531\u7684\u6761\u4ef6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.652639902638275, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9926129168402734, "meta-llama/Meta-Llama-3-8B": 0.9513751810221277, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9791028592746541}}, {"question": "\u5728\u4e00\u4e2a\u53cc\u661f\u7cfb\u7edf\u4e2d\uff0c\u4e3b\u661f\u4e0e\u4f34\u661f\u7684\u6e29\u5ea6\u76f8\u5dee\u4e0d\u591a\uff0c\u4e3b\u661f\u7684\u5149\u5ea6\u7ea6\u4e3a\u4f34\u661f\u768482\u500d\uff0c\u5149\u5ea6\u5dee\u5f02\u4e3b\u8981\u6e90\u4e8e\u4e24\u9897\u661f\u534a\u5f84\u7684\u5dee\u5f02\u3002\u6839\u636e\u4ee5\u4e0a\u4fe1\u606f\uff0c\u4f30\u7b97\u4e3b\u661f\u4e0e\u4f34\u661f\u7684\u534a\u5f84\u4e4b\u6bd4\u7ea6\u4e3a\nA. 8.5\nB. 9.1\nC. 9.7\nD. 10.3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3527809287490976, "itpossible/Chinese-Mistral-7B-v0.1": 0.29539205153207015, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.26560468668687814, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.35286194820709277}}, {"question": "\u7528()\u53ef\u4ee5\u4f30\u7b977.9804*3.025\u7684\u79ef\u5927\u7ea6\u662f\u591a\u5c11\nA. 7*4\nB. 8*4\nC. 7*3\nD. 8*3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.33065623127838456, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c14\u538b\u4e0e\u6d77\u62d4\u7684\u5173\u7cfb\u662f[ ]\u3002\nA. \u6b63\u6bd4\u5173\u7cfb\nB. \u6d77\u62d4\u6108\u9ad8\uff0c\u6c14\u538b\u6108\u9ad8\nC. \u6d77\u62d4\u6108\u9ad8\uff0c\u6c14\u538b\u6108\u4f4e\nD. \u53cd\u6bd4\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5970440396605693, "meta-math/MetaMath-Mistral-7B": 0.33767879625406966, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.839078511357405, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.371068906204966, "meta-llama/Meta-Llama-3-8B": 0.47935414222805517, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9864219702647851}}, {"question": "\u7eaa\u67d0\u4e0e\u590f\u67d0\u5230\u5a5a\u59fb\u767b\u8bb0\u673a\u5173\u7533\u8bf7\u767b\u8bb0\u7ed3\u5a5a\uff0c\u5a5a\u59fb\u767b\u8bb0\u673a\u5173\u4f9d\u6cd5\u4e88\u4ee5\u767b\u8bb0\u5e76\u53d1\u7ed9\u7ed3\u5a5a\u8bc1\u4e66\u3002\u636e\u6b64\uff0c\u4ea7\u751f\u7eaa\u67d0\u4e0e\u590f\u67d0\u592b\u59bb\u5173\u7cfb\u7684\u4e8b\u5b9e\u5c5e\u4e8e\nA. \u4e8b\u5b9e\u5173\u7cfb\nB. \u4e8b\u5b9e\u6784\u6210\nC. \u5355\u4e00\u7684\u6cd5\u5f8b\u4e8b\u5b9e\nD. \u6cd5\u5f8b\u4e8b\u4ef6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6570\u636e\u79d1\u5b66\u5bb6\u53ef\u80fd\u4f1a\u540c\u65f6\u4f7f\u7528\u591a\u4e2a\u7b97\u6cd5\uff08\u6a21\u578b\uff09\u8fdb\u884c\u9884\u6d4b\uff0c \u5e76\u4e14\u6700\u540e\u628a\u8fd9\u4e9b\u7b97\u6cd5\u7684\u7ed3\u679c\u96c6\u6210\u8d77\u6765\u8fdb\u884c\u6700\u540e\u7684\u9884\u6d4b\uff08\u96c6\u6210\u5b66\u4e60\uff09\uff0c\u4ee5\u4e0b\u5bf9\u96c6\u6210\u5b66\u4e60\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u5355\u4e2a\u6a21\u578b\u4e4b\u95f4\u6709\u9ad8\u76f8\u5173\u6027\nB. \u5355\u4e2a\u6a21\u578b\u90fd\u662f\u7528\u7684\u4e00\u4e2a\u7b97\u6cd5\nC. \u5355\u4e2a\u6a21\u578b\u4e4b\u95f4\u6709\u4f4e\u76f8\u5173\u6027\nD. \u5728\u96c6\u6210\u5b66\u4e60\u4e2d\u4f7f\u7528\u201c\u5e73\u5747\u6743\u91cd\u201d\u800c\u4e0d\u662f\u201c\u6295\u7968\u201d\u4f1a\u6bd4\u8f83\u597d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8249296805136787, "meta-math/MetaMath-Mistral-7B": 0.9976215502224335, "itpossible/Chinese-Mistral-7B-v0.1": 0.4280201266346246, "HuggingFaceH4/zephyr-7b-beta": 0.9983174069236186, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8672974114866204, "meta-llama/Meta-Llama-3-8B": 0.8897819828199236, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9201732466925712}}, {"question": "\u5bf9\u5f71\u54cd\u793e\u4f1a\u4e8b\u5b9e\u53d1\u751f\u3001\u53d8\u5316\u7684\u4e3b\u5ba2\u89c2\u56e0\u7d20\uff0c\u4ece\u5176\u56e0\u679c\u8054\u7cfb\u4e0a\u52a0\u4ee5\u8bf4\u660e\u7684\u8fc7\u7a0b\uff0c\u6307\u7684\u662f\u793e\u4f1a\u5b66\u7684\nA. \u89e3\u91ca\u6027\u529f\u80fd\nB. \u63cf\u8ff0\u6027\u529f\u80fd\nC. \u9884\u6d4b\u6027\u529f\u80fd\nD. \u89c4\u8303\u6027\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8129851336093732, "meta-math/MetaMath-Mistral-7B": 0.9562248457458443, "itpossible/Chinese-Mistral-7B-v0.1": 0.9228500155068334, "HuggingFaceH4/zephyr-7b-beta": 0.9657058589853441, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9069227070372403, "meta-llama/Meta-Llama-3-8B": 0.7534613912964941, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.912935528002872}}, {"question": "\u5173\u4e8e\u6cd5\u7684\u4f5c\u7528\u7684\u5bf9\u8c61\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u6709\nA. \u6cd5\u5f8b\u7684\u6307\u5f15\u4f5c\u7528\u7684\u5bf9\u8c61\u662f\u4e00\u822c\u4eba\u7684\u884c\u4e3a\nB. \u6cd5\u5f8b\u7684\u8bc4\u4ef7\u4f5c\u7528\u7684\u5bf9\u8c61\u662f\u4eba\u4eec\u76f8\u4e92\u95f4\u7684\u884c\u4e3a\nC. \u6cd5\u5f8b\u7684\u9884\u6d4b\u4f5c\u7528\u7684\u5bf9\u8c61\u662f\u81ea\u5df1\u7684\u884c\u4e3a\nD. \u6cd5\u7684\u5f3a\u5236\u4f5c\u7528\u7684\u5bf9\u8c61\u662f\u8fdd\u6cd5\u72af\u7f6a\u8005\u7684\u884c\u4e3a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7302477457306907, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "___\u7684\u5747\u6570\u7b49\u4e8e\u65b9\u5dee\u3002\nA. Poisson\u5206\u5e03\nB. \u4e8c\u9879\u5206\u5e03\nC. \u6b63\u6001\u5206\u5e03\nD. \u5bf9\u6570\u6b63\u6001\u5206\u5e03\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4191688632147307, "meta-math/MetaMath-Mistral-7B": 0.671834260155504, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47357793626764294, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "SM4 \u662f\u4e00\u79cd\u5206\u7ec4\u5bc6\u7801\u7b97\u6cd5\uff0c\u5176\u5206\u7ec4\u957f\u5ea6\u548c\u5bc6\u94a5\u957f\u5ea6\u5206\u522b\u4e3a\nA. 64 \u4f4d\u548c 128 \u4f4d\nB. 128 \u4f4d\u548c 128 \u4f4d\nC. 256 \u4f4d\u548c 256 \u4f4d\nD. 128 \u4f4d\u548c 256 \u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3331832353540621, "meta-math/MetaMath-Mistral-7B": 0.47249888692361725, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4516954533836804, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6504342747892524}}, {"question": "\u60a3\u8005\uff0c\u7537\uff0c60 \u5c81\uff0c10 \u5929\u524d\u884c\u80c3\u764c\u6839\u6cbb\u672f\uff0c\u8fd1 5 \u5929\u6765\u4f53\u6e29 38\u2103\u5de6\u53f3\uff0c\u80f8\u7247\u6b63\u5e38\uff0c\u5c3f\u5e38\u89c4\u672a\u89c1\u5f02\u5e38\uff0c\u8179\u90e8\u4f24\u53e3\u6108\u5408\u597d\uff0c\u5df2\u62c6\u7ebf\uff0c\u4e0a\u8179\u90e8 B \u8d85\u672a\u89c1\u79ef\u6db2\uff0c\u67e5\u4f53\u53d1\u73b0\u5de6\u5c0f\u817f\u5fae\u80bf\uff0c\u8153\u80a0\u808c\u6709\u538b\u75db\u3002\u53ef\u80fd\u7684\u8bca\u65ad\u662f\nA. \u5de6\u4e0b\u80a2\u6df1\u9759\u8109\u8840\u6813\u5f62\u6210\nB. \u5de6\u4e0b\u80a2\u808c\u7b4b\u819c\u708e\nC. \u5de6\u819d\u5173\u8282\u708e\nD. \u5de6\u4e0b\u80a2\u6d45\u9759\u8109\u708e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5766451634523272, "meta-math/MetaMath-Mistral-7B": 0.9113652272476671, "itpossible/Chinese-Mistral-7B-v0.1": 0.6784187613900476, "HuggingFaceH4/zephyr-7b-beta": 0.9310520102660137, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6633513248871756, "meta-llama/Meta-Llama-3-8B": 0.466673650729599, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6145070093040109}}, {"question": "\u6700\u6df1\u7ea2\u7c92\u7684\u5c0f\u9ea6\u548c\u767d\u7c92\u5c0f\u9ea6\u6742\u4ea4\uff0cF1\u4e3a\u4e2d\u95f4\u7c7b\u578b\u7684\u7ea2\u7c92\uff0cF2\u4e2d\u5927\u7ea6\u67091/64\u4e3a\u767d\u7c92\uff0c\u5176\u4f59\u4e3a\u7531\u6df1\u81f3\u6d45\u7684\u7ea2\u8272\u7c7d\u7c92\u3002\u7531\u6b64\u53ef\u4ee5\u5224\u65ad\u63a7\u5236\u8be5\u6027\u72b6\u7684\u57fa\u56e0\u6709\nA. 1\u5bf9\nB. 3\u5bf9\nC. 4\u5bf9\nD. 2\u5bf9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.42689714545069396, "itpossible/Chinese-Mistral-7B-v0.1": 0.3148300531811561, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3984585044016447, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u4e16\u7eaa\u540e\u671f\u4f4d\u4e8e\u4e1c\u897f\u65b9\u4e4b\u95f4\u7684\u67d0\u4e2a\u519b\u4e8b\u5f3a\u56fd\uff0c\u5360\u636e\u4e86\u91cd\u8981\u7684\u5730\u7406\u4f4d\u7f6e\uff0c\u63a7\u5236\u4e86\u4e9a\u6b27\u5546\u8def\u3002\u8fd9\u4e2a\u56fd\u5bb6\u662f\nA. \u6cd5\u5170\u514b\u738b\u56fd\nB. \u5965\u65af\u66fc\u5e1d\u56fd\nC. \u62dc\u5360\u5ead\u5e1d\u56fd\nD. \u897f\u7f57\u9a6c\u5e1d\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6784313027751547, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6508189551426302, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u52b3\u529b\u8fc7\u5ea6\u4e3b\u8981\u4f24\u53ca\nA. \u7cbe\nB. \u6c14\nC. \u795e\nD. \u8840\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2904324311152722, "meta-math/MetaMath-Mistral-7B": 0.381902052120576, "itpossible/Chinese-Mistral-7B-v0.1": 0.40252652413078177, "HuggingFaceH4/zephyr-7b-beta": 0.3354456100429363, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.40568367469707994, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4228420831128636}}, {"question": "\u6838\u7206\u70b8\u4ea7\u751f\u7684\u6740\u4f24\u56e0\u7d20\u4e2d\uff0c\u5bf9\u4eba\u5458\u548c\u5730\u9762\u76ee\u6807\u6740\u4f24\u80fd\u91cf\u6700\u5927\u7684\u662f\nA. \u65e9\u671f\u6838\u8f90\u5c04\nB. \u653e\u5c04\u6027\u6cbe\u67d3\nC. \u6838\u8f90\u5c04\u7684\u6301\u7eed\u5f71\u54cd\nD. \u51b2\u51fb\u6ce2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36900628425811965, "meta-math/MetaMath-Mistral-7B": 0.6334848253325773, "itpossible/Chinese-Mistral-7B-v0.1": 0.6574799030276495, "HuggingFaceH4/zephyr-7b-beta": 0.6909834230813519, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7223367314494383, "meta-llama/Meta-Llama-3-8B": 0.6801518609700945, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9690829550553354}}, {"question": "DNA\u534a\u4fdd\u7559\u590d\u5236\u7684\u542b\u4e49\u662f\u65b0\u5408\u6210\u7684DNA\u5206\u5b50\u4e2d\nA. \u4fdd\u7559\u4eb2\u4ee3DNA\u5206\u5b50\u4e00\u6761\u5355\u94fe\nB. \u4fdd\u7559\u4eb2\u4ee3DNA\u5206\u5b50\u4e00\u534a\u7684\u6838\u7cd6\nC. \u4fdd\u7559\u4eb2\u4ee3DNA\u5206\u5b50\u534a\u6761\u53cc\u94fe\nD. \u4fdd\u7559\u4eb2\u4ee3DNA\u5206\u5b50\u4e00\u534a\u7684\u78b1\u57fa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4588123714082308, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u5411\u4e59\u8868\u793a\u81ea\u5df1\u613f\u610f\u51fa\u9ad8\u4ef7\u201c\u4e70\u201d\u59bb\uff0c\u4e59\u4e0e\u5176\u59bb\u4e19\u5546\u91cf\uff0c\u8ba9\u4e19\u5047\u626e\u4e3a\u88ab\u62d0\u5356\u5987\u5973\uff0c\u5e76\u5c06\u4e19\u201c\u51fa\u5356\u201d\u7ed9\u7532\u3002\u4e09\u5929\u540e\uff0c\u4e59\u534f\u52a9\u4e19\u9003\u79bb\u7532\u5bb6\u3002\u5bf9\u6b64\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u7532\u6784\u6210\u62d0\u5356\u5987\u5973\u7f6a\nB. \u4e59\u6784\u6210\u8bc8\u9a97\u7f6a\nC. \u4e59\u6784\u6210\u62d0\u5356\u5987\u5973\u7f6a\nD. \u4e19\u4e0d\u6784\u6210\u72af\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5982\u679c\u6709\u8f83\u9ad8\u51c6\u786e\u7387\uff0c\u5e76\u4e0d\u96be\u603b\u662f\u8bf4\u660e\u8fd9\u4e2a\u5206\u7c7b\u5668\u662f\u597d\u7684\nB. \u6211\u4eec\u4e0d\u53ef\u4ee5\u4f7f\u7528\u805a\u7c7b\u201c\u7c7b\u522bid\u201d\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u7684\u7279\u5f81\u9879\uff0c \u7136\u540e\u518d\u7528\u76d1\u7763\u5b66\u4e60\u5206\u522b\u8fdb\u884c\u5b66\u4e60\nC. \u5982\u679c\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\uff0c \u90a3\u4e48\u6a21\u578b\u7684\u6d4b\u8bd5\u9519\u8bef\u7387\u603b\u662f\u4f1a\u964d\u4f4e\nD. \u5982\u679c\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\uff0c \u90a3\u4e48\u6a21\u578b\u7684\u8bad\u7ec3\u9519\u8bef\u7387\u603b\u662f\u4f1a\u964d\u4f4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.2963332999770349, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4310339605590007, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u9879\u7ef4\u751f\u7d20\u53c2\u4e0e\u611f\u5149\u7269\u8d28\u6784\u6210\uff0c\u7f3a\u4e4f\u53ef\u81f4\u591c\u76f2\u75c7\nA. \u7ef4\u751f\u7d20A\nB. \u70df\u9178\nC. \u7ef4\u751f\u7d20C\nD. \u03b2\u80e1\u841d\u535c\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.9044533659037908, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8567277974095694, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e16\u754c\u4e0a\u7b2c\u4e00\u652f\u6fc0\u5149\u67aa\u51fa\u73b0\u5728\nA. 1978\u5e74\u7f8e\u56fd\nB. 1975\u5e74\u7f8e\u56fd\nC. 1973\u5e74\u82cf\u8054\nD. 1976\u5e74\u5fb7\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.38957470397236993, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.486409916760432}}, {"question": "\u6559\u80b2\u5b66\u7684\u6e90\u6cc9\u662f\nA. \u6559\u80b2\u89c4\u5f8b\nB. \u6559\u80b2\u5b9e\u8df5\nC. \u6559\u80b2\u7406\u8bba\nD. \u6559\u80b2\u9700\u8981\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.38336772865499974, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3436615088034303, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1848\u5e74\u6cd5\u56fd\u7206\u53d1\u7684\u201c\u4e8c\u6708\u9769\u547d\u201d\uff0c\u63a8\u7ffb\u4e86\u201c\u4e03\u6708\u738b\u671d\u201d\uff0c\u91cd\u65b0\u5efa\u7acb\u4e86\u5171\u548c\u56fd\uff0c\u8be5\u5171\u548c\u56fd\u7684\u540d\u79f0\u662f\nA. \u6cd5\u5170\u897f\u7b2c\u4e8c\u5171\u548c\u56fd\nB. \u6cd5\u5170\u897f\u7b2c\u56db\u5171\u548c\u56fd\nC. \u6cd5\u5170\u897f\u7b2c\u4e94\u5171\u548c\u56fd\nD. \u6cd5\u5170\u897f\u7b2c\u4e09\u5171\u548c\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6970518701143158}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u5b88\u6cd5\u7684\u8868\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u5b88\u6cd5\u7684\u6700\u4f4e\u72b6\u6001\u5c31\u662f\u4e0d\u8fdd\u6cd5\u72af\u7f6a\nB. \u5b88\u6cd5\u7684\u4e3b\u4f53\u4e0d\u5305\u62ec\u65e0\u6c11\u4e8b\u884c\u4e3a\u80fd\u529b\u4eba\nC. \u5b88\u6cd5\u7684\u8303\u56f4\u5305\u542b\u5baa\u6cd5\u3001\u6cd5\u5f8b\u53ca\u98ce\u4fd7\u4e60\u60ef\nD. \u5b88\u6cd5\u7684\u5185\u5bb9\u5c31\u662f\u5168\u9762\u884c\u4f7f\u6cd5\u5b9a\u6743\u5229\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u88ab\u80e1\u9002\u8ba4\u4e3a\u8ffd\u6c42\u201c\u7f8e\u4e0e\u7231\u4e0e\u81ea\u7531\u201d\u7684\u201c\u5355\u7eaf\u4fe1\u4ef0\u201d\u7684\u4eba\u751f\u89c2\u7684\u4f5c\u5bb6\u662f\nA. \u95fb\u4e00\u591a\nB. \u6c88\u4ece\u6587\nC. \u5f90\u5fd7\u6469\nD. \u6797\u8bed\u5802\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28874768259290584, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3689108554330873, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3468666740605408, "meta-llama/Meta-Llama-3-8B": 0.42096269224722166, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3668428019448374}}, {"question": "16\u6c7d\u4fee\uff082\uff09\u73ed\u603b\u2f08\u6570\u662f50\uff0c\u5176\u4e2d\u559c\u6b22\u84dd\u7403\u7684\u670921\u2f08\uff0c\u559c\u6b22\u2f7b\u2f51\u7403\u7684\u670919\u2f08\uff0c\u65e2\u4e0d\u559c\u6b22\u7bee\u7403\u2f1c\u4e0d\u559c\u6b22\u2f7b\u2f51\u7403\u7684\u670915\u2f08\uff0c\u90a3\u4e48\u65e2\u559c\u6b22\u7bee\u7403\u2f1c\u559c\u6b22\u2f7b\u2f51\u7403\u7684\u6709\u2f0f\u2f08\uff1f\nA. 5\u4eba\nB. 6\u4eba\nC. 7\u4eba\nD. 4\u4eba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.28850952576306876, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4319475585678708}}, {"question": "\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u6cd5\u5f8b\u5bf9\u653f\u7b56\u5177\u6709\u6307\u5bfc\u4f5c\u7528\nB. \u6cd5\u5f8b\u4e0e\u9053\u5fb7\u90fd\u662f\u7531\u56fd\u5bb6\u5f3a\u5236\u529b\u4fdd\u8bc1\u5b9e\u65bd\u7684\u793e\u4f1a\u89c4\u8303\nC. \u6cd5\u5f8b\u7684\u5185\u5bb9\u6700\u7ec8\u662f\u7531\u7edf\u6cbb\u9636\u7ea7\u7684\u610f\u5fd7\u51b3\u5b9a\u7684\nD. \u6cd5\u5f8b\u7684\u4ea7\u751f\u7ecf\u5386\u4e86\u4e00\u4e2a\u4ece\u4e0e\u9053\u5fb7\u3001\u5b97\u6559\u89c4\u8303\u6df7\u4e3a\u4e00\u4f53\u5230\u76f8\u5bf9\u72ec\u7acb\u7684\u8fc7\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7310635192455666, "meta-math/MetaMath-Mistral-7B": 0.9346854638037783, "itpossible/Chinese-Mistral-7B-v0.1": 0.33767905931288267, "HuggingFaceH4/zephyr-7b-beta": 0.9975726310395553, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8956868588115232, "meta-llama/Meta-Llama-3-8B": 0.7804280398280519, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5509855489973132}}, {"question": "\u201c\u5fc5\u7136\u738b\u56fd\u201d\u548c\u201c\u81ea\u7531\u738b\u56fd\u201d\u662f\nA. \u7a7a\u95f4\u6027\u6982\u5ff5\nB. \u7269\u8d28\u6027\u6982\u5ff5\nC. \u5386\u53f2\u6027\u6982\u5ff5\nD. \u65f6\u95f4\u6027\u6982\u5ff5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7197382466213271, "meta-math/MetaMath-Mistral-7B": 0.9718084367538931, "itpossible/Chinese-Mistral-7B-v0.1": 0.5872565511231962, "HuggingFaceH4/zephyr-7b-beta": 0.9968085347886765, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8998034307453495, "meta-llama/Meta-Llama-3-8B": 0.8475813650683766, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.896113208958704}}, {"question": "\u9488\u5bf9\u6570\u636e\u5305\u8fc7\u6ee4\u548c\u5e94\u7528\u7f51\u5173\u6280\u672f\u5b58\u5728\u7684\u7f3a\u70b9\u800c\u5f15\u5165\u7684\u9632\u706b\u5899\u6280\u672f\uff0c\u8fd9\u662f\u4e0b\u5217\u54ea\u4e00\u79cd\u9632\u706b\u5899\u7684\u7279\u70b9\u3002\nA. \u4ee3\u7406\u670d\u52a1\u578b\nB. \u5e94\u7528\u7ea7\u7f51\u5173\u578b\nC. \u590d\u5408\u578b\u9632\u706b\u5899\nD. \u5305\u8fc7\u6ee4\u578b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2010\u5e743\u670817\u65e5\u51cc\u6668\uff0c\u7f8e\u56fd\u56fd\u4f1a\u518d\u6b21\u629b\u51fa\u201c\u8212\u9ed8\u8bae\u6848\u201d\uff0c\u8fd8\u6709100\u591a\u540d\u7f8e\u56fd\u8bae\u5458\u201c\u547c\u5401\u201d\u5bf9\u4e2d\u56fd\u8f93\u7f8e\u4ea7\u54c1\u5f81\u6536\u53cd\u8865\u8d34\u7a0e\u3002\u800c\u4e2d\u56fd\u4ecd\u7136\u8868\u793a\uff0c\u201c\u4eba\u6c11\u5e01\u6c47\u7387\u5408\u7406\uff0c\u7f8e\u56fd\u4e0d\u5e94\u8be5\u5c06\u6c47\u7387\u95ee\u9898\u653f\u6cbb\u5316\u201d\u3002\u5047\u5982\u4eba\u6c11\u5e01\u4e0d\u5347\u503c\uff0c\u7f8e\u56fd\u4f1a\u91c7\u53d6\u66f4\u52a0\u6fc0\u70c8\u7684\u624b\u6bb5\uff0c\u6bd4\u5982\u8054\u5408\u6b27\u76df\u5236\u88c1\u4e2d\u56fd\u7ecf\u6d4e\uff0c\u7ee7\u7eed\u589e\u52a0\u5bf9\u4e2d\u56fd\u4ea7\u54c1\u7684\u5904\u7f5a\u6027\u5173\u7a0e\uff0c\u8fd9\u6837\u52bf\u5fc5\u9020\u6210\u5bf9\u4e2d\u56fd\u7ecf\u6d4e\u5bf9\u5916\u8d38\u6613\u7684\u5de8\u5927\u51b2\u51fb\u3002\u8fd9\u4ece\u4e00\u4e2a\u4fa7\u9762\u53cd\u6620\u4e86\nA. \u4e16\u754c\u5e02\u573a\u5448\u73b0\u4e00\u4f53\u5316\u7684\u8d8b\u52bf\nB. \u7ecf\u6d4e\u5168\u7403\u5316\u5728\u4e0d\u65ad\u5730\u53d1\u5c55\nC. \u56fd\u5bb6\u95f4\u7684\u7ecf\u6d4e\u8054\u7cfb\u65e5\u76ca\u7d27\u5bc6\nD. \u4e16\u754c\u5e02\u573a\u7684\u7ade\u4e89\u65e5\u76ca\u6fc0\u70c8\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.614482665819669, "meta-math/MetaMath-Mistral-7B": 0.4439175967604887, "itpossible/Chinese-Mistral-7B-v0.1": 0.547690360045609, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.35540643906208824, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4e8e\u5185\u5b58\u7684\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u5b58\u50a8\u901f\u5ea6\u5f88\u5feb\nB. \u643a\u5e26\u6bd4\u8f83\u65b9\u4fbf \nC. \u75c5\u6bd2\u4e00\u822c\u4e0d\u4f1a\u611f\u67d3\u5b83\nD. \u4ef7\u683c\u76f8\u5bf9\u4e8e\u786c\u76d8\u6765\u8bf4\u6bd4\u8f83\u4fbf\u5b9c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4527235718585295, "meta-math/MetaMath-Mistral-7B": 0.9606549748577321, "itpossible/Chinese-Mistral-7B-v0.1": 0.7493258688284876, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5901591685251878, "meta-llama/Meta-Llama-3-8B": 0.5014154751172246, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8524379650528412}}, {"question": "\u5ba3\u544a\u5efa\u7acb\u6b27\u6d32\u8054\u76df\u7684\u6761\u7ea6\u662f\nA. \u7f57\u9a6c\u6761\u7ea6\nB. \u9a6c\u65af\u7279\u91cc\u8d6b\u7279\u6761\u7ea6\nC. \u963f\u59c6\u65af\u7279\u4e39\u6761\u7ea6\nD. \u5e03\u9c81\u585e\u5c14\u6761\u7ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5340639057355574, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3498694049171309, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9731946381976136}}, {"question": "\u7532\u5348\u56fd\u6b87\uff0c\u50ac\u5316\u4e86\u7ef4\u65b0\u601d\u60f3\u7684\u53d1\u9175\u548c\u7ef4\u65b0\u7fa4\u4f53\u7684\u805a\u5408\uff0c\u5e76\u5f88\u5feb\u53d1\u5c55\u6210\u4e3a\u7231\u56fd\u6551\u4ea1\u7684\u7ef4\u65b0\u8fd0\u52a8\u3002\u4e0b\u5217\u4e8b\u4ef6\uff0c\u62c9\u5f00\u4e86\u7ef4\u65b0\u8fd0\u52a8\u5e8f\u5e55\u7684\u662f\nA. \u521b\u529e\u4eac\u5e08\u5927\u5b66\u5802\nB. \u7b7e\u8ba2\u300a\u9a6c\u5173\u6761\u7ea6\u300b\nC. \u9881\u5e03\u201c\u660e\u5b9a\u56fd\u662f\u201d\u8bcf\nD. \u53d1\u52a8\u201c\u516c\u8f66\u4e0a\u4e66\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.378444236021072, "meta-math/MetaMath-Mistral-7B": 0.540162145577072, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5781049390933074, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.432336935349251, "meta-llama/Meta-Llama-3-8B": 0.6615533871426005, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4117253467977964}}, {"question": "\u5608\u6742\uff1a\u73af\u5883\u5b89\u9759\nA. \u75b2\u52b3\uff1a\u9a7e\u9a76\u5b89\u5168\nB. \u8fdf\u7f13\uff1a\u5de5\u4f5c\u6548\u7387\nC. \u6e29\u6696\uff1a\u62b5\u5fa1\u5bd2\u51ac\nD. \u7c97\u5fc3\uff1a\u5934\u8111\u6e05\u9192\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5481967800721393, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u54ea\u4e9b\u7b97\u6cd5, 1. KNN\uff1b2. \u7ebf\u6027\u56de\u5f52\uff1b3.\u5bf9\u6570\u51e0\u7387\u56de\u5f52\u3002\u53ef\u4ee5\u7528\u795e\u7ecf\u7f51\u7edc\u53bb\u6784\u9020:\nA. 2 \u548c 3\nB. 1\u548c 2\nC. \u4ee5\u4e0a\u90fd\u4e0d\u662f\nD. 1, 2 \u548c 3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8def\u4e2d\u4e24\u4e2a\u5e76\u8054\u7684\u7535\u5bb9\u5143\u4ef6\uff08\uff09\u76f8\u7b49\u3002\nA. \u7535\u538b\nB. \u80fd\u91cf\nC. \u7535\u6d41\nD. \u7535\u8377\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2885095257630687, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.40841291921960077, "meta-llama/Meta-Llama-3-8B": 0.32366165066143693, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f57\u9a6c\u5171\u548c\u56fd\u65e9\u671f\uff0c\u5f53\u7f57\u9a6c\u906d\u53d7\u5916\u65cf\u8fdb\u653b\u65f6\uff0c\u5e73\u6c11\u66fe\u591a\u6b21\u5c06\u81ea\u5df1\u7ec4\u7ec7\u7684\u961f\u4f0d\u64a4\u79bb\u7f57\u9a6c\uff0c\u62d2\u7edd\u4f5c\u6218\uff0c\u8feb\u4f7f\u8d35\u65cf\u5728\u653f\u6cbb\u4e0a\u505a\u51fa\u8ba9\u6b65\u3002\u300a\u5341\u4e8c\u94dc\u8868\u6cd5\u300b\u7684\u5236\u5b9a\u5c31\u662f\u8fd9\u79cd\u6597\u4e89\u7684\u6210\u679c\u4e4b\u4e00\u3002\u53ef\u89c1\u5f53\u65f6\nA. \u8d35\u65cf\u9010\u6b65\u4e27\u5931\u5236\u5b9a\u6cd5\u5f8b\u7684\u4e3b\u5bfc\u5730\u4f4d\nB. \u5e73\u6c11\u4e0e\u8d35\u65cf\u7684\u653f\u6cbb\u8bc9\u6c42\u65e5\u8d8b\u4e00\u81f4\nC. \u5e73\u6c11\u91c7\u53d6\u6709\u6548\u65b9\u5f0f\u4e89\u53d6\u81ea\u8eab\u6743\u76ca\nD. \u8d35\u65cf\u8ba9\u6b65\u5728\u6cd5\u5236\u53d1\u5c55\u4e2d\u8d77\u51b3\u5b9a\u4f5c\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6191819273627134, "meta-math/MetaMath-Mistral-7B": 0.8817180089374788, "itpossible/Chinese-Mistral-7B-v0.1": 0.5061801362296187, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6282368877687033, "meta-llama/Meta-Llama-3-8B": 0.9256834582597989, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.903092511852756}}, {"question": "\u9a6c\u514b\u601d\u4e3b\u4e49\u6c11\u65cf\u5e73\u7b49\u601d\u60f3\u8ba4\u4e3a\u79c1\u6709\u5236\u662f\u9020\u6210\u6c11\u65cf\u5265\u524a\u548c\u538b\u8feb\u5173\u7cfb\u7684\u5236\u5ea6\u6839\u6e90\uff0c\u201c\u4eba\u5bf9\u4eba\u7684\u5265\u524a\u4e00\u6d88\u706d\uff0c\u6c11\u65cf\u5bf9\u6c11\u65cf\u7684\u5265\u524a\u5c31\u4f1a\u968f\u4e4b\u6d88\u706d\u201d\uff0c\u662f\u9a6c\u6069\u66fe\u5728\u90a3\u7bc7\u6587\u7ae0\u4e2d\u63d0\u5230\u7684\nA. \u300a\u5171\u4ea7\u515a\u5ba3\u8a00\u300b\nB. \u300a\u795e\u5723\u5bb6\u65cf\uff0c\u6216\u5bf9\u6279\u5224\u7684\u6279\u5224\u6240\u4f5c\u7684\u6279\u5224\u300b\nC. \u300a1857-1858\u5e74\u7ecf\u6d4e\u5b66-\u54f2\u5b66\u624b\u7a3f\u300b\nD. \u300a\u5173\u4e8e\u8d39\u5c14\u5df4\u54c8\u7684\u63d0\u7eb2\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5172105356604503, "itpossible/Chinese-Mistral-7B-v0.1": 0.31122966560092724, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u4eca\u5929\u6211\uff08\uff09\u6ca1\u6709\u53bb\u8bad\u7ec3\uff0c\uff08\uff09\u6211\u611f\u5192\u4e86\uff0c\u8eab\u4f53\u4e0d\u8212\u670d\u3002\u201d\u8fd9\u53e5\u8bdd\u4e2d\u7684\u5173\u8054\u8bcd\u5c31\u586b\nA. \u56e0\u4e3a\u2026\u2026\u6240\u4ee5\u2026\u2026\nB. \u4e0d\u662f\u2026\u2026\u800c\u662f\u2026\u2026\nC. \u65e2\u7136\u2026\u2026\u5c31\u2026\u2026\nD. \u4e4b\u6240\u4ee5\u2026\u2026\u662f\u56e0\u4e3a\u2026\u2026\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.397399161987674, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u7269\u7684\u4f9d\u8d56\u6027\u5173\u7cfb\u201d\u662f\nA. \u8d44\u672c\u4e3b\u4e49\u793e\u4f1a\u4ee5\u524d\u7684\u4eba\u4e0e\u4eba\u4e4b\u95f4\u7684\u5173\u7cfb\nB. \u5171\u4ea7\u4e3b\u4e49\u793e\u4f1a\u4e4b\u4e2d\u7684\u4eba\u4e0e\u4eba\u4e4b\u95f4\u7684\u5173\u7cfb\nC. \u793e\u4f1a\u4e3b\u4e49\u793e\u4f1a\u4e4b\u4e2d\u7684\u4eba\u4e0e\u4eba\u4e4b\u95f4\u7684\u5173\u7cfb\nD. \u8d44\u672c\u4e3b\u4e49\u793e\u4f1a\u4e4b\u4e2d\u7684\u4eba\u4e0e\u4eba\u4e4b\u95f4\u7684\u5173\u7cfb \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8106463743211585, "meta-math/MetaMath-Mistral-7B": 0.9682927823384784, "itpossible/Chinese-Mistral-7B-v0.1": 0.3782541812927621, "HuggingFaceH4/zephyr-7b-beta": 0.9969351710994382, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8924962077694553, "meta-llama/Meta-Llama-3-8B": 0.8395174546463485, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6670876765326501}}, {"question": "\u4e00\u4e2a\u5b57\u8282\u5bf9\u5e94\u7684\u4e8c\u8fdb\u5236\u4f4d\u6570\u662f\nA. 1\nB. 2\nC. 8\nD. 4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6415918425350917, "meta-math/MetaMath-Mistral-7B": 0.7198352104690076, "itpossible/Chinese-Mistral-7B-v0.1": 0.4578706792802334, "HuggingFaceH4/zephyr-7b-beta": 0.638463256514233, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8307224156472419, "meta-llama/Meta-Llama-3-8B": 0.6558143487807616, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9771019535394863}}, {"question": "\u4e0b\u5217\u75be\u75c5\u4e2d\u53ef\u5f15\u8d77\u7eb5\u9694\u6251\u52a8\u7684\u662f\nA. \u5f20\u529b\u6027\u6c14\u80f8\nB. \u95ed\u5408\u6027\u6c14\u80f8\nC. \u5f00\u653e\u6027\u6c14\u80f8\nD. \u8fdb\u884c\u6027\u8840\u6c14\u80f8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u4e0d\u4f46\u6000\u7591\u4e59\u53d1\u7ed9\u4ed6\u7684\u4fe1\u906d\u4eba\u7be1\u6539\uff0c\u800c\u4e14\u6000\u7591\u4e59\u7684\u516c\u94a5\u4e5f\u662f\u88ab\u4eba\u5192\u5145\u7684\u3002\u4e3a\u4e86\u6d88\u9664\u7532\u7684\u7591\u8651\uff0c\u7532\u548c\u4e59\u9700\u8981\u627e\u4e00\u4e2a\u53cc\u65b9\u90fd\u4fe1\u4efb\u7684\u7b2c\u4e09\u65b9\u6765\u7b7e\u53d1\u6570\u5b57\u8bc1\u4e66\uff0c\u8fd9\u4e2a\u7b2c\u4e09\u65b9\u662f\nA. \u56fd\u5bb6\u4fe1\u606f\u5b89\u5168\u6d4b\u8bc4\u8ba4\u8bc1\u4e2d\u5fc3\nB. \u6ce8\u518c\u4e2d\u5fc3 RA\nC. \u8ba4\u8bc1\u4e2d\u5fc3CA\nD. \u56fd\u9645\u7535\u4fe1\u8054\u76dfITU\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9649852467056576, "meta-math/MetaMath-Mistral-7B": 0.9966476318641122, "itpossible/Chinese-Mistral-7B-v0.1": 0.9308422033356264, "HuggingFaceH4/zephyr-7b-beta": 0.9997244406906641, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9895384373162417, "meta-llama/Meta-Llama-3-8B": 0.7261732980458873, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6539\u9769\u5f00\u653e\u540e\uff0c\u4e2d\u5171\u4e2d\u592e\u660e\u786e\u5c06\u4e2d\u5171\u516b\u5927\u4f5c\u4e3a\u63a2\u7d22\u4e2d\u56fd\u793e\u4f1a\u4e3b\u4e49\u5efa\u8bbe\u9053\u8def\u7684\u5f00\u7aef\u3002\u8fd9\u662f\u56e0\u4e3a\u4e2d\u5171\u516b\u5927\nA. \u5b9e\u73b0\u4e86\u5de5\u4f5c\u91cd\u5fc3\u5411\u793e\u4f1a\u4e3b\u4e49\u73b0\u4ee3\u5316\u5efa\u8bbe\u7684\u8f6c\u53d8\nB. \u5168\u9762\u603b\u7ed3\u4e86\u65b0\u6c11\u4e3b\u4e3b\u4e49\u9769\u547d\u7684\u7ecf\u9a8c\nC. \u5ba2\u89c2\u5206\u6790\u4e86\u5f53\u65f6\u7684\u56fd\u5185\u5f62\u52bf\u4e0e\u4e3b\u8981\u77db\u76fe\u7684\u53d8\u5316\nD. \u660e\u786e\u63d0\u51fa\u793e\u4f1a\u4e3b\u4e49\u521d\u7ea7\u9636\u6bb5\u7684\u7406\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4084129141920774, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4350764809881977, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u897f\u8499\u4e3a\u4ee3\u8868\u7684\u51b3\u7b56\u7406\u8bba\u5b66\u6d3e\u63d0\u51fa\u7684\u51b3\u7b56\u51c6\u5219\u662f\nA. \u6700\u4f18\u5316\nB. \u516c\u5e73\nC. \u6c11\u4e3b\u5316\nD. \u6ee1\u610f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c34\u679c\u548c\u852c\u83dc\u7684\u98ce\u5473\u548c\u9999\u5473\u5f52\u56e0\u4e8e\u5404\u79cd\u5316\u5408\u7269\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4ece\u800c\u4ea7\u751f\u72ec\u7279\u548c\u4e0e\u4f17\u4e0d\u540c\u7684\u7279\u5f81\uff0c\u8fd9\u4e9b\u5316\u5408\u7269\u4e0d\u5305\u62ec\nA. \u4e59\u919b\nB. \u6709\u673a\u9178\nC. \u4e59\u9187\nD. \u78f7\u5316\u7269\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.660175261137191, "HuggingFaceH4/zephyr-7b-beta": 0.789136851733393, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4678874439401799, "meta-llama/Meta-Llama-3-8B": 0.44661208169375366, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.45842081068123747}}, {"question": "\u4e0b\u5217\u5c5e\u4e8e\u8bb8\u5730\u5c71\u7684\u6563\u6587\u96c6\u662f\nA. \u300a\u7a7a\u5c71\u7075\u96e8\u300b\nB. \u300a\u7f00\u7f51\u52b3\u86db\u300b\nC. \u300a\u5730\u4e4b\u5b50\u300b\nD. \u300a\u56da\u7eff\u8bb0\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4227611334008299, "itpossible/Chinese-Mistral-7B-v0.1": 0.30889363229415834, "HuggingFaceH4/zephyr-7b-beta": 0.5754438285376463, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5837308368711829, "meta-llama/Meta-Llama-3-8B": 0.25756620677129083, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.49441951195961253}}, {"question": "\u5728\u5176\u5c04\u7a0b\u5185\uff0c\u5bf9\u5766\u514b\u3001\u98de\u673a\u3001\u8230\u8247\u3001\u6865\u6881\u7b49\u70b9\u76ee\u6807\u6709\u591a\u5c11\u4ee5\u4e0a\u76f4\u63a5\u547d\u4e2d\u6982\u7387\u7684\u6b66\u5668\u53ef\u79f0\u4e4b\u4e3a\u7cbe\u786e\u5236\u5bfc\u6b66\u5668\nA. 50%\nB. 30%\nC. 60%\nD. 70%\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.33065623127838456, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u690d\u7269\u4ece\u6697\u4e2d\u8f6c\u79fb\u5230\u5149\u4e0b\uff0c\u7c7b\u56ca\u4f53\u819c\u4e0a\u7684\u7389\u7c73\u9ec4\u7d20\u548c\u7d2b\u9ec4\u7d20\u542b\u91cf\u7684\u53d8\u5316\u5206\u522b\u662f\nA. \u964d\u4f4e\u3001\u964d\u4f4e\nB. \u5347\u9ad8\u3001\u964d\u4f4e\nC. \u5347\u9ad8\u3001\u5347\u9ad8\nD. \u964d\u4f4e\u3001\u5347\u9ad8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3306562312783845, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3967757715930558, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4656430048571539}}, {"question": "\u76f8\u5173\u7cfb\u6570\u68c0\u9a8c\u7684\u65e0\u6548\u5047\u8bbe$H_1$\u662f.\nA. $\\rho=0$\uff0c\nB. $\\rho=1$\uff0c\nC. $\\rho\\neq0$\nD. $\\rho>0$\uff0c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4645878064131203, "meta-math/MetaMath-Mistral-7B": 0.6327650609309493, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7859863573598797, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9452552999560637}}, {"question": "\u6839\u636e\u300a\u751f\u4ea7\u5b89\u5168\u4e8b\u6545\u62a5\u544a\u4e0e\u8c03\u67e5\u5904\u7406\u6761\u4f8b\u300b\uff08\u56fd\u52a1\u9662\u4ee4\u7b2c493\u53f7\uff09\uff0c\u5b89\u5168\u751f\u4ea7\u76d1\u7763\u7ba1\u7406\u90e8\u95e8\u4e0e\u8d1f\u6709\u5b89\u5168\u751f\u4ea7\u76d1\u7763\u7ba1\u7406\u804c\u8d23\u7684\u6709\u5173\u90e8\u95e8\u9010\u7ea7\u4e0a\u62a5\u4e8b\u6545\u60c5\u51b5\uff0c\u6bcf\u7ea7\u4e0a\u62a5\u7684\u65f6\u95f4\u4e0d\u7684\u8d85\u8fc7\uff08\uff09\u5c0f\u65f6\u3002\nA. 3\nB. 1\nC. 4\nD. 2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8bcd\u8bed\u4e2d\u6ca1\u6709\u9519\u522b\u5b57\u7684\u4e00\u9879\u662f( )\nA. \u6ca7\u832b \u6025\u71e5 \u5b09\u620f \u7c97\u5236\u6ee5\u9020\nB. \u7280\u5229 \u7bf7\u52c3 \u611a\u949d \u7ffb\u6765\u8986\u53bb\nC. \u7fe1\u7fe0 \u6ede\u7559 \u8fc1\u5f99 \u4e0d\u53ef\u660e\u72b6\nD. \u7f9e\u6127 \u51c4\u60e8 \u5965\u79d8 \u62d6\u6ce5\u5e26\u6c34\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u4e0e\u4e59\u6709\u4ec7\uff0c\u6b32\u7f6e\u4e59\u4e8e\u6b7b\u5730\u3002\u67d0\u65e5\uff0c\u7532\u89c1\u4e59\u4e0e\u4e19\u88ab\u7ef3\u7d22\u60ac\u540a\u4e8e\u534a\u7a7a\u4e2d\u6d17\u64e6\u697c\u623f\u5916\u5899\u73bb\u7483\uff0c\u4fbf\u7528\u5200\u5272\u65ad\u7ef3\u7d22\uff0c\u81f4\u4e59\u3001\u4e19\u4e00\u8d77\u5760\u5730\u6b7b\u4ea1\u3002\u5bf9\u6b64\uff0c\u7532\u7684\u4e3b\u89c2\u7f6a\u8fc7\u5f62\u5f0f\u662f\nA. \u5bf9\u4e59\u5c5e\u76f4\u63a5\u6545\u610f\uff0c\u5bf9\u4e19\u4e5f\u662f\u76f4\u63a5\u6545\u610f\nB. \u5bf9\u4e59\u5c5e\u76f4\u63a5\u6545\u610f\uff0c\u5bf9\u4e19\u5c5e\u610f\u5916\u4e8b\u4ef6\uff0c\u53ea\u5177\u6709\u4e00\u4e2a\u7f6a\u8fc7\nC. \u5bf9\u4e59\u5c5e\u76f4\u63a5\u6545\u610f\uff0c\u5bf9\u4e19\u5c5e\u95f4\u63a5\u6545\u610f\uff0c\u5177\u6709\u4e24\u4e2a\u7f6a\u8fc7\nD. \u5bf9\u4e59\u5c5e\u76f4\u63a5\u6545\u610f\uff0c\u5bf9\u4e19\u5c5e\u8f7b\u4fe1\u8fc7\u5931\uff0c\u5177\u6709\u4e24\u4e2a\u7f6a\u8fc7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c11\u8425\u7ecf\u6d4e\u7684\u8fc5\u901f\u53d1\u5c55\u662f\u6211\u56fd\u6539\u9769\u5f00\u653e\u4ee5\u6765\u6700\u5177\u5386\u53f2\u610f\u4e49\u7684\u793e\u4f1a\u7ecf\u6d4e\u73b0\u8c61\u4e4b\u4e00\uff0c\u6c11\u8425\u7ecf\u6d4e\u4ea7\u503c\u4ee5\u6bcf\u5e7470%\u7684\u901f\u5ea6\u589e\u957f\uff0c\u6c11\u8425\u4f01\u4e1a\u7684\u5c31\u4e1a\u4eba\u6570\u6bcf\u5e74\u4ee541%\u7684\u901f\u5ea6\u589e\u957f\uff0c\u5bf9\u4e2d\u56fdGDP\u7684\u8d21\u732e\u8d85\u8fc7\u4e8650%\u3002\u6c11\u8425\u7ecf\u6d4e\u53d1\u5c55\u8fc5\u901f\u662f\u56e0\u4e3a\uff1aa\u6c11\u8425\u7ecf\u6d4e\u673a\u5236\u65b0\u3001\u6709\u6d3b\u529b\uff0c\u9002\u5e94\u5e02\u573a\u8981\u6c42\uff1bb\u6c11\u8425\u7ecf\u6d4e\u662f\u6211\u56fd\u73b0\u9636\u6bb5\u7684\u91cd\u8981\u7ecf\u6d4e\u6210\u5206\uff1bc\u6c11\u8425\u7ecf\u6d4e\u6700\u9002\u5e94\u793e\u4f1a\u5316\u5927\u751f\u4ea7\u7684\u53d1\u5c55\u8981\u6c42\uff1bd\u56fd\u5bb6\u9f13\u52b1\u3001\u652f\u6301\u6c11\u8425\u7ecf\u6d4e\u7684\u53d1\u5c55\uff0c\u5e76\u4e3a\u5176\u53d1\u5c55\u5236\u5b9a\u4e00\u7cfb\u5217\u653f\u7b56\nA. ac\nB. ad\nC. ab\nD. bc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u80ba\u8840\u6813\u6813\u585e\u75c7\u7684\u57fa\u672c\u6cbb\u7597\u4e2d\uff0c\u5c1a\u4e0d\u80fd\u6ee1\u8db3\u8981\u6c42\u7684\u662f\nA. \u963f\u53f8\u5339\u6797\nB. \u534e\u6cd5\u6797\nC. \u666e\u901a\u809d\u7d20\nD. \u5229\u4f10\u6c99\u73ed\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4290007666576758, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5404\u7ec4\u5b57\uff0c\u4e0e\u201c\u4fe1\u2014\u4f38\u201d\u4e24\u5b57\u95f4\u5173\u7cfb\u76f8\u540c\u7684\u4e00\u7ec4\u662f\nA. \u8d74\u2014\u8ba3\nB. \u4e88\u2014\u8206\nC. \u5269\u2014\u8cf8\nD. \u6cea\u2014\u6dda\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3160424181481997, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6559\u80b2\u8981\u9002\u5e94\u4eba\u7684\u53d1\u5c55\u7684\u4e2a\u522b\u5dee\u5f02\u6027\uff0c\u505a\u5230\nA. \u9632\u5fae\u675c\u6e10\nB. \u5faa\u5e8f\u6e10\u8fdb\nC. \u6559\u5b66\u76f8\u957f\nD. \u56e0\u6750\u65bd\u6559\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9233951638069885, "meta-math/MetaMath-Mistral-7B": 0.9964007775305312, "itpossible/Chinese-Mistral-7B-v0.1": 0.9540985792855088, "HuggingFaceH4/zephyr-7b-beta": 0.9998540640450166, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9779945745420242, "meta-llama/Meta-Llama-3-8B": 0.9778169773764549, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.99950231780431}}, {"question": "\u9057\u4f20\u6f02\u53d8\u6307\u7684\u662f\nA. \u57fa\u56e0\u9891\u7387\u7684\u964d\u4f4e\nB. \u57fa\u56e0\u9891\u7387\u5728\u5c0f\u7fa4\u4f53\u4e2d\u7684\u968f\u673a\u589e\u51cf\nC. \u57fa\u56e0\u7531A\u53d8\u4e3aa\u6216\u7531a\u53d8\u4e3aA\nD. \u57fa\u56e0\u9891\u7387\u7684\u589e\u52a0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9236090675218906, "meta-math/MetaMath-Mistral-7B": 0.9951145485565495, "itpossible/Chinese-Mistral-7B-v0.1": 0.8671569011552483, "HuggingFaceH4/zephyr-7b-beta": 0.9940135859373527, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9182462080304221, "meta-llama/Meta-Llama-3-8B": 0.9728967165055724, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9986379021440989}}, {"question": "\u7f8e\u56fd\u5fc3\u7406\u5b66\u5bb6\u5965\u82cf\u4f2f\u5c14\u4f9d\u636e\u5b66\u4e60\u6750\u6599\u4e0e\u5b66\u4e60\u8005\u7684\u539f\u6709\u77e5\u8bc6\u7684\u5173\u7cfb\uff0c\u628a\u5b66\u4e60\u5206\u4e3a\nA. \u610f\u4e49\u5b66\u4e60\u548c\u673a\u68b0\u5b66\u4e60\nB. \u610f\u4e49\u5b66\u4e60\u548c\u63a5\u53d7\u5b66\u4e60\nC. \u53d1\u73b0\u5b66\u4e60\u548c\u673a\u68b0\u5b66\u4e60\nD. \u63a5\u53d7\u5b66\u4e60\u548c\u53d1\u73b0\u5b66\u4e60\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4871416446306822, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1802\u5e74\u82f1\u56fd\u51fa\u53f0\u7b2c\u4e00\u90e8\u300a\u5de5\u5382\u6cd5\u300b\uff0c\u6cd5\u5f8b\u89c4\u5b9a\uff1a9\u5c81\u4ee5\u4e0a\u7684\u513f\u7ae5\u6bcf\u5929\u53ef\u4ee5\u5de5\u4f5c8\u5c0f\u65f6\uff0c14\u5c81\u4ee5\u4e0a\u7684\u513f\u7ae5\u6bcf\u5929\u53ef\u4ee5\u5de5\u4f5c12\u5c0f\u65f6\uff1b\u4ed6\u4eec\u4e0d\u80fd\u5728\u51cc\u66686\u70b9\u4e4b\u524d\u5de5\u4f5c\uff0c\u4ed6\u4eec\u7761\u89c9\u7684\u65f6\u95f4\u4e0d\u80fd\u665a\u4e8e\u51cc\u66682\u70b9\u3002\u7531\u6b64\u53ef\u4ee5\u63a8\u65ad\nA. \u5de5\u4eba\u7f62\u5de5\u6597\u4e89\u8d62\u5f97\u4e86\u5408\u6cd5\u6743\u76ca\nB. \u5de5\u4e1a\u9769\u547d\u4e2d\u5de5\u4eba\u751f\u5b58\u72b6\u51b5\u6076\u52a3\nC. \u5de5\u5382\u89c4\u8303\u5316\u548c\u5236\u5ea6\u5316\u4f53\u7cfb\u51fa\u73b0\nD. \u5de5\u5382\u4e3b\u6ce8\u91cd\u4fdd\u969c\u513f\u7ae5\u6b63\u5f53\u6743\u76ca\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8562845766480065, "meta-math/MetaMath-Mistral-7B": 0.9781287098378091, "itpossible/Chinese-Mistral-7B-v0.1": 0.48533997841984466, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.859744133106919, "meta-llama/Meta-Llama-3-8B": 0.7845480212845977, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5316\u5b66\u4e0e\u751f\u4ea7\u3001\u751f\u6d3b\u3001\u793e\u4f1a\u5bc6\u5207\u76f8\u5173\uff0c\u4e0b\u5217\u6709\u5173\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u5730\u6c9f\u6cb9\u7ecf\u8fc7\u52a0\u5de5\u5904\u7406\u540e\u53ef\u7528\u6765\u5236\u751f\u7269\u67f4\u6cb9\u548c\u80a5\u7682\nB. \u5927\u91cf\u4f7f\u7528\u85aa\u67f4\u4e3a\u71c3\u6599\uff0c\u8df5\u884c\u4f4e\u78b3\u751f\u6d3b\nC. \u4f7f\u7528\u586b\u57cb\u6cd5\u5904\u7406\u672a\u7ecf\u5206\u7c7b\u7684\u751f\u6d3b\u5783\u573e\nD. \u6d77\u6c34\u4e2d\u542b\u6709\u4e30\u5bcc\u7684\u7898\u5143\u7d20\uff0c\u56e0\u6b64\u7898\u88ab\u79f0\u4e3a\u201c\u6d77\u6d0b\u5143\u7d20\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.51078687437039, "meta-math/MetaMath-Mistral-7B": 0.7635417562588562, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45252084867615433, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9888\u690e\u538b\u7f29\u6027\u9aa8\u6298\u5408\u5e76\u8131\u4f4d\uff0c\u9996\u9009\u7684\u6cbb\u7597\u65b9\u6cd5\u662f\nA. \u9885\u9aa8\u7275\u5f15\nB. \u9888\u6795\u5e26\u7275\u5f15\nC. \u5207\u5f00\u590d\u4f4d\nD. \u624b\u6cd5\u590d\u4f4d\uff0c\u77f3\u818f\u56fa\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67093\u4e2a\u5927\u4eba\u30012\u4e2a\u5c0f\u5b69\u8981\u4e00\u6b21\u540c\u65f6\u8fc7\u6cb3\uff0c\u6e21\u53e3\u6709\u5927\u8239\u3001\u4e2d\u8239\u3001\u5c0f\u8239\u5404\u4e00\u53ea\uff0c\u5927\u8239\u6700\u591a\u80fd\u8f7dl\u4e2a\u5927\u4eba\u30012\u4e2a\u5c0f\u5b69\uff0c\u4e2d\u8239\u6700\u591a\u80fd\u8f7d\u5927\u4eba\u3001\u5c0f\u5b69\u5404l\u4eba\uff0c\u5c0f\u8239\u6700\u591a\u80fd\u8f7d\u5927\u4ebal\u4eba\uff0c\u4e3a\u4e86\u5b89\u5168\uff0c\u5c0f\u5b69\u9700\u5927\u4eba\u966a\u540c'\u5219\u4e58\u8239\u7684\u65b9\u5f0f\u6709\u591a\u5c11\u79cd?\nA. 18\nB. 12\nC. 6\nD. 24\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5de5\u4f11\u671f\u95f4\uff0c\u5efa\u7b51\u5de5\u4eba\u7532\u5728\u5de5\u5730\u4e0a\u5c06\u4e0e\u81ea\u5df1\u76f8\u4e92\u5b09\u95f9\u7684\u5de5\u53cb\u4e59\u63a8\u5012\uff0c\u81f4\u4e59\u8dcc\u843d\u3002\u7532\u7684\u884c\u4e3a\u5e94\u8ba4\u5b9a\u4e3a\nA. \u6545\u610f\u4f24\u5bb3\u7f6a\nB. \u8fc7\u5931\u81f4\u4eba\u6b7b\u4ea1\u7f6a\nC. \u91cd\u5927\u8d23\u4efb\u4e8b\u6545\u7f6a\nD. \u610f\u5916\u4e8b\u4ef6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u793e\u533a\u53d1\u5c55\u4e2d\uff0c\u5e94\u6ee1\u8db3\u793e\u4f1a\u5c45\u6c11\u7684\u5171\u540c\u9700\u6c42\u4e3a\u91cd\u70b9\uff0c\u7279\u522b\u5e94\u4ece\u89e3\u51b3\u793e\u533a\u9762\u4e34\u7684\u8feb\u5207\u95ee\u9898\u5165\u624b\uff0c\u8fd9\u662f\u793e\u533a\u53d1\u5c55\u7684\nA. \u6c11\u4f17\u9700\u8981\u539f\u5219\nB. \u81ea\u4e0b\u800c\u4e0a\u4e0e\u81ea\u4e0a\u800c\u4e0a\u76f8\u7ed3\u5408\u7684\u539f\u5219\nC. \u81ea\u5229\u539f\u5219\nD. \u6c11\u4e3b\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.40528209665916054, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6291\u764c\u57fa\u56e0\u7684\u53d9\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u5177\u6709\u6291\u5236\u7ec6\u80de\u8fc7\u5ea6\u589e\u6b96\u7684\u4f5c\u7528\nB. \u6291\u764c\u57fa\u56e0\u53ea\u5b58\u5728\u4e8e\u80bf\u7624\u7ec6\u80de\u4e2d\nC. \u7ef4\u6301\u7ec6\u80de\u7684\u6b63\u5e38\u751f\u957f\uff0c\u53ea\u8981\u6291\u764c\u57fa\u56e0\u6b63\u5e38\u8868\u8fbe\u5373\u53ef\nD. \u4eba\u7c7b\u6b63\u5e38\u7ec6\u80de\u4e2d\u4e0d\u5b58\u5728\u6291\u764c\u57fa\u56e0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9000260441803635, "meta-math/MetaMath-Mistral-7B": 0.994826659116444, "itpossible/Chinese-Mistral-7B-v0.1": 0.8645942122745864, "HuggingFaceH4/zephyr-7b-beta": 0.9999638670389697, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8028108936619986, "meta-llama/Meta-Llama-3-8B": 0.9105398903226064, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.703248642305698}}, {"question": "\u82e6\u3001\u4e50\u3001\u4e0d\u82e6\u4e0d\u4e50\u662f\u5c5e\u4e8e()\u7684\u8303\u56f4\uff1f\nA. \u53d6\nB. \u60f3\nC. \u89e6\nD. \u53d7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4745338752360199, "meta-math/MetaMath-Mistral-7B": 0.5146287889717713, "itpossible/Chinese-Mistral-7B-v0.1": 0.35347162922091135, "HuggingFaceH4/zephyr-7b-beta": 0.999654686318178, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8445059728262341, "meta-llama/Meta-Llama-3-8B": 0.5327251004904839, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u542b\u6709\u5c0f\u8bf4\u300a\u4f24\u901d\u300b\u7684\u9c81\u8fc5\u4f5c\u54c1\u96c6\u662f\nA. \u300a\u671d\u82b1\u5915\u62fe\u300b\nB. \u300a\u5450\u558a\u300b\nC. \u300a\u6545\u4e8b\u65b0\u7f16\u300b\nD. \u300a\u5f77\u5fa8\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.325455072595945, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u7537\uff0c25\u5c81\u3002\u524d\u65e5\u5f00\u59cb\uff0c\u53f3\u80f8\u80c1\u90e8\u6cdb\u8d77\u75b1\u75b9\uff0c\u707c\u70ed\u75bc\u75db\uff0c\u53e3\u82e6\u54bd\u5e72\uff0c\u70e6\u8e81\u4e0d\u5b89\u3001\u6eb2\u8d64\u4fbf\u5e72\uff0c\u820c\u8fb9\u7ea2\u82d4\u9ec4\u817b\uff0c\u8109\u5f26\u6570\u3002\u4e34\u5e8a\u8bca\u65ad\u6700\u53ef\u80fd\u662f\nA. \u809d\u80c6\u6e7f\u70ed\u8bc1\nB. \u8840\u70ed\u8bc1\nC. \u6e7f\u70ed\u8574\u813e\u8bc1\nD. \u8840\u70ed\u751f\u98ce\u8bc1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.27340131319076255, "meta-math/MetaMath-Mistral-7B": 0.32214924591241917, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f01\u4e1a\u5229\u7528\u6d88\u8d39\u8005\u4ef0\u6155\u540d\u724c\u5546\u54c1\u6216\u540d\u5e97\u58f0\u671b\u7684\u5fc3\u7406\u6765\u786e\u5b9a\u5546\u54c1\u7684\u4ef7\u683c\uff0c\u6545\u610f\u628a\u4ef7\u683c\u5b9a\u6210\u6574\u6570\u6216\u9ad8\u4ef7\u3002\u8fd9\u79cd\u5fc3\u7406\u5b9a\u4ef7\u7b56\u7565\u5c5e\u4e8e\nA. \u6574\u6570\u5b9a\u4ef7\nB. \u62db\u5f95\u5b9a\u4ef7\nC. \u58f0\u671b\u5b9a\u4ef7\nD. \u5c3e\u6570\u5b9a\u4ef7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9451515274339314, "meta-math/MetaMath-Mistral-7B": 0.9913349073704052, "itpossible/Chinese-Mistral-7B-v0.1": 0.9320539865506781, "HuggingFaceH4/zephyr-7b-beta": 0.9997328497347787, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9851490656937245, "meta-llama/Meta-Llama-3-8B": 0.9097782322472188, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9696480108262724}}, {"question": "\u8ba1\u7b97\u673a\u7f51\u7edc\u6700\u7a81\u51fa\u7684\u4f18\u70b9\u662f\nA. \u5b58\u50a8\u5bb9\u91cf\u5927\nB. \u53ef\u4ee5\u4e92\u76f8\u901a\u4fe1\u3001\u6570\u636e\u5171\u4eab\nC. \u8fd0\u7b97\u6548\u7387\u9ad8\nD. \u5904\u7406\u901f\u5ea6\u5feb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9817344719760431, "meta-math/MetaMath-Mistral-7B": 0.9985584833461278, "itpossible/Chinese-Mistral-7B-v0.1": 0.9732799519787263, "HuggingFaceH4/zephyr-7b-beta": 0.9999847386369705, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9955686268662183, "meta-llama/Meta-Llama-3-8B": 0.9658548546258439, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9990451216030437}}, {"question": "\u4e0d\u5c5e\u4e8e\u6fc0\u52b1\u56e0\u7d20\u7684\u662f\nA. \u804c\u4e1a\u53d1\u5c55\nB. \u5de5\u4f5c\u8d23\u4efb\nC. \u5de5\u4f5c\u6210\u5c31\nD. \u5de5\u8d44\u6536\u5165\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.852028669774015, "meta-math/MetaMath-Mistral-7B": 0.9829271565287249, "itpossible/Chinese-Mistral-7B-v0.1": 0.45505423392341127, "HuggingFaceH4/zephyr-7b-beta": 0.9207635211312651, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6329239927612941, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u5bfc\u5f39\u4e4b\u7236\u4e3a\nA. \u9093\u7a3c\u5148\nB. \u9ec4\u7eac\u7984\nC. \u94b1\u5b66\u68ee\nD. \u738b\u6de6\u660c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3545765782702778, "meta-math/MetaMath-Mistral-7B": 0.4041552969236844, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4961966837307516, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6824601284073669, "meta-llama/Meta-Llama-3-8B": 0.394184811204869, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u56e0\u542b\u7837\u7c7b\u5bf9\u80be\u6709\u5f71\u54cd\u7684\u4e2d\u836f\u662f\nA. \u7ec6\u8f9b\nB. \u9c7c\u80c6\nC. \u96c4\u9ec4\nD. \u6731\u7802\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6184469845382073, "HuggingFaceH4/zephyr-7b-beta": 0.49687755104111697, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.34031470637689565, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5217\u5b81\u5f97\u51fa\u793e\u4f1a\u4e3b\u4e49\u53ef\u80fd\u5728\u4e00\u56fd\u6216\u6570\u56fd\u9996\u5148\u53d6\u5f97\u80dc\u5229\u7684\u7ed3\u8bba\u4f9d\u636e\u662f\nA. \u65e0\u4ea7\u9636\u7ea7\u662f\u6700\u5148\u8fdb\u3001\u6700\u9769\u547d\u7684\u9636\u7ea7\u7684\u539f\u7406\nB. \u8d44\u672c\u4e3b\u4e49\u5fc5\u7136\u706d\u4ea1\u3001\u793e\u4f1a\u4e3b\u4e49\u548c\u5171\u4ea7\u4e3b\u4e49\u5fc5\u7136\u80dc\u5229\u7684\u89c4\u5f8b\nC. \u5e1d\u56fd\u4e3b\u4e49\u65f6\u4ee3\u8d44\u672c\u4e3b\u4e49\u653f\u6cbb\u7ecf\u6d4e\u53d1\u5c55\u4e0d\u5e73\u8861\u7684\u89c4\u5f8b \nD. \u8d44\u672c\u4e3b\u4e49\u56fd\u5bb6\u65e0\u4ea7\u9636\u7ea7\u4e0e\u8d44\u4ea7\u9636\u7ea7\u6597\u4e89\u7684\u89c4\u5f8b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3118866287896564, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5837308223879009, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u8ff0\u53d9\u8ff0\u6b63\u786e\u7684\u662f\nA. \u8f6f\u4ef6\u548c\u786c\u4ef6\u7684\u754c\u7ebf\u4e0d\u662f\u7edd\u5bf9\u7684\uff0c\u6709\u65f6\u529f\u80fd\u662f\u7b49\u6548\u7684\nB. \u8f6f\u4ef6\u4e0d\u53ef\u2f64\u786c\u4ef6\u4ee3\u66ff\nC. \u786c\u4ef6\u7cfb\u7edf\u4e0d\u53ef\u2f64\u8f6f\u4ef6\u4ee3\u66ff\nD. \u8ba1\u7b97\u673a\u6027\u80fd\u5b8c\u5168\u53d6\u51b3\u4e8eCPU\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6379769575529707, "meta-math/MetaMath-Mistral-7B": 0.7919159148492547, "itpossible/Chinese-Mistral-7B-v0.1": 0.5117503890665857, "HuggingFaceH4/zephyr-7b-beta": 0.9956442063886571, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8504782773776145, "meta-llama/Meta-Llama-3-8B": 0.8711309376101415, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9941200443348155}}, {"question": "1892\u5e74\uff0c\u7ef4\u65b0\u601d\u60f3\u5bb6\u5b8b\u6055\u63d0\u51fa\u201c\u6b32\u66f4\u5b98\u5236\u3001\u8bbe\u8bae\u9662\u3001\u6539\u8bd5\u4ee4\uff0c\u5fc5\u81ea\u6613\u897f\u670d\u59cb\u201d\u3002\u5eb7\u6709\u4e3a\u5728\u594f\u8bae\u4e2d\u4e5f\u4e0d\u6b62\u4e00\u6b21\u63d0\u53ca\u201c\u6613\u670d\u201d\u3002\u7ef4\u65b0\u6d3e\u5982\u6b64\u91cd\u89c6\u6613\u670d\u7684\u4e3b\u8981\u539f\u56e0\u662f\nA. \u6539\u5236\u4e2d\u6613\u670d\u66f4\u6613\u63a8\u884c\nB. \u610f\u5728\u8425\u9020\u6539\u5236\u7684\u793e\u4f1a\u6c1b\u56f4\nC. \u4e2d\u56fd\u9700\u6539\u53d8\u5bf9\u5916\u5f62\u8c61\nD. \u957f\u888d\u9a6c\u8902\u4ee3\u8868\u4e86\u5b88\u65e7\u52bf\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6731278233003863, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7103696988501279}}, {"question": "\u5929\u7136\u6cb3\u9053\u4e2d\u7684\u6d2a\u6c34\u53d7\u5230\u7ed3\u51b0\u5f71\u54cd\u65f6\uff0c\u6c34\u4f4d\u6d41\u91cf\u5173\u7cfb\u70b9\u636e\u7684\u5206\u5e03\uff0c\u603b\u7684\u8d8b\u52bf\u662f\u504f\u5728\u7545\u6d41\u671f\u6c34\u4f4d\u6d41\u91cf\u5173\u7cfb\u66f2\u7ebf\u7684 ( )\u3002\nA. \u4e0a\u4e0b\u6446\u52a8\nB. \u4ee5\u4e0b\nC. \u4e0d\u53d8\nD. \u4ee5\u4e0a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3386874459099419, "itpossible/Chinese-Mistral-7B-v0.1": 0.2980747244459592, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3236616466071801, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "19\u4e16\u7eaa\u4e2d\u671f\u4ee5\u540e\uff0c\u4e2d\u56fd\u5e02\u573a\u4e0a\u7684\u6d0b\u8d27\u65e5\u76ca\u589e\u591a\uff0c\u706b\u67f4\u3001\u6d0b\u5e03\u7b49\u65e5\u7528\u54c1\uff0c\u201c\u867d\u7a77\u4e61\u50fb\u58e4\uff0c\u6c42\u4e4b\u4e8e\u5e02\uff0c\u5fc5\u6709\u6240\u4f9b\u3002\u201d\u8fd9\u79cd\u72b6\u51b5\u8868\u660e\nA. \u4e2d\u56fd\u5173\u7a0e\u4e3b\u6743\u5f00\u59cb\u4e27\u5931\nB. \u4e2d\u56fd\u5e02\u573a\u7531\u88ab\u52a8\u5f00\u653e\u8f6c\u4e3a\u4e3b\u52a8\u5f00\u653e\nC. \u5546\u54c1\u7ecf\u6d4e\u57fa\u672c\u53d6\u4ee3\u81ea\u7136\u7ecf\u6d4e\nD. \u65e5\u5e38\u751f\u6d3b\u4e0e\u4e16\u754c\u5e02\u573a\u8054\u7cfb\u65e5\u8d8b\u5bc6\u5207\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7588141802220364, "meta-math/MetaMath-Mistral-7B": 0.8013697746757276, "itpossible/Chinese-Mistral-7B-v0.1": 0.566543706486443, "HuggingFaceH4/zephyr-7b-beta": 0.9950220135395836, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8900014410506141, "meta-llama/Meta-Llama-3-8B": 0.7848511175203898, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9468821638777222}}, {"question": "\u4e0d\u63a5\u53d7\u81ea\u53d8\u91cf\u4f5c\u7528\u7684\u5b9e\u9a8c\u5bf9\u8c61\u5728\u63a7\u5236\u5b9e\u9a8c\u4e2d\u6784\u6210\nA. \u201c\u89c2\u6d4b\u7ec4\u201d\nB. \u201c\u5b9e\u9a8c\u7ec4\u201d\nC. \u201c\u63a7\u5236\u7ec4\"\nD. \u201c\u5bf9\u7167\u7ec4\"\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u5728\u8ddd\u79bb\u6211\u4eec\u592a\u9633\u7cfb10pc\u7684\u8303\u56f4\u4e4b\u5185\uff0c\u5929\u6587\u89c2\u6d4b\u5df2\u7ecf\u8bc1\u5b9e\u5b58\u5728\u7684\u6570\u76ee\u6700\u591a\u7684\u662f\u4ee5\u4e0b\u54ea\u7c7b\u6052\u661f\nA. O\u578b\nB. G\u578b\nC. M\u578b\nD. \u8910\u77ee\u661f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5530426963669699}}, {"question": "\u4e0b\u5217\u54ea\u4e00\u79cd\u662f\u8230\u8247\u88c5\u5907\u6700\u5e7f\u6cdb\u7684\u4e3b\u529b\u6218\u8230\nA. \u9a71\u9010\u8230\nB. \u5de1\u6d0b\u8230\nC. \u6218\u5217\u8230\nD. \u62a4\u536b\u8230\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u9053\u8def\u4ea4\u901a\u5b89\u5168\u6cd5\u300b\u662f\u4e3a\u4e86\u7ef4\u62a4\u9053\u8def\u4ea4\u901a\u79e9\u5e8f\uff0c\uff08\uff09\uff0c\u63d0\u9ad8\u901a\u884c\u6548\u7387\u3002\nA. \u5706\u6ee1\u5b8c\u6210\u8fd0\u8f93\u4efb\u52a1\nB. \u4fdd\u62a4\u516c\u6c11\u5408\u6cd5\u6743\u76ca\nC. \u51cf\u5c11\u4ea4\u901a\u4e8b\u6545\nD. \u4fdd\u8bc1\u8f66\u8f86\u9ad8\u901f\u884c\u9a76\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7197921696120733, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\uff0c29\u5c81\u3002\u8f6c\u79fb\u6027\u53f3\u4e0b\u8179\u75db\u4f34\u53d1\u70ed\uff0c\u8bca\u65ad\u4e3a\u6025\u6027\u9611\u5c3e\u708e\u3002\u67e5\u4f53\uff1a\u5631\u60a3\u8005\u4ef0\u5367\uff0c\u4f7f\u53f3\u9acb\u548c\u53f3\u5927\u817f\u5c48\u66f2\uff0c\u7136\u540e\u533b\u751f\u65cb\u8f6c\u5176\u4e0b\u80a2\uff0c\u5f15\u8d77\u60a3\u8005\u53f3\u4e0b\u8179\u75bc\u75db\u3002\u63d0\u793a\u9611\u5c3e\u7684\u4f4d\u7f6e\u5728\nA. \u76c6\u4f4d\nB. \u53f3\u4e0b\u8179\u9ea6\u6c0f\u70b9\nC. \u809d\u4e0b\u4f4d\nD. \u76f2\u80a0\u540e\u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5404\u79cd\u7269\u8d28\u5728\u4e00\u5b9a\u6761\u4ef6\u4e0b\u53cd\u5e94\uff0c\u6240\u5f97\u4ea7\u7269\u79cd\u7c7b\u4e0e\u53cd\u5e94\u7269\u7684\u7528\u91cf\u6216\u6761\u4ef6\u65e0\u5173\u7684\u662fa: Fe\uff0bO2\uff1bb: NH4HCO3\uff0bNaOH\uff1bc: Cu\uff0bCl2\uff1bd: Ca(HCO3)2\uff0bCa(OH)2\uff1be: C\uff0bSiO2\nA. abd\nB. cd\nC. ade\nD. bc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5177\u6709\u201c\u538b\u5236\u4e2a\u4eba\u76ee\u6807\uff0c\u4f7f\u7ec4\u7ec7\u76ee\u6807\u51cc\u9a7e\u4e8e\u4e2a\u4eba\u76ee\u6807\u4e4b\u4e0a\u201d\u7279\u5f81\u7684\u662f\nA. \u76ee\u6807\u7ba1\u7406\nB. \u4f20\u7edf\u7ba1\u7406\nC. \u73b0\u4ee3\u7ba1\u7406\nD. \u7ec4\u7ec7\u7ba1\u7406\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6081568545606945, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4737435512623867, "meta-llama/Meta-Llama-3-8B": 0.7708483894039974, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5404\u5f0f\u4e2d[ ]\u662f\u2f45\u7a0b\nA. X\nB. 4.7+5.2=9.9 \nC. 3X+6\nD. 3X+6=27\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5941606612808507, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.42886488324950367, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u642d\u914d\u6b63\u786e\u7684\u662f\nA. \u4e91\u5357---\u6ee1\u65cf---\u706b\u628a\u8282\nB. \u65b0\u7586---\u626d\u79e7\u6b4c---\u8461\u8404\uff0c\u54c8\u5bc6\u74dc \nC. \u897f\u85cf---\u6652\u4f5b\u8282---\u9165\u6cb9\u8336\uff0c\u7ccc\u7c91\nD. \u9655\u5317---\u5b89\u585e\u8170\u9f13---\u540a\u811a\u697c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.34624084333667404, "HuggingFaceH4/zephyr-7b-beta": 0.41209693851201173, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe $\\lim _{x \\rightarrow a} f(x)=A\uff0c \\lim _{x \\rightarrow a} g(x)$ \u4e0d\u5b58\u5728\uff0c $\\lim _{x \\rightarrow a} h(x)$ \u4e0d\u5b58\u5728\uff0c \u5219(1) $\\lim _{x \\rightarrow a}[f(x) \\cdot g(x)]$ \u4e0d\u5b58\u5728;(2) $\\lim _{x \\rightarrow a}[g(x)+h(x)]$ \u4e0d\u5b58\u5728;(3) $\\lim _{x \\rightarrow a}[h(x) \\cdot g(x)]$ \u4e0d\u5b58\u5728;(4) $\\lim _{x \\rightarrow a}[g(x)+f(x)]$ \u4e0d\u5b58\u5728;\u4ee5\u4e0a\u547d\u9898\u4e2d\u6b63\u786e\u7684\u4e2a\u6570\u662f\nA. 0\nB. 2\nC. 3\nD. 1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.29863342676099575, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "A\u3001B\u4e24\u53ea\u9752\u86d9\u8fdb\u884c\u8df3\u8dc3\u6bd4\u8d5b\uff0cA\u6bcf\u6b21\u8df310\u5398\u7c73\uff0cB\u6bcf\u6b21\u8df315\u5398\u7c73\uff0c\u4ed6\u4eec\u6bcf\u79d2\u90fd\u53ea\u8df31\u6b21\uff0c\u4e14\u4e00\u8d77\u4ece\u8d77\u70b9\u5f00\u59cb\u5411\u540c\u4e00\u65b9\u5411\u8df3\u8dc3\u3002\u5728\u6bd4\u8d5b\u9014\u4e2d\uff0c\u6bcf\u969412\u5398\u7c73\u6709\u4e00\u9677\u9631\uff0c\u5f53\u5b83\u4eec\u4e2d\u7b2c\u4e00\u53ea\u6389\u8fdb\u9677\u9631\u65f6\uff0c\u53e6\u4e00\u53ea\u8ddd\u79bb\u5b83\u6700\u8fd1\u7684\u9677\u9631\u6709()\u5398\u7c73\nA. 6\nB. 2\nC. 4\nD. \u65e0\u6cd5\u786e\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3313635536266756}}, {"question": "\u90ed\u6cab\u82e5\u521b\u4f5c\u7684\u300a\u5c48\u539f\u300b\u662f\nA. \u5bcc\u6709\u73b0\u5b9e\u4e3b\u4e49\u8272\u5f69\u7684\u60b2\u5267\nB. \u878d\u8bd7\u3001\u6b4c\u3001\u821e\u4e8e\u4e00\u4f53\u7684\u6c11\u65cf\u65b0\u6b4c\u5267\nC. \u5bcc\u6709\u6d6a\u6f2b\u4e3b\u4e49\u8272\u5f69\u7684\u559c\u5267\nD. \u5bcc\u6709\u6d6a\u6f2b\u4e3b\u4e49\u8272\u5f69\u7684\u8bd7\u5267\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7526240333638293, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u519c\u4e1a\u81ea\u7136\u8d44\u6e90\u7684\u6574\u4f53\u6027\u662f\u6307\nA. \u81ea\u7136\u8d44\u6e90\u4e2d\u5404\u79cd\u8d44\u6e90\u6570\u91cf\u4e0a\u6709\u4e00\u5b9a\u6bd4\u4f8b\u5173\u7cfb\nB. \u519c\u4e1a\u81ea\u7136\u8d44\u6e90\u7531\u591a\u79cd\u8d44\u6e90\u7ec4\u6210\nC. \u4ee5\u4e0a\u8bf4\u6cd5\u90fd\u4e0d\u5bf9\nD. \u81ea\u7136\u8d44\u6e90\u4e2d\u5404\u79cd\u8d44\u6e90\u662f\u76f8\u4e92\u8054\u7cfb\u3001\u76f8\u4e92\u5236\u7ea6\uff0c\u7ec4\u6210\u4e00\u4e2a\u7cfb\u7edf\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.934962745747149, "meta-math/MetaMath-Mistral-7B": 0.9978595964781888, "itpossible/Chinese-Mistral-7B-v0.1": 0.9636098249324778, "HuggingFaceH4/zephyr-7b-beta": 0.9999253397899045, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.993707100768931, "meta-llama/Meta-Llama-3-8B": 0.9413864422500565, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9954683627271653}}, {"question": "\u5728\u6587\u5316\u7684\u7ed3\u6784\u4e2d\uff0c\u4e00\u8f86\u9a6c\u8f66\u662f\nA. \u6587\u5316\u7279\u8d28\nB. \u6587\u5316\u96c6\u4e1b\nC. \u6587\u5316\u6a21\u5f0f\nD. \u6587\u5316\u7279\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.26463422059185693, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.626717967499141}}, {"question": "\u4e0d\u5c5e\u4e8e\u5168\u9762\u63a7\u5236\u6cd5\u7684\u662f\nA. \u635f\u76ca\u63a7\u5236\u6cd5\nB. \u6295\u8d44\u62a5\u916c\u7387\u5206\u6790\u6cd5\nC. \u9879\u76ee\u9884\u7b97\u6cd5\nD. \u7ba1\u7406\u5ba1\u8ba1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f26\u7406\u5173\u7cfb\u662f\u4e00\u79cd\nA. \u4ec5\u4ec5\u5305\u542b\u5728\u5bb6\u5ead\u5173\u7cfb\u4e2d\u7684\u793e\u4f1a\u5173\u7cfb\nB. \u4eba\u4eec\u81ea\u884c\u8bbe\u8ba1\uff0c\u5e76\u7ecf\u4eba\u4eec\u52aa\u529b\u521b\u9020\u7684\u4e00\u79cd\u5ba2\u89c2\u4e8b\u5b9e\nC. \u72ec\u7acb\u4e8e\u5176\u5b83\u793e\u4f1a\u5173\u7cfb\u4e4b\u5916\u7684\u4e00\u79cd\u5173\u7cfb\nD. \u5ba2\u89c2\u7684\u3001\u5fc5\u7136\u7684\u793e\u4f1a\u5173\u7cfb\u4e8b\u5b9e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.38812073416459414, "HuggingFaceH4/zephyr-7b-beta": 0.9914937591771841, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6955814447917761, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d5b\u524d\u8bad\u7ec3\u5468\u6280\u672f\u8bad\u7ec3\u7684\u57fa\u672c\u7279\u70b9\u662f\nA. \u540c\u65f6\u589e\u52a0\u5b8c\u6574\u548c\u5206\u89e3\u7ec3\u4e60\nB. \u589e\u52a0\u5b8c\u6574\u7ec3\u4e60\nC. \u589e\u52a0\u5206\u89e3\u7ec3\u4e60\nD. \u51cf\u5c11\u5b8c\u6574\u7ec3\u4e60\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3516316150354567, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3230435121167668, "HuggingFaceH4/zephyr-7b-beta": 0.5867781215594156, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u8001\u5e74\u4eba\u72af\u7f6a\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u6709\nA. \u5df2\u6ee175\u5c81\u7684\u4eba\u72af\u7f6a\u7684\uff0c\u5e94\u5f53\u4e00\u5f8b\u51cf\u8f7b\u5904\u7f5a\nB. \u5df2\u6ee175\u5c81\u7684\u4eba\u8fc7\u5931\u72af\u7f6a\u7684\uff0c\u5e94\u5f53\u4ece\u8f7b\u6216\u8005\u51cf\u8f7b\u5904\u7f5a\nC. \u5df2\u6ee175\u5c81\u7684\u4eba\u6545\u610f\u72af\u7f6a\u7684\uff0c\u5e94\u5f53\u4ece\u8f7b\u6216\u8005\u51cf\u8f7b\u5904\u7f5a\nD. \u5df2\u6ee175\u5c81\u7684\u4eba\u72af\u7f6a\u7684\uff0c\u5e94\u5f53\u4e00\u5f8b\u514d\u9664\u5904\u7f5a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5257541078379924, "meta-math/MetaMath-Mistral-7B": 0.7799029100120952, "itpossible/Chinese-Mistral-7B-v0.1": 0.46482722799024745, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5583097592932078, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9647115165853953}}, {"question": "\u5728\u8d44\u6e90\u7ba1\u7406\u5668\u4e2d\uff0c\u6587\u4ef6\u5939\u6811\u4e2d\u7684\u67d0\u4e2a\u6587\u4ef6\u5939\u7684\u5de6\u8fb9\u7684\u201c+\u201d\u8868\u793a\nA. \u8be5\u6587\u4ef6\u5939\u4e3a\u7a7a\nB. \u8be5\u6587\u4ef6\u5939\u542b\u6709\u7cfb\u7edf\u6587\u4ef6\nC. \u8be5\u6587\u4ef6\u5939\u542b\u6709\u5b50\u6587\u4ef6\u5939\nD. \u8be5\u6587\u4ef6\u5939\u542b\u6709\u9690\u85cf\u6587\u4ef6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9627624336561851, "meta-math/MetaMath-Mistral-7B": 0.9962454205466074, "itpossible/Chinese-Mistral-7B-v0.1": 0.816331050185441, "HuggingFaceH4/zephyr-7b-beta": 0.9997990184448877, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.97904303360681, "meta-llama/Meta-Llama-3-8B": 0.9613245769155682, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9930573719033867}}, {"question": "\u4e0b\u5217\u6709\u5173\u7269\u8d28\u53ca\u6210\u5206\u3001\u6027\u8d28\u548c\u7528\u9014\u90fd\u6b63\u786e\u7684\u662f\nA. \u7269\u8d28\u53ca\u6210\u5206\uff1a\u78c1\u6027\u6c27\u5316\u94c1(Fe2O3)\u3002\u6027\u8d28\uff1a\u96be\u6eb6\u4e8e\u6c34\uff0c\u7ea2\u8272\u3002\u7528\u9014\uff1a\u5236\u9020\u7ea2\u8272\u6d82\u6599\u3002\nB. \u7269\u8d28\u53ca\u6210\u5206\uff1a\u5c0f\u82cf\u6253(Na2CO3)\u3002\u6027\u8d28\uff1a\u4e0e\u9178\u53cd\u5e94\u4ea7\u751f\u4e8c\u6c27\u5316\u78b3\u3002\u7528\u9014\uff1a\u4f5c\u53d1\u9175\u7c89\u3002\nC. \u7269\u8d28\u53ca\u6210\u5206\uff1a\u84dd\u77fe(CuSO4\u00b75H2O)\u3002\u6027\u8d28\uff1a\u84dd\u8272\u3002\u7528\u9014\uff1a\u68c0\u9a8c\u6c34\u84b8\u6c14\u3002\nD. \u7269\u8d28\u53ca\u6210\u5206\uff1a\u4e8c\u6c27\u5316\u786b(SO2)\u3002\u6027\u8d28\uff1a\u80fd\u548c\u67d0\u4e9b\u6709\u8272\u7269\u8d28\u53cd\u5e94\u751f\u6210\u65e0\u8272\u7269\u8d28\u3002\u7528\u9014\uff1a\u6f02\u767d\u8349\u7ec7\u54c1\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u751f\u6d3b\u73b0\u8c61\u7684\u8bf4\u6cd5\u9519\u8bef\u7684\u662f\nA. \u6302\u949f\u7535\u6c60\u8017\u5c3d\u540e\uff0c\u79d2\u9488\u56e0\u53d7\u5230\u91cd\u529b\u77e9\u7684\u4f5c\u7528\uff0c\u4f1a\u505c\u572830\u79d2\u7684\u4f4d\u7f6e\nB. \u81ea\u6765\u6c34\u4e2d\u542b\u6709\u5c11\u91cf\u6b21\u6c27\u9178\uff0c\u56e0\u6b64\u4e0d\u5b9c\u76f4\u63a5\u7528\u81ea\u6765\u6c34\u517b\u9c7c\nC. \u65e0\u98ce\u65f6\u843d\u53f6\u5404\u5904\u53d7\u51b7\u7a7a\u6c14\u4f5c\u7528\u529b\u4e0d\u5747\uff0c\u56e0\u6b64\u5448\u66f2\u7ebf\u7ffb\u8f6c\u843d\u4e0b\nD. \u8863\u670d\u6e7f\u540e\u5bf9\u5149\u7ebf\u7684\u53cd\u5c04\u80fd\u529b\u51cf\u5f31\uff0c\u56e0\u6b64\u989c\u8272\u6bd4\u5e72\u7684\u65f6\u5019\u6df1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.46418505510299635, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4285537155312629, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8699135575749006}}, {"question": "\u7532\u5c06\u623f\u5c4b\u51fa\u79df\u7ed9\u4e59\u3002\u79df\u8d41\u671f\u95f4\u7532\u5c06\u623f\u5c4b\u5356\u7ed9\u4e86\u4e19\uff0c\u529e\u7406\u4e86\u8fc7\u6237\u767b\u8bb0\u624b\u7eed\uff0c\u53cc\u65b9\u7ea6\u5b9a\u5269\u4f59\u79df\u671f\u7684\u79df\u91d1\u7531\u4e19\u6536\u53d6\u3002\u5176\u540e\u7532\u5c06\u4e0a\u8ff0\u4e8b\u5b9e\u544a\u77e5\u4e59\u3002\u79df\u671f\u5c4a\u6ee1\u540e\uff0c\u4e59\u7ee7\u7eed\u5360\u6709\u8be5\u623f\u5c4b\u3002\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u4e19\u53ea\u80fd\u8bf7\u6c42\u7532\u4ea4\u4ed8\u623f\u5c4b\nB. \u4e19\u53ea\u80fd\u8bf7\u6c42\u4e59\u5411\u7532\u8fd4\u8fd8\u623f\u5c4b\nC. \u4e19\u65e2\u53ef\u4ee5\u8bf7\u6c42\u7532\u4ea4\u4ed8\u623f\u5c4b\uff0c\u4e5f\u53ef\u4ee5\u8bf7\u6c42\u4e59\u8fd4\u8fd8\u623f\u5c4b\nD. \u4e19\u53ea\u80fd\u8bf7\u6c42\u4e59\u8fd4\u8fd8\u623f\u5c4b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4805588466043234, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8967586296926171, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u793e\u4f1a\u4e3b\u4e49\u653f\u6cbb\u5236\u5ea6\u7684\u57fa\u672c\u7279\u5f81\u662f\nA. \u65e0\u4ea7\u9636\u7ea7\u653f\u515a\u7684\u9886\u5bfc\nB. \u65e0\u4ea7\u9636\u7ea7\u4e13\u653f\u7684\u653f\u6743\nC. \u575a\u6301\u793e\u4f1a\u4e3b\u4e49\u65b9\u5411\nD. \u9a6c\u514b\u601d\u4e3b\u4e49\u7684\u6307\u5bfc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5683411037475465, "meta-math/MetaMath-Mistral-7B": 0.556431679578697, "itpossible/Chinese-Mistral-7B-v0.1": 0.5113041468242412, "HuggingFaceH4/zephyr-7b-beta": 0.9504573053904929, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6869708442481717, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u5c0f\u5b66\u7ec4\u7ec76\u4e2a\u5e74\u7ea7\u7684\u5b66\u751f\u5916\u51fa\u53c2\u89c2\u5305\u62ecA\u79d1\u6280\u9986\u5728\u5185\u76846\u4e2a\u79d1\u6280\u9986\uff0c\u6bcf\u4e2a\u5e74\u7ea7\u4efb\u9009\u4e00\u4e2a\u79d1\u6280\u9986\u53c2\u89c2\uff0c\u5219\u6709\u4e14\u53ea\u6709\u4e24\u4e2a\u5e74\u7ea7\u9009\u62e9A\u79d1\u6280\u9986\u7684\u65b9\u6848\u5171\u6709\nA. 3800\u79cd\nB. 9375\u79cd\nC. 1800\u79cd\nD. 18750\u79cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2801288226217134, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4221216124438063, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5207728147196982}}, {"question": "\u51af\u7279\u521b\u7acb\u7684\u5fc3\u7406\u5b66\u6d3e\u662f\nA. \u884c\u4e3a\u4e3b\u4e49\u5fc3\u7406\u5b66\nB. \u683c\u5f0f\u5854\u5fc3\u7406\u5b66\nC. \u673a\u80fd\u5fc3\u7406\u5b66\nD. \u6784\u9020\u5fc3\u7406\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4535223341046808, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5194457205120052, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.47406983492991867, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u79cd\u519c\u4ea7\u54c1\u5728\u5e02\u573a\u7684\u96f6\u552e\u4ef7\u548c\u6279\u53d1\u4ef7\u7684\u5dee\u989d\u79f0\u4e3a\nA. \u6279\u96f6\u5dee\u4ef7\nB. \u65f6\u95f4\u5dee\u4ef7\nC. \u5730\u533a\u5dee\u4ef7\nD. \u5b63\u8282\u5dee\u4ef7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9182642946042054, "meta-math/MetaMath-Mistral-7B": 0.9980247552717924, "itpossible/Chinese-Mistral-7B-v0.1": 0.7528427046863518, "HuggingFaceH4/zephyr-7b-beta": 0.9999034560114736, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9962134790624626, "meta-llama/Meta-Llama-3-8B": 0.9544350193558017, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.996648325148879}}, {"question": "\u9a6c\u4e01\u00b7\u8def\u5fb7\u5b97\u6559\u6539\u9769\u7406\u8bba\u6839\u57fa\u7684\u6838\u5fc3\u662f\nA. \u56e0\u4fe1\u79f0\u4e49\nB. \u4eba\u4eba\u7686\u50e7\u4fa3\nC. \u5341\u5b57\u67b6\u795e\u5b66\nD. \u5584\u529f\u79f0\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4958066921952073, "meta-math/MetaMath-Mistral-7B": 0.5151454849524386, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9553639733055053, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9929197869599662}}, {"question": "\u793e\u4f1a\u4e3b\u4e49\u5404\u4e2a\u9636\u6bb5\u7684\u5212\u5206\u6700\u7ec8\u5e94\u4ee5\nA. \u4ee5\u9636\u7ea7\u6597\u4e89\u4e3a\u6807\u51c6\nB. \u751f\u4ea7\u8d44\u6599\u7684\u516c\u6709\u5236\u7a0b\u5ea6\u4e3a\u6807\u51c6\nC. \u751f\u4ea7\u529b\u7684\u53d1\u5c55\u4e3a\u6807\u51c6\nD. \u751f\u4ea7\u5173\u7cfb\u4e3a\u6807\u51c6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.49572668857386315, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.44876258312973644, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u521d\u7ea7\u7fa4\u4f53\u7684\u89c4\u6a21\u4e00\u822c\u5728\nA. 0\u201410 \u4eba\nB. 0\u201420 \u4eba\nC. 0\u201430 \u4eba\nD. 0\u201440 \u4eba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4874271373144424}}, {"question": "\u5728\u793e\u4f1a\u5236\u5ea6\u7684\u6784\u6210\u8981\u7d20\u4e2d\uff0c\u5236\u5ea6\u7684\u7075\u9b42\u662f\nA. \u8bbe\u5907\u8981\u7d20\nB. \u7ec4\u7ec7\u8981\u7d20\nC. \u4ef7\u503c\u8981\u7d20\nD. \u89c4\u8303\u8981\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5017797442264189, "meta-math/MetaMath-Mistral-7B": 0.7494318529363136, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9421734641645788, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7458015731550931, "meta-llama/Meta-Llama-3-8B": 0.6837774247210165, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8472917631096734}}, {"question": "\u5173\u4e8e\u5b8c\u5168\u7ade\u4e89\u5e02\u573a\uff0c\u4ee5\u4e0b\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u5b8c\u5168\u7ade\u4e89\u5e02\u573a\u91cd\u89c6\u793e\u4f1a\u603b\u6548\u7528\u6700\u5927\u5316\nB. \u5b8c\u5168\u7ade\u4e89\u5e02\u573a\u4fc3\u4f7f\u4eba\u4eec\u9010\u6e10\u5f62\u6210\u5fe0\u8bda\u3001\u5584\u826f\u548c\u5173\u6000\u7684\u7f8e\u5fb7\nC. \u5b8c\u5168\u7ade\u4e89\u5e02\u573a\u91cd\u89c6\u9700\u6c42\u3001\u53d1\u5c55\u53ca\u5e73\u7b49\u7684\u6b63\u4e49\nD. \u5b8c\u5168\u7ade\u4e89\u662f\u5e02\u573a\u901a\u8fc7\u5f15\u5bfc\u4e70\u65b9\u548c\u5356\u65b9\u4ee5\u5b8c\u7f8e\u7684\u6548\u7387\u5212\u5206\u3001\u4f7f\u7528\u548c\u5206\u914d\u5546\u54c1\uff0c\u4f7f\u4ed6\u4eec\u7684\u6548\u7528\u6700\u5927\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9658465724622063, "meta-math/MetaMath-Mistral-7B": 0.9990262624140869, "itpossible/Chinese-Mistral-7B-v0.1": 0.8014015647240958, "HuggingFaceH4/zephyr-7b-beta": 0.9999778177302014, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9713123197744821, "meta-llama/Meta-Llama-3-8B": 0.7319938533806801, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c23\u5c81\u3002\u56e0\u4e4f\u529b10\u5929\u3001\u7259\u9f88\u51fa\u8840\u4f34\u76ae\u80a4\u6de4\u65914 \u5929\u5165\u9662\uff0c\u65e2\u5f80\u4f53\u5065\u3002\u5316\u9a8c\u8840 Hb 76 g/L\uff0cWBC 25X10^9/L\uff0cPlt 29X10^9/L\uff0c\u9aa8\u9ad3\u589e\u751f\u660e\u663e\u6d3b\u8dc3\uff0c\u539f\u59cb\u7ec6\u80de\u536060%\uff0cPOX\u67d3\u8272(-)\uff0cPAS\u67d3\u8272(+)\u6210\u5757\uff0cNSE \u67d3\u8272(-)\u3002\u8be5\u60a3\u8005\u9996\u9009\u7684\u6cbb\u7597\u65b9\u6848\u662f\nA. COP\nB. ABVD\nC. DVLP\nD. DA\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.34686667406054084, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u67d3\u8272\u4f53\u7ed3\u6784\u53d8\u5f02\u4f53\u4e2d\uff0c\u751f\u6d3b\u529b\u548c\u914d\u5b50\u80b2\u6027\u6700\u4f4e\u7684\u662f\nA. \u5012\u4f4d\u7eaf\u5408\u4f53\nB. \u7f3a\u5931\u7eaf\u5408\u4f53\nC. \u6613\u4f4d\u7eaf\u5408\u4f53\nD. \u91cd\u590d\u7eaf\u5408\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7145058437179861, "meta-math/MetaMath-Mistral-7B": 0.9563691118022992, "itpossible/Chinese-Mistral-7B-v0.1": 0.8236802162924135, "HuggingFaceH4/zephyr-7b-beta": 0.9973950347643261, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.867451142415645, "meta-llama/Meta-Llama-3-8B": 0.6561505118446186, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8335609159500358}}, {"question": "\u5728\u793e\u533a\u7814\u7a76\u4e2d\uff0c\u5168\u8c8c\u7814\u7a76\u5c31\u662f\nA. \u628a\u793e\u533a\u89c6\u4e3a\u4eba\u7c7b\u805a\u5c45\u751f\u6d3b\u7684\u7279\u6b8a\u7684\u7a7a\u95f4\u73b0\u8c61\nB. \u5206\u6790\u201c\u793e\u4f1a\u201d\u548c\u201c\u793e\u533a\u201d\u4e24\u79cd\u5bf9\u7acb\u7684\u793e\u4f1a\u8054\u7cfb\u5f62\u5f0f\u7684\u7c7b\u578b\nC. \u7efc\u5408\u6027\u7684\u8bb0\u5f55\u8c03\u67e5\uff0c\u8f83\u5c11\u7406\u8bba\u5206\u6790\nD. \u63cf\u8ff0\u793e\u533a\u7684\u5404\u4e2a\u4e0d\u540c\u90e8\u5206\u5e76\u89e3\u91ca\u8fd9\u4e9b\u4e0d\u540c\u90e8\u5206\u7684\u76f8\u4e92\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.48876095956139387, "meta-math/MetaMath-Mistral-7B": 0.5311212419733762, "itpossible/Chinese-Mistral-7B-v0.1": 0.5885469212539914, "HuggingFaceH4/zephyr-7b-beta": 0.9454859968123731, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5759519186285699, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5481730319485998}}, {"question": "\u7802\u7c92\u6837\u7ec6\u80de\u6838\u4e3b\u8981\u89c1\u4e8e\u7684\u75c5\u6bd2\u6027\u809d\u708e\u662f\nA. \u4e19\u578b\u809d\u708e\nB. \u620a\u578b\u809d\u708e\nC. \u4e59\u578b\u809d\u708e\nD. \u7532\u578b\u809d\u708e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4908800361925515, "meta-math/MetaMath-Mistral-7B": 0.6361127519366742, "itpossible/Chinese-Mistral-7B-v0.1": 0.3403147063768956, "HuggingFaceH4/zephyr-7b-beta": 0.5383855293185753, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5893012202001757, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5363029429694058}}, {"question": "\u5149\u901f\u4e3a$3\\times10^{8}$\u7c73\u6bcf\u79d2\uff0c\u592a\u9633\u4e0e\u5730\u7403\u4e4b\u95f4\u7684\u8ddd\u79bb\u4e3a$1.5\\times10^{11}$\u7c73\uff0c\u5219\u592a\u9633\u5149\u5230\u5730\u7403\u9700\u8981\uff08\uff09\u79d2\nA. $5\\times10^{3}$\nB. $5\\times10^{4}$\nC. $5\\times10^{2}$\nD. $2\\times10^{2}$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3051050775177134, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.35686329776860143}}, {"question": "\u5728\u4ee5\u4e0b\u68c0\u9a8c\u65b9\u6cd5\u4e2d\uff0c\u4e0d\u5c5e\u4e8e\u975e\u53c2\u6570\u7edf\u8ba1\u65b9\u6cd5()\nA. $\\mathrm{T}$\u68c0\u9a8c\nB. $\\mathrm{t}$\u68c0\u9a8c\nC. $\\mathrm{H}$\u68c0\u9a8c\nD. $\\chi^2$\u68c0\u9a8c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.26560468668687814, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8fd1\u51e0\u5e74\u5168\u7403\u6380\u8d77\u4e86Internet\u70ed\uff0c\u5728Internet\u4e0a\u80fd\u591f\nA. \u6253\u56fd\u9645\u957f\u9014\u7535\u8bdd\nB. \u7f51\u4e0a\u8d2d\u7269\nC. \u4ee5\u4e0a\u90fd\u5bf9\nD. \u7f51\u4e0a\u770b\u75c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9369413468059179, "meta-math/MetaMath-Mistral-7B": 0.9868525471419416, "itpossible/Chinese-Mistral-7B-v0.1": 0.7104235029220421, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9897087270905567, "meta-llama/Meta-Llama-3-8B": 0.6781788346961699, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9870423119722361}}, {"question": "\u6e05\u653f\u5e9c\u9274\u4e8e\u8fb9\u7586\u5730\u533a\u4e0e\u4e2d\u539f\u4e4b\u95f4\u3001\u8fb9\u7586\u5404\u5730\u533a\u4e4b\u95f4\u5386\u53f2\u6587\u5316\u4f20\u7edf\u5747\u6709\u8f83\u5927\u5dee\u5f02\uff0c\u56e0\u6b64\u91c7\u53d6\u201c\u4fee\u5176\u6559\u4e0d\u6613\u5176\u4fd7\uff0c\u9f50\u5176\u653f\u4e0d\u6613\u5176\u5b9c\u201d\u7684\u653f\u7b56\u3002\u8fd9\u53cd\u6620\u51fa\u6e05\u671d\u6cbb\u7406\u8fb9\u7586\u7684\u57fa\u672c\u653f\u7b56\u662f\nA. \u519b\u4e8b\u63a7\u5236\nB. \u56e0\u5730\u5236\u5b9c\nC. \u62db\u629a\u6388\u5b98\nD. \u6269\u5927\u8d38\u6613\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8943218377042439, "meta-math/MetaMath-Mistral-7B": 0.9940684159702176, "itpossible/Chinese-Mistral-7B-v0.1": 0.9073240924936364, "HuggingFaceH4/zephyr-7b-beta": 0.979820273967847, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9485278754578027, "meta-llama/Meta-Llama-3-8B": 0.9437327528424052, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9986374219274622}}, {"question": "\u9053\u5fb7\u5efa\u8bbe\u7684\u6838\u5fc3\uff0c\u4f53\u73b0\u5e76\u51b3\u5b9a\u7740\u793e\u4f1a\u9053\u5fb7\u5efa\u8bbe\u7684\u6839\u672c\u6027\u8d28\u548c\u53d1\u5c55\u65b9\u5411\u3002\u6211\u56fd\u793e\u4f1a\u4e3b\u4e49\u9053\u5fb7\u5efa\u8bbe\u7684\u6838\u5fc3\u662f\nA. \u515a\u7684\u9886\u5bfc\nB. \u4ee5\u5fb7\u6cbb\u56fd\nC. \u96c6\u4f53\u4e3b\u4e49\nD. \u4e3a\u4eba\u6c11\u670d\u52a1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8302762168401713, "meta-math/MetaMath-Mistral-7B": 0.9640734571688371, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9935845560230676, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728 k-\u5747\u503c\u7b97\u6cd5\u4e2d\uff0c\u4ee5\u4e0b\u54ea\u4e2a\u9009\u9879\u53ef\u7528\u4e8e\u83b7\u5f97\u5168\u5c40\u6700\u5c0f\uff1f\nA. \u4ee5\u4e0a\u6240\u6709\nB. \u627e\u5230\u96c6\u7fa4\u7684\u6700\u4f73\u6570\u91cf\nC. \u8c03\u6574\u8fed\u4ee3\u7684\u6b21\u6570\nD. \u5c1d\u8bd5\u4e3a\u4e0d\u540c\u7684\u8d28\u5fc3\uff08centroid\uff09\u521d\u59cb\u5316\u8fd0\u884c\u7b97\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5725761621239777, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u52a8\u8109\u8212\u5f20\u538b\u4e3b\u8981\u53cd\u6620\nA. \u5927\u52a8\u8109\u987a\u5e94\u6027\nB. \u5faa\u73af\u8840\u91cf\nC. \u5916\u5468\u963b\u529b\nD. \u5fc3\u640f\u51fa\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.47315990612848036, "meta-math/MetaMath-Mistral-7B": 0.5424536191525299, "itpossible/Chinese-Mistral-7B-v0.1": 0.6137511835335298, "HuggingFaceH4/zephyr-7b-beta": 0.8688483923840555, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3500288890872089, "meta-llama/Meta-Llama-3-8B": 0.3748084697576336, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6366932490595311}}, {"question": "\u201c\u76d7\u2014\u7a83\u2014\u6518\u201d\u8fd9\u7ec4\u540c\u4e49\u8bcd\u5728\uff08\uff09\u4e0a\u6709\u6240\u4e0d\u540c\u3002\nA. \u4f7f\u7528\u6761\u4ef6\nB. \u611f\u60c5\u8272\u5f69\nC. \u8bcd\u4e49\u8f7b\u91cd\nD. \u8bed\u6cd5\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4403307015169764, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9717601343740662, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5171541280018161, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9272221411575583}}, {"question": "\u5386\u53f2\u6d2a\u6c34\u7684\u6d2a\u5cf0\u6d41\u91cf\u662f\u7531( )\u5f97\u5230\u7684\u3002\nA. \u7531\u8c03\u67e5\u7684\u5386\u53f2\u6d2a\u6c34\u7684\u6d2a\u5cf0\u6c34\u4f4d\u67e5\u6c34\u4f4d\u6d41\u91cf\u5173\u7cfb\u66f2\u7ebf\nB. \u5411\u7fa4\u4f17\u8c03\u67e5\nC. \u67e5\u5f53\u5730\u6d2a\u5cf0\u6d41\u91cf\u7684\u9891\u7387\u66f2\u7ebf\nD. \u5728\u8c03\u67e5\u65ad\u9762\u8fdb\u884c\u6d4b\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6806653832942486, "meta-math/MetaMath-Mistral-7B": 0.9921939530133375, "itpossible/Chinese-Mistral-7B-v0.1": 0.8336637063737012, "HuggingFaceH4/zephyr-7b-beta": 0.9999866152449847, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9665153055692668, "meta-llama/Meta-Llama-3-8B": 0.8760584915237589, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8161259231295245}}, {"question": "\u4e16\u754c\u4e0a\u6700\u5927\u7684\u9cb8\u662f\u4ec0\u4e48\u9cb8\nA. \u864e\u9cb8\nB. \u62b9\u9999\u9cb8\nC. \u89d2\u9cb8\nD. \u84dd\u9cb8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8370536182506161, "meta-math/MetaMath-Mistral-7B": 0.9544350012108515, "itpossible/Chinese-Mistral-7B-v0.1": 0.9540985783066259, "HuggingFaceH4/zephyr-7b-beta": 0.9992228588848139, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7868727847064041, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u2f00\u7269\u4f53\u9759\u2f4c\u5728\u5347\u964d\u673a\u7684\u2f54\u5e73\u5e95\u677f\u4e0a\uff0c\u5728\u5347\u964d\u673a\u52a0\u901f\u4e0a\u5347\u7684\u8fc7\u7a0b\u4e2d\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u91cd\u2f12\u5bf9\u7269\u4f53\u505a\u6b63\u529f\nB. \u5e95\u677f\u5bf9\u7269\u4f53\u7684\u2f40\u6301\u2f12\u4e0d\u505a\u529f\nC. \u5408\u5916\u2f12\u5bf9\u7269\u4f53\u4e0d\u505a\u529f\nD. \u5e95\u677f\u5bf9\u7269\u4f53\u7684\u2f40\u6301\u2f12\u505a\u6b63\u529f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31110415356189847, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4944953719015023, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4fc3\u4f7f\u9053\u5fb7\u8ba4\u8bc6\u548c\u9053\u5fb7\u60c5\u611f\u5b9e\u9645\u53d1\u751f\u4f5c\u7528\u7684\u662f\nA. \u9053\u5fb7\u7406\u6027\nB. \u9053\u5fb7\u611f\u6027\nC. \u9053\u5fb7\u89c2\u5ff5\nD. \u9053\u5fb7\u610f\u5fd7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7141627887137693, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u7c7b\u56fd\u5bb6\u9009\u62e9\u7684\u662f\u52b3\u52a8\u96c6\u7ea6\u578b\u7684\u519c\u4e1a\u73b0\u4ee3\u5316\u53d1\u5c55\u9053\u8def\nA. \u7f8e\u56fd\nB. \u6cd5\u56fd\u3001\u5fb7\u56fd\nC. \u65e5\u672c\nD. \u52a0\u62ff\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4980398245889473, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.583673613959256}}, {"question": "\u6839\u636e\u300a\u5546\u6807\u6cd5\u300b\u7684\u6709\u5173\u89c4\u5b9a\uff0c\u5546\u6807\u4e13\u7528\u6743\u7684\u4fdd\u62a4\u671f\u9650\u662f\nA. 15\u5e74\nB. 20\u5e74\nC. 30\u5e74\nD. 10\u5e74\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f5b\u6559\u6700\u65e9\u4f20\u5165\u4e2d\u56fd\u7684\u7ecf\u5178\u662f\nA. \u822c\u82e5\u7ecf\nB. \u6cd5\u53e5\u7ecf\nC. \u963f\u542b\u7ecf\nD. \u56db\u5341\u4e8c\u7ae0\u7ecf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u2f00\u822c\u5c06\u8ba1\u7b97\u673a\u7684\u53d1\u5c55\u5206\u4e3a\u56db\u4e2a\u9636\u6bb5\uff0c\u5176\u4e2d\u2f24\u89c4\u6a21\u96c6\u6210\u7535\u8def\u8ba1\u7b97\u673a\u9636\u6bb5\u51fa\u73b0\u4e86\nA. \u2fbc\u7ea7\u7a0b\u5e8f\u8bbe\u8ba1\u8bed\u2f94\nB. \u64cd\u4f5c\u7cfb\u7edf\nC. \u6c47\u7f16\u8bed\u2f94\nD. \u6570\u636e\u5e93\u7cfb\u7edf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728Word\u7684\u7f16\u8f91\u72b6\u6001\uff0c\u6267\u884c\u7f16\u8f91\u83dc\u5355\u4e2d\u201c\u590d\u5236\u201d\u547d\u4ee4\u540e\nA. \u63d2\u5165\u70b9\u6240\u5728\u7684\u6bb5\u843d\u5185\u5bb9\u88ab\u590d\u5236\u5230\u526a\u8d34\u677f\nB. \u5149\u6807\u6240\u5728\u7684\u6bb5\u843d\u5185\u5bb9\u88ab\u590d\u5236\u5230\u526a\u8d34\u677f\nC. \u88ab\u9009\u62e9\u7684\u5185\u5bb9\u88ab\u590d\u5236\u5230\u63d2\u5165\u70b9\u5904\nD. \u88ab\u9009\u62e9\u7684\u5185\u5bb9\u88ab\u590d\u5236\u5230\u526a\u8d34\u677f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8113316303400834, "meta-math/MetaMath-Mistral-7B": 0.969891183495399, "itpossible/Chinese-Mistral-7B-v0.1": 0.6249629574439266, "HuggingFaceH4/zephyr-7b-beta": 0.9973465369960646, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8965172651091298, "meta-llama/Meta-Llama-3-8B": 0.873379683732212, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u5408\u540c\u6cd5\u7684\u89c4\u5b9a\uff0c\u5408\u540c\u6210\u7acb\u7684\u65f6\u95f4\u662f\nA. \u5408\u540c\u786e\u8ba4\u4e66\u7b7e\u8ba2\u65f6\nB. \u627f\u8bfa\u751f\u6548\u65f6\nC. \u627f\u8bfa\u53d1\u51fa\u65f6\nD. \u8981\u7ea6\u5230\u8fbe\u53d7\u8981\u7ea6\u4eba\u65f6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7201578783369469, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6193566562873832, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bf7\u9009\u51fa\u4e0b\u5217\u7684\u6210\u8bed\u4e3b\u4eba\u516c\u642d\u914d\u6b63\u786e\u7684\u4e00\u9879\nA. \u4e50\u4e0d\u601d\u8700\u2014\u2014\u5218\u7985\nB.  \u671b\u6885\u6b62\u6e34\u2014\u2014\u5218\u90a6\nC. \u6307\u9e7f\u4e3a\u9a6c\u2014\u2014\u8d75\u6784\nD. \u97e6\u7f16\u4e09\u7edd\u2014\u2014\u8001\u5b50\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6411027397809216, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7719798912289793, "meta-llama/Meta-Llama-3-8B": 0.30308918294772175, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6587\u660e\u793c\u8c8c\u3001\u52a9\u4eba\u4e3a\u4e50\u3001\u7231\u62a4\u516c\u7269\u3001\u4fdd\u62a4\u73af\u5883\u3001\u9075\u7eaa\u5b88\u6cd5\uff0c\u662f\u6211\u56fd\u793e\u4f1a\u4e3b\u4e49\u9053\u5fb7\u5efa\u8bbe\u4e2d\nA. \u793e\u4f1a\u516c\u5fb7\u7684\u4e3b\u8981\u5185\u5bb9\nB. \u5bb6\u5ead\u7f8e\u5fb7\u7684\u4e3b\u8981\u5185\u5bb9\nC. \u73af\u5883\u9053\u5fb7\u7684\u4e3b\u8981\u5185\u5bb9\nD. \u804c\u4e1a\u9053\u5fb7\u7684\u4e3b\u8981\u5185\u5bb9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9039308912643009, "meta-math/MetaMath-Mistral-7B": 0.9794636485243919, "itpossible/Chinese-Mistral-7B-v0.1": 0.878156481078362, "HuggingFaceH4/zephyr-7b-beta": 0.999976880167264, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.99607941368194, "meta-llama/Meta-Llama-3-8B": 0.9754258105952738, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9977681210997862}}, {"question": "\u63d0\u51fa\u6559\u80b2\u5177\u6709\u76f8\u5bf9\u72ec\u7acb\u6027\uff0c\u4e3b\u8981\u662f\u5f3a\u8c03\u6559\u80b2 ()\nA. \u4e0d\u53d7\u751f\u4ea7\u53d1\u5c55\u5236\u7ea6\nB. \u6709\u81ea\u8eab\u7684\u7279\u70b9\u548c\u89c4\u5f8b\nC. \u5bf9\u653f\u6cbb\u3001\u7ecf\u6d4e\u6709\u4fc3\u8fdb\u4f5c\u7528\nD. \u53ef\u4ee5\u8d85\u8d8a\u793e\u4f1a\u5386\u53f2\u800c\u5b58\u5728\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6612779629520119, "meta-math/MetaMath-Mistral-7B": 0.6884642892967837, "itpossible/Chinese-Mistral-7B-v0.1": 0.8049257332539287, "HuggingFaceH4/zephyr-7b-beta": 0.9932098984081434, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8145803099553383, "meta-llama/Meta-Llama-3-8B": 0.9254297902255058, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9500286841776103}}, {"question": "2019\u5e745\u6708\uff0cSpaceX\u516c\u53f8\u901a\u8fc7\u4e00\u7bad60\u661f\u7684\u65b9\u5f0f\u5c06\u9996\u6279\uff08\uff09\u536b\u661f\u9001\u5165\u592a\u7a7a\uff0cIAU\u8868\u793a\u8fd9\u4f1a\u5bf9\u5929\u6587\u89c2\u6d4b\u4ea7\u751f\u4e25\u91cd\u7684\u5f71\u54cd\nA. Starlink\nB. Telesat\nC. \u7acb\u65b9\u661f\nD. OneWeb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8809614602147664, "meta-math/MetaMath-Mistral-7B": 0.9935988251641836, "itpossible/Chinese-Mistral-7B-v0.1": 0.9428402073340573, "HuggingFaceH4/zephyr-7b-beta": 0.9986736536421903, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9251787018748255, "meta-llama/Meta-Llama-3-8B": 0.5981025891503571, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.523391355471761}}, {"question": "\u5375\u5706\u7a9d\u4f4d\u4e8e\nA. \u53f3\u5fc3\u5ba4\nB. \u5de6\u5fc3\u623f\nC. \u5de6\u5fc3\u5ba4\nD. \u53f3\u5fc3\u623f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.29068935354339726, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5661394068176863, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u300a\u7ee7\u627f\u6cd5\u300b\u7b2c16\u6761\u7b2c2\u6b3e\u89c4\u5b9a\uff1a\u516c\u6c11\u53ef\u4ee5\u7acb\u9057\u5631\u5c06\u4e2a\u4eba\u8d22\u4ea7\u6307\u5b9a\u7531\u6cd5\u5b9a\u7ee7\u627f\u4eba\u7684\u4e00\u4eba\u6216\u8005\u6570\u4eba\u7ee7\u627f\u3002\u4ece\u6cd5\u7684\u89c4\u8303\u4f5c\u7528\u770b\uff0c\u8be5\u9879\u89c4\u5b9a\u5c5e\u4e8e\u4e0b\u5217\u54ea\u79cd\u60c5\u51b5\uff1f\nA. \u6709\u9009\u62e9\u6307\u5f15\nB. \u975e\u89c4\u8303\u6027\u6307\u5f15\nC. \u786e\u5b9a\u6307\u5f15\nD. \u4e2a\u522b\u6307\u5f15\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4134295178319914, "itpossible/Chinese-Mistral-7B-v0.1": 0.2731272040287072, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.724517558851211, "meta-llama/Meta-Llama-3-8B": 0.49708932203180867, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6216532143177583}}, {"question": "\u53d8\u538b\u5668\u6295\u5207\u65f6\u4f1a\u4ea7\u751f\nA. \u64cd\u4f5c\u8fc7\u7535\u538b\nB. \u5927\u6c14\u8fc7\u7535\u538b\nC. \u96f7\u51fb\u8fc7\u7535\u538b\nD. \u7cfb\u7edf\u8fc7\u7535\u538b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.284388353700082, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4915116336064247, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5528630996364458, "meta-llama/Meta-Llama-3-8B": 0.3586670609407181, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7089109927746081}}, {"question": "\u4e00\u53f0\u591a\u5a92\u4f53\u7535\u8111\u4e2d\uff0c\u4e0b\u5217\u54ea\u4e00\u79cd\u8bbe\u5907\u662f\u7528\u6765\u5904\u7406\u58f0\u97f3\u7684\nA. \u58f0\u5361 \nB. \u6444\u50cf\u5934\nC. \u5149\u9a71\nD. \u626b\u63cf\u4eea\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9885148999823274, "meta-math/MetaMath-Mistral-7B": 0.9997957479060045, "itpossible/Chinese-Mistral-7B-v0.1": 0.9620086032926324, "HuggingFaceH4/zephyr-7b-beta": 0.9999783001933553, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9985818089287797, "meta-llama/Meta-Llama-3-8B": 0.9903498765168018, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9998380449094539}}, {"question": "\u957f\u6c5f\u4e09\u5ce1\u5de5\u7a0b\u7684\u6821\u6838\u6d2a\u6c34\u4f4d\u548c\u8bbe\u8ba1\u6d2a\u6c34\u4f4d\u5206\u522b\u4e3a[ ]\u3002\nA. 185.0m\u3001 180.0m\nB. 175.0m\u3001180.0m\nC. 180.4m\u3001175.0m\nD. 155.0m\u3001145.0m\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4690665965583234, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8329519459309158, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4006796901109581, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8900182398106254}}, {"question": "\u6cd5\u56fd\u6cd5\u5f8b\u89c4\u5b9a\uff1a\u4e0d\u52a8\u4ea7\u7684\u6cd5\u5b9a\u7ee7\u627f\u4f9d\u4e0d\u52a8\u4ea7\u6240\u5728\u5730\u6cd5\uff1b\u5fb7\u56fd\u6cd5\u5f8b\u89c4\u5b9a\uff1a\u4e0d\u52a8\u4ea7\u6cd5\u5b9a\u7ee7\u627f\u4f9d\u6b7b\u8005\u672c\u56fd\u6cd5\uff1b\u4e14\u4e24\u56fd\u90fd\u8ba4\u4e3a\u81ea\u5df1\u6307\u5b9a\u7684\u6cd5\u5f8b\u4e5f\u5305\u62ec\u51b2\u7a81\u6cd5\u3002\u73b0\u6709\u4e00\u5fb7\u56fd\u516c\u6c11\u6b7b\u4e8e\u6cd5\u56fd\u5e76\u5728\u6cd5\u56fd\u7559\u4e0b\u4e0d\u52a8\u4ea7\uff0c\u4e3a\u6b64\u4e0d\u52a8\u4ea7\u6cd5\u5b9a\u7ee7\u627f\u53d1\u751f\u4e89\u8bbc\uff0c\u5176\u7ed3\u679c\u662f\nA. \u5728\u6cd5\u56fd\u8d77\u8bc9\u4f1a\u53d1\u751f\u53cd\u81f4\nB. \u5728\u4e24\u56fd\u4e2d\u4efb\u4e00\u56fd\u8d77\u8bc9\u90fd\u4e0d\u4f1a\u53d1\u751f\u53cd\u81f4\nC. \u5728\u5fb7\u56fd\u8d77\u8bc9\u4f1a\u53d1\u751f\u53cd\u81f4\nD. \u5728\u4e24\u56fd\u4e2d\u4efb\u4e00\u56fd\u8d77\u8bc9\u90fd\u4f1a\u53d1\u751f\u53cd\u81f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3060136256597631, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.31292412927280244, "meta-llama/Meta-Llama-3-8B": 0.4528728380746746, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "HIV \u7684\u201c\u50a8\u5907\u6c60\u201d\u662f\nA. CD4+T \u7ec6\u80de\nB. CD8+T \u7ec6\u80de\nC. B\u6dcb\u5df4\u7ec6\u80de\nD. \u6ee4\u6ce1\u6811\u7a81\u72b6\u7ec6\u80de\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3967757715930558, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u7fa4\u4f53\u7684\u5173\u7cfb\u7ed3\u6784\u548c\u7ec4\u7ec7\u529f\u80fd\u4e3a\u6807\u51c6\uff0c\u53ef\u4ee5\u628a\u7fa4\u4f53\u5206\u4e3a\nA. \u6b63\u5f0f\u7fa4\u4f53\u548c\u975e\u6b63\u5f0f\u7fa4\u4f53\nB. \u521d\u7ea7\u7fa4\u4f53\u548c\u6b21\u7ea7\u7fa4\u4f53\nC. \u6210\u5458\u7fa4\u4f53\u548c\u53c2\u7167\u7fa4\u4f53\nD. \u5185\u7fa4\u4f53\u548c\u5916\u7fa4\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33932272093070465, "meta-math/MetaMath-Mistral-7B": 0.32199616367329575, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6308803820978645, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.48148338235486976}}, {"question": "\u7532\u6bcf\u79d2\u8dd13\u7c73\uff0c\u4e59\u6bcf\u79d2\u8dd12\u7c73\uff0c\u4e19\u6bcf\u79d2\u8dd14\u7c73\uff0c\u4e09\u4eba\u6cbf600\u7c73\u7684\u73af\u5f62\u8dd1\u9053\u4ece\u540c\u4e00\u70b9\u540c\u65f6\u540c\u5411\u8dd1\u6b65\uff0c\u7ecf\u8fc7()\u79d2\u4e09\u4eba\u53c8\u540c\u65f6\u4ece\u51fa\u53d1\u70b9\u51fa\u53d1\nA. 300\nB. 12\nC. \u65e0\u6cd5\u786e\u5b9a\nD. 600\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u56fd\u9645\u5173\u7cfb\u4e2d\uff0c\u4e3b\u6743\u56fd\u5bb6\u4f1a\u5728\u5efa\u4ea4\u56fd\u8bbe\u7acb\u5916\u4ea4\u4ee3\u8868\u673a\u6784\uff0c\u5916\u4ea4\u4ee3\u8868\u673a\u6784\u901a\u5e38\u5212\u4e3a\u4e09\u4e2a\u7b49\u7ea7\uff0c\u4ee5\u4e0b\u9009\u9879\u4e2d\uff0c\u4e0d\u5c5e\u4e8e\u4e09\u4e2a\u7b49\u7ea7\u7684\u5916\u4ea4\u4ee3\u8868\u673a\u6784\u662f\nA. \u9886\u4e8b\u9986\nB. \u4ee3\u529e\u5904\nC. \u516c\u4f7f\u9986\nD. \u5927\u4f7f\u9986\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7028036952081694}}, {"question": "\u526f\u6eb6\u8840\u6027\u5f27\u83cc\u5f15\u8d77\u7684\u98df\u6e90\u6027\u75be\u75c5\u5360\u4eba\u7fa4\u7684\nA. 0.055\nB. 0.025\nC. 0.015\nD. 0.035\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3837616204620929, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3128363857141096, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6cd5\u5f8b\u89c4\u5219\u5206\u4e3a\u6388\u6743\u6027\u89c4\u5219\u3001\u547d\u4ee4\u6027\u89c4\u5219\u548c\u7981\u6b62\u6027\u89c4\u5219\uff0c\u5176\u5206\u7c7b\u6807\u51c6\u662f\nA. \u7279\u5b9a\u884c\u4e3a\u4ee5\u524d\u662f\u5426\u6709\u8c03\u6574\u89c4\u5219\nB. \u8c03\u6574\u65b9\u5f0f\u7684\u4e0d\u540c\nC. \u884c\u4e3a\u6a21\u5f0f\u7684\u4e0d\u540c\nD. \u6cd5\u5f8b\u89c4\u8303\u5185\u5bb9\u662f\u5426\u786e\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u751f\u7269\u8fdb\u5316\u7684\u89c2\u70b9\u4e2d\uff0c\u7b26\u5408\u73b0\u4ee3\u751f\u7269\u8fdb\u5316\u7406\u8bba\u7684\u662f\nA. \u751f\u7269\u8fdb\u5316\u7684\u5b9e\u8d28\u662f\u79cd\u7fa4\u57fa\u56e0\u9891\u7387\u7684\u6539\u53d8\uff0c\u751f\u7269\u8fdb\u5316\u4e00\u5b9a\u4f1a\u4ea7\u751f\u65b0\u7684\u7269\u79cd\nB. \u53ea\u6709\u751f\u6b96\u9694\u79bb\u80fd\u963b\u6b62\u79cd\u7fa4\u95f4\u7684\u57fa\u56e0\u4ea4\u6d41\uff0c\u81ea\u7136\u9009\u62e9\u662f\u7269\u79cd\u671d\u4e0d\u540c\u65b9\u5411\u53d1\u5c55\u7684\u51b3\u5b9a\u6027\u56e0\u7d20\nC. \u57fa\u56e0\u7a81\u53d8\u5177\u6709\u201c\u5c11\u5229\u591a\u5bb3\u201d\u7684\u7279\u6027\uff0c\u81ea\u7136\u9009\u62e9\u5bfc\u81f4\u7a81\u53d8\u4ea7\u751f\u5e76\u5bf9\u6709\u5229\u53d8\u5f02\u8fdb\u884c\u5b9a\u5411\u79ef\u7d2f\nD. \u81ea\u7136\u9009\u62e9\u548c\u9057\u4f20\u6f02\u53d8\u7b49\u4f1a\u6539\u53d8\u79cd\u7fa4\u7684\u57fa\u56e0\u9891\u7387\uff0c\u5176\u5076\u7136\u6027\u968f\u79cd\u7fa4\u6570\u91cf\u589e\u52a0\u800c\u51cf\u5c0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.768763854390101, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u7b97\u6cd5\u8f93\u51fa\u7684\u53d9\u8ff0\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u7b97\u6cd5\u2f00\u5b9a\u6ca1\u6709\u8f93\u51fa\nB. \u7b97\u6cd5\u53ef\u4ee5\u6ca1\u6709\u8f93\u51fa\nC. \u7b97\u6cd5\u2f84\u5c11\u6709\u2f00\u4e2a\u8f93\u51fa\nD. \u7b97\u6cd5\u5fc5\u987b\u6709\u591a\u4e2a\u8f93\u51fa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5530830332201014, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5086910302762513, "HuggingFaceH4/zephyr-7b-beta": 0.9974144031900135, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8618906146259807, "meta-llama/Meta-Llama-3-8B": 0.7936427071141706, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.995419789683819}}, {"question": "\u5728\u76f4\u2ec6\u5750\u6807\u7cfb\u4e2d\uff0c\u5df2\u77e5A\uff083\uff0c3\uff09\uff0c\u5728x\u8f74\u3001y\u8f74\u4e0a\u786e\u5b9a\u2f00\u70b9P\uff0c\u4f7f\u25b3AOP\u4e3a\u7b49\u8170\u4e09\u2ec6\u5f62\uff0c\u5219\u7b26\u5408\u6761\u4ef6\u7684\u70b9P\u5171\u6709\nA. 6\u4e2a \nB. 8\u4e2a \nC. 10\u4e2a \nD. 4\u4e2a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2611628089355486, "meta-math/MetaMath-Mistral-7B": 0.33482349867891054, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53d9\u8ff0\u6b63\u786e\u7684\nA. \u5229\u7528\u5316\u5b66\u65b9\u6cd5\uff0c\u6211\u4eec\u53ef\u4ee5\u5236\u9020\u51fa\u65b0\u7684\u5206\u5b50\uff0c\u4f46\u4e0d\u80fd\u5236\u9020\u51fa\u65b0\u7684\u539f\u5b50\nB. \u5411\u6eb6\u6db2\u4e2d\u52a0\u5165BaCl2\u6eb6\u6db2\uff0c\u4ea7\u751f\u4e0d\u6eb6\u4e8e\u785d\u9178\u7684\u767d\u8272\u6c89\u6dc0\uff0c\u8be5\u6eb6\u6db2\u4e2d\u4e00\u5b9a\u542b\u6709Ag\uff0b\nC. \u660e\u77fe\u6c34\u89e3\u65f6\u4ea7\u751f\u5177\u6709\u5438\u9644\u6027\u7684\u80f6\u4f53\u7c92\u5b50\uff0c\u6545\u660e\u77fe\u53ef\u4f5c\u6f02\u767d\u5242\nD. \u201c\u4f4e\u78b3\u7ecf\u6d4e\u201d\u63d0\u5021\u5927\u91cf\u4f7f\u7528\u7164\u3001\u77f3\u6cb9\u3001\u5929\u7136\u6c14\u7b49\u5316\u77f3\u71c3\u6599\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3291538467926473, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.35286194820709277, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u5211\u4e8b\u8bc9\u8bbc\u6cd5\u7684\u89c4\u5b9a\uff0c()\u5728\u5ba1\u5224\u5211\u4e8b\u6848\u4ef6\u7684\u8fc7\u7a0b\u4e2d\uff0c\u53ef\u4ee5\u4f9d\u6cd5\u8ba4\u5b9a\u6050\u6016\u6d3b\u52a8\u7ec4\u7ec7\u548c\u4eba\u5458\u3002\nA. \u6700\u9ad8\u4eba\u6c11\u6cd5\u9662\nB. \u6709\u7ba1\u8f96\u6743\u7684\u9ad8\u7ea7\u4ee5\u4e0a\u4eba\u6c11\u6cd5\u9662\nC. \u6709\u7ba1\u8f96\u6743\u7684\u57fa\u5c42\u4ee5\u4e0a\u4eba\u6c11\u6cd5\u9662\nD. \u6709\u7ba1\u8f96\u6743\u7684\u4e2d\u7ea7\u4ee5\u4e0a\u4eba\u6c11\u6cd5\u9662\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u884c\u4e3a\u79d1\u5b66\u7684\u7814\u7a76\uff0c\u4eba\u7684\u9700\u8981\u3001\u52a8\u673a\u3001\u884c\u4e3a\u4e0e\u6ee1\u8db3\u4e4b\u95f4\u7684\u5173\u7cfb\u53ef\u4ee5\u7528\u7b80\u5355\u7684\u6a21\u5f0f\u8868\u793a\u4e3a\nA. \u9700\u6c42\u2192\u884c\u4e3a\u2192\u52a8\u673a\u2192\u6ee1\u8db3\nB. \u884c\u4e3a\u2192\u52a8\u673a\u2192\u9700\u6c42\u2192\u6ee1\u8db3\nC. \u52a8\u673a\u2192\u9700\u6c42\u2192\u884c\u4e3a\u2192\u6ee1\u8db3\nD. \u9700\u6c42\u2192\u52a8\u673a\u2192\u884c\u4e3a\u2192\u6ee1\u8db3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5101191018276479, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5553635629679738, "HuggingFaceH4/zephyr-7b-beta": 0.9991566859637598, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4503887184158778, "meta-llama/Meta-Llama-3-8B": 0.7693515612330021, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u843d\u5b9e\u65bd\u5de5\u751f\u4ea7\u5b89\u5168\u4e8b\u6545\u62a5\u544a\u548c\u8c03\u67e5\u5904\u7406\u201c\u56db\u4e0d\u653e\u8fc7\u201d\u539f\u5219\u7684\u6838\u5fc3\u73af\u8282\u662f\nA. \u4e8b\u6545\u5904\u7406\nB. \u4e8b\u6545\u62a5\u544a\nC. \u4e8b\u6545\u8c03\u67e5\nD. \u4e8b\u6545\u95ee\u8d23\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f20\u67d0\u5728\u4e08\u592b\u53bb\u4e16\u540e\uff0c\u4e0e\u5176\u4fdd\u59c6\u674e\u67d0\u7b7e\u8ba2\u4e86\u9057\u8d60\u6276\u517b\u534f\u8bae\uff0c\u5f20\u67d0\u7684\u5b50\u5973\u5f97\u77e5\u540e\u4e0d\u8ba4\u53ef\u8be5\u9057\u8d60\u6276\u517b\u534f\u8bae\u7684\u6548\u529b\u3002\u8be5\u9057\u8d60\u6276\u517b\u534f\u8bae\u7684\u6548\u529b\u4e3a\nA. \u6709\u6548\nB. \u6548\u529b\u5f85\u5b9a\nC. \u53ef\u64a4\u9500\nD. \u65e0\u6548\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u56fd\u5bb6\u673a\u6784\u662f\u56fd\u5bb6\u4e3a\u5b9e\u73b0\u5176\u7ba1\u7406\u793e\u4f1a\u804c\u80fd\u800c\u5efa\u7acb\u8d77\u6765\u7684\u56fd\u5bb6\u673a\u5173\u7684\u603b\u548c\u3002\u5728\u6211\u56fd\u56fd\u5bb6\u673a\u6784\u4e2d\u5c45\u4e8e\u6700\u9ad8\u5730\u4f4d\u7684\u56fd\u5bb6\u673a\u5173\u662f\nA. \u5168\u56fd\u4eba\u5927\u53ca\u5176\u5e38\u59d4\u4f1a\nB. \u6700\u9ad8\u4eba\u6c11\u6cd5\u9662\nC. \u56fd\u52a1\u9662\nD. \u56fd\u5bb6\u4e3b\u5e2d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8345145157648683, "meta-math/MetaMath-Mistral-7B": 0.9899433990370591, "itpossible/Chinese-Mistral-7B-v0.1": 0.9491601569565667, "HuggingFaceH4/zephyr-7b-beta": 0.9997665470708319, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8379729167858266, "meta-llama/Meta-Llama-3-8B": 0.9844831977490945, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9265623578073536}}, {"question": "\u626b\u63cf\u4eea\u5c5e\u4e8e\nA. \u5916\u5b58\u50a8\u5668\nB. \u8f93\u5165\u8bbe\u5907\nC. \u5185\u5b58\u50a8\u5668\nD. \u8f93\u51fa\u8bbe\u5907\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9828778006975173, "meta-math/MetaMath-Mistral-7B": 0.9995674088393831, "itpossible/Chinese-Mistral-7B-v0.1": 0.9069629523553449, "HuggingFaceH4/zephyr-7b-beta": 0.9999980053208909, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9978572434487912, "meta-llama/Meta-Llama-3-8B": 0.951569394018377, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9888719657197458}}, {"question": "\u4e0a\u7ea7\u7ba1\u7406\u7ec4\u7ec7\u6216\u4eba\u5458\u628a\u5404\u9879\u7ba1\u7406\u653f\u7b56\u53ca\u7ec4\u7ec7\u76ee\u6807\u3001\u5de5\u4f5c\u7a0b\u5e8f\u3001\u89c4\u7ae0\u5236\u5ea6\u9010\u7ea7\u5411\u4e0b\u4f20\u9012\u7684\u6c9f\u901a\u5c5e\u4e8e\nA. \u94fe\u5f0f\u6c9f\u901a\nB. \u53cc\u5411\u6c9f\u901a\nC. \u4e0b\u884c\u6c9f\u901a\nD. \u4e0a\u884c\u6c9f\u901a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5358178410422609, "meta-math/MetaMath-Mistral-7B": 0.8990051676839073, "itpossible/Chinese-Mistral-7B-v0.1": 0.30359886656533613, "HuggingFaceH4/zephyr-7b-beta": 0.8492540793901737, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5752682877942145, "meta-llama/Meta-Llama-3-8B": 0.7699977063013081, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.37883284186959215}}, {"question": "\u7528\u5730\u8d28\u7f57\u76d8\u6d4b\u5f97\u5ca9\u5c42\u9762\u503e\u5411\u4e3a\u5357\u504f\u4e1c30\u00b0\uff0c\u503e\u89d2\u4e3a60\u00b0\u8868\u793a\nA. 30Z150\nB. 30230\nC. 602150\nD. 150260\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\uff0c\u52a8\u5bbe\u5173\u7cfb\u5c5e\u4e8e\u4e3a\u52a8\u5173\u7cfb\u7684\u4e00\u53e5\u662f\nA. \u6545\u541b\u4eba\u8005\uff0c\u52de\u65bc\u7d22\u4e4b\uff0c\u800c\u4f11\u65bc\u4f7f\u4e4b\u3002\nB. \u541b\u805e\u800c\u8ce2\u4e4b\u3002\nC. \u6c42\u4e5f\u9000\uff0c\u6545\u9032\u4e4b\uff1b\u7531\u4e5f\u517c\u4eba\uff0c\u6545\u9000\u4e4b\u3002\nD. \u4f2f\u5937\u6b7b\u540d\u65bc\u9996\u967d\u4e4b\u4e0b\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4f20\u8f93\u4ecb\u8d28\u4e2d\uff0c\u6570\u636e\u4f20\u8f93\u5165\u80fd\u529b\u6700\u5f3a\u7684\u662f\nA. \u5149\u7ea4\nB. \u53cc\u7ede\u7ebf\nC. \u7535\u8bdd\u7ebf\nD. \u540c\u8f74\u7535\u7f06\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9429015613902586, "meta-math/MetaMath-Mistral-7B": 0.9985329703607037, "itpossible/Chinese-Mistral-7B-v0.1": 0.7201578723308697, "HuggingFaceH4/zephyr-7b-beta": 0.997524893157339, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9958209995068726, "meta-llama/Meta-Llama-3-8B": 0.8851347148118853, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9972851052828311}}, {"question": "\u300a\u9ec4\u5e1d\u5185\u7ecf\u300b\u4e2d\u63d0\u5230\u4e00\u79cd\u53e4\u8001\u7684\u9152\uff0c\u662f\u7528\u52a8\u7269\u7684\u4e73\u6c41\u917f\u6210\u7684\u751c\u9152\uff0c\u5373\nA. \u79ec\u9b2f\nB. \u91b4\u916a\nC. \u733f\u9152\nD. \u918d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5055494812777196, "itpossible/Chinese-Mistral-7B-v0.1": 0.7620353329590909, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f71\u54cd\u7ba1\u7406\u7cfb\u7edf\u751f\u5b58\u548c\u53d1\u5c55\u7684\u4e00\u5207\u8981\u7d20\u7684\u603b\u548c\u662f\nA. \u7ba1\u7406\u624b\u6bb5\nB. \u7ba1\u7406\u65b9\u6cd5\nC. \u7ba1\u7406\u73af\u5883\nD. \u7ba1\u7406\u9053\u5fb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9236305516601395, "meta-math/MetaMath-Mistral-7B": 0.9960535231533286, "itpossible/Chinese-Mistral-7B-v0.1": 0.9244997909173092, "HuggingFaceH4/zephyr-7b-beta": 0.999277087297565, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8938142970615985, "meta-llama/Meta-Llama-3-8B": 0.9516647206113972, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9931463139564577}}, {"question": "\u7f57\u9a6c\u6cd5\u8c1a\uff1a\u201c\u6cd5\u5f8b\u4e0d\u7406\u7410\u788e\u4e4b\u4e8b\u3002\u201d\u5bf9\u6b64\u8868\u8ff0\uff0c\u6b63\u786e\u7684\u7406\u89e3\u662f\nA. \u201c\u51e1\u4e8b\u7686\u8bc9\u8bbc\u201d\u662f\u73b0\u4ee3\u6cd5\u6cbb\u56fd\u5bb6\u7684\u6807\u5fd7\nB. \u6cd5\u5f8b\u5bf9\u4e8e\u81ea\u8eab\u80fd\u591f\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u7684\u95ee\u9898\u4e00\u5f8b\u52a0\u4ee5\u89c4\u5236\nC. \u6cd5\u5f8b\u65e0\u6cd5\u5bf9\u6240\u6709\u7684\u793e\u4f1a\u95ee\u9898\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\nD. \u201c\u7410\u788e\u4e4b\u4e8b\u201d\u4e0d\u5c5e\u4e8e\u6cd5\u5f8b\u8c03\u6574\u7684\u8303\u56f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6411639253299657, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9991174046717206, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.836569470719285, "meta-llama/Meta-Llama-3-8B": 0.7105484269432784, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679c\u628a\u6708\u7403\u538b\u6210\u9ed1\u6d1e\uff0c\u5b83\u7684\u53f2\u74e6\u897f\u534a\u5f84\u7ea6\nA. 0.1\u6beb\u7c73\nB. 1\u6beb\u7c73\nC. 1\u5398\u7c73\nD. 1\u5206\u7c73\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3492104466579257, "meta-math/MetaMath-Mistral-7B": 0.4467021344273751, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8782521894205056, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6054069806143936, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6211\u56fd\u8131\u8d2b\u653b\u575a\u5de5\u4f5c\u8bf4\u6cd5\u51c6\u786e\u7684\u662f\nA. \u52a8\u5458\u5168\u515a\u5168\u56fd\u5168\u793e\u4f1a\u529b\u91cf\uff0c\u575a\u6301\u7cbe\u51c6\u6276\u8d2b\uff0c\u7cbe\u51c6\u8131\u8d2b\nB. \u575a\u6301\u5148\u6276\u5fd7\uff0c\u518d\u6276\u667a\uff0c\u540e\u6276\u6280\u7684\u987a\u5e8f\nC. \u575a\u6301\u4e2d\u592e\u7edf\u7b79\u7701\u8d1f\u603b\u8d23\u53bf\u4e61\u6293\u843d\u5b9e\u7684\u5de5\u4f5c\u673a\u5236\nD. \u5f3a\u5316\u884c\u653f\u4e00\u628a\u624b\u8d1f\u603b\u8d23\u7684\u8d23\u4efb\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3768925079020566, "meta-math/MetaMath-Mistral-7B": 0.3444545218043904, "itpossible/Chinese-Mistral-7B-v0.1": 0.49165259161913094, "HuggingFaceH4/zephyr-7b-beta": 0.5978057410406162, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5723103380893134, "meta-llama/Meta-Llama-3-8B": 0.3879740469430858, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u665a\u81ea\u4e60\u65f6\uff0c\u9ad8\u8001\u5e08\u53d1\u73b0\u73ed\u4e0a\u7684\u4e00\u4f4d\u7537\u751f\u5728\u7ed9\u4e00\u4f4d\u5973\u751f\u9012\u7eb8\u6761\u3002\u9ad8\u8001\u5e08\u8d70\u4e0a\u524d\u53bb\u5bf9\u4ed6\u4eec\u8bf4\uff1a\u201c\u4f60\u4eec\u5728\u5e72\u561b?\u662f\u4e0d\u662f\u5728\u9012\u60c5\u4e66\u554a?\u73b0\u5728\u53ef\u4e0d\u662f\u8c08\u604b\u7231\u7684\u65f6\u5019\u554a\uff0c\u8003\u4e0a\u5927\u5b66\u540e\u518d\u8c08\u5427\u3002\u201d\u9ad8\u8001\u5e08\u7684\u58f0\u97f3\u4e0d\u5927\u4f46\u540c\u5b66\u4eec\u90fd\u542c\u5230\u4e86\uff0c\u8fd9\u4e24\u4f4d\u540c\u5b66\u987f\u65f6\u7f9e\u7ea2\u4e86\u8138\u3002\u5173\u4e8e\u9ad8\u8001\u5e08\u7684\u505a\u6cd5\uff0c\u4ee5\u4e0b\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u9009\u9879\u662f\nA. \u6709\u4eb2\u548c\u529b\uff0c\u5de7\u5999\u675c\u7edd\u65e9\u604b\nB. \u65b9\u6cd5\u7c97\u66b4\uff0c\u4fb5\u72af\u5b66\u751f\u9690\u79c1\nC. \u660e\u5bdf\u79cb\u6beb\uff0c\u53ca\u65f6\u5f15\u5bfc\u5b66\u751f\nD. \u5de5\u4f5c\u6b66\u65ad\uff0c\u4f24\u5bb3\u5b66\u751f\u81ea\u5c0a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u60a3\u8005\uff0c\u7537\uff0c48\u5c81\uff0c\u80c3\u8118\u80c0\u75db2\u5468\uff0c\u8fde\u53ca\u80c1\u808b\uff0c\u55f3\u6c14\u540e\u75bc\u75db\u51cf\u8f7b\uff0c\u6bcf\u4e8e\u60c5\u5fd7\u523a\u6fc0\u65f6\u80c3\u75db\u52a0\u91cd\uff0c\u98df\u6b32\u4e0d\u632f\uff0c\u5608\u6742\u541e\u9178\uff0c\u820c\u7ea2\uff0c\u82d4\u8584\u767d\uff0c\u8109\u5f26\u3002\u9488\u5bf9\u6b64\u8bc1\u5e94\u91c7\u7528\u7684\u6cbb\u6cd5\u662f\nA. \u758f\u809d\u7406\u6c14\uff0c\u548c\u80c3\u6b62\u75db\nB. \u5bfc\u6ede\u548c\u80c3\nC. \u758f\u809d\u6cc4\u70ed\uff0c\u548c\u80c3\u6b62\u75db\nD. \u6e29\u4e2d\u6563\u5bd2\uff0c\u548c\u80c3\u6b62\u75db\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f53\u73b0\u4e70\u5356\u53cc\u65b9\u5171\u540c\u610f\u5fd7\u5efa\u7acb\u8d77\u6765\u7684\u5ba2\u89c2\u5173\u7cfb\u7684\u662f\nA. \u8bda\u4fe1\nB. \u501f\u8d37\nC. \u4fe1\u7528\nD. \u4fe1\u6258\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c5e\u4e8e\u8179\u819c\u5916\u4f4d\u5668\u5b98\u7684\u662f\nA. \u5347\u7ed3\u80a0\nB. \u9611\u5c3e\nC. \u80c6\u56ca\nD. \u80be\u810f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u822c\uff0ck-NN\u6700\u8fd1\u90bb\u65b9\u6cd5\u5728\uff08\uff09\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u8f83\u597d\nA. \u6837\u672c\u5448\u56e2\u72b6\u5206\u5e03\nB. \u6837\u672c\u8f83\u591a\u4f46\u5178\u578b\u6027\u4e0d\u597d\nC. \u6837\u672c\u5448\u94fe\u72b6\u5206\u5e03\nD. \u6837\u672c\u8f83\u5c11\u4f46\u5178\u578b\u6027\u597d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.32205625344145955, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.927692268316721, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u636e\u53f2\u7c4d\u8bb0\u8f7d\uff0c\u4e1c\u6c49\u65f6\u671f\u201c\u5927\u79e6\u738b\u5b89\u6566\u9063\u4f7f\u81ea\u65e5\u5357\u5fbc\u5916\u732e\u8c61\u7259\u3001\u7280\u89d2\u3001\u73b3\u7441\uff0c\u59cb\u4e43\u4e00\u901a\u7109\u3002\u5176\u6240\u8868\u8d21\uff0c\u5e76\u65e0\u73cd\u5f02\u2026\u2026\u201d\u6587\u4e2d\u7684\u201c\u5927\u79e6\u201d\u662f\u6307\nA. \u5965\u65af\u66fc\u5e1d\u56fd\nB. \u7f57\u9a6c\u5e1d\u56fd\nC. \u6ce2\u65af\u5e1d\u56fd\nD. \u4e9a\u5386\u5c71\u5927\u5e1d\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8992166711292721, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9867099274741796}}, {"question": "\u4e0b\u5217\u6709\u5173\u6cd5\u7684\u666e\u904d\u6027\u7684\u8bf4\u6cd5\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u6cd5\u7684\u666e\u904d\u7ea6\u675f\u529b\u662f\u4ee5\u5916\u5728\u5f3a\u5236\u529b\u4e3a\u7279\u5f81\u7684\u7ea6\u675f\uff0c\u800c\u5176\u4ed6\u793e\u4f1a\u89c4\u8303\u4ee5\u5185\u5728\u5f3a\u5236\u4e3a\u4e3b\u8981\u7279\u5f81\nB. \u6cd5\u5177\u6709\u666e\u904d\u6027\uff0c\u56e0\u6b64\uff0c\u4e00\u5207\u5177\u4f53\u7684\u6cd5\u5f8b\u7684\u6548\u529b\u90fd\u662f\u5b8c\u5168\u76f8\u540c\u7684\nC. \u6cd5\u7684\u666e\u904d\u6027\u5728\u7a7a\u95f4\u4e0a\u662f\u4ee5\u56fd\u5bb6\u4e3b\u6743\u7ba1\u8f96\u8303\u56f4\u4e3a\u754c\uff0c\u56e0\u6b64\uff0c\u5b83\u4e0d\u662f\u7edd\u5bf9\u7684\u548c\u65e0\u9650\u7684\nD. \u6cd5\u7684\u666e\u904d\u6027\u662f\u6307\u6cd5\u5728\u56fd\u5bb6\u6743\u529b\u7ba1\u8f96\u8303\u56f4\u5185\u666e\u904d\u6709\u6548\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8456163386937094, "meta-math/MetaMath-Mistral-7B": 0.9786411833419019, "itpossible/Chinese-Mistral-7B-v0.1": 0.8082301972330272, "HuggingFaceH4/zephyr-7b-beta": 0.9996171262796452, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9916332335956036, "meta-llama/Meta-Llama-3-8B": 0.9337755315199089, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8977964166354414}}, {"question": "\u4e3a\u4e86\u6539\u53d8\u5b66\u751f\u4ece\u8bfe\u672c\u4e2d\u627e\u201c\u6807\u51c6\u7b54\u6848\u201d\u7684\u4e60\u60ef\uff0c\u5218\u8001\u5e08\u7ecf\u5e38\u5728\u8bfe\u5802\u4e0a\u8bbe\u8ba1\u4e00\u4e9b\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5f15\u5bfc\u5b66\u751f\u81ea\u7531\u8ba8\u8bba\u3001\u63a2\u7d22\u7b54\u6848\u3002\u540c\u4e8b\u9a6c\u8001\u5e08\u5bf9\u5218\u8001\u5e08\u8bf4\uff1a\u201c\u4f60\u8fd9\u6837\u4f1a\u4f7f\u5b66\u751f\u601d\u7ef4\u592a\u53d1\u6563\uff0c\u4e5f\u6d6a\u8d39\u65f6\u95f4\uff0c\u5c06\u6765\u8003\u8bd5\u80af\u5b9a\u4f1a\u5403\u4e8f\u7684\uff0c\u6211\u4ece\u4e0d\u8fd9\u6837\u505a!\u201d\u4ee5\u4e0b\u9009\u62e9\u4e2d\u6b63\u786e\u7684\u9009\u9879\u662f\nA. \u5218\u8001\u5e08\u7684\u505a\u6cd5\u5f97\u5f53\uff0c\u6709\u5229\u4e8e\u57f9\u517b\u5b66\u751f\u521b\u65b0\u610f\u8bc6\nB. \u9a6c\u8001\u5e08\u7684\u8bf4\u6cd5\u6b20\u59a5\uff0c\u4e0d\u5229\u4e8e\u7ef4\u6301\u8bfe\u5802\u6559\u5b66\u79e9\u5e8f\nC. \u9a6c\u8001\u5e08\u7684\u8bf4\u6cd5\u5408\u7406\uff0c\u6709\u5229\u4e8e\u63d0\u9ad8\u5b66\u751f\u5b66\u4e60\u6210\u7ee9\nD. \u5218\u8001\u5e08\u7684\u505a\u6cd5\u6b20\u59a5\uff0c\u4e0d\u5229\u4e8e\u4fdd\u8bc1\u6b63\u5e38\u6559\u5b66\u8fdb\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7843978563860856, "meta-math/MetaMath-Mistral-7B": 0.9859145846278652, "itpossible/Chinese-Mistral-7B-v0.1": 0.6147326441139662, "HuggingFaceH4/zephyr-7b-beta": 0.9669842400694583, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9278025676255168, "meta-llama/Meta-Llama-3-8B": 0.9515693919582131, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9236388586026361}}, {"question": "\u53ea\u6709\u5728\u4e00\u4e2a\uff08\uff09\u7684\u5e02\u573a\u6761\u4ef6\u4e0b\uff0c\u751f\u4ea7\u8981\u7d20\u7684\u4ef7\u683c\u624d\u80fd\u5145\u5206\u53cd\u6620\u8981\u7d20\u7684\u7a00\u7f3a\u7a0b\u5ea6\uff0c\u4ea7\u54c1\u4ef7\u683c\u624d\u80fd\u771f\u6b63\u4f53\u73b0\u4f9b\u6c42\u5173\u7cfb\nA. \u5b8c\u5168\u5784\u65ad\nB. \u5b8c\u5168\u7ade\u4e89\nC. \u7ade\u4e89\u5784\u65ad\nD. \u5784\u65ad\u7ade\u4e89\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7352035758135705, "meta-math/MetaMath-Mistral-7B": 0.9486191357399862, "itpossible/Chinese-Mistral-7B-v0.1": 0.8245869753966475, "HuggingFaceH4/zephyr-7b-beta": 0.9999779949875575, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9887243963573958, "meta-llama/Meta-Llama-3-8B": 0.7567935984900285, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u5206\u5b50\u52a8\u7406\u8bba\u548c\u7269\u4f53\u7684\u5185\u80fd\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u7269\u4f53\u4ece\u5916\u754c\u5438\u6536\u70ed\u91cf\uff0c\u5176\u5185\u80fd\u4e00\u5b9a\u589e\u52a0\nB. \u7269\u4f53\u7684\u6e29\u5ea6\u5347\u9ad8\uff0c\u7269\u4f53\u5185\u5927\u91cf\u5206\u5b50\u70ed\u8fd0\u52a8\u7684\u5e73\u5747\u52a8\u80fd\u589e\u5927\nC. \u6db2\u4f53\u5206\u5b50\u7684\u65e0\u89c4\u5219\u8fd0\u52a8\u79f0\u4e3a\u5e03\u6717\u8fd0\u52a8\nD. \u6c14\u4f53\u7684\u6e29\u5ea6\u5347\u9ad8\uff0c\u6c14\u4f53\u7684\u538b\u5f3a\u4e00\u5b9a\u589e\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6988036100603955, "meta-math/MetaMath-Mistral-7B": 0.9506542224741317, "itpossible/Chinese-Mistral-7B-v0.1": 0.5388577254431183, "HuggingFaceH4/zephyr-7b-beta": 0.9553427251336539, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7559610435821289, "meta-llama/Meta-Llama-3-8B": 0.5463490849541556, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.48896455894179536}}, {"question": "\u7531\u4e8e\u5fb7\u56fd\u8d44\u672c\u4e3b\u4e49\u8d77\u6b65\u665a\uff0c\u5728\u5411\u8d44\u672c\u4e3b\u4e49\u8fdb\u519b\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5c01\u5efa\u5bb9\u514b\u548c\u519b\u9600\u9886\u52bf\u529b\u5f3a\u5927\uff0c\u52a0\u4e4b\u662f\u5c01\u5efa\u5bb9\u514b\u548c\u519b\u9600\u9886\u5bfc\u7684\u738b\u671d\u6218\u4e89\u5b9e\u73b0\u4e86\u5fb7\u56fd\u7684\u7edf\u4e00\uff0c\u4e3a\u5fb7\u56fd\u7684\u53d1\u5c55\u521b\u9020\u4e86\u6761\u4ef6\u3002\u56e0\u6b64\u5176\u5baa\u6cd5\u5e26\u6709\u5c01\u5efa\u548c\u519b\u56fd\u4e3b\u4e49\u6b8b\u4f59\uff0c\u662f\u56e0\u4e3a\nA. \u662f\u6ca1\u6709\u5f7b\u5e95\u624b\u672f\u7559\u4e0b\u7684\u6bd2\u7624\nB. \u662f\u8d44\u4ea7\u9636\u7ea7\u529b\u91cf\u5f31\u5c0f\u7684\u53cd\u6620\nC. \u662f\u5fb7\u56fd\u4eba\u6c11\u5bf9\u5c01\u5efa\u5bb9\u514b\u548c\u519b\u9600\u7684\u5956\u52b1\nD. \u662f\u5f53\u65f6\u5fb7\u56fd\u53d1\u5c55\u7684\u5ba2\u89c2\u9700\u8981 \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3964093993692024, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5469058178302001, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9661094568031645}}, {"question": "\u751f\u7406\u60c5\u51b5\u4e0b\uff0c\u4eba\u7684\u4e2d\u5fc3\u9759\u8109\u538b\u5347\u9ad8\u53ef\u89c1\u4e8e\nA. \u5fc3\u810f\u5c04\u8840\u80fd\u529b\u52a0\u5f3a\nB. \u7531\u5438\u6c14\u76f8\u6539\u4e3a\u547c\u6c14\u76f8\nC. \u4ece\u884c\u8d70\u6539\u4e3a\u7ad9\u7acb\nD. \u4f53\u4f4d\u7531\u76f4\u7acb\u53d8\u4e3a\u5e73\u5367\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6650199684779817, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u80fd\u5438\u9644\u5bbf\u4e3b\u7ec6\u80de\u534f\u52a9\u75c5\u6bd2\u4fb5\u5165\u7ec6\u80de\u7684\u90e8\u5206\u662f\nA. \u6838\u9178\nB. \u58f3\u7c92\nC. \u5305\u819c\nD. \u8863\u58f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u91c7\u53d6\u9002\u5f53\u7684\u63aa\u65bd\uff0c\u4f7f\u71c3\u70e7\u56e0\u7f3a\u4e4f\u6216\u65ad\u7edd\u6c27\u6c14\u800c\u7184\u706d\uff0c\u8fd9\u79cd\u65b9\u6cd5\u79f0\u4f5c\nA. \u964d\u6c34\u706d\u706b\u53d1\nB. \u9694\u79bb\u706d\u706b\u6cd5\nC. \u7a92\u606f\u706d\u706b\u6cd5\nD. \u51b7\u5374\u706d\u706b\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u5b66\u751f\u79ef\u6781\u53c2\u52a0\u73ed\u7ea7\u6d3b\u52a8\uff0c\u4e0e\u540c\u5b66\u56e2\u7ed3\u53cb\u7231\uff0c\u5bf9\u6559\u5e08\u70ed\u7231\u5c0a\u91cd\uff0c\u4f53\u73b0\u4e86\u6027\u683c\u4e0a\u7684\uff08\uff09\u7279\u5f81\u3002\nA. \u7406\u667a\nB. \u6001\u5ea6\nC. \u610f\u5fd7\nD. \u60c5\u7eea\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3786933543716561, "meta-math/MetaMath-Mistral-7B": 0.8782255651001362, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9926115541847071, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8577489734750848, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u79cd\u4ea7\u54c1\u7684\u5e02\u573a\u5904\u4e8e\u5747\u8861\u72b6\u6001\u662f\u6307\nA. \u6d88\u8d39\u8005\u60f3\u8981\u8d2d\u4e70\u7684\u6570\u91cf\u6070\u597d\u7b49\u4e8e\u4f9b\u7ed9\u8005\u60f3\u8981\u51fa\u552e\u7684\u6570\u91cf\nB. \u9700\u6c42\u66f2\u7ebf\u5411\u53f3\u4e0b\u65b9\u503e\u659c\uff0c\u4f9b\u7ed9\u66f2\u7ebf\u5411\u53f3\u4e0a\u65b9\u503e\u659c\nC. \u6574\u4e2a\u793e\u4f1a\u7684\u603b\u9700\u6c42\u7b49\u4e8e\u6574\u4e2a\u793e\u4f1a\u7684\u603b\u4f9b\u7ed9\nD. \u4e70\u548c\u5356\u7684\u91cf\u76f8\u7b49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8743291054030701, "meta-math/MetaMath-Mistral-7B": 0.9339187334861264, "itpossible/Chinese-Mistral-7B-v0.1": 0.4907780216811601, "HuggingFaceH4/zephyr-7b-beta": 0.9990195770546632, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8037517139796644, "meta-llama/Meta-Llama-3-8B": 0.8525524541915089, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.804040825304944}}, {"question": "\u98df\u54c1\u7684\u8d2e\u5b58\u5305\u62ec\u51b7\u85cf\u548c\u51b7\u51bb\u4e24\u79cd\u65b9\u5f0f\uff0c\u90a3\u4e48\u98df\u54c1\u51b7\u85cf\u8d2e\u5b58\u6e29\u5ea6\u662f\u6307\u591a\u5c11\u5ea6\nA. 4-10\u00b0C\nB. 0-29\u00b0C\nC. 0-10\u00b0C\nD. 0-18\u00b0C\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9930079735407054, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6447827897321473, "meta-llama/Meta-Llama-3-8B": 0.4703069298158966, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7512053547231579}}, {"question": "\u5173\u4e8e\u7cd6\u9175\u89e3\u7684\u6b63\u786e\u63cf\u8ff0\u662f\nA. \u5728\u7ec6\u80de\u8d28\u4e2d\u8fdb\u884c\nB. \u751f\u621038\u5206\u5b50ATP\nC. \u5168\u8fc7\u7a0b\u662f\u53ef\u9006\u7684\nD. \u4e0d\u6d88\u8017ATP\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3403147063768956, "itpossible/Chinese-Mistral-7B-v0.1": 0.39935991204190846, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3788721596190015, "meta-llama/Meta-Llama-3-8B": 0.5482603453203221, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5\u4e24\u51cf\u56e0\u751f\u5b58\u6a21\u578b:$q_x^{(1)}=0.02\uff0cq_x^{(2)}=0.05$\u3002\u5047\u8bbe\u5728\u6bcf\u4e00\u5e74\u9f84\u7684\u5e74\u7ec8\u6b62\u529b\u4e3a\u5e38\u6570\uff0c\u5219$q_x^{\\prime(1)}$\u548c$q_x^{\\prime(2)}$\u7684\u503c\u5206\u522b\u4e3a( )\u3002\nA. $0.0205\uff0c0.9795$\nB. $0.0505\uff0c0.9795$\nC. $0.0205\uff0c0.0505$\nD. $0.0505\uff0c0.0205$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3419216904345335, "meta-math/MetaMath-Mistral-7B": 0.34239623393788804, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5371129438687818, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3874556190002601, "meta-llama/Meta-Llama-3-8B": 0.308176776288574, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4014884389780015}}, {"question": "\u8001\u5e74\u4eba\u5403\u83dc\u5e94\u591a\u52a0\u4e9b\u4ec0\u4e48\nA. \u7cd6\nB. \u918b\nC. \u6cb9\nD. \u76d0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.28486648950718824, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.48617931675193277, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.41251570012808325}}, {"question": "\u674e\u9e3f\u7ae0\u5728\u300a\u7b79\u8bae\u5236\u9020\u8f6e\u8239\u672a\u53ef\u88c1\u64a4\u6298\u300b\u4e2d\u6307\u51fa\uff1a\u201c\u82e5\u6211\u679c\u6df1\u901a\u5176\u6cd5\uff0c\u6108\u5b66\u6108\u7cbe\uff0c\u6108\u63a8\u6108\u5e7f\uff0c\u5b89\u89c1\u767e\u6570\u5341\u5e74\u540e\u4e0d\u80fd\u6518\u5937\u800c\u81ea\u7acb\u8036\uff1f\u201d\u636e\u6b64\u53ef\u77e5\uff0c\u6b64\u65f6\u7684\u674e\u9e3f\u7ae0\u529b\u4e3b\u63a8\u5e7f\nA. \u6cbb\u56fd\u6cd5\u5f8b\nB. \u6559\u80b2\u4e4b\u6cd5\nC. \u7ecf\u5546\u4e4b\u6cd5\nD. \u5236\u9020\u4e4b\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7584088902824269, "meta-math/MetaMath-Mistral-7B": 0.4656815401217194, "itpossible/Chinese-Mistral-7B-v0.1": 0.8707670437416318, "HuggingFaceH4/zephyr-7b-beta": 0.9917575175938776, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5188730819588593, "meta-llama/Meta-Llama-3-8B": 0.8978706820995664, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7528\u91cd\u91cf\u6cd5\u6d4b\u5b9aBaCl2\u00b7nH2O\u4e2d\u7ed3\u6676\u6c34\u6570\u76ee\u7684\u5b9e\u9a8c\uff1a\u51c6\u786e\u79f0\u53d67.91 g\u8be5\u6676\u4f53\uff0c\u52a0\u5165\u9002\u91cf\u76d0\u9178\uff0c\u52a0\u70ed\u4f7f\u5176\u5b8c\u5168\u6eb6\u89e3\uff0c\u51b7\u5374\uff0c\u914d\u5236100 mL\u6eb6\u6db2\u3002\u53d620.00 mL\u8be5\u6eb6\u6db2\uff0c\u5411\u5176\u4e2d\u6ef4\u52a0100 mL 0.05 mol\u00b7L^\uff0d1 Na2SO4\u6eb6\u6db2\u6070\u597d\u4f7fBa2\uff0b\u5b8c\u5168\u6c89\u6dc0\u3002n\u7b49\u4e8e\nA. 4\nB. 2\nC. 8\nD. 6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c06word\u6587\u6863\u4e2d\u7684\u5185\u5bb9\u5168\u90e8\u9009\u5b9a\u7684\u5feb\u6377\u952e\u662f\nA. CTRL+Z\nB. CTRL+A\nC. CTRL+C\nD. CTRL+V\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9685808525210241, "meta-math/MetaMath-Mistral-7B": 0.9970916170450641, "itpossible/Chinese-Mistral-7B-v0.1": 0.8811533475147507, "HuggingFaceH4/zephyr-7b-beta": 0.9734825411908138, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.997496033120832, "meta-llama/Meta-Llama-3-8B": 0.9270592307751829, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9995339382853207}}, {"question": "\u4e2d\u592e\u628a\u4eba\u624d\u95ee\u9898\u63d0\u5347\u5230\u56fd\u5bb6\u6218\u7565\u7684\u9ad8\u5ea6\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\nA. \u6211\u56fd\u4eba\u624d\u8d44\u6e90\u532e\u4e4f\uff0c\u4eba\u624d\u6d41\u5931\u4e25\u91cd\nB. \u5f53\u4eca\u56fd\u9645\u7ade\u4e89\u662f\u4ee5\u7ecf\u6d4e\u548c\u79d1\u6280\u5b9e\u529b\u4e3a\u57fa\u7840\u7684\u7efc\u5408\u56fd\u529b\u7684\u8f83\u91cf\nC. \u4eba\u624d\u8d44\u6e90\u662f\u7b2c\u4e00\u8d44\u6e90\nD. \u4eba\u624d\u8d44\u6e90\u7684\u914d\u7f6e\u662f\u4ee5\u56fd\u5bb6\u5b8f\u89c2\u8c03\u63a7\u4e3a\u57fa\u7840\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u65bd\u5de5\u603b\u627f\u5305\u7ba1\u7406\u65b9\u8d23\u4efb\u7684\u8bf4\u6cd5\uff0c\u6b63\u786e\u7684\u662f\nA. \u7ec4\u7ec7\u548c\u6307\u6325\u65bd\u5de5\u603b\u627f\u5305\u5355\u4f4d\u7684\u65bd\u5de5\nB. \u4e0e\u5206\u5305\u5355\u4f4d\u7b7e\u8ba2\u5206\u5305\u5408\u540c\nC. \u627f\u62c5\u9879\u76ee\u65bd\u5de5\u4efb\u52a1\u5e76\u5bf9\u5176\u5de5\u7a0b\u8d28\u91cf\u8d1f\u8d23\nD. \u8d1f\u8d23\u5bf9\u6240\u6709\u5206\u5305\u5355\u4f4d\u7684\u7ba1\u7406\u53ca\u7ec4\u7ec7\u534f\u8c03\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6430375567004805, "meta-math/MetaMath-Mistral-7B": 0.9142555882350065, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9997370783003467, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9232125650753135, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6cd5\u5f8b\u4e2d\u5bf9\u4e8e\u6709\u5173\u95ee\u9898\u6ca1\u6709\u76f4\u63a5\u7684\u660e\u6587\u89c4\u5b9a\uff0c\u5373\u51fa\u73b0\u4e86\u6cd5\u5f8b\u6f0f\u6d1e\u65f6\uff0c\u5e94\u91c7\u7528\nA. \u5f52\u7eb3\u63a8\u7406\nB. \u6f14\u7ece\u63a8\u7406\nC. \u7c7b\u6bd4\u63a8\u7406\nD. \u5b9e\u8d28\u63a8\u7406\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3403147063768956, "HuggingFaceH4/zephyr-7b-beta": 0.8235724883340765, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u516c\u5171\u5173\u7cfb\u7684\u5de5\u4f5c\u8fdb\u7a0b\u901a\u5e38\u5206\u4e3a\u8c03\u67e5\u5206\u6790\u3001\u5236\u5b9a\u8ba1\u5212\u3001\u5b9e\u65bd\u4f20\u64ad\u548c\u8bc4\u4f30\u6548\u679c\u56db\u4e2a\u6b65\u9aa4\uff0c\u53c8\u79f0\nA. \u8c03\u67e5\u5206\u6790\u6cd5\nB. \u4f20\u64ad\u6c9f\u901a\u6cd5\nC. \u5236\u5b9a\u8ba1\u5212\u6cd5\nD. \u56db\u6b65\u5de5\u4f5c\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9407383772947321, "meta-math/MetaMath-Mistral-7B": 0.9842059720248132, "itpossible/Chinese-Mistral-7B-v0.1": 0.9035542151900183, "HuggingFaceH4/zephyr-7b-beta": 0.9999751763272344, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9828529355726816, "meta-llama/Meta-Llama-3-8B": 0.9532516827316269, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9215147446075487}}, {"question": "\u827a\u672f\u6982\u8bba\u7814\u7a76\u7684\u5bf9\u8c61\u662f\u4eba\u7c7b\u7684\nA. \u60c5\u611f\u6d3b\u52a8\nB. \u5ba1\u7f8e\u6d3b\u52a8\nC. \u7cbe\u795e\u6d3b\u52a8\nD. \u827a\u672f\u6d3b\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4397683349492627, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5758813233185172, "meta-llama/Meta-Llama-3-8B": 0.655578319054012, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7005955548300131}}, {"question": "\u201c\u5386\u53f2\u662f\u903b\u8f91\u7684\u57fa\u7840\uff0c\u903b\u8f91\u662f\u5386\u53f2\u7684\u4fee\u6b63\u201d\uff0c\u8fd9\u4e00\u89c2\u70b9\u662f\nA. \u5272\u88c2\u5386\u53f2\u4e0e\u903b\u8f91\u7edf\u4e00\u7684\u89c2\u70b9\nB. \u4e3b\u89c2\u552f\u5fc3\u4e3b\u4e49\u7684\u89c2\u70b9 \nC. \u5386\u53f2\u4e0e\u903b\u8f91\u76f8\u7edf\u4e00\u7684\u89c2\u70b9\nD. \u7247\u9762\u5f3a\u8c03\u903b\u8f91\u91cd\u8981\u6027\u7684\u89c2\u70b9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8853617903351826, "meta-math/MetaMath-Mistral-7B": 0.9939115693779598, "itpossible/Chinese-Mistral-7B-v0.1": 0.7820872640320758, "HuggingFaceH4/zephyr-7b-beta": 0.9980785541025169, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9113816915575089, "meta-llama/Meta-Llama-3-8B": 0.7255908375338561, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8353379862543779}}, {"question": "\u4e0b\u5217\u6709\u5173\u80bf\u7624\u751f\u957f\u7279\u70b9\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u6709\nA. \u5f02\u578b\u6027\u5927\u7684\u6076\u6027\u80bf\u7624\u751f\u957f\u8f83\u5feb\nB. \u591a\u6570\u6076\u6027\u80bf\u7624\u7ec6\u80de\u7684\u500d\u589e\u65f6\u95f4\u6bd4\u6b63\u5e38\u7ec6\u80de\u5feb\nC. \u80bf\u7624\u7ec6\u80de\u751f\u6210\u4e0e\u6b7b\u4ea1\u6bd4\u4f8b\u4e0e\u5176\u6301\u7eed\u751f\u957f\u6709\u5173\nD. \u751f\u957f\u5206\u6570\u9ad8\u7684\u80bf\u7624\u5bf9\u4e8e\u5316\u5b66\u6cbb\u7597\u654f\u611f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3527809287490976, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u516c\u5143\u524d406\u5e74\uff0c\u96c5\u5178\u53d6\u5f97\u963f\u57fa\u7ebd\u897f\u6d77\u6218\u80dc\u5229\uff0c\u4f46\u4e5f\u9020\u6210\u4e86\u6570\u5343\u58eb\u5175\u56e0\u8239\u7834\u6c89\u6ca1\u6eba\u6c34\u8eab\u4ea1\u3002\u5728\u968f\u540e\u7684\u516c\u6c11\u5927\u4f1a\u4e0a\uff0c\u6307\u6325\u8fd9\u6b21\u6218\u5f79\u76846\u4f4d\u5c06\u519b\u88ab\u6307\u63a7\u6551\u63f4\u4e0d\u529b\u3002\u5728\u6ca1\u6709\u542c\u53d66\u4f4d\u5c06\u519b\u4e2a\u4eba\u7533\u8fa9\u7684\u60c5\u51b5\u4e0b\uff0c\u5c31\u5c06\u4ed6\u4eec\u5224\u5904\u6b7b\u5211\u3002\u8fd9\u53cd\u6620\u4e86\u5f53\u65f6\u96c5\u5178\nA. \u88ab\u544a\u4eba\u6ca1\u6709\u6cd5\u5ead\u7533\u8fa9\u6743\u5229\nB. \u516c\u6c11\u5927\u4f1a\u7684\u51b3\u5b9a\u5177\u6709\u81f3\u4e0a\u6743\u5a01\nC. \u516c\u6c11\u5927\u4f1a\u638c\u63e1\u57ce\u90a6\u76ee\u6cd5\u6743\nD. \u519b\u961f\u4e0e\u516c\u6c11\u5927\u4f1a\u4e4b\u95f4\u5b58\u5728\u51b2\u7a81\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4301816307692888, "meta-math/MetaMath-Mistral-7B": 0.3346581372147677, "itpossible/Chinese-Mistral-7B-v0.1": 0.6027603439651981, "HuggingFaceH4/zephyr-7b-beta": 0.6594021613949134, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5057615504596514, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.573015187142965}}, {"question": "\u4e00\u9636\u903b\u8f91\u5373\u662f\u6307\nA. \u56fe\u5f0f\u903b\u8f91\nB. \u8c13\u8bcd\u903b\u8f91\nC. \u6570\u503c\u903b\u8f91\nD. \u6982\u7387\u903b\u8f91\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8724436055558017, "meta-math/MetaMath-Mistral-7B": 0.8188818518350022, "itpossible/Chinese-Mistral-7B-v0.1": 0.6209050376025551, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7811640521054725, "meta-llama/Meta-Llama-3-8B": 0.4243460165911567, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9272520257887525}}, {"question": "\u53e4\u4eba\u4e91\uff1a\u201c\u8a00\u4e0d\u5bbf\u8bfa\uff0c\u884c\u4e0d\u82df\u4ece\u201d\u3001\u201c\u8584\u4e8e\u8eab\u800c\u539a\u4e8e\u6c11\uff0c\u7ea6\u4e8e\u8eab\u800c\u5e7f\u4e8e\u4e16\u201d\u3002\u4eca\u5929\uff0c\u6211\u4eec\u5927\u529b\u5021\u5bfc\u201c\u7231\u56fd\u5b88\u6cd5\u3001\u660e\u793c\u8bda\u4fe1\u3001\u56e2\u7ed3\u53cb\u5584\u3001\u52e4\u4fed\u81ea\u5f3a\u3001\u656c\u4e1a\u5949\u732e\u201d\u7684\u57fa\u672c\u9053\u5fb7\u89c4\u8303\uff1b\u91cd\u89c6\u601d\u60f3\u9053\u5fb7\u5efa\u8bbe\uff0c\u662f\u56e0\u4e3a\u5b83 a\u662f\u4e2d\u534e\u6c11\u65cf\u7684\u4f20\u7edf\u9053\u5fb7\uff1bb\u89c4\u5b9a\u7740\u6587\u5316\u5efa\u8bbe\u7684\u65b9\u5411\uff0c\u662f\u6587\u5316\u5efa\u8bbe\u7684\u7075\u9b42\uff1bc\u5728\u6bcf\u4e2a\u65f6\u4ee3\u90fd\u5177\u6709\u76f8\u540c\u7684\u5185\u6db5\uff1bd\u662f\u53d1\u5c55\u793e\u4f1a\u4e3b\u4e49\u5e02\u573a\u7ecf\u6d4e\u7684\u5185\u5728\u8981\u6c42\nA. bc\nB. cd\nC. bd\nD. ab\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2821833983601388, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3632935697889166}}, {"question": "\u4e0b\u5217\u54ea\u79cd\u8bc1\u4ef6\u662f\u9a7e\u9a76\u673a\u52a8\u8f66\u4e0a\u8def\u884c\u9a76\u5e94\u5f53\u968f\u8f66\u643a\u5e26\nA. \u673a\u52a8\u8f66\u767b\u8bb0\u8bc1\nB. \u51fa\u5382\u5408\u683c\u8bc1\u660e\nC. \u673a\u52a8\u8f66\u884c\u9a76\u8bc1\nD. \u673a\u52a8\u8f66\u4fdd\u9669\u5355\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6633024507272026, "meta-math/MetaMath-Mistral-7B": 0.8423608103997678, "itpossible/Chinese-Mistral-7B-v0.1": 0.9116481129762924, "HuggingFaceH4/zephyr-7b-beta": 0.9159812319727618, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.953522187754738, "meta-llama/Meta-Llama-3-8B": 0.5789936156902674, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9483040558401041}}, {"question": "\u8d44\u672c\u4e3b\u4e49\u7531\u81ea\u7531\u7ade\u4e89\u9636\u6bb5\u8fdb\u4eba\u5784\u65ad\u9636\u6bb5\uff0c\u6700\u6839\u672c\u7684\u6807\u5fd7\u5728\u4e8e\nA. \u56fd\u5bb6\u5784\u65ad\u4ee3\u66ff\u79c1\u4eba\u5784\u65ad\u5728\u7ecf\u6d4e\u751f\u6d3b\u4e2d\u5360\u7edf\u6cbb\u5730\u4f4d\nB. \u8d44\u672c\u8f93\u51fa\u4ee3\u66ff\u5546\u54c1\u8f93\u51fa\u5728\u7ecf\u6d4e\u751f\u6d3b\u4e2d\u5360\u7edf\u6cbb\u5730\u4f4d \nC. \u5784\u65ad\u4ee3\u66ff\u81ea\u7531\u7ade\u4e89\u5728\u7ecf\u6d4e\u751f\u6d3b\u4e2d\u5360\u7edf\u6cbb\u5730\u4f4d\nD. \u94f6\u884c\u8d44\u672c\u4ee3\u66ff\u5de5\u4e1a\u8d44\u672c\u5728\u7ecf\u6d4e\u751f\u6d3b\u4e2d\u5360\u7edf\u6cbb\u5730\u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.619488312756237, "meta-math/MetaMath-Mistral-7B": 0.8419160549472389, "itpossible/Chinese-Mistral-7B-v0.1": 0.5262386562523383, "HuggingFaceH4/zephyr-7b-beta": 0.9967789555898852, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7986304645378215, "meta-llama/Meta-Llama-3-8B": 0.5613323696185212, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6313772674101914}}, {"question": "\u6839\u636e\u5211\u6cd5\u89c4\u5b9a\uff0c\u53ef\u4ee5\u4ece\u8f7b\u3001\u51cf\u8f7b\u6216\u8005\u514d\u9664\u5904\u7f5a\u7684\u60c5\u5f62\u662f\nA. \u76f2\u4eba\u72af\u7f6a\nB. \u72af\u7f6a\u672a\u9042\nC. \u4e0d\u6ee118\u5468\u5c81\u7684\u4eba\u72af\u7f6a\nD. \u5c1a\u672a\u5b8c\u5168\u4e27\u5931\u8fa8\u8ba4\u548c\u63a7\u5236\u80fd\u529b\u7684\u7cbe\u795e\u75c5\u4eba\u72af\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3607860911977634, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u80f0\u817a\u5047\u6027\u56ca\u80bf\u7684\u53d9\u8ff0\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u56ca\u58c1\u4e0a\u76ae\u53ef\u5206\u6ccc\u7c98\u6db2\nB. \u4e3b\u8981\u4f53\u5f81\u662f\u4e0a\u8179\u5305\u5757\nC. \u8bca\u65ad\u540e\u5c3d\u65e9\u624b\u672f\u5207\u9664\nD. \u591a\u7ee7\u53d1\u4e8e\u8179\u90e8\u5916\u4f24\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5497592250009111, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5\u67d0\u9669\u79cd\u7684\u5b9e\u9645\u635f\u5931\u989d$X$\u7684\u5206\u5e03\u51fd\u6570\u4e3a:$$F_X(x)=1-0.8 e^{-0.02 x}-0.2 e^{-0.001 x}\uff0cx \\geqslant 0$$\u82e5\u4fdd\u5355\u89c4\u5b9a\uff1a\u635f\u5931\u989d\u4f4e\u4e8e 1000 \u5143\u5c31\u5168\u90e8\u8d54\u507f\uff0c\u82e5\u635f\u5931\u989d\u9ad8\u4e8e 1000 \u5143\u5219\u53ea\u8d54\u507f 1000 \u5143\u3002\u5219\u88ab\u4fdd\u9669\u4eba\u6240\u83b7\u5f97\u7684\u5b9e\u9645\u8d54\u4ed8\u989d\u671f\u671b\u4e3a( )\u3002\nA. 166.4\nB. 206.8\nC. 126.4\nD. 40.0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.28661281177772774, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3672229297879873}}, {"question": "\u5173\u4e8e\u5927\u5b66\u751f\u6027\u81ea\u6170\u884c\u4e3a\uff0c\u4e0b\u5217\u8868\u8ff0\u4e0d\u6b63\u786e\u7684\u662f\nA. \u81ea\u6170\u65b9\u5f0f\u8981\u5408\u7406\u3002\nB. \u6b63\u786e\u5bf9\u5f85\u81ea\u6170\uff1a\u4f60\u4e0d\u81ea\u6170\uff0c\u4e5f\u53ef\u4ee5\u5f88\u5feb\u4e50\uff0c\u5e76\u4e0d\u662f\u6bcf\u4e2a\u4eba\u90fd\u81ea\u6170\u3002\nC. \u6027\u81ea\u6170\u662f\u4e00\u79cd\u6b63\u5e38\u73b0\u8c61\uff0c\u4e0d\u59a8\u788d\u65e5\u5e38\u751f\u6d3b\u4f5c\u606f\uff0c\u5bf9\u5065\u5eb7\u65e0\u5bb3\uff0c\u53ef\u4ee5\u65e0\u8282\u5236\u800c\u4e3a\u3002\nD. \u81ea\u6170\u662f\u4e00\u79cd\u4e2a\u4eba\u9690\u79c1\u3002\u8981\u5728\u5b89\u5168\u7684\u73af\u5883\u4e0b\u8fdb\u884c\uff0c\u4e5f\u4e0d\u8981\u5bf9\u4ed6\u4eba\u6784\u6210\u9a9a\u6270\u6216\u653b\u51fb\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9088978037511117, "meta-math/MetaMath-Mistral-7B": 0.9863396158020645, "itpossible/Chinese-Mistral-7B-v0.1": 0.9265503078501482, "HuggingFaceH4/zephyr-7b-beta": 0.998017171871612, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8838194301262932, "meta-llama/Meta-Llama-3-8B": 0.9468821665326101, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9964814651529196}}, {"question": "\u4e0b\u5217\u5404\u9879\u4e2d\uff0c\u5c5e\u4e8e\u8d26\u5b9e\u6838\u5bf9\u7684\u662f\nA. \u94f6\u884c\u5b58\u6b3e\u65e5\u8bb0\u8d26\u4e0e\u94f6\u884c\u5b58\u6b3e\u4f59\u989d\u8c03\u8282\u8868\u6838\u5bf9\nB. \u94f6\u884c\u5b58\u6b3e\u65e5\u8bb0\u8d26\u4e0e\u94f6\u884c\u5b58\u6b3e\u4ed8\u6b3e\u51ed\u8bc1\u6838\u5bf9\nC. \u94f6\u884c\u5b58\u6b3e\u65e5\u8bb0\u8d26\u4e0e\u94f6\u884c\u5b58\u6b3e\u603b\u8d26\u6838\u5bf9\nD. \u94f6\u884c\u5b58\u6b3e\u65e5\u8bb0\u8d26\u4e0e\u94f6\u884c\u5bf9\u8d26\u5355\u6838\u5bf9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4045257283646438, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u793e\u4f1a\u8d44\u6e90\u7684\u57fa\u672c\u7c7b\u578b\u4e0d\u5305\u62ec\nA. \u7ecf\u6d4e\u8d44\u6e90\nB. \u653f\u6cbb\u8d44\u6e90\nC. \u6559\u80b2\u8d44\u6e90\nD. \u6587\u5316\u8d44\u6e90\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.485402636708518, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4430435777731413, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6330268894214387, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3919626083521393}}, {"question": "\u4e0b\u5217\u6240\u5217\u51b2\u7a81\u89c4\u8303\u4e2d\u5c5e\u4e8e\u91cd\u53e0\u9002\u7528\u51c6\u636e\u6cd5\u7684\u51b2\u7a81\u89c4\u8303\u662f\nA. \u5728\u672c\u4eba\u4e0e\u7b2c\u4e09\u4eba\u5173\u7cfb\u4e2d\uff0c\u4ee3\u7406\u6743\u7684\u5b58\u5728\u4e0e\u8303\u56f4\u4ee5\u53ca\u4ee3\u7406\u4eba\u884c\u4f7f\u6216\u610f\u56fe\u884c\u4f7f\u4ee3\u7406\u6743\u6240\u4ea7\u751f\u7684\u6548\u529b\uff0c\u5e94\u9002\u7528\u4ee3\u7406\u4eba\u4e3a\u6709\u5173\u884c\u4e3a\u65f6\u5176\u8425\u4e1a\u6240\u6240\u5728\u5730\u6cd5\nB. \u79bb\u5a5a\u4f9d\u5176\u539f\u56e0\u53d1\u751f\u65f6\u592b\u4e4b\u672c\u56fd\u6cd5\uff0c\u4f46\u5176\u539f\u56e0\u4e8b\u5b9e\u975e\u65e5\u672c\u6cd5\u4ea6\u8ba4\u4e3a\u79bb\u5a5a\u4e4b\u539f\u56e0\u8005\uff0c\u6cd5\u9662\u4e0d\u5f97\u4e3a\u79bb\u5a5a\u4e4b\u5ba3\u544a\nC. \u592b\u5987\u8d22\u4ea7\u5951\u7ea6\u7684\u7f14\u7ed3\uff0c\u5173\u4e8e\u5176\u65b9\u5f0f\u6216\u4f9d\u636e\u7f14\u7ed3\u5951\u7ea6\u56fd\u4e4b\u6cd5\u5f8b\uff0c\u6216\u4f9d\u5a5a\u59fb\u4e3e\u884c\u65f6\u5a5a\u59fb\u5f53\u4e8b\u4eba\u5404\u81ea\u4e4b\u672c\u56fd\u6cd5\uff0c\u6216\u4f9d\u5a5a\u59fb\u7ee7\u7eed\u4e2d\u5173\u4e8e\u592b\u59bb\u5404\u81ea\u4e4b\u672c\u56fd\u6cd5\uff0c\u5747\u4e3a\u6709\u6548\nD. \u5728\u4efb\u4f55\u60c5\u51b5\u4e0b\uff0c\u5916\u56fd\u7684\u6cd5\u5f8b\u548c\u6cd5\u89c4\u3001\u4e00\u4e2a\u7ec4\u7ec7\u6216\u6cd5\u4eba\u7684\u7ae0\u7a0b\u548c\u89c4\u5b9a\uff0c\u4ee5\u53ca\u79c1\u4eba\u95f4\u7684\u89c4\u5b9a\u548c\u534f\u8bae\uff0c\u5982\u679c\u8fdd\u53cd\u516c\u5e8f\u826f\u4fd7\uff0c\u5728\u610f\u5927\u5229\u9886\u571f\u4e0a\u65e0\u6548\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6025\u6027\u975e ST\u6bb5\u62ac\u9ad8\u5fc3\u808c\u6897\u6b7b\u6cbb\u7597\u65f6\u4e0d\u5b9c\u91c7\u7528\u6eb6\u6813\u7597\u6cd5\u7684\u4e3b\u8981\u539f\u56e0\u662f\nA. \u75c5\u60c5\u5371\u6025\u7a0b\u5ea6\u8f83\u8f7b\nB. \u51a0\u8109\u963b\u585e\u4e0d\u5b8c\u5168\nC. \u51a0\u8109\u75c9\u631b\u662f\u53d1\u75c5\u7684\u4e3b\u8981\u56e0\u7d20\nD. \u51a0\u8109\u5185\u4e3b\u8981\u662f\u767d\u8840\u6813\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2998701595284818, "HuggingFaceH4/zephyr-7b-beta": 0.8974620080488976, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.511429617326952, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3191958056930821}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u4f1a\u8ba1\u51ed\u8bc1\u7684\u610f\u4e49\u548c\u79cd\u7c7b\u7684\u8868\u8ff0\u4e2d\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u4f1a\u8ba1\u51ed\u8bc1\u6309\u7167\u586b\u5236\u7a0b\u4ea8\u548c\u7ecf\u6d4e\u4e1a\u52a1\u5185\u5bb9\u4e0d\u540c\uff0c\u53ef\u5206\u4e3a\u539f\u59cb\u51ed\u8bc1\u548c\u8bb0\u8d26\u51ed\u8bc1\nB. \u8bb0\u5f55\u7ecf\u6d4e\u4e1a\u52a1\uff0c \u63d0\u4f9b\u8bb0\u8d26\u4f9d\u636e\nC. \u660e\u786e\u7ecf\u6d4e\u8d23\u4efb\uff0c\u5f3a\u5316\u5185\u90e8\u63a7\u5236\nD. \u5408\u7406\u7684\u53d6\u5f97\u3001\u6b63\u786e\u7684\u586b\u5236\u548c\u5ba1\u6838\u4f1a\u8ba1\u51ed\u8bc1\uff0c \u662f \u4f1a\u8ba1\u6838\u7b97\u7684\u57fa\u672c\u65b9\u6cd5\u4e4b\u4e00\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3013812289234467, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3779088862695841, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u300a\u5211\u6cd5\u300b\u7b2c21\u6761\u7b2c1\u6b3e\u89c4\u5b9a\uff1a\u201c\u4e3a\u4e86\u4f7f\u56fd\u5bb6\u3001\u516c\u5171\u5229\u76ca\u3001\u672c\u4eba\u6216\u8005\u4ed6\u4eba\u7684\u4eba\u8eab\u3001\u8d22\u4ea7\u548c\u5176\u4ed6\u6743\u5229\u514d\u53d7\u6b63\u5728\u53d1\u751f\u7684\u5371\u9669\uff0c\u4e0d\u5f97\u5df2\u91c7\u53d6\u7684\u7d27\u6025\u907f\u9669\u884c\u4e3a\uff0c\u9020\u6210\u635f\u5bb3\u7684\uff0c\u4e0d\u8d1f\u5211\u4e8b\u8d23\u4efb\u3002\u201d\u5173\u4e8e\u8be5\u6cd5\u6761\u4e2d\u5305\u542b\u7684\u6cd5\u5f8b\u89c4\u5219\u7684\u903b\u8f91\u7ed3\u6784\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u5047\u5b9a\u6761\u4ef6\u3001\u884c\u4e3a\u6a21\u5f0f\u548c\u6cd5\u5f8b\u540e\u679c\nB. \u5047\u5b9a\u6761\u4ef6\u548c\u6cd5\u5f8b\u540e\u679c\nC. \u6cd5\u5f8b\u540e\u679c\u548c\u884c\u4e3a\u6a21\u5f0f\nD. \u5047\u5b9a\u6761\u4ef6\u548c\u884c\u4e3a\u6a21\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6366680569669386, "meta-math/MetaMath-Mistral-7B": 0.9584566145438019, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9776076228237747, "meta-llama/Meta-Llama-3-8B": 0.5277408414344443, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5612835042362446}}, {"question": "\u4e0b\u5217\u5e38\u7528\u82f1\u8bed\u6c42\u6551\u5355\u8bcd\u4e0e\u610f\u601d\u5bf9\u5e94\u6b63\u786e\u7684\u662f\nA. S0S\u6c42\u6551\nB. HELP\u9001\u51fa\nC. SEND\u8ff7\u5931\nD. LOST\u53d7\u56f0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5432322853624897, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4676965188148211, "HuggingFaceH4/zephyr-7b-beta": 0.9999361023638451, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7847974082481969, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5341388886111852}}, {"question": "\u5982\u679c\u592a\u9633\u98ce\u6bcf\u79d2\u4ece\u592a\u9633\u8868\u9762\u5439\u51fa\u7684\u7c92\u5b50\u6570\u662fn=1\u00d710^{36}\uff0c\u4e14\u56e0\u4e3a\u6838\u5fc3\u6c22\u7684\u6838\u805a\u53d8\u53cd\u5e94\uff0c\u592a\u9633\u7684\u8d28\u91cf\u4e5f\u5728\u540c\u65f6\u51cf\u5c11\uff0c\u5219\u592a\u9633\u98ce\u4e0e\u6838\u805a\u53d8\u7684\u8d28\u91cf\u635f\u5931\u7387\u4e4b\u6bd4\u6700\u63a5\u8fd1\u7684\u662f\u3002\uff08\u5df2\u77e5\uff1a\u592a\u9633\u5149\u5ea63.8\u00d710^{26}W\uff0c\u7535\u5b50\u8d28\u91cf9.1\u00d710^{-31}kg\uff0c\u8d28\u5b50\u8d28\u91cf1.7\u00d710^{-27}kg\u3002\uff09\nA. 1:50\nB. 1:5\nC. 2:50\nD. 2:5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.417305741279239, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.34239623393788804, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6020156482290361}}, {"question": "\u65e0\u4ea7\u9636\u7ea7\u4e13\u653f\u7684\u76ee\u6807\u662f\nA. \u4e0d\u65ad\u5de9\u56fa\u3001\u53d1\u5c55\u65e0\u4ea7\u9636\u7ea7\u653f\u6743\u548c\u793e\u4f1a\u4e3b\u4e49\u5236\u5ea6\nB. \u9632\u6b62\u56fd\u5916\u654c\u4eba\u7684\u4fb5\u7565\u548c\u98a0\u8986\uff0c\u652f\u6301\u4e16\u754c\u4eba\u6c11\u7684\u9769\u547d\u6597\u4e89\nC. \u5efa\u8bbe\u793e\u4f1a\u4e3b\u4e49\u6c11\u4e3b\nD. \u8981\u6d88\u706d\u5265\u524a\u3001\u6d88\u706d\u9636\u7ea7\uff0c\u8fdb\u5230\u65e0\u9636\u7ea7\u793e\u4f1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7576251218897948, "meta-math/MetaMath-Mistral-7B": 0.9627553209903438, "itpossible/Chinese-Mistral-7B-v0.1": 0.5754195250065764, "HuggingFaceH4/zephyr-7b-beta": 0.9929238329623108, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8426456190935065, "meta-llama/Meta-Llama-3-8B": 0.4888566993486958, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7210298962719515}}, {"question": "\u6c11\u4e3b\u653f\u6cbb\u53d6\u4ee3\u4e13\u5236\u653f\u6cbb\uff0c\u8fd9\u662f\u516c\u5171\u5173\u7cfb\u4ea7\u751f\u7684\nA. \u5386\u53f2\u6761\u4ef6\nB. \u653f\u6cbb\u6761\u4ef6\nC. \u7ecf\u6d4e\u6761\u4ef6\nD. \u6587\u5316\u6761\u4ef6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.626073898655821, "meta-math/MetaMath-Mistral-7B": 0.5951110622274491, "itpossible/Chinese-Mistral-7B-v0.1": 0.6122081785109881, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5918385766787481, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7463452490013995}}, {"question": "\u7ba1\u7406\u8005\u6839\u636e\u81ea\u5df1\u7684\u8d23\u4efb\u548c\u6743\u9650\uff0c\u501f\u52a9\u6307\u793a\u3001\u547d\u4ee4\u7b49\u6743\u529b\u624b\u6bb5\u548c\u6743\u5a01\uff0c\u6709\u6548\u5730\u6307\u5bfc\u4e0b\u5c5e\u673a\u6784\u548c\u4eba\u5458\u5c65\u884c\u5176\u804c\u8d23\uff0c\u5b9e\u73b0\u7ba1\u7406\u76ee\u6807\u7684\u9886\u5bfc\u6d3b\u52a8\u3002\u8fd9\u662f\u5b66\u6821\u7ba1\u7406\u7684\nA. \u63a7\u5236\u804c\u80fd\nB. \u534f\u8c03\u804c\u80fd\nC. \u7ec4\u7ec7\u804c\u80fd\nD. \u6307\u6325\u804c\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.47408283366262655, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7030404172120638, "HuggingFaceH4/zephyr-7b-beta": 0.5538723421739431, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7430881621686196, "meta-llama/Meta-Llama-3-8B": 0.6054070091463254, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9317162828817861}}, {"question": "\u4eba\u7c7b\u793e\u4f1a\u7684\u9053\u5fb7\u53d1\u5c55\u662f\u968f\u7740\u793e\u4f1a\u7ecf\u6d4e\u5173\u7cfb\u7684\u4e0d\u65ad\u5b8c\u5584\u800c\u8fdb\u6b65\u53d1\u5c55\u7684\u3002\u73b0\u65f6\u4ee3\u6700\u5148\u8fdb\u7684\u9053\u5fb7\u662f\nA. \u4f20\u7edf\u9053\u5fb7\nB. \u8d44\u4ea7\u9636\u7ea7\u9053\u5fb7\nC. \u793e\u4f1a\u4e3b\u4e49\u9053\u5fb7\nD. \u5b97\u6559\u9053\u5fb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9641354186074584, "meta-math/MetaMath-Mistral-7B": 0.9967139791448061, "itpossible/Chinese-Mistral-7B-v0.1": 0.9776007959663686, "HuggingFaceH4/zephyr-7b-beta": 0.9981512212617651, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.924537789863999, "meta-llama/Meta-Llama-3-8B": 0.963211964823917, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9963758195920916}}, {"question": "\u4e0b\u5217\u4e34\u5e8a\u4e0a\u5e38\u89c1\u51fa\u73b0\u8109\u538b\u51cf\u5c0f\u7684\u75c5\u53d8\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u4e3b\u52a8\u8109\u74e3\u72ed\u7a84\nB. \u91cd\u5ea6\u4e8c\u5c16\u74e3\u5173\u95ed\u4e0d\u5168\nC. \u5fc3\u5305\u79ef\u6db2\nD. \u5fc3\u529b\u8870\u7aed\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9645142716419114, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe $F(x)$ \u548c $G(x)$ \u5747\u4e3a\u968f\u673a\u53d8\u91cf\u7684\u5206\u5e03\u51fd\u6570\uff0c \u5219\u4e0b\u5217\u53ef\u4ee5\u4f5c\u4e3a\u67d0\u968f\u673a\u53d8\u91cf\u7684\u5206\u5e03 \u51fd\u6570\u7684\u662f ( )\nA. $2 F(x)-G(x)$\nB. $\\frac{1}{3} F(x)+\\frac{2}{3} G(x)$\nC. $F(x)+G(x)$\nD. $F\\left(x^2\\right)$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53\u4f01\u4e1a\u62e5\u6709\u5355\u4e00\u7684\u4ea7\u54c1\u5927\u7c7b\uff0c\u9762\u5bf9\u4e0d\u540c\u504f\u597d\u7684\u6d88\u8d39\u7fa4\u4f53\u540c\u65f6\u4f7f\u7528\u4e0d\u540c\u7684\u5206\u9500\u6e20\u9053\u65f6\uff0c\u6700\u9002\u5b9c\u91c7\u7528\u7684\u5e02\u573a\u8425\u9500\u7ec4\u7ec7\u7c7b\u578b\u662f\nA. \u4ea7\u54c1\u578b\u7ec4\u7ec7\nB. \u804c\u80fd\u578b\u7ec4\u7ec7\nC. \u5730\u7406\u578b\u7ec4\u7ec7\nD. \u5e02\u573a\u578b\u7ec4\u7ec7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5896104085915327, "meta-math/MetaMath-Mistral-7B": 0.9506597366256649, "itpossible/Chinese-Mistral-7B-v0.1": 0.6093855805230416, "HuggingFaceH4/zephyr-7b-beta": 0.9996563856930452, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7812639561610816, "meta-llama/Meta-Llama-3-8B": 0.5190768443635878, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9543991862126243}}, {"question": "\u5343\u4e07\u4e0d\u8981\u5230()\u53bb\u73a9\u800d\nA. \u6c99\u6ee9\nB. \u5efa\u7b51\u5de5\u5730\nC. \u4e50\u56ed\nD. \u64cd\u573a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6777366528946094, "meta-math/MetaMath-Mistral-7B": 0.9105398854254269, "itpossible/Chinese-Mistral-7B-v0.1": 0.7546246854650442, "HuggingFaceH4/zephyr-7b-beta": 0.9943463236924965, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9726877391495599, "meta-llama/Meta-Llama-3-8B": 0.8904732817161028, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.925654721490362}}, {"question": "\u7f57\u9a6c\u6cd5\u89c4\u5b9a\uff0c\u5728\u8d22\u4ea7\u7ee7\u627f\u65b9\u9762\uff0c\u82e5\u9057\u5631\u88ab\u8ba4\u4e3a\u4e0d\u7b26\u5408\u4eba\u4f26\u9053\u5fb7\uff0c\u9057\u5631\u4eba\u7684\u8fd1\u4eb2\u53ef\u4ee5\u63d0\u8d77\u201c\u9057\u5631\u9006\u4f26\u4e4b\u8bc9\u201d\uff0c\u4ee5\u8bf7\u6c42\u64a4\u9500\u9057\u5631\u3002\u8fd9\u4e00\u89c4\u5b9a\u8bf4\u660e\u7f57\u9a6c\u6cd5\nA. \u5bf9\u5e02\u6c11\u7684\u8d22\u4ea7\u81ea\u7531\u8fdb\u884c\u9650\u5236\nB. \u8981\u6c42\u5728\u7ee7\u627f\u5173\u7cfb\u4e2d\u8fd1\u4eb2\u4f18\u5148\nC. \u5728\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u517c\u987e\u4e86\u793e\u4f1a\u6c11\u60c5\nD. \u653e\u677e\u4e86\u5bf9\u516c\u6c11\u8d22\u4ea7\u7684\u7ee7\u627f\u4fdd\u62a4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.399162304558403, "meta-math/MetaMath-Mistral-7B": 0.470389574003656, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6594839618827847, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9933323329999976}}, {"question": "\u6839\u636e\u5408\u540c\u5f53\u4e8b\u4eba\u662f\u5426\u4e92\u76f8\u4eab\u6709\u6743\u5229\u8d1f\u6709\u4e49\u52a1\uff0c\u53ef\u5c06\u5408\u540c\u5206\u4e3a\nA. \u6709\u540d\u5408\u540c\u4e0e\u65e0\u540d\u5408\u540c\nB. \u5355\u52a1\u5408\u540c\u4e0e\u53cc\u52a1\u5408\u540c\nC. \u6709\u507f\u5408\u540c\u4e0e\u65e0\u507f\u5408\u540c\nD. \u8981\u5f0f\u5408\u540c\u4e0e\u5b9e\u8df5\u5408\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5794769015462363, "itpossible/Chinese-Mistral-7B-v0.1": 0.950145863034163, "HuggingFaceH4/zephyr-7b-beta": 0.6838912832211329, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9266270240819497, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9085793307714064}}, {"question": "\u83e9\u8428\u4fee\u884c\u7684\u4f4d\u6b21\u521d\u5730\u662f\u6307\nA. \u79bb\u57a2\u5730\nB. \u8fdc\u884c\u5730\nC. \u6b22\u559c\u5730\nD. \u53d1\u5149\u5730\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1788\u5e747\u670810\u65e5\uff0c\u7ebd\u7ea6\u67d0\u62a5\u7eb8\u4ee5\u300a\u8239\u8baf\u2014\u2014\u53f7\u5916\u300b\u4e3a\u9898\u53d1\u5e03\u901a\u544a\uff1a\u4e07\u4e16\u8054\u5408\u8239\u4e3b\u7684\u5e78\u798f\u8239\uff0c\u5df2\u8f7d\u7740\u5341\u4e09\u5305\u201c\u8054\u5408\u3001\u548c\u5e73\u548c\u53cb\u8c0a\u201d\u8fdb\u6e2f\uff0c\u2026\u2026\u611a\u8822\u8239\u4e3b\u7684\u8239\u5df2\u8f7d\u7740\u5730\u65b9\u504f\u89c1\u3001\u4e0d\u548c\u7684\u79cd\u5b50\u7b49\u51fa\u6e2f\u3002\u5b83\u8d5e\u7f8e\u7684\u662f\nA. \u4e3b\u6743\u5728\u6c11\u539f\u5219\nB. \u5206\u6743\u5236\u8861\u539f\u5219\nC. \u5171\u548c\u5236\u5ea6\nD. \u8054\u90a6\u4f53\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3638582843838116, "HuggingFaceH4/zephyr-7b-beta": 0.720664016297756, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4382091167752508, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.796741173229441}}, {"question": "\u7528\u771f\u503c\u8868\u68c0\u9a8c\u91cd\u8a00\u5f0f\u4e0e\u77db\u76fe\u5f0f\u9700\u8981\u9488\u5bf9\u6240\u8ba8\u8bba\u7684\u547d\u9898\u5efa\u7acb\u4e00\u4e2a\nA. \u77db\u76fe\u5f0f\nB. \u91cd\u8a00\u5f0f\nC. \u771f\u503c\u884c\nD. \u771f\u503c\u5217\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4008670121220472, "meta-math/MetaMath-Mistral-7B": 0.36464267153587787, "itpossible/Chinese-Mistral-7B-v0.1": 0.5005725520129989, "HuggingFaceH4/zephyr-7b-beta": 0.9983285780965551, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6666987731408125, "meta-llama/Meta-Llama-3-8B": 0.48591480744674537, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ec6\u80de\u8272\u7d20\u4f53\u7cfb\u4e2d\u80fd\u4e0eCO\u548c\u6c30\u5316\u7269\u7ed3\u5408\u4f7f\u7535\u5b50\u4e0d\u80fd\u4f20\u9012\u7ed9\u6c27\u800c\u4f7f\u547c\u5438\u94fe\u4e2d\u65ad\u7684\u662f\nA. \u7ec6\u80de\u8272\u7d20A3\nB. \u7ec6\u80de\u8272\u7d20B1\nC. \u7ec6\u80de\u8272\u7d20C\nD. \u7ec6\u80de\u8272\u7d20B\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.2885095257630687, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3704758786566308, "meta-llama/Meta-Llama-3-8B": 0.47406982006883397, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.630697800064522}}, {"question": "\u8eab\u4f53\u7d20\u8d28\u662f\u4eba\u7684\u4f53\u80fd\u72b6\u6001\u7684\u53cd\u6620\uff0c\u662f\u6307\u4eba\u4f53\u5728\u8fd0\u52a8\u3001\u5de5\u4f5c\u548c\u751f\u6d3b\u4e2d\u6240\u8868\u73b0\u51fa\u6765\u7684\u529b\u91cf\u3001\u8010\u529b\u548c\u3002\nA. \u901f\u5ea6\nB. \u547c\u5438\u7cfb\u7edf\nC. \u8eab\u4f53\u673a\u80fd\nD. \u9aa8\u9abc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6b21\u7ea7\u80c6\u6c41\u9178\u662f\nA. \u5728\u809d\u5185\u7531\u521d\u7ea7\u6e38\u79bb\u80c6\u6c41\u9178\u8f6c\u53d8\u751f\u6210\nB. \u5728\u80a0\u5185\u7531\u521d\u7ea7\u80c6\u6c41\u9178\u8f6c\u53d8\u751f\u6210\nC. \u5728\u80a0\u5185\u7531\u80c6\u56fa\u9187\u8f6c\u53d8\u751f\u6210\nD. \u5728\u809d\u5185\u7531\u521d\u7ea7\u7ed3\u5408\u80c6\u6c41\u9178\u8f6c\u53d8\u751f\u6210\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3194451802654172, "meta-math/MetaMath-Mistral-7B": 0.38754811480062784, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3528619536675028, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5630475047893382}}, {"question": "\u7d2f\u53ca\u8840\u7ba1\u5927\u5c0f\u53ef\u53d8\u7684\u7cfb\u7edf\u6027\u8840\u7ba1\u708e\u662f\nA. \u5ddd\u5d0e\u75c5\nB. \u8d1d\u8d6b\u5207\u7279\u75c5\nC. \u8089\u82bd\u80bf\u6027\u591a\u8840\u7ba1\u708e\nD. IgA\u8840\u7ba1\u708e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.43983232232244734}}, {"question": "\u80ba\u4e0b\u754c\u7684\u4f53\u8868\u6295\u5f71\u5728\u814b\u4e2d\u7ebf\u76f8\u4ea4\u4e8e\nA. \u7b2c8\u808b\nB. \u7b2c6\u808b\nC. \u7b2c5\u808b\nD. \u7b2c7\u808b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ecf\u8425\u8005\u63d0\u4f9b\u5546\u54c1\u6216\u8005\u670d\u52a1\u6709\u6b3a\u8bc8\u884c\u4e3a\u7684\uff0c\u5e94\u5f53\u6309\u7167\u6d88\u8d39\u8005\u7684\u8981\u6c42\u589e\u52a0\u8d54\u507f\u5176\u53d7\u5230\u7684\u635f\u5931\uff0c\u589e\u52a0\u8d54\u507f\u7684\u91d1\u989d\u4e3a\u6d88\u8d39\u8005\u8d2d\u4e70\u5546\u54c1\u7684\u4ef7\u6b3e\u6216\u63a5\u53d7\u670d\u52a1\u7684\u8d39\u7528\u7684\u591a\u5c11\u500d\nA. 4\u500d\nB. 3\u500d\nC. 1\u500d\nD. 2\u500d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.46945937254202624, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.42588617478863083}}, {"question": "\u8d28\u91cf10g\u3001\u4ee50.70km/s\u2edc\u2f8f\u7684\u2f26\u5f39\u4e0e\u8d28\u91cf60kg\u3001\u4ee510m/s\u5954\u8dd1\u7684\u8fd0\u52a8\u5458\u76f8\u2f50\nA. \u2f26\u5f39\u7684\u52a8\u80fd\u8f83\u2f24\nB. \u2f06\u8005\u7684\u52a8\u80fd\u2f00\u6837\u2f24\nC. \u2f46\u6cd5\u2f50\u8f83\u5b83\u4eec\u7684\u52a8\u80fd\nD. \u8fd0\u52a8\u5458\u7684\u52a8\u80fd\u8f83\u2f24\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u521b\u9020\u793e1921\u5e747\u6708\u6210\u7acb\u65e5\u672c\u4e1c\u4eac\uff0c\u90fd\u662f\u5f53\u65f6\u5728\u65e5\u672c\u4e1c\u4eac\u7684\u7559\u5b66\u751f\u3002\u4ed6\u4eec\u521b\u529e\u4e86\u300a\u521b\u9020\u300b\u5b63\u520a\u3001\u300a\u521b\u9020\u5468\u520a\u300b\u3001\u300a\u521b\u9020\u65e5\u300b\u3001\u300a\u6d2a\u6c34\u300b\u7b49\u520a\u7269\uff0c\u4e3b\u5f20\u201c\u4e3a\u827a\u672f\u800c\u827a\u672f\u201d\uff0c\u5f3a\u8c03\u6587\u5b66\u5fc5\u987b\u5fe0\u5b9e\u5730\u8868\u73b0\u4f5c\u8005\u81ea\u5df1\u201c\u5185\u5fc3\u7684\u8981\u6c42\u201d\uff0c\u91cd\u89c6\u6587\u5b66\u7684\u7f8e\u611f\u4f5c\u7528\u3002\u5c5e\u4e8e\u521b\u9020\u793e\u6210\u5458\u7684\u4e00\u7ec4\u662f\nA. \u90ed\u6cab\u82e5\u3001\u5f20\u8d44\u5e73\u3001\u90c1\u8fbe\u592b\u3001\u6210\u5f77\u543e\u3001\u7530\u5bff\u660c\nB. \u6c88\u96c1\u51b0\u3001\u5f20\u8d44\u5e73\u3001\u90c1\u8fbe\u592b\u3001\u6210\u5f77\u543e\u3001\u8bb8\u5730\u5c71\nC. \u5468\u4f5c\u4eba\u3001\u5f20\u8d44\u5e73\u3001\u90c1\u8fbe\u592b\u3001\u6210\u5f77\u543e\u3001\u9c81\u8fc5\nD. \u90ed\u6cab\u82e5\u3001\u5f20\u8d44\u5e73\u3001\u90c1\u8fbe\u592b\u3001\u6210\u5f77\u543e\u3001\u6797\u8bed\u5802\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4541039773223436, "meta-math/MetaMath-Mistral-7B": 0.5246138854189192, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6203535984729758, "meta-llama/Meta-Llama-3-8B": 0.34239623393788804, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5927\u79b9\u6cbb\u6c34\u201d\u7684\u6545\u4e8b\u5bb6\u55bb\u6237\u6653\uff0c\u5927\u79b9\u6cbb\u7406\u7684\u662f\u54ea\u4e2a\u6d41\u57df\u7684\u6d2a\u6c34\nA. \u6d1e\u5ead\u6e56\u6c34\u57df\nB. \u9131\u9633\u6e56\u6c34\u57df\nC. \u9ec4\u6cb3\u6d41\u57df\nD. \u957f\u6c5f\u6d41\u57df\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4974058721781516, "meta-math/MetaMath-Mistral-7B": 0.6719171327996907, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9291380554259678, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6461760924187371, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u79cd\u60c5\u51b5\u8bf4\u660e\u5e02\u573a\u9700\u6c42\u53ef\u80fd\u7f3a\u4e4f\u5f39\u6027\uff1f\nA. \u8d2d\u4e70\u8005\u79ef\u6781\u5bfb\u6c42\u8f83\u4e3a\u4fbf\u5b9c\u7684\u4e1c\u897f\nB. \u8d2d\u4e70\u8005\u8d2d\u4e70\u4e60\u60ef\u5f88\u5bb9\u6613\u6539\u53d8\nC. \u8d2d\u4e70\u8005\u5bf9\u9ad8\u4ef7\u683c\u5f88\u5728\u610f\nD. \u5e02\u573a\u4e0a\u6ca1\u6709\u66ff\u4ee3\u54c1\u6216\u6ca1\u6709\u7ade\u4e89\u8005\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4458433307734352, "meta-math/MetaMath-Mistral-7B": 0.6429364522187536, "itpossible/Chinese-Mistral-7B-v0.1": 0.4890223473100958, "HuggingFaceH4/zephyr-7b-beta": 0.8330934358688434, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7095423502460974, "meta-llama/Meta-Llama-3-8B": 0.6487062281175798, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4f20\u64ad\u7684\u8fc7\u7a0b\uff0c\u62c9\u65af\u97e6\u5c14\u63d0\u51fa\u7684\u6a21\u5f0f\u662f\nA. \u5bf9\u79f0\u6a21\u5f0f\nB. \u6570\u5b66\u6a21\u5f0f\nC. 5W\u6a21\u5f0f\nD. \u5e73\u8861\u6a21\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43122525814466456, "meta-math/MetaMath-Mistral-7B": 0.7072987956618573, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5309377157392581, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8986386187116489, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7902199442093087}}, {"question": "\u4f18\u7f8e\u3001\u559c\u5267\u3001\u4e11\u548c\u8352\u8bde\u7b49\u5ba1\u7f8e\u5f62\u6001\u6210\u4e3a\u4e16\u754c\u5404\u6c11\u65cf\u5171\u540c\u7684\u5ba1\u7f8e\u5f62\u6001\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\nA. \u827a\u672f\u7684\u53d8\u8fc1\nB. \u793e\u4f1a\u7684\u53d1\u5c55\nC. \u5b9e\u8df5\u7684\u63d0\u5347\nD. \u6587\u5316\u7684\u8ba4\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u73ed\u4e3b\u4efb\u4e86\u89e3\u548c\u7814\u7a76\u5b66\u751f\u7684\u4e3b\u8981\u65b9\u6cd5\u662f\nA. \u89c2\u5bdf\u6cd5\nB. \u8c03\u67e5\u6cd5\nC. \u8c08\u8bdd\u6cd5\nD. \u8003\u6838\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4002031857725596, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5199402063248852, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1919\u5e747\u6708\uff0c\u8fdb\u6b65\u62a5\u520a\u300a\u6bcf\u5468\u8bc4\u8bba\u300b\u63d0\u51fa\uff1a\u201c\u771f\u6b63\u7684\u89e3\u653e\uff0c\u4e0d\u662f\u592e\u6c42\u4eba\u5bb6\u7f51\u5f00\u4e00\u9762\u2026\u2026\u662f\u8981\u9760\u81ea\u5df1\u7684\u52aa\u529b\uff0c\u628a\u4ed6\u6253\u7834\uff0c\u4ece\u9ed1\u6697\u7684\u7262\u72f1\u4e2d\uff0c\u6253\u51fa\u4e00\u9053\u5149\u660e\u6765\u3002\u540c\u4e00\u65f6\u671f\u7684\u300a\u5357\u5f00\u65e5\u520a\u300b\u5ba3\u79f0\uff1a\u201c\u53ea\u8981\u6709\u5e72\u6d89\u653f\u6cbb\u7684\u51b3\u5fc3\uff0c\u4e0d\u6015\u653f\u5e9c\u4e0d\u987a\u4ece\u6c11\u610f\u3002\u201d\u8fd9\u8868\u660e\u5f53\u65f6\nA. \u5e7f\u5927\u6c11\u4f17\u5f00\u59cb\u62e5\u6709\u51b3\u5b9a\u56fd\u5bb6\u653f\u7b56\u7684\u6743\u529b\nB. \u77e5\u8bc6\u9636\u5c42\u5177\u6709\u53c2\u4e0e\u653f\u5e9c\u7ba1\u7406\u7684\u610f\u8bc6\nC. \u8fdb\u6b65\u77e5\u8bc6\u5206\u5b50\u5bf9\u793e\u4f1a\u53d8\u9769\u65b9\u5f0f\u6709\u4e86\u65b0\u8ba4\u8bc6\nD. \u4e0e\u514b\u601d\u4e3b\u4e49\u5728\u4e2d\u56fd\u5f97\u5230\u5e7f\u6cdb\u6df1\u5165\u7684\u4f20\u64ad\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5928299288544577, "meta-math/MetaMath-Mistral-7B": 0.7682960634746829, "itpossible/Chinese-Mistral-7B-v0.1": 0.5829521285210238, "HuggingFaceH4/zephyr-7b-beta": 0.9649265784125055, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6537887707732538, "meta-llama/Meta-Llama-3-8B": 0.6651255498011539, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6705731168497282}}, {"question": "\u5929\u6d25\u8457\u540d\u4e2d\u836f\u4f01\u4e1a\u8fbe\u4ec1\u5802\u5982\u4eca\u5df2\u6709\u767e\u5e74\u5386\u53f2\u300220\u4e16\u7eaa50\u5e74\u4ee3\u7eb3\u5165\u56fd\u5bb6\u96c6\u4e2d\u7edf\u4e00\u7ba1\u7406\u540e\uff0c\u539f\u6709\u5206\u53f7\u4e0e\u5176\u5168\u90e8\u8131\u94a9\uff0c\u8fbe\u4ec1\u5802\u53ea\u7ba1\u751f\u4ea7\uff0c\u4e0d\u7ba1\u9500\u552e\u3002\u572820 \u4e16\u7eaa90 \u5e74\u4ee3\u4e2d\u671f\u7684\u53d1\u5c55\u4e2d\uff0c\u8be5\u4f01\u4e1a\u66fe\u4e00\u5ea6\u201c\u62d4\u5251\u56db\u987e\u5fc3\u832b\u7136\u201d\u3002\u9020\u6210\u5176\u201c\u832b\u7136\u201d\u7684\u4e3b\u8981\u539f\u56e0\u662f\nA. \u4f01\u4e1a\u5c1a\u672a\u9002\u5e94\u5e02\u573a\u7ecf\nB. \u4f01\u4e1a\u5931\u53bb\u5206\u53f7\uff0c\u529b\u91cf\u524a\u5f31\nC. \u4f01\u4e1a\u4ea7\u54c1\u8d28\u91cf\u4e0b\u6ed1\nD. \u96c6\u4e2d\u7edf\u4e00\u7ba1\u7406\u4f7f\u4f01\u4e1a\u5931\u53bb\u6d3b\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6280\u672f\u6587\u5316\u666f\u89c2\u662f\u4ee5\u4e0b\u5217\u54ea\u4e00\u9879\u4e3a\u6838\u5fc3\u7684\nA. \u6280\u672f\u4f20\u5a92\nB. \u6280\u672f\u7406\u5ff5\nC. \u6280\u672f\u53d1\u5c55\nD. \u6280\u672f\u89c4\u8303\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3671700228158848, "meta-math/MetaMath-Mistral-7B": 0.5113041678545609, "itpossible/Chinese-Mistral-7B-v0.1": 0.778351903714324, "HuggingFaceH4/zephyr-7b-beta": 0.9992208638555854, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5740062320612715, "meta-llama/Meta-Llama-3-8B": 0.2801288226217134, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8080787153003008}}, {"question": "\u4e0b\u5217\u54ea\u9879\u4e0d\u662f\u4eba\u5de5\u81ea\u52a8\u514d\u75ab\u4f7f\u7528\u7684\u5236\u5242\nA. \u810a\u9ad3\u7070\u8d28\u708e\u6d3b\u75ab\u82d7\nB. \u9ebb\u75b9\u6d3b\u75ab\u82d7\nC. \u5361\u4ecb\u82d7\nD. \u4e19\u79cd\u7403\u86cb\u767d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5603111661930648, "meta-math/MetaMath-Mistral-7B": 0.5912916217681976, "itpossible/Chinese-Mistral-7B-v0.1": 0.5014154826951555, "HuggingFaceH4/zephyr-7b-beta": 0.642936440932372, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6805214714027491, "meta-llama/Meta-Llama-3-8B": 0.307023896814742, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5765268646422097}}, {"question": "19\u4e16\u7eaa\u516d\u4e03\u5341\u5e74\u4ee3\uff0c\u5916\u56fd\u4eba\u5c06\u81ea\u5df1\u7684\u540d\u5b57\u79df\u501f\u7ed9\u4e2d\u56fd\u4eba\u7ecf\u529e\u65b0\u5f0f\u4f01\u4e1a\u7684\u505a\u6cd5\uff0c\u5728\u901a\u5546\u53e3\u5cb8\u8f83\u4e3a\u76db\u884c\u3002\u8fd9\u4e00\u505a\u6cd5\nA. \u52a0\u5267\u4e86\u5916\u56fd\u8d44\u672c\u5bf9\u4e2d\u56fd\u7684\u8f93\u5165\nB. \u6709\u5229\u4e8e\u4e2d\u56fd\u65b0\u7684\u793e\u4f1a\u9636\u5c42\u53d1\u5c55\nC. \u5bfc\u81f4\u6c11\u95f4\u8bbe\u5382\u9ad8\u6f6e\u5c40\u9762\u7684\u51fa\u73b0\nD. \u626d\u8f6c\u4e86\u4e2d\u56fd\u5bf9\u5916\u8d38\u6613\u5165\u8d85\u5c40\u9762\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5176492478570136}}, {"question": "\u5728\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u80c3\u6240\u5177\u6709\u7684\u8fd0\u52a8\u5f62\u5f0f\u662f\nA. \u5206\u8282\u8fd0\u52a8\nB. \u8815\u52a8\u51b2\nC. \u5bb9\u53d7\u6027\u8212\u5f20\nD. \u888b\u72b6\u5f80\u8fd4\u8fd0\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c47\u5c81\uff0c\u8179\u80c0\u3001\u7eb3\u5dee\u534a\u5e74\uff0c6\u5c0f\u65f6\u524d\u7a81\u53d1\u5455\u8840\u6765\u8bca\u3002\u65e2\u5f80HBsAg(+)\u3002\u67e5\u4f53\uff1aP125\u6b21/\u5206\uff0cBP70/50mmHg\uff0c\u5de9\u819c\u8f7b\u5ea6\u9ec4\u67d3\uff0c\u809d\u813e\u808b\u4e0b\u672a\u89e6\u53ca\uff0c\u79fb\u52a8\u6027\u6d4a\u97f3(+)\uff0c\u4e0b\u80a2\u53ef\u51f9\u6027\u6c34\u80bf\u3002\u5f15\u8d77\u8be5\u60a3\u8005\u5455\u8840\u7684\u6700\u53ef\u80fd\u75c5\u56e0\u662f\nA. \u809d\u786c\u5316\nB. \u6d88\u5316\u6027\u6e83\u75a1\nC. \u80c3\u98df\u7ba1\u53cd\u6d41\u75c5\nD. \u80c3\u764c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3602912461121416, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.46454998140218834, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53d8\u538b\u5668\u7684\u63a5\u7ebf\u7ec4\u522b\u8868\u793a\u662f\u53d8\u538b\u5668\u7684\u9ad8\u3001\u4f4e\u538b\u4fa7\uff08\uff09\u95f4\u7684\u76f8\u4f4d\u5173\u7cfb\u3002\nA. \u76f8\u7535\u538b\nB. \u76f8\u7535\u6d41\nC. \u7ebf\u7535\u6d41\nD. \u7ebf\u7535\u538b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8525454804824542, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5344466407803383, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7535\u8111\u7531\u4e8e\u4e0d\u660e\u539f\u56e0\u5192\u70df\u65f6\uff0c\u5e94\u907f\u5f00\uff08\uff09\u8fdb\u884c\u5904\u7f6e\uff0c\u4ee5\u9632\u7206\u70b8\u4f24\u4eba\u3002\nA. \u5c4f\u5e55\u540e\u9762\nB. \u5c4f\u5e55\u6b63\u9762\nC. \u5c4f\u5e55\u4fa7\u9762\nD. \u5c4f\u5e55\u4e0b\u65b9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28373022896248634, "meta-math/MetaMath-Mistral-7B": 0.3918190646891617, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u524d\u63d0\u662f\u81ea\u6211\u6b3a\u9a97\u6027\u8bed\u53e5\uff0c\u5373\nA. \u524d\u63d0\u865a\u5047\u8c2c\u8bef\nB. \u4ee5\u4e0a\u90fd\u4e0d\u5bf9\nC. \u9884\u671f\u7406\u7531\u8c2c\u8bef\nD. \u524d\u63d0\u81ea\u76f8\u77db\u76fe\u8c2c\u8bef\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.36087106371472144, "HuggingFaceH4/zephyr-7b-beta": 0.9992098807319711, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5284271800947281, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4389239893731963}}, {"question": "\u58a8\u2ec4\u54e5\u57ce\u6bcf\u5e7411\u2f49\u2f84\u6b21\u5e744\u30015\u2f49\u591a\u53d1\u2f63\u4e25\u91cd\u7684\u5149\u5316\u5b66\u70df\u96fe\u4e8b\u4ef6\uff0c\u4e3b\u8981\u539f\u56e0\u6709\nA. \u6c7d\u2ecb\u5c3e\u2f53\u548c\u2f63\u4ea7\u3001\u2f63\u6d3b\u5e9f\u2f53\u6392\u653e\u91cf\u2f29\nB. \u8be5\u671f\u95f4\u591a\u2f53\u65cb\u6d3b\u52a8\uff0c\u5f71\u54cd\u6c61\u67d3\u2f53\u4f53\u7684\u6269\u6563\nC. \u53d7\u6e29\u5ba4\u6548\u5e94\u5f71\u54cd\nD. \u8be5\u671f\u95f4\u5929\u2f53\u6674\u6717\uff0c\u5149\u7167\u5f3a\uff1b\u4e14\u76c6\u5730\u5730\u5f62\uff0c\u591a\u9006\u6e29\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8130437330909187, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5670184138430419, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e0d\u5c5e\u4e8e\u5973\u6027\u5728\u751f\u7406\u4e0a\u7684\u4e09\u4e2a\u7279\u522b\u65f6\u671f\u7684\u662f\nA. \u6000\u5b55\u4e0e\u54fa\u4e73\u671f\nB. \u6708\u7ecf\u671f\nC. \u7edd\u7ecf\u671f\nD. \u9752\u6625\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u4fe1\u53f7\u5206\u5b50\u7684\u76f8\u5173\u53d9\u8ff0\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u7ec6\u80de\u95f4\u4f20\u9012\u4fe1\u53f7\u7684\u5206\u5b50\u90fd\u662f\u7531\u7ec6\u80de\u5185\u7684\u6838\u7cd6\u4f53\u5408\u6210\u7684\nB. \u4fe1\u53f7\u5206\u5b50\u5728\u5b8c\u6210\u4fe1\u606f\u4f20\u9012\u540e\u6570\u91cf\u548c\u6027\u8d28\u53d1\u751f\u6539\u53d8\nC. \u6709\u7684\u4fe1\u53f7\u5206\u5b50\u5408\u6210\u540e\u53ef\u4ee5\u901a\u8fc7\u81ea\u7531\u6269\u6563\u7684\u65b9\u5f0f\u8fd0\u51fa\u7ec6\u80de\u5916\nD. \u5411\u542b\u6709\u4fc3\u4f7f\u8840\u7cd6\u964d\u4f4e\u7684\u4fe1\u53f7\u5206\u5b50\u7684\u6eb6\u6db2\u4e2d\u52a0\u5165\u53cc\u7f29\u8132\u8bd5\u5242\u540e\u6eb6\u6db2\u5448\u7d2b\u8272\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5980568734527913, "meta-math/MetaMath-Mistral-7B": 0.9288160565477468, "itpossible/Chinese-Mistral-7B-v0.1": 0.5847369558666086, "HuggingFaceH4/zephyr-7b-beta": 0.9979696485436614, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8146522436268613, "meta-llama/Meta-Llama-3-8B": 0.6869708570656677, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.49498911018909447}}, {"question": "\u76ee\u524d\uff0c\u6df7\u5408\u6240\u6709\u5236\u7ecf\u6d4e\u5df2\u7ecf\u5360\u6211\u56fd\u603b\u4f53\u7ecf\u6d4e\u768440%\u5de6\u53f3\uff0c\u518d\u8fc75\uff5e10\u5e74\uff0c\u8fd9\u4e2a\u6bd4\u4f8b\u5c06\u8fbe\u523080%\u5de6\u53f3\u3002\u8fd9\u610f\u5473\u7740\u6df7\u5408\u6240\u6709\u5236\u7ecf\u6d4e\nA. \u5c06\u6210\u4e3a\u6211\u56fd\u5404\u79cd\u6240\u6709\u5236\u7684\u4e3b\u8981\u5b9e\u73b0\u5f62\u5f0f\nB. \u662f\u4fc3\u8fdb\u6211\u56fd\u7ecf\u6d4e\u53d1\u5c55\u7684\u91cd\u8981\u5f62\u5f0f\nC. \u662f\u6211\u56fd\u793e\u4f1a\u4e3b\u4e49\u7ecf\u6d4e\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\nD. \u5c06\u6210\u4e3a\u6211\u56fd\u56fd\u6c11\u7ecf\u6d4e\u7684\u4e3b\u5bfc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45985432590515235, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6784\u9020\u771f\u503c\u8868\uff0c\u57fa\u4e8e\u7684\u903b\u8f91\u7b97\u5b50\u89c4\u5219\u4e0d\u5305\u62ec\nA. \u547d\u9898\nB. \u5408\u53d6\nC. \u5426\u5b9a\nD. \u7b49\u503c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.32205625344145955, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6743726736616787}}, {"question": "\u8bed\u8a00\u662f\u793e\u4f1a\u7ea6\u5b9a\u4fd7\u6210\u7684\u5e76\u4e14\u662f\u6bd4\u8f83\u9ad8\u7ea7\u548c\u590d\u6742\u7684\nA. \u4fe1\u606f\nB. \u7b26\u53f7\nC. \u5a92\u4ecb\u6240\u6709\u8005\nD. \u827a\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7289637854922927, "meta-math/MetaMath-Mistral-7B": 0.7513725741378183, "itpossible/Chinese-Mistral-7B-v0.1": 0.8317154965757286, "HuggingFaceH4/zephyr-7b-beta": 0.9798525292404326, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8150111606018778, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7567541638322016}}, {"question": "\u4e00\u6bb5\u7ef3\u5b50\uff0c\u7b2c\u4e00\u6b21\u7528\u53bb\u5168\u957f\u76842/5\uff0c\u7b2c\u4e8c\u6b21\u7528\u53bb\u5168\u957f\u76841/4\uff0c\u5269\u4e0b\u7684\u6bd4\u7b2c\u4e8c\u6b21\u7528\u53bb\u7684\u957f20\u7c73\uff0c\u8fd9\u6839\u7ef3\u5b50\u539f\u6765\u6709\nA. 150\u7c73\nB. 180\u7c73\nC. 200\u7c73\nD. 60\u7c73\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34478590299221606, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.34239623393788804, "HuggingFaceH4/zephyr-7b-beta": 0.5894805454576868, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3509604826644439, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4596151413217764}}, {"question": "\u4e0b\u957f\u5761\u65f6\uff0c\u63a7\u5236\u8f66\u901f\u9664\u4e86\u5239\u8f66\u5236\u52a8\u4ee5\u5916\u8fd8\u6709\u4ec0\u4e48\u6709\u6548\u7684\u8f85\u52a9\u65b9\u6cd5\nA. \u5229\u7528\u53d1\u52a8\u673a\u5236\u52a8\nB. \u6302\u5165\u7a7a\u6321\u6ed1\u884c\nC. \u8e0f\u4e0b\u79bb\u5408\u5668\u6ed1\u884c\nD. \u5173\u95ed\u53d1\u52a8\u673a\u7184\u706b\u6ed1\u884c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5337436524773918, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5162439802406337, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5647043806942662}}, {"question": "20\u4e16\u7eaa70\u5e74\u4ee3\u82cf\u7f8e\u4e89\u9738\u7684\u6001\u52bf\u662f\nA. \u82cf\u8054\u91c7\u53d6\u653b\u52bf\uff0c\u7f8e\u56fd\u5904\u4e8e\u5b88\u52bf\nB. \u7f8e\u82cf\u4e92\u6709\u653b\u5b88\nC. \u7f8e\u82cf\u5173\u7cfb\u5168\u9762\u7f13\u548c\nD. \u7f8e\u56fd\u91c7\u53d6\u653b\u52bf\uff0c\u82cf\u8054\u5904\u4e8e\u5b88\u52bf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3756727668405692, "meta-math/MetaMath-Mistral-7B": 0.4034949082514381, "itpossible/Chinese-Mistral-7B-v0.1": 0.4560775741310175, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5143\u592a\u7956\u94c1\u6728\u771f\u662f\u8499\u53e4\u8349\u539f\u4e0a\u7684\u82f1\u96c4\uff0c\u4ed6\u88ab\u4eba\u4eec\u5c0a\u79f0\u4e3a\u201c\u6210\u5409\u601d\u6c57\u201d\uff0c\u201c\u6c57\u201d\u7684\u610f\u601d\u662f\u5927\u738b\uff0c\u90a3\u4e48\u201c\u6210\u5409\u601d\u6c57\u201d\u7684\u610f\u601d\u662f\nA. \u5927\u6d77\nB. \u5929\u7a7a\nC. \u8349\u539f\nD. \u9ad8\u5c71\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u822c\u6765\u8bf4\uff0c\u6c11\u65cf\u5f62\u6210\u7684\u65f6\u95f4\u662f\nA. \u5974\u96b6\u5236\u793e\u4f1a\nB. \u5c01\u5efa\u5236\u793e\u4f1a\nC. \u8d44\u672c\u4e3b\u4e49\u793e\u4f1a\nD. \u539f\u59cb\u793e\u4f1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "()\u662f\u6307\u6ce8\u610f\u5728\u540c\u4e00\u5bf9\u8c61\u6216\u6d3b\u52a8\u4e0a\u6240\u4fdd\u6301\u65f6\u95f4\u7684\u957f\u77ed\u3002\u5b83\u662f\u6ce8\u610f\u7684\u65f6\u95f4\u7279\u5f81\u3002\nA. \u6ce8\u610f\u7684\u7a33\u5b9a\u6027\nB. \u6ce8\u610f\u7684\u5206\u914d\nC. \u6ce8\u610f\u7684\u5206\u5fc3\nD. \u6ce8\u610f\u7684\u8f6c\u79fb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6120843855007865, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9804276508183967, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9120952191599526}}, {"question": "\u8774\u8776\u9600\u5206\u89e3\u524d\u8981\u505a\u7684\u8bd5\u9a8c\u662f\nA. \u4f4e\u6cb9\u538b\u64cd\u4f5c\u8bd5\u9a8c\nB. \u9759\u6c34\u542f\u95ed\u548c\u65e0\u6c34\u542f\u95ed\u8bd5\u9a8c\nC. \u5907\u7528\u6cb9\u6e90\u8bd5\u9a8c\nD. \u65e0\u6c34\u542f\u95ed\u8bd5\u9a8c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3898115540279693, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4800666736249149, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7942494436875392}}, {"question": "\u6211\u56fd\u300a\u6c11\u6cd5\u901a\u5219\u300b\u7b2c143\u6761\u89c4\u5b9a\uff1a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u516c\u6c11\u5b9a\u5c45\u56fd\u5916\u7684\uff0c\u5176\u6c11\u4e8b\u884c\u4e3a\u80fd\u529b\u53ef\u4ee5\u9002\u7528\nA. \u6cd5\u9662\u5730\u6cd5\u5f8b\nB. \u4e2d\u56fd\u6cd5\u5f8b\nC. \u5b9a\u5c45\u56fd\u6cd5\u5f8b\nD. \u884c\u4e3a\u5730\u6cd5\u5f8b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3461131492142801, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5326190993940749, "meta-llama/Meta-Llama-3-8B": 0.6463379372683372, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7050725070044279}}, {"question": "\u611f\u89c9\u9600\u9650\u4e0e\u611f\u53d7\u6027\u5927\u5c0f\u7684\u5173\u7cfb\u662f\nA. \u5e73\u884c\u5173\u7cfb\nB. \u6b63\u6bd4\u5173\u7cfb\nC. \u97e6\u4f2f\u6bd4\u7387\nD. \u53cd\u6bd4\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6569118386312389, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.48618609864626156, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u6c11\u672c\u201d\u8bed\u51fa\u300a\u5c1a\u4e66\u00b7\u590f\u4e66\u00b7\u4e94\u5b50\u4e4b\u6b4c\u300b\uff1a\u201c\u6c11\u60df\u90a6\u672c\uff0c\u672c\u56fa\u90a6\u5b81\u3002\u201d\u610f\u8c13\u4eba\u6c11\u662f\u56fd\u5bb6\u7684\u6839\u672c\uff0c\u6839\u672c\u5de9\u56fa\uff0c\u56fd\u5bb6\u624d\u80fd\u5b89\u5b81\u3002\u201c\u6c11\u4e3b\u201d\u4e00\u8bcd\uff0c\u6700\u65e9\u89c1\u4e8e\u53e4\u5e0c\u814a\u5386\u53f2\u5b66\u5bb6\u5e0c\u7f57\u591a\u5fb7\u7684\u300a\u5386\u53f2\u300b\u4e00\u4e66\uff0c\u5b83\u7531\u201c\u4eba\u6c11\u201d\u548c\u201c\u6743\u5229\u201d\u4e24\u4e2a\u8bcd\u7ec4\u5408\u800c\u6210\uff0c\u5176\u5b57\u9762\u610f\u601d\u5c31\u662f\u201c\u4eba\u6c11\u7684\u6743\u5229\u201d\u3001\u201c\u4eba\u6c11\u4e3b\u6743\u201d\u548c\u201c\u591a\u6570\u4eba\u7684\u7edf\u6cbb\u201d\u3002\u4e0b\u5217\u5173\u4e8e\u201c\u6c11\u672c\u201d\u4e0e\u201c\u6c11\u4e3b\u201d\u7684\u8868\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u4e24\u8005\u90fd\u9f13\u52b1\u767e\u59d3\u4e3a\u5b9e\u73b0\u81ea\u5df1\u7684\u8ffd\u6c42\u800c\u594b\u6597\nB. \u4e24\u8005\u90fd\u5177\u6709\u9650\u5236\u548c\u7ea6\u675f\u7edf\u6cbb\u8005\u884c\u4e3a\u7684\u4f5c\u7528\nC. \u4e24\u8005\u7684\u533a\u522b\u5728\u4e8e\u201c\u6c11\u201d\u662f\u5426\u662f\u56fd\u5bb6\u7684\u57fa\u7840\nD. \u4e24\u8005\u90fd\u5229\u4e8e\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u767e\u59d3\u7684\u793e\u4f1a\u5730\u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.42876490286251856, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8584573896046209}}, {"question": "\u6211\u56fd\u5904\u7406\u5bf9\u5916\u5173\u7cfb\u7684\u57fa\u672c\u51c6\u5219\u662f\nA. \u548c\u5e73\u5171\u5904\u4e94\u9879\u539f\u5219\nB. \u5e7f\u6cdb\u7ed3\u76df\nC. \u4e89\u53d6\u5168\u7403\u9886\u5bfc\u6743\nD. \u81ea\u7ed9\u81ea\u8db3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9606354837185817, "meta-math/MetaMath-Mistral-7B": 0.9945036936557415, "itpossible/Chinese-Mistral-7B-v0.1": 0.8894221736061826, "HuggingFaceH4/zephyr-7b-beta": 0.9999553088174757, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9969841611309853, "meta-llama/Meta-Llama-3-8B": 0.9331087884348331, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9243864261249589}}, {"question": "\u4e0d\u540c\u7684\u51b3\u7b56\u89c4\u5219\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7684\u7ed3\u679c\uff0c\u80fd\u591f\u8fbe\u5230\u201c\u5e15\u7d2f\u6258\u6700\u4f18\u201d\u7684\u51b3\u7b56\u89c4\u5219\u662f\nA. \u7edd\u5bf9\u591a\u6570\u89c4\u5219\nB. \u5168\u4f53\u4e00\u81f4\u89c4\u5219\nC. \u76f8\u5bf9\u591a\u6570\u89c4\u5219\nD. \u7b80\u5355\u591a\u6570\u89c4\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.7734649414809546, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6153421576782531, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u5efa\u8bbe\u94f6\u884c\u53d1\u884c\u7684\u4fe1\u7528\u5361\u662f\nA. \u9f99\u5361\nB. \u7261\u4e39\u5361\nC. \u957f\u57ce\u5361\nD. \u592a\u5e73\u6d0b\u5361\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3713976413174735, "itpossible/Chinese-Mistral-7B-v0.1": 0.49300652014628704, "HuggingFaceH4/zephyr-7b-beta": 0.4842756328948756, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4014886105774974, "meta-llama/Meta-Llama-3-8B": 0.4861860837564744, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u957f\u6c5f\u4e2d\u4e0a\u6e38\u6587\u5316\u6d41\u57df\u5e7f\u5927\u5730\u533a\u996e\u98df\u98ce\u5473\u4f53\u7cfb\u7684\u4ee3\u8868\nA. \u6c5f\u82cf\u83dc\nB. \u6d59\u6c5f\u83dc\nC. \u5ddd\u83dc\nD. \u6e56\u5357\u83dc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39857072748279515, "meta-math/MetaMath-Mistral-7B": 0.5199028554703753, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5609717583820025, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6237889909047183, "meta-llama/Meta-Llama-3-8B": 0.832723158205106, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.728857048741701}}, {"question": "\u8d44\u672c\u793e\u4f1a\u5316\u7684\u6700\u9ad8\u5f62\u5f0f\u662f\nA. \u56fd\u5bb6\u5784\u65ad\u8d44\u672c\u4e3b\u4e49\nB. \u5784\u65ad\u8d44\u672c\u4e3b\u4e49\nC. \u751f\u4ea7\u793e\u4f1a\u5316\nD. \u7ecf\u8425\u7ba1\u7406\u793e\u4f1a\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6742\u5408\u4f53\u51cf\u6570\u5206\u88c2\u7ec8\u53d8\u671f\u4f1a\u51fa\u73b0\u56db\u4f53\u73af\u6216\u56db\u4f53\u94fe\u7684\u67d3\u8272\u4f53\u7ed3\u6784\u53d8\u5f02\u7c7b\u578b\u662f\nA. \u6613\u4f4d\nB. \u91cd\u590d\nC. \u5012\u4f4d\nD. \u7f3a\u5931\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3702582991635287, "meta-math/MetaMath-Mistral-7B": 0.45772068377448727, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3900852433273489, "meta-llama/Meta-Llama-3-8B": 0.3118033757242151, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8154887570401038}}, {"question": "\u7ecf\u6d4e\u5168\u7403\u5316\u4f7f\u4f01\u4e1a\u95f4\u7684\u7ade\u4e89\u5728\u77ed\u65f6\u95f4\u5185\u6269\u5c55\u4e3a\u4e16\u754c\u8303\u56f4\u7684\u4e89\u593a\u3002\u5382\u5bb6\u6839\u672c\u65e0\u6cd5\u9884\u6599\u5728\u4ec0\u4e48\u65f6\u5019\u3001\u4ec0\u4e48\u5730\u65b9\u4f1a\u51fa\u73b0\u4ec0\u4e48\u6837\u7684\u7ade\u4e89\u5bf9\u624b\u3002\u4ece\u4e16\u754c\u5e02\u573a\u65b0\u7279\u70b9\u770b\uff0c\u4e0a\u8ff0\u6750\u6599\u4e3b\u8981\u53cd\u6620\u4e86\nA. \u533a\u57df\u96c6\u56e2\u5316\u8d8b\u52bf\u52a0\u5f3a\nB. \u5e02\u573a\u7ade\u4e89\u6fc0\u70c8\u5316\nC. \u56fd\u9645\u5206\u5de5\uff0c\u751f\u4ea7\u4e13\u4e1a\u5316\nD. \u8de8\u56fd\u516c\u53f8\u7684\u8fc5\u901f\u53d1\u5c55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.754634782041377, "meta-math/MetaMath-Mistral-7B": 0.9452005078009336, "itpossible/Chinese-Mistral-7B-v0.1": 0.557748400253107, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6956145892841293, "meta-llama/Meta-Llama-3-8B": 0.6612446849830954, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7699695217455933}}, {"question": "\u7626\u8089\u578b\u751f\u957f\u80a5\u80b2\u732a\u5b9c\u91c7\u7528\u7684\u80a5\u80b2\u65b9\u6cd5\u662f\nA. \u9636\u6bb5\u80a5\u80b2\u6cd5\nB. \u524d\u655e\u540e\u9650\u80a5\u80b2\u6cd5\nC. \u524d\u540e\u655e\u5f00\u80a5\u80b2\u6cd5\nD. \u540a\u67b6\u5b50\u80a5\u80b2\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u516c\u5171\u573a\u6240\u4eba\u4eba\u90fd\u53ef\u80fd\u4f1a\u9047\u5230\u4e00\u4e9b\u7a81\u53d1\u6027\u707e\u7978\uff0c\u5982\u8f66\u7978\u3001\u6eba\u6c34\u3001\u610f\u5916\u75c5\u75db\u7b49\uff0c\u8fd9\u5c31\u9700\u8981\u4eba\u4eec\u89c1\u4e49\u52c7\u4e3a\uff0c\u4e34\u5371\u4e0d\u60e7\uff0c\u79ef\u6781\u4e3a\u4ed6\u4eba\u6392\u5fe7\u89e3\u96be\u3002\u8fd9\u4f53\u73b0\u7684\u662f\u793e\u4f1a\u516c\u5fb7\u4e2d\nA. \u52a9\u4eba\u4e3a\u4e50\u7684\u8981\u6c42\nB. \u7231\u62a4\u516c\u7269\u7684\u8981\u6c42\nC. \u4fdd\u62a4\u73af\u5883\u7684\u8981\u6c42\nD. \u6587\u660e\u793c\u8c8c\u7684\u8981\u6c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9609798721580355, "meta-math/MetaMath-Mistral-7B": 0.9982675173603683, "itpossible/Chinese-Mistral-7B-v0.1": 0.9269108360064983, "HuggingFaceH4/zephyr-7b-beta": 0.9997092967346822, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9909162643690999, "meta-llama/Meta-Llama-3-8B": 0.8484333641184348, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9568322852899633}}, {"question": "\u8fd0\u52a8\u5458\u5728\u505a\u53f2\u5bc6\u65af\u63a8\u80a9\u65f6\u51fa\u73b0\u80a9\u5173\u8282\u5916\u4fa7\u75bc\u75db\uff0c\u6211\u4eec\u5e94\u8be5\u677e\u89e3\u4ee5\u4e0b\u54ea\u5757\u808c\u8089\u5e2e\u52a9\u5176\u6539\u5584\u75bc\u75db\u5b8c\u6210\u8bad\u7ec3\uff1f\nA. \u5188\u4e0b\u808c\nB. \u5188\u4e0a\u808c\nC. \u5c0f\u5706\u808c\nD. \u8179\u76f4\u808c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.38422187401517677, "meta-math/MetaMath-Mistral-7B": 0.5473230844577356, "itpossible/Chinese-Mistral-7B-v0.1": 0.47506905614182626, "HuggingFaceH4/zephyr-7b-beta": 0.8004568783057813, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5818756488811256, "meta-llama/Meta-Llama-3-8B": 0.3418241248222891, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3786604407238381}}, {"question": "APT\u653b\u51fb\u662f\u4e00\u79cd\u4ee5\u5546\u4e1a\u6216\u8005\u653f\u6cbb\u76ee\u7684\u4e3a\u524d\u63d0\u7684\u7279\u5b9a\u653b\u51fb\uff0c\u5176\u4e2d\u653b\u51fb\u8005\u91c7\u7528\u53e3\u4ee4\u7a83\u542c\u3001\u6f0f\u6d1e\u653b\u51fb\u7b49\u65b9\u5f0f\u5c1d\u8bd5\u8fdb\u4e00\u6b65\u5165\u4fb5\u7ec4\u7ec7\u5185\u90e8\u7684\u4e2a\u4eba\u7535\u8111\u548c\u670d\u52a1\u5668\uff0c\u4e0d\u65ad\u63d0\u5347\u81ea\u5df1\u7684\u6743\u9650\uff0c\u76f4\u81f3\u83b7\u5f97\u6838\u5fc3\u7535\u8111\u548c\u670d\u52a1\u5668\u63a7\u5236\u6743\u7684\u8fc7\u7a0b\u88ab\u79f0\u4e3a\nA. \u6a2a\u5411\u6e17\u900f\nB. \u9632\u7ebf\u7a81\u7834\nC. \u901a\u9053\u5efa\u7acb\nD. \u60c5\u62a5\u6536\u96c6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4403424185345663, "meta-math/MetaMath-Mistral-7B": 0.5321685175679838, "itpossible/Chinese-Mistral-7B-v0.1": 0.3164901664353555, "HuggingFaceH4/zephyr-7b-beta": 0.999854705710174, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7302618482068987, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9661286283150506}}, {"question": "\u5173\u4e8e\u6211\u56fd\u5211\u6cd5\u6eaf\u53ca\u529b\u7684\u9002\u7528\uff0c\u4e0b\u5217\u8868\u8ff0\u4e2d\u6b63\u786e\u7684\u662f\uff08 \uff09\u3002\nA. \u53f8\u6cd5\u89e3\u91ca\u5e94\u9002\u7528\u4ece\u65b0\u517c\u4ece\u8f7b\u539f\u5219\nB. \u5904\u5211\u8f83\u8f7b\u662f\u6307\u6cd5\u9662\u5224\u5904\u7684\u5ba3\u544a\u5211\u8f83\u8f7b\nC. \u5e94\u4ee5\u201c\u5ba1\u5224\u65f6\u201d\u4f5c\u4e3a\u65b0\u65e7\u6cd5\u9009\u62e9\u9002\u7528\u7684\u5224\u65ad\u57fa\u7840\nD. \u6309\u7167\u5ba1\u5224\u76d1\u7763\u7a0b\u5e8f\u91cd\u65b0\u5ba1\u5224\u7684\u6848\u4ef6\u9002\u7528\u884c\u4e3a\u65f6\u7684\u6cd5\u5f8b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9620769802879345, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.41166574743774986, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f4e\u8840\u94be\u7684\u60a3\u8005\u7ed9\u4e88\u8865\u94be\u6cbb\u7597\u540e\u4ecd\u7136\u4f4e\u8840\u94be\uff0c\u6b64\u65f6\u5e94\u8be5\u8003\u8651\u5408\u5e76\nA. \u4f4e\u78f7\u8840\u75c7\nB. \u4f4e\u9541\u8840\u75c7\nC. \u4f4e\u94a0\u8840\u75c7\nD. \u4f4e\u9499\u8840\u75c7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.37859167732908794, "itpossible/Chinese-Mistral-7B-v0.1": 0.4227611188552607, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3236616425529228, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6587\u5b57\u7f16\u8f91\u65f6\u201cEnter\u201d\u7684\u529f\u80fd\u662f\nA. \u5220\u9664\u5b57\u7b26\nB. \u6362\u884c \nC. \u5927\u5c0f\u5199\u63a7\u5236\nD. \u7ffb\u9875\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9871131324236785, "meta-math/MetaMath-Mistral-7B": 0.9995928362618928, "itpossible/Chinese-Mistral-7B-v0.1": 0.9560617717193067, "HuggingFaceH4/zephyr-7b-beta": 0.9999492249181621, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9989356296266073, "meta-llama/Meta-Llama-3-8B": 0.9863415223202276, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9998787008723369}}, {"question": "\u6b63\u5f0f\u89e3\u91ca\u5206\u4e3a\u7acb\u6cd5\u89e3\u91ca\u3001\u53f8\u6cd5\u89e3\u91ca\u3001\u884c\u653f\u89e3\u91ca\uff0c\u5176\u5212\u5206\u6807\u51c6\u662f\nA. \u89e3\u91ca\u7684\u5c3a\u5ea6\u4e0d\u540c\nB. \u89e3\u91ca\u7684\u4e3b\u4f53\u4e0d\u540c\nC. \u89e3\u91ca\u7684\u6548\u529b\u4e0d\u540c\nD. \u89e3\u91ca\u7684\u4f53\u5236\u4e0d\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43521553698256354, "meta-math/MetaMath-Mistral-7B": 0.6554679754317998, "itpossible/Chinese-Mistral-7B-v0.1": 0.3710689062049661, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4124578775795746, "meta-llama/Meta-Llama-3-8B": 0.6109588524261916, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9181422749756479}}, {"question": "\u5356\u65b9\u8d1f\u8d23\u63d0\u4f9b\u8d27\u7269\u3001\u79df\u8239\u8fd0\u8f93\u3001\u8d27\u7269\u4fdd\u9669\uff0c\u4e70\u65b9\u627f\u62c5\u4ed8\u6b3e\u3001\u5728\u8d27\u7269\u5230\u8fbe\u76ee\u7684\u6e2f\u6536\u53d6\u8d27\u7269\uff0c\u8fd9\u4e00\u4ef7\u683c\u6761\u4ef6\u662f\nA. CFR\nB. FCA\nC. CIF\nD. FOB\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.40979327494059936, "HuggingFaceH4/zephyr-7b-beta": 0.9831488704407839, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8fdb\u5165\u516c\u5171\u573a\u6240\uff0c\u4e00\u5b9a\u8981\u6ce8\u610f\u89c2\u5bdf___\nA. \u5b89\u5168\u6307\u793a\u6807\u5fd7\nB. \u758f\u6563\u65b9\u5411\nC. \u758f\u6563\u901a\u9053\u4f4d\u7f6e\nD. \u5176\u4ed6\u9009\u9879\u5747\u53ef\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.43023427730341035, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u76ee\u524d\u5168\u56fd\u6c34\u4f4d\u7edf\u4e00\u91c7\u7528\u7684\u57fa\u51c6\u9762\u662f()\nA. \u5927\u6cbd\u57fa\u9762\nB. \u73e0\u6c5f\u57fa\u9762\nC. \u5434\u6dde\u57fa\u9762\nD. \u9ec4\u6d77\u57fa\u9762\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53\u524d\u94a2\u94c1\u3001\u6c34\u6ce5\u3001\u5e73\u677f\u73bb\u7483\u7b49\u884c\u4e1a\u8fd0\u884c\u51fa\u73b0\u56f0\u96be\uff0c\u5176\u4ea7\u80fd\u5229\u7528\u7387\u5206\u522b\u4e3a75.8%\u300175.3%\u548c96.8%\uff0c\u5206\u522b\u8f83\u5168\u7403\u5e73\u5747\u6c34\u5e73\u4f4e2.4\u30010.4\u548c1.5\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u8fd8\u6709\u5e9e\u5927\u7684\u5728\u5efa\u548c\u62df\u5efa\u4ea7\u80fd\u3002\u4e3a\u6291\u5236\u8fd9\u4e9b\u884c\u4e1a\u4ea7\u80fd\u8fc7\u5269\uff0c\u56fd\u5bb6\u5c06\u4e25\u683c\u5e02\u573a\u51c6\u5165\uff1b\u5f3a\u5316\u73af\u5883\u76d1\u7ba1\uff0c\u4e0d\u8fbe\u6807\u7684\u4f01\u4e1a\u4e00\u5f8b\u4e0d\u5f97\u5f00\u5de5\uff1b\u8fd9\u4e9b\u4ea7\u4e1a\u9879\u76ee\u7684\u6838\u51c6\u6743\u7531\u5730\u65b9\u6536\u5f52\u4e2d\u592e\uff0c\u5168\u90e8\u7531\u53d1\u6539\u59d4\u5ba1\u6279\u3002\u8fd9\u4e3b\u8981\u8868\u660e\nA. \u52a0\u5f3a\u5b8f\u89c2\u8c03\u63a7\u662f\u793e\u4f1a\u4e3b\u4e49\u5e02\u573a\u7ecf\u6d4e\u7684\u6839\u672c\u76ee\u6807\nB. \u575a\u6301\u516c\u6709\u5236\u7684\u4e3b\u4f53\u5730\u4f4d\uff0c\u662f\u793e\u4f1a\u4e3b\u4e49\u5e02\u573a\u7ecf\u6d4e\u7684\u57fa\u672c\u6807\u5fd7\nC. \u52a0\u5f3a\u5b8f\u89c2\u8c03\u63a7\u5c31\u80fd\u5f25\u8865\u5e02\u573a\u8c03\u8282\u7684\u7f3a\u9677\u548c\u4e0d\u8db3\nD. \u793e\u4f1a\u4e3b\u4e49\u56fd\u5bb6\u80fd\u591f\u5b9e\u884c\u5f3a\u6709\u529b\u7684\u5b8f\u89c2\u8c03\u63a7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9562913851487964, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6720329211538737, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2015\u5e74\u5730\u7403\u2f47\u6d3b\u52a8\u4e3b\u9898\u4e3a\uff1a\u201c\u73cd\u60dc\u5730\u7403\u8d44\u6e90\u8f6c\u53d8\u53d1\u5c55\u2f45\u5f0f\ufe63\ufe63\u63d0\u2fbc\u8d44\u6e90\u5229\u2f64\u6548\u76ca\u201d\uff0c\u4e0b\u5217\u505a\u6cd5\u4e0e\u8be5\u4e3b\u9898\u601d\u60f3\u4e0d\u7b26\u5408\u7684\u662f\nA. \u2f24\u2f12\u63a8\u2f34\u2f00\u6b21\u6027\u9910\u5177\u2f64\u54c1\nB. \u9010\u6b65\u5173\u505c\u2fbc\u8017\u80fd\u91cd\u6c61\u67d3\u7684\u2f2f\u2f1a\nC. \u5783\u573e\u5206\u7c7b\uff0c\u8d44\u6e90\u518d\u5229\u2f64\nD. \u2f24\u2f12\u53d1\u5c55\u8282\u2f54\u519c\u4e1a\u548c\u8282\u2f54\u2f2f\u4e1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.45816797923171626, "meta-math/MetaMath-Mistral-7B": 0.8691224893444406, "itpossible/Chinese-Mistral-7B-v0.1": 0.3964696390054703, "HuggingFaceH4/zephyr-7b-beta": 0.9405807146768345, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6880620286011201, "meta-llama/Meta-Llama-3-8B": 0.579334751126242, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7855345926069979}}, {"question": "\u5982\u679c\u5728\u534e\u4f57\u53bb\u4e16\u540e\u4e0d\u53ca\uff0c\u66f9\u64cd\u8fd8\u60f3\u5bfb\u627e\u4e00\u4f4d\u540d\u533b\u4e3a\u81ea\u5df1\u6cbb\u75c5\uff0c\u4ed6\u53ef\u4ee5\u627e\u4e0b\u5217\u7684\u54ea\u4e00\u4f4d\nA. \u6241\u9e4a\nB. \u5f20\u4ef2\u666f\nC. \u5b59\u601d\u9088\nD. \u674e\u65f6\u73cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33077570779618426, "meta-math/MetaMath-Mistral-7B": 0.697201491663079, "itpossible/Chinese-Mistral-7B-v0.1": 0.490778006785068, "HuggingFaceH4/zephyr-7b-beta": 0.797603627549133, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4855401515625167, "meta-llama/Meta-Llama-3-8B": 0.6487062127360178, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9587982011796551}}, {"question": "\u6bd4\u8f83\u4e24\u4e2a\u5730\u533a\u7684\u6d41\u611f\u6d41\u884c\u7a0b\u5ea6\u65f6\u7528\nA. \u7f79\u60a3\u7387\nB. \u53d1\u75c5\u7387\nC. \u6807\u5316\u53d1\u75c5\u7387\nD. \u60a3\u75c5\u7387\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5490183239487869, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u64cd\u573a\u6216\u5ba4\u5916\u5e94\u8be5\u600e\u6837\u907f\u9707?\u8bf7\u95ee\uff1a\u4ee5\u4e0b\u65b9\u5f0f\u54ea\u79cd\u662f\u4e0d\u6b63\u786e\u7684\nA. \u4e0d\u8981\u4e71\u8dd1\u3001\u4e71\u6324\uff0c\u5f85\u5730\u9707\u8fc7\u53bb\u540e\uff0c\u518d\u6309\u8001\u5e08\u6307\u6325\u884c\u52a8\nB. \u6ce8\u610f\u907f\u5f00\u9ad8\u5927\u5efa\u7b51\u7269\u6216\u5371\u9669\u7269\nC. \u53d1\u751f\u5730\u9707\u65f6\u8d76\u7d27\u56de\u5230\u6559\u5ba4\u53bb\nD. \u82e5\u5728\u5f00\u9614\u5730\u65b9\uff0c\u53ef\u539f\u5730\u4e0d\u52a8\uff0c\u8e72\u4e0b\uff0c\u6ce8\u610f\u4fdd\u62a4\u5934\u90e8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8936327910566743, "meta-math/MetaMath-Mistral-7B": 0.9821295190535823, "itpossible/Chinese-Mistral-7B-v0.1": 0.8344029655729174, "HuggingFaceH4/zephyr-7b-beta": 0.9751994211731382, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9508804951199179, "meta-llama/Meta-Llama-3-8B": 0.8890055408703338, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9412363239320182}}, {"question": "\u5728\u793e\u533a\u7814\u7a76\u4e2d\uff0c\u5168\u8c8c\u7814\u7a76\u5c31\u662f\nA. \u63cf\u8ff0\u793e\u533a\u7684\u5404\u4e2a\u4e0d\u540c\u90e8\u5206\u5e76\u89e3\u91ca\u8fd9\u4e9b\u4e0d\u540c\u90e8\u5206\u7684\u76f8\u4e92\u5173\u7cfb\u3002\nB. \u628a\u793e\u533a\u89c6\u4e3a\u4eba\u7c7b\u805a\u5c45\u751f\u6d3b\u7684\u7279\u6b8a\u7684\u7a7a\u95f4\u73b0\u8c61\u3002\nC. \u5206\u6790\u201c\u793e\u4f1a\u201d\u548c\u201c\u793e\u533a\u201d\u4e24\u79cd\u5bf9\u7acb\u7684\u793e\u4f1a\u8054\u7cfb\u5f62\u5f0f\u7684\u7c7b\u578b\u3002\nD. \u7efc\u5408\u6027\u7684\u8bb0\u5f55\u8c03\u67e5\uff0c\u8f83\u5c11\u7406\u8bba\u5206\u6790\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6499179591208726, "meta-math/MetaMath-Mistral-7B": 0.873347643314207, "itpossible/Chinese-Mistral-7B-v0.1": 0.4650131013460436, "HuggingFaceH4/zephyr-7b-beta": 0.8607519680741942, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.592301048274313, "meta-llama/Meta-Llama-3-8B": 0.6694333334952759, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9328565215932556}}, {"question": "\u4ece\u6295\u2f0a\u2014\u4ea7\u51fa\u7684\u2f2f\u4e1a\u8054\u7cfb\u770b\uff0c\u5e94\u96c6\u805a\u7684\u2f2f\u2f1a\u662f\nA. \u6c7d\u2ecb\u2f1a\u2014\u2014\u5316\u7ea4\u2f1a\nB. \u2f83\u2f8f\u2ecb\u2f1a\u2014\u2014\u7535\u89c6\u673a\u2f1a\nC. \u5370\u67d3\u2f1a\u2014\u2014\u7ec7\u5e03\u2f1a\nD. \u94a2\u94c1\u2f1a\u2014\u2014\u5370\u67d3\u2f1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.28966338381871215, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4332739826147697}}, {"question": "\u4eba\u4eec\u5bf9\u6cd5\u5f8b\u7684\u529f\u80fd\u3001\u4f5c\u7528\u548c\u5b9e\u65bd\u6240\u6301\u6709\u7684\u5185\u5fc3\u4fe1\u5ff5\u548c\u89c2\u5ff5\uff0c\u6307\u5bfc\u4e00\u56fd\u6cd5\u5f8b\u5236\u5ea6\u8bbe\u8ba1\u548c\u53f8\u6cd5\u3001\u6267\u6cd5\u3001\u5b88\u6cd5\u5b9e\u8df5\u7684\u601d\u60f3\u57fa\u7840\u548c\u4e3b\u5bfc\u4ef7\u503c\uff0c\u79f0\u4e3a\nA. \u6cd5\u5f8b\u601d\u7ef4\nB. \u6cd5\u6cbb\u7406\u5ff5\nC. \u6cd5\u5f8b\u610f\u8bc6\nD. \u6cd5\u5f8b\u6587\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.739231827770907, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9966687922413017}}, {"question": "\u5728\u4e0b\u5217\u5404\u9879\u4e2d\uff0c\u7531\u653f\u6cbb\u5229\u76ca\u52a8\u673a\u5bfc\u81f4\u4f1a\u8ba1\u804c\u4e1a\u9053\u5fb7\u7f3a\u5931\u7684\u662f\nA. \u65b0\u4e0a\u4efb\u7684\u7ba1\u7406\u4eba\u5458\u7c89\u9970\u8d22\u52a1\u62a5\u8868\nB. \u672c\u5730\u4f01\u4e1a\u4eab\u6709\u7684\u7a0e\u6536\u4f18\u60e0\u591a\u4e8e\u5916\u5730\u4f01\u4e1a\nC. \u4f01\u4e1a\u5077\u7a0e\nD. \u5728\u7533\u8bf7\u94f6\u884c\u8d37\u6b3e\u65f6\u865a\u62a5\u4f01\u4e1a\u5229\u6da6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6048941167959009}}, {"question": "\u4e16\u754c\u4e0a\u5f00\u529e\u6700\u65e9\u7684\u56fd\u9645\u65b0\u95fb\u7535\u89c6\u673a\u6784\u662f=\nA. \u7f8e\u56fd\u7684\u4e16\u754c\u7535\u89c6\u7f51\nB. \u7f8e\u56fd\u7684\u6709\u7ebf\u7535\u89c6\u65b0\u95fb\u7f51\nC. \u82f1\u56fd\u7684\u4e16\u754c\u7535\u89c6\u53f0\nD. \u9999\u6e2f\u7684\u536b\u661f\u7535\u89c6\u53f0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u5b75\u5316\u5ba4\u6e29\u5ea6\u4e3a24\uff5e26\u00b0C\u7684\u6761\u4ef6\u4e0b\uff0c\u7acb\u4f53\u5b75\u5316\u5668\u5b75\u5316\u9e21\u86cb\u7684\u6700\u9002\u6e29\u5ea6\u4e3a\nA. 36\u00b0C\nB. 37.8\u00b0C\nC. 36.5\u00b0C\nD. 37\u00b0C\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3545899205706617, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3860362730051736, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6807674197496925}}, {"question": "\u5728\u5f53\u4ee3\uff0c\u6559\u80b2\u88ab\u4eba\u4eec\u89c6\u4e3a\u4e00\u79cd\u6295\u8d44\uff0c\u4e00\u79cd\u4eba\u529b\u8d44\u672c\uff0c\u8fd9\u662f\u56e0\u4e3a\u6559\u80b2\u5177\u6709\nA. \u6587\u5316\u529f\u80fd\nB. \u7ecf\u6d4e\u529f\u80fd\nC. \u4eba\u53e3\u529f\u80fd\nD. \u653f\u6cbb\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8808602376753452, "meta-math/MetaMath-Mistral-7B": 0.9898554575638728, "itpossible/Chinese-Mistral-7B-v0.1": 0.9626167077599749, "HuggingFaceH4/zephyr-7b-beta": 0.9999518703235938, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9755395099442901, "meta-llama/Meta-Llama-3-8B": 0.8430603565233287, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9804276457071749}}, {"question": "\u5f3a\u5316\u98df\u54c1\u662f\u5728\u98df\u54c1\u4e2d\u6dfb\u52a0\nA. \u5316\u5b66\u6210\u5206\nB. \u529f\u80fd\u6210\u5206\nC. \u5929\u7136\u7269\u8d28\nD. \u5f3a\u5316\u5242\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8166923996285345, "meta-math/MetaMath-Mistral-7B": 0.8477928612970934, "itpossible/Chinese-Mistral-7B-v0.1": 0.49431842833152895, "HuggingFaceH4/zephyr-7b-beta": 0.9997244667887424, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8308786233945217, "meta-llama/Meta-Llama-3-8B": 0.9528292615228849, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8988140196198914}}, {"question": "\u5e0c\u814a\u795e\u8bdd\u4e2d\u7684\u8be5\u4e9a\u662f\nA. \u5929\u540e\nB. \u6d77\u795e\nC. \u5929\u795e\nD. \u5927\u5730\u4e4b\u6bcd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2957129572682819, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3789723868131119, "HuggingFaceH4/zephyr-7b-beta": 0.9750530977574409, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4861793253544494, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9543991856449049}}, {"question": "\u6700\u7f8e\u5988\u5988\u201d\u5434\u83ca\u840d\u3001\u201c\u6700\u7f8e\u8001\u5e08\u201d\u5f20\u4e3d\u8389\u3001\u201c\u6700\u7f8e\u53f8\u673a\u201d\u5434\u658c\u2026\u2026\u751f\u6d3b\u4e2d\uff0c\u8fd9\u4e9b\u5e73\u51e1\u800c\u53c8\u6734\u5b9e\u7684\u6545\u4e8b\uff0c\u8ba9\u6211\u4eec\u7684\u5fc3\u7075\u4e00\u6b21\u53c8\u4e00\u6b21\u5f97\u5230\u9707\u64bc\u548c\u6d17\u6da4\u3002\u5728\u90a3\u4e9b\u201c\u6700\u7f8e\u201d\u7684\u77ac\u95f4\uff0c\u4ed6\u4eec\u9996\u5148\u60f3\u5230\u7684\u662f\u522b\u4eba\u3002\u8fd9\u4f53\u73b0\u4e86\nA. \u4ef7\u503c\u5224\u65ad\u548c\u4ef7\u503c\u9009\u62e9\u7684\u793e\u4f1a\u5386\u53f2\u6027\nB. \u4ef7\u503c\u89c2\u4e0d\u540c\uff0c\u4ef7\u503c\u5224\u65ad\u4e0e\u4ef7\u503c\u9009\u62e9\u4e5f\u4e0d\u540c\nC. \u4eba\u751f\u4ef7\u503c\u7684\u5b9e\u73b0\u9700\u8981\u987d\u5f3a\u62fc\u640f\u3001\u81ea\u5f3a\u4e0d\u606f\u7684\u7cbe\u795e\nD. \u793e\u4f1a\u5ba2\u89c2\u6761\u4ef6\u662f\u5b9e\u73b0\u4eba\u751f\u4ef7\u503c\u7684\u57fa\u7840\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5900199510866502, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5206\u5e03\u4e8e\u9762\u988a\u90e8\u7684\u7ecf\u8109\u662f\nA. \u8db3\u5c11\u9633\u80c6\u7ecf\nB. \u624b\u592a\u9633\u5c0f\u80a0\u7ecf\nC. \u8db3\u9633\u660e\u80c3\u7ecf\nD. \u624b\u5c11\u9633\u4e09\u7126\u7ecf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2896636654578384, "meta-math/MetaMath-Mistral-7B": 0.3645134912202329, "itpossible/Chinese-Mistral-7B-v0.1": 0.2821833983601388, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.31911523504877376, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e8c\u91cd\u5377\u8fb9\u7ed3\u6784\u5171\u6709\nA. 2\u5c42\nB. 3\u5c42\nC. 5\u5c42\nD. 7\u5c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3326991911298114, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3386364065941128, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3287145937103622, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c0f\u8bf4\u300a\u68cb\u738b\u300b\u7684\u4e3b\u4eba\u516c\u662f\nA. \u738b\u533b\u751f\nB. \u738b\u4e00\u751f\nC. \u963f\u6210\nD. \u963f\u57ce\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37598890286540065, "meta-math/MetaMath-Mistral-7B": 0.8268086929349432, "itpossible/Chinese-Mistral-7B-v0.1": 0.4201996374046127, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6909093609208341, "meta-llama/Meta-Llama-3-8B": 0.3881207483196859, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6137571638774524}}, {"question": "\u5927\u5b66\u751f\u6811\u7acb\u79d1\u5b66\u604b\u7231\u89c2\u4e0d\u5305\u62ec\nA. \u6446\u6b63\u7231\u60c5\u7684\u4f4d\u7f6e\nB. \u63d0\u5021\u5fd7\u540c\u9053\u5408\u7684\u7231\u60c5\nC. \u61c2\u5f97\u7231\u662f\u4e00\u79cd\u8d23\u4efb\u548c\u5949\u732e\nD. \u5b66\u4e60\u7231\u7684\u6280\u5de7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4777635491413945, "itpossible/Chinese-Mistral-7B-v0.1": 0.57595191084186, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9440829346438847, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5a\u662f\u4e00\u4e2a\u771f\u5206\u6570\uff0cb\u662f\u4e00\u4e2a\u5047\u5206\u6570\uff0c\u5728\u4e0b\u5217\u7b97\u5f0f\u4e2d\u7b54\u6848\u4e00\u5b9a\u5927\u4e8e1\u7684\u7b97\u5f0f\u662f\nA. a+b\nB. a*b\nC. a/b\nD. a-b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8f6f\u4ef6\u4e2d\u4e0d\u5c5e\u4e8e\u7cfb\u7edf\u8f6f\u4ef6\u7684\u662f\nA. Word\nB. DOS\nC. Linux\nD. Windows\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7505551430205994, "meta-math/MetaMath-Mistral-7B": 0.9774346778543453, "itpossible/Chinese-Mistral-7B-v0.1": 0.6879345085709951, "HuggingFaceH4/zephyr-7b-beta": 0.9997202883460419, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8929822470829366, "meta-llama/Meta-Llama-3-8B": 0.9581038419655721, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9633375603403912}}, {"question": "\u7537\u6027\uff0c55\u5c81\uff0c\u4e59\u809d\u75c5\u53f215\u5e74\uff0cCT\u63d0\u793a\u53f3\u809d\u76f4\u5f848cm\u80bf\u7269\uff0c\u9760\u8fd1\u7b2c\u4e00\u809d\u95e8\uff0cChild\u5206\u7ea7C\u7ea7\uff0cAFP890ng/ml\uff0c\u8003\u8651\u809d\u764c\u3002\u6700\u4f73\u6cbb\u7597\u65b9\u6848\nA. \u5c40\u90e8\u653e\u5c04\u6cbb\u7597\nB. \u5168\u8eab\u5316\u7597\nC. \u9009\u62e9\u6027\u809d\u52a8\u8109\u6813\u585e\nD. \u5c40\u90e8\u65e0\u6c34\u9152\u7cbe\u6ce8\u5c04\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5596514517318217, "meta-math/MetaMath-Mistral-7B": 0.884108296122139, "itpossible/Chinese-Mistral-7B-v0.1": 0.46185612141658194, "HuggingFaceH4/zephyr-7b-beta": 0.972931489883636, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7141737627403321, "meta-llama/Meta-Llama-3-8B": 0.47340444460231085, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.784878915787729}}, {"question": "1995\u5e74\u4e4b\u540e\u4fe1\u606f\u7f51\u7edc\u5b89\u5168\u95ee\u9898\u5c31\u662f\nA. \u8bbf\u95ee\u63a7\u5236\nB. \u56de\u907f\u98ce\u9669\nC. \u98ce\u9669\u7ba1\u7406\nD. \u6d88\u9664\u98ce\u9669\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5556499834702745, "meta-math/MetaMath-Mistral-7B": 0.8015779920910419, "itpossible/Chinese-Mistral-7B-v0.1": 0.7689164638805439, "HuggingFaceH4/zephyr-7b-beta": 0.9733347655053345, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.557260454355704, "meta-llama/Meta-Llama-3-8B": 0.5970437434582615, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eac\u6caa\u2fbc\u94c1\u662f\u2f00\u6761\u8fde\u63a5\u5317\u4eac\u5e02\u4e0e\u4e0a\u6d77\u5e02\u7684\u2fbc\u901f\u94c1\u8def\uff0c\u5168\u7a0b\u6865\u6881\u2f5080%\uff0c\u8bbe\u8ba1\u65f6\u901f350\u5343\u2f76\uff0c\u4e8e2011\u5e746\u2f4930\u2f47\u5168\u7ebf\u6b63\u5f0f\u901a\u2ecb\uff0c\u8fd0\u8425\u65f6\u901f300\uff5e350\u5343\u2f76\u3002\u4eac\u6caa\u2fbc\u94c1\nA. \u6cbf\u7ebf\u4f4e\u2f2d\u4e18\u9675\u2f34\u5e03\uff0c\u67b6\u6865\u53ef\u4ee5\u4fdd\u8bc1\u8fd0\u8425\u901f\u5ea6\nB. \u8d77\u2f4c\u70b9\u57ce\u5e02\u8fd0\u8f93\u2f45\u5f0f\u591a\u6837\uff0c\u5229\u4e8e\u8854\u63a5\u548c\u8f6c\u8fd0\nC. \u8fde\u63a5\u9014\u5f84\u5404\u7701\u4f1a\uff0c\u4f18\u5148\u6ee1\u2f9c\u9700\u6c42\u2f24\u7684\u4ea4\u901a\u70b9\nD. \u8fd0\u2f8f\u65f6\u901f\u2f29\u4e8e\u8bbe\u8ba1\u65f6\u901f\u4f53\u73b0\u4e86\u56e0\u5730\u5236\u5b9c\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.32643394968626843, "HuggingFaceH4/zephyr-7b-beta": 0.5730151871429651, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3171201089282235, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "$x^2$\u503c\u7684\u53d6\u503c\u8303\u56f4\u4e3a\nA. $x^2\\geqslant 1$\nB. $-\\infty<x^2<+\\infty$\nC. $0 \\leqslant x^2 \\leqslant+\\infty$\nD. $x^2 \\leqslant 1$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6940943662065282, "meta-math/MetaMath-Mistral-7B": 0.6231069179505528, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.987274048041224, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5719662639361683, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e73\u7ba1\u5185\u4e73\u5934\u72b6\u7624\u6700\u5e38\u51fa\u73b0\u7684\u4e73\u5934\u6ea2\u6db2\u662f\nA. \u8840\u6027\u6ea2\u6db2\nB. \u8113\u6027\u6ea2\u6db2\nC. \u6d46\u6db2\u6027\u6ea2\u6db2\nD. \u9ec4\u7eff\u8272\u6ea2\u6db2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ece\u4e00\u6279\u7fbd\u6bdb\u7403\u4ea7\u54c1\u4e2d\u4efb\u53d6\u4e00\u4e2a\uff0c\u5176\u8d28\u91cf\u5c0f\u4e8e4.8g\u7684\u6982\u7387\u4e3a0.3\uff0c\u8d28\u91cf\u5c0f\u4e8e4.85g\u7684\u6982\u7387\u4e3a0.32\uff0c\u90a3\u4e48\u8d28\u91cf\u5728[4.8,4.85)g\u8303\u56f4\u5185\u7684\u6982\u7387\u662f\nA. 0.68\nB. 0.38\nC. 0.02\nD. 0.62\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u201cDNA\u7684\u7c97\u63d0\u53d6\u4e0e\u9274\u5b9a\u201d\u5b9e\u9a8c\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. 2 mol/L\u548c0.14 mol/L\u7684NaCl\u6eb6\u6db2\u5747\u53ef\u6eb6\u89e3DNA\nB. \u5728\u6cb8\u6c34\u6d74\u6761\u4ef6\u4e0bDNA\u4e0e\u4e8c\u82ef\u80fa\u53cd\u5e94\u5448\u73b0\u84dd\u8272\nC. \u9175\u6bcd\u83cc\u548c\u83dc\u82b1\u5747\u53ef\u4f5c\u4e3a\u63d0\u53d6DNA\u7684\u6750\u6599\nD. \u7814\u78e8\u690d\u7269\u7ec6\u80de\u65f6\u52a0\u5165\u6d17\u6da4\u5242\u662f\u4e3a\u4e86\u6eb6\u89e3DNA\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3784442360210721, "meta-math/MetaMath-Mistral-7B": 0.8282898667474273, "itpossible/Chinese-Mistral-7B-v0.1": 0.34239623393788804, "HuggingFaceH4/zephyr-7b-beta": 0.9953595669807525, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9064624011264362, "meta-llama/Meta-Llama-3-8B": 0.8744996165236343, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u5c5e\u4e8e\u6211\u56fd\u6b63\u5f0f\u6cd5\u5f8b\u6e0a\u6e90\u7684\u662f\nA. \u67d0\u5e02\u6ee8\u6e56\u533a\u653f\u5e9c\u53d1\u5e03\u7684\u300a\u5916\u6765\u52a1\u5de5\u4eba\u5458\u7ba1\u7406\u6682\u884c\u529e\u6cd5\u300b\nB. \u56fd\u52a1\u9662\u9881\u5e03\u7684\u300a\u804c\u5de5\u5e26\u85aa\u5e74\u4f11\u5047\u6761\u4f8b\u300b\nC. \u6700\u9ad8\u4eba\u6c11\u6cd5\u9662\u53d1\u5e03\u7684\u6307\u5bfc\u6027\u6848\u4f8b\nD. \u300a\u4e2d\u56fd\u5171\u4ea7\u515a\u7ae0\u7a0b\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.44872184976297047, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6558143285996518, "meta-llama/Meta-Llama-3-8B": 0.45352234650356354, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u65f6\u9645\u6cd5\u5f8b\u51b2\u7a81\u89e3\u51b3\u7684\u95ee\u9898\u4e0a\uff0c\u5bf9\u4e8e\u5f53\u4e8b\u4eba\u6240\u9009\u7684\u6cd5\u5f8b\u4e8b\u540e\u53d1\u751f\u53d8\u66f4\u5e94\u5982\u4f55\u9002\u7528\u6cd5\u5f8b\u7684\u95ee\u9898\uff0c\u4e00\u76f4\u5b58\u5728\u8f83\u5927\u7684\u4e89\u8bae\uff0c\u6211\u56fd\u5b66\u8005\u4e00\u822c\u4e3b\u5f20\nA. \u9002\u7528\u9009\u62e9\u65f6\u7684\u6cd5\u5f8b\nB. \u9002\u7528\u65e7\u6cd5\nC. \u9002\u7528\u65b0\u6cd5\nD. \u539f\u5219\u4e0a\u7531\u5f53\u4e8b\u4eba\u534f\u5546\u89e3\u51b3\u51c6\u636e\u6cd5\u7684\u53d8\u66f4\u95ee\u9898\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.533743622810805, "HuggingFaceH4/zephyr-7b-beta": 0.5110391198696641, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45504750169710273, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65e5\u672c\u6d77\u4e0a\u81ea\u536b\u961f\u7684\"\u5b99\u65af\u76fe\"\u578b\u9632\u7a7a\u9a71\u9010\u8230\u79f0\u4e3a\nA. \u201c\u91d1\u521a\u201d\u7ea7\nB. \u201c\u9ad8\u6ce2\u201d\u7ea7\nC. \u201c\u6751\u96e8\u201d\u7ea7\nD. \u201c\u699b\u540d\u201d\u7ea7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.27416108226793107, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.37480846975763354, "meta-llama/Meta-Llama-3-8B": 0.5243223150121866, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6750\u6599\u5728\u7edd\u5bf9\u5bc6\u5ea6\u72b6\u6001\u7684\u4f53\u79ef\u4e0e\u81ea\u7136\u72b6\u6001\u4f53\u79ef\u7684\u767e\u5206\u6bd4\u4e3a\nA. \u5806\u79ef\u5bc6\u5ea6\nB. \u5b54\u9699\u7387\nC. \u8868\u89c2\u5bc6\u5ea6\nD. \u5bc6\u5b9e\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9850347252376723, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5329708955361997, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e24\u56e0\u7d20\u4e4b\u95f4\u6709\u663e\u8457\u6027\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u610f\u5473\u7740\nA. \u4e00\u4e2a\u56e0\u7d20\u7684\u5404\u6c34\u5e73\u5bf9\u5b9e\u9a8c\u7ed3\u679c\u7684\u5f71\u54cd\u968f\u53e6\u4e00\u4e2a\u56e0\u7d20\u6c34\u5e73\u7684\u6539\u53d8\u800c\u6539\u53d8\nB. \u56e0\u7d20 $A$ \u7684\u4f5c\u7528\u968f\u56e0\u7d20 $B$ \u7684\u4f5c\u7528\u53d8\u5316\u800c\u53d8\u5316\nC. \u56e0\u7d20 A \u7684\u4f5c\u7528\u968f\u56e0\u7d20 $B$ \u7684\u4f5c\u7528\u51cf\u5f31\u800c\u51cf\u5f31\nD. \u56e0\u7d20 $\\mathrm{A}$ \u7684\u4f5c\u7528\u968f\u56e0\u7d20 $\\mathrm{B}$ \u7684\u4f5c\u7528\u589e\u5f3a\u800c\u589e\u5f3a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7197642769615286}}, {"question": "\u201c\u53cc\u7c27\u201d\u662f\u6211\u4eec\u4e00\u95e8\u72ec\u7279\u7684\u66f2\u827a\u827a\u672f\uff0c\u5b83\u5f97\u540d\u4e8e\nA. \u8868\u6f14\u8005\u4f7f\u7528\u7684\u4e50\u5668\u53eb\u505a\u7c27\nB. \u8868\u6f14\u8005\u8eab\u7740\u9ec4\u9a6c\u8902\nC. \u521d\u6f14\u7684\u4e24\u4e2a\u4eba\u90fd\u59d3\u9ec4\nD. \u8868\u6f14\u8005\u5de7\u820c\u5982\u7c27\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u8102\u7c7b\u6d4b\u5b9a\u4e2d\u4ee5\u4e0b\u54ea\u9879\u4e0d\u5c5e\u4e8e\u6eb6\u5242\u8403\u53d6\u6cd5\nA. \u5df4\u5e03\u79d1\u514b\u6cd5\nB. \u7d22\u6c0f\u63d0\u53d6\u6cd5\nC. \u78b1\u6027\u4e59\u919a\u63d0\u53d6\u6cd5\nD. \u9178\u6027\u4e59\u919a\u63d0\u53d6\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6684858101044147, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.41605132742014833}}, {"question": "\u7532\u4e59\u4e24\u4eba\u5206\u522b\u4ece\u968f\u673a\u6570\u5b57\u8868\u62bd\u5f97 30 \u4e2a (\u5404\u53d6\u4e24\u4f4d\u6570\u5b57) \u968f\u673a\u6570\u5b57\u4f5c\u4e3a\u4e24\u4e2a\u6837\u672c\uff0c\u6c42\u5f97 $\\overline{X_1}\uff0c S_1^2\uff0c \\overline{X_2}\uff0c S_2^2$\uff0c\u5219\u7406\u8bba\u4e0a\nA. $\\overline{X_1}=\\overline{X_2}\uff0c S_1^2=S_2^2$\nB. \u5206\u522b\u7531\u7532\u3001\u4e59\u4e24\u6837\u672c\u6c42\u51fa\u7684\u603b\u4f53\u5747\u6570\u7684 $95 \\%$ \u53ef\u4fe1\u533a\u95f4\uff0c\u5f88\u53ef\u80fd\u6709\u91cd\u53e0\nC. \u4f5c\u4e24\u65b9\u5dee\u9f50\u6027\u7684 $\\mathrm{F}$ \u68c0\u9a8c\uff0c\u5fc5\u7136\u65b9\u5dee\u9f50\nD. \u4f5c\u4e24\u6837\u672c\u5747\u6570\u7684 $\\mathrm{t}$ \u68c0\u9a8c\uff0c\u5fc5\u7136\u5f97\u51fa\u65e0\u5dee\u522b\u7684\u7ed3\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4039771099029434, "meta-math/MetaMath-Mistral-7B": 0.3872903633425944, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5326190845563333, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6308502482156867}}, {"question": "\u5728\u7814\u7a76\u52a8\u7269\u7ec6\u80de\u6709\u4e1d\u5206\u88c2\u8fc7\u7a0b\u4e2d\u53d1\u73b0\uff1a\u53ea\u6709\u5f53\u6240\u6709\u67d3\u8272\u4f53\u90fd\u6392\u5217\u5230\u8d64\u9053\u677f\u4e0a\u624d\u542f\u52a8\u540e\u671f\uff0c\u7136\u540e\uff0c\u9ecf\u8fde\u59d0\u59b9\u67d3\u8272\u5355\u4f53\u7684\u9ecf\u8fde\u86cb\u767d\u88ab\u201c\u5206\u79bb\u9176\u201d\u964d\u89e3\uff0c\u59d0\u59b9\u67d3\u8272\u5355\u4f53\u5206\u79bb\u3002\u82e5\u7528\u6fc0\u5149\u7279\u5f02\u6027\u5730\u7834\u574f\u6ede\u540e\u67d3\u8272\u4f53\uff08\u672a\u79fb\u5230\u8d64\u9053\u677f\uff09\u4e0a\u7684\u5c1a\u672a\u4e0e\u7eba\u9524\u4e1d\u8fde\u63a5\u7684\u7740\u4e1d\u70b9\uff0c\u53d1\u73b0\u8be5\u67d3\u8272\u4f53\u4f9d\u7136\u6ede\u540e\uff0c\u540e\u671f\u5374\u53ef\u4ee5\u542f\u52a8\u3002\u4e0b\u5217\u6709\u5173\u53d9\u8ff0\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u59d0\u59b9\u67d3\u8272\u5355\u4f53\u5206\u79bb\u5f62\u6210\u76842\u6761\u67d3\u8272\u4f53\u4ee5\u4e0d\u540c\u7684\u901f\u7387\u79fb\u5411\u7ec6\u80de\u4e24\u6781\nB. \u63a7\u5236\u201c\u5206\u79bb\u9176\u201d\u5408\u6210\u7684\u57fa\u56e0\u5728\u4e2d\u671f\u5f00\u59cb\u8f6c\u5f55\u5e76\u7ffb\u8bd1\nC. \u52a8\u7269\u7ec6\u80de\u4e2d\u6240\u6709\u7eba\u9524\u4e1d\u90fd\u4e0e\u7740\u4e1d\u70b9\u8fde\u63a5\nD. \u672a\u8fde\u63a5\u7eba\u9524\u4e1d\u7684\u7740\u4e1d\u70b9\u53ef\u80fd\u4f1a\u6291\u5236\u7ec6\u80de\u5468\u671f\u5411\u4e0b\u4e00\u9636\u6bb5\u8fd0\u8f6c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3760559659175853, "meta-math/MetaMath-Mistral-7B": 0.46550964355826063, "itpossible/Chinese-Mistral-7B-v0.1": 0.36617667716213176, "HuggingFaceH4/zephyr-7b-beta": 0.9912209257147221, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.42693270069524886, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u77ed\u671f\u6765\u770b\uff0c\u65e5\u8d8b\u6fc0\u70c8\u7684\u7ade\u4e89\u5fc5\u5c06\u5bfc\u81f4\u76f8\u5e94\u533a\u57df\u7684\u4e3b\u9898\u516c\u56ed__\uff1b\u800c\u4ece\u4e2d\u957f\u671f\u6765\u770b\uff0c__\u4e86\u56fd\u5185\u5916\u4e3b\u9898\u516c\u56ed\u7cbe\u7cb9\u7684\u96c6\u7fa4\u533a\u57df\u5c06\u66f4\u6709\u52a9\u4e8e\u5438\u5f15\u6765\u81ea\u56fd\u5185\u5916\u7684\u589e\u91cf\u5ba2\u6d41\uff0c\u8fd9\u5bf9\u8eab\u5904\u5176\u4e2d\u7684\u672c\u571f\u4e3b\u9898\u516c\u56ed\u800c\u8a00\uff0c\u53c8\u662f\u5f88\u597d\u7684\u673a\u9047\u3002\u4f9d\u6b21\u586b\u5165\u5212\u6a2a\u7ebf\u90e8\u5206\u6700\u6070\u5f53\u7684\u4e00\u9879\u662f\nA. \u7269\u7ade\u5929\u62e9\u878d\u6c47\nB. \u6b64\u6d88\u5f7c\u957f\u4f1a\u805a\nC. \u4f18\u80dc\u52a3\u6c70\u805a\u96c6\nD. \u5f31\u8089\u5f3a\u98df\u835f\u8403\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.965459709883649, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5323950959741666, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u54f2\u5b66\u7684\u57fa\u672c\u95ee\u9898\u662f\nA. \u601d\u7ef4\u4e0e\u5b58\u5728\u7684\u5173\u7cfb\u95ee\u9898\nB. \u793e\u4f1a\u548c\u81ea\u7136\u7684\u5173\u7cfb\u95ee\u9898\nC. \u5b9e\u8df5\u548c\u7406\u8bba\u7684\u5173\u7cfb\u95ee\u9898\nD. \u653f\u6cbb\u548c\u7ecf\u6d4e\u7684\u5173\u7cfb\u95ee\u9898\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8240691876753778, "meta-math/MetaMath-Mistral-7B": 0.9912646906567983, "itpossible/Chinese-Mistral-7B-v0.1": 0.7643891421258375, "HuggingFaceH4/zephyr-7b-beta": 0.9999599373175668, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9158727717579488, "meta-llama/Meta-Llama-3-8B": 0.958963372253239, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9947978150107123}}, {"question": "\u8bbe $f(x)=\\left\\{\\begin{array}{ll}x^2\uff0c & x \\leq 0\uff0c \\\\ x^2+x\uff0c & x>0\uff0c\\end{array}\\right.$ \u5219 $f(-x)=___ $.\nA. $\\begin{cases}x^2\uff0c & x \\leq 0 \\\\ x^2-x\uff0c & x>0\\end{cases}$\nB. $\\begin{cases}x^2-x\uff0c & x<0 \\\\ x^2\uff0c & x \\geq 0\\end{cases}$\nC. $\\begin{cases}-\\left(x^2+x\\right)\uff0c & x<0 \\\\ -x^2\uff0c & x \\geq 0\\end{cases}$\nD. $\\begin{cases}-x^2\uff0c & x \\leq 0 \\\\ -\\left(x^2+x\\right)\uff0c & x>0\\end{cases}$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3160424181481997, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3454901449522389}}, {"question": "\u7528\u76f4\u63a5\u7535\u955c\u6cd5\u53ef\u505a\u51fa\u65e9\u671f\u5feb\u901f\u8bca\u65ad\u7684\u75c5\u6bd2\u662f\nA. \u8f6e\u72b6\u75c5\u6bd2\nB. \u75b1\u75b9\u75c5\u6bd2\nC. \u5de8\u7ec6\u80de\u75c5\u6bd2\nD. \u6d41\u611f\u75c5\u6bd2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.36150852560561064, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6882563107026121}}, {"question": "\uff08\uff09\u662f\u56fd\u5bb4\u7684\u53e6\u4e00\u79cd\u8868\u73b0\u5f62\u5f0f\uff0c\u65f6\u95f4\u5728\u665a\u4e0a\u4e3e\u884c\uff0c\u5176\u89c4\u683c\u548c\u6807\u51c6\u4e0e\u56fd\u5bb4\u76f8\u540c\uff0c\u9686\u91cd\u70ed\u70c8\nA. \u665a\u5bb4\nB. \u5bb6\u5bb4\nC. \u4fbf\u5bb4\nD. \u65e9\u5bb4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9645721085347302, "meta-math/MetaMath-Mistral-7B": 0.9991335596457797, "itpossible/Chinese-Mistral-7B-v0.1": 0.8359002905682463, "HuggingFaceH4/zephyr-7b-beta": 0.999474771036497, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9957870551537631, "meta-llama/Meta-Llama-3-8B": 0.8383749204036363, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.971522912996483}}, {"question": "\u675c\u8fd0\u71ee\u7684\u8bd7\u4f5c\u300a\u5c71\u300b\u7684\u610f\u8574\u662f\nA. \u5bf9\u8ffd\u6c42\u8005\u5728\u8fdb\u53d6\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u9047\u5230\u7684\u632b\u6298\u7684\u8b66\u6212\nB. \u5bf9\u8ffd\u6c42\u8005\u6c38\u4e0d\u6ee1\u8db3\u3001\u4e0d\u65ad\u6500\u5347\u5374\u6c38\u8fdc\u5bc2\u5bde\u7684\u60cb\u60dc\nC. \u5bf9\u8ffd\u6c42\u8005\u4e0d\u65ad\u6500\u5347\u800c\u6b32\u6c42\u6c38\u8fdc\u4e0d\u80fd\u6ee1\u8db3\u7684\u611f\u6168\nD. \u5bf9\u8ffd\u6c42\u8005\u6c38\u4e0d\u6ee1\u8db3\u3001\u6c38\u8fdc\u8fdb\u53d6\u7684\u7cbe\u795e\u6c14\u6982\u7684\u8d5e\u53f9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5327251004904839, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5c5e\u4e8e\u6076\u6027\u80bf\u7624\u7684\u662f\nA. \u521b\u4f24\u6027\u795e\u7ecf\u7624\nB. \u810a\u819c\u7624\nC. \u9ec4\u8272\u7624\nD. \u5f25\u6f2b\u578b\u661f\u5f62\u7ec6\u80de\u7624\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4520495068530242, "meta-math/MetaMath-Mistral-7B": 0.8457584030106615, "itpossible/Chinese-Mistral-7B-v0.1": 0.6793296468240335, "HuggingFaceH4/zephyr-7b-beta": 0.9859613507785561, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45038871841587785, "meta-llama/Meta-Llama-3-8B": 0.8078236125858229, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6408458291327694}}, {"question": "\u57fa\u7763\u6559\u7684\u7cbe\u795e\u672c\u8d28\u662f\u5c06\u4eba\u63d0\u5230\u5149\u660e\u5723\u6d01\u7684\u795e\u6027\u751f\u6d3b\uff0c\u800c\u4e2d\u4e16\u7eaa\u6559\u4f1a\u7684\u5b9e\u8df5\u6d3b\u52a8\u5374\u628a\u57fa\u7763\u5f92\u4eec\u5f15\u5411\u4e86\nA. \u72ac\u5112\u4e3b\u4e49\u4e4b\u4e2d\nB. \u552f\u7075\u4e3b\u4e49\u4e4b\u4e2d\nC. \u6700\u7c97\u9119\u91ce\u86ee\u7684\u8089\u6b32\u653e\u7eb5\u4e4b\u4e2d\nD. \u795e\u79d8\u4e3b\u4e49\u4e4b\u4e2d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9224052886306334, "meta-math/MetaMath-Mistral-7B": 0.9934321286294221, "itpossible/Chinese-Mistral-7B-v0.1": 0.8714763104734555, "HuggingFaceH4/zephyr-7b-beta": 0.992762108708468, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9537176890228356, "meta-llama/Meta-Llama-3-8B": 0.6650609661451794, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9542996110578512}}, {"question": "\u5728\u7ba1\u7406\u4e0a\u5f3a\u8c03\u5916\u90e8\u63a7\u5236\uff0c\u4e3b\u5f20\u4f9d\u9760\u6743\u5a01\u7684\u529b\u91cf\u4f5c\u4e3a\u6307\u6325\u548c\u63a7\u5236\u624b\u6bb5\u7684\u662f\nA. Y\u7406\u8bba\nB. \u9886\u5bfc\u65b9\u683c\u7406\u8bba\nC. \u4e0d\u6210\u719f-\u6210\u719f\u7406\u8bba\nD. X\u7406\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7221728315760043, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5674426365424174, "meta-llama/Meta-Llama-3-8B": 0.5722792347020408, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u81ed\u8c46\u8150\uff1a\u9999\u83c7\nA. \u5c0f\u9ea6\uff1a\u5927\u7c73\nB. \u9ed1\u829d\u9ebb\uff1a\u767d\u83dc\nC. \u70ed\u5e72\u9762\uff1a\u51c9\u6c34\nD. \u751c\u83dc\uff1a\u82e6\u74dc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2883873487231113, "meta-math/MetaMath-Mistral-7B": 0.3236616466071801, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.44972445627596996}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u8702\u7a9d\u7ec7\u708e\u7684\u53d9\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u7ec6\u83cc\u4e0d\u5bb9\u6613\u7ecf\u7ec4\u7ec7\u95f4\u9699\u3001\u6dcb\u5df4\u7ba1\u548c\u8840\u9053\u8513\u5ef6\u6269\u6563\nB. \u5e38\u7531\u8349\u7eff\u8272\u94fe\u7403\u83cc\u611f\u67d3\u5f15\u8d77\nC. \u75c5\u53d8\u7279\u70b9\u4e0e\u94fe\u7403\u83cc\u5206\u6ccc\u7684\u94fe\u6fc0\u9176\u548c\u900f\u660e\u8d28\u9178\u9176\u6709\u5173\nD. \u5e38\u89c1\u90e8\u4f4d\u662f\u5185\u810f\u5668\u5b98\u3001\u808c\u8089\u548c\u9611\u5c3e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3171201089282235, "meta-math/MetaMath-Mistral-7B": 0.5856919254596261, "itpossible/Chinese-Mistral-7B-v0.1": 0.37672193122020287, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5443943063547284, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u51cf\u8f7b\u5904\u7f5a\uff0c\u4e0b\u5217\u8bf4\u6cd5\u9519\u8bef\u7684\u6709\nA. \u72af\u7f6a\u5206\u5b50\u4e0d\u5177\u6709\u6cd5\u5b9a\u51cf\u8f7b\u5904\u7f5a\u60c5\u8282\u7684\uff0c\u7edd\u5bf9\u4e0d\u80fd\u5728\u6cd5\u5b9a\u5211\u4ee5\u4e0b\u5224\u5904\u5211\u7f5a\nB. \u72af\u7f6a\u5206\u5b50\u5177\u6709\u51cf\u8f7b\u5904\u7f5a\u60c5\u8282\uff0c\u4f46\u662f\u5176\u6240\u72af\u4e4b\u7f6a\u5177\u6709\u6570\u4e2a\u91cf\u5211\u5e45\u5ea6\u7684\uff0c\u53ea\u80fd\u5728\u6cd5\u5b9a\u91cf\u5211\u5e45\u5ea6\u7684\u4e0b\u4e00\u4e2a\u91cf\u5211\u5e45\u5ea6\u5185\u5224\u5904\u5211\u7f5a\nC. \u72af\u7f6a\u5206\u5b50\u65e2\u6709\u51cf\u8f7b\u5904\u7f5a\u60c5\u8282\uff0c\u4e5f\u6709\u4ece\u8f7b\u5904\u7f5a\u60c5\u8282\u7684\uff0c\u4e5f\u53ea\u80fd\u5728\u6cd5\u5b9a\u91cf\u5211\u5e45\u5ea6\u7684\u4e0b\u4e00\u4e2a\u91cf\u5211\u5e45\u5ea6\u5185\u5224\u5904\u5211\u7f5a\nD. \u72af\u7f6a\u5206\u5b50\u5177\u6709\u51cf\u8f7b\u5904\u7f5a\u60c5\u8282\u7684\uff0c\u5e94\u5f53\u5728\u6cd5\u5b9a\u5211\u4ee5\u4e0b\u5224\u5904\u5211\u7f5a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.31283638571410965, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u725b\u90ce\u7ec7\u5973\u300b\u7684\u4f53\u88c1\u662f\nA. \u7ae5\u8bdd\nB. \u795e\u8bdd\nC. \u4f20\u8bf4\nD. \u5bd3\u8a00\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3833677286549997, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u5728\u81ea\u7136\u6761\u4ef6\u4e0b\uff0c\u67d0\u968f\u673a\u4ea4\u914d\u79cd\u7fa4\u4e2d\u7b49\u4f4d\u57fa\u56e0A\u3001a\u9891\u7387\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u6301\u7eed\u9009\u62e9\u6761\u4ef6\u4e0b\uff0c\u4e00\u79cd\u57fa\u56e0\u7684\u9891\u7387\u53ef\u4ee5\u964d\u4e3a\u96f6\nB. \u8be5\u79cd\u7fa4\u57fa\u56e0\u9891\u7387\u7684\u53d8\u5316\u53ea\u4e0e\u73af\u5883\u7684\u9009\u62e9\u4f5c\u7528\u6709\u5173\nC. \u4e00\u822c\u6765\u8bf4\uff0c\u9891\u7387\u9ad8\u7684\u57fa\u56e0\u6240\u63a7\u5236\u7684\u6027\u72b6\u66f4\u9002\u5e94\u73af\u5883\nD. \u5728\u67d0\u79cd\u6761\u4ef6\u4e0b\u4e24\u79cd\u57fa\u56e0\u7684\u9891\u7387\u53ef\u4ee5\u76f8\u7b49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35866706094071804, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4716472104458297, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u63a7\u5236\u67d0\u4e00\u6027\u72b6\u76843\u4e2a\u62163\u4e2a\u4ee5\u4e0a\u7684\u7b49\u4f4d\u57fa\u56e0\u79f0\u4e3a\nA. \u5fae\u6548\u591a\u57fa\u56e0\nB. \u590d\u7b49\u4f4d\u57fa\u56e0\nC. \u62df\u7b49\u4f4d\u57fa\u56e0\nD. \u8fde\u9501\u57fa\u56e0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43843607764045495, "meta-math/MetaMath-Mistral-7B": 0.7471176285291683, "itpossible/Chinese-Mistral-7B-v0.1": 0.7333071429388314, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5157888980022398, "meta-llama/Meta-Llama-3-8B": 0.35160429054563636, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u6211\u56fd\u6cd5\u5f8b\u6709\u5173\u89c4\u5b9a\uff0c\u5bf9\u4ee5\u4e0b\u54ea\u79cd\u884c\u4e3a\u4e0d\u5c5e\u4e8e\u51cf\u8f7b\u6216\u514d\u9664\u6cd5\u5f8b\u8d23\u4efb\u7684\u60c5\u5f62\uff1f\nA. \u738b\u67d0\u5bb6\u8d2b\u5982\u6d17\u5374\u4e0d\u5c0f\u5fc3\u5728\u6253\u5de5\u7684\u4f01\u4e1a\u635f\u574f\u4e86\u4ef7\u503c30\u4e07\u5143\u7684\u4eea\u5668\nB. \u5218\u67d0\u5077\u4e86\u4e00\u8f86\u4ef7\u503c100\u5143\u7684\u81ea\u884c\u8f66\uff0c14\u5e74\u540e\u88ab\u4eba\u67e5\u51fa\nC. \u5f20\u67d0\u66b4\u529b\u963b\u6b62\u674e\u67d0\u4e0e\u8d75\u67d0\u7ed3\u5a5a\uff0c\u88ab\u5bb3\u4eba\u6ca1\u6709\u8d77\u8bc9\nD. \u674e\u67d0\u9047\u52303\u4e2a\u624b\u62ff\u5229\u5203\u7684\u6b79\u5f92\u62a2\u52ab\u65f6\u594b\u8d77\u53cd\u6297\uff0c\u593a\u8fc7\u5200\u5c06\u5176\u4e2d\u4e00\u4e2a\u6b79\u5f92\u523a\u6210\u91cd\u4f24\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6cd5\u4f5c\u4e3a\u4e00\u79cd\u5e26\u6709\u4ef7\u503c\u5224\u65ad\u7684\u884c\u4e3a\u89c4\u8303\uff0c\u53ef\u4ee5\u8bc4\u5224\u3001\u8861\u91cf\u4ed6\u4eba\u884c\u4e3a\u7684\u5408\u6cd5\u6027\nA. \u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u4ef2\u88c1\u6cd5\u300b\nB. \u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u5211\u4e8b\u8bc9\u8bbc\u6cd5\u300b\nC. \u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u884c\u653f\u8bc9\u8bbc\u6cd5\u300b\nD. \u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u6c11\u6cd5\u901a\u5219\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9176\u7684\u7279\u5f02\u6027\u662f\u6307\nA. \u9176\u50ac\u5316\u53cd\u5e94\u7684\u673a\u5236\u5404\u4e0d\u540c\nB. \u9176\u5bf9\u5176\u6240\u50ac\u5316\u7684\u5e95\u7269\u6709\u7279\u5f02\u7684\u9009\u62e9\u6027\nC. \u9176\u5728\u7ec6\u80de\u4e2d\u7684\u5b9a\u4f4d\u662f\u7279\u5f02\u6027\u7684\nD. \u9176\u4e0e\u8f85\u9176\u7279\u5f02\u7684\u7ed3\u5408\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9554469590065239, "meta-math/MetaMath-Mistral-7B": 0.9976174468007113, "itpossible/Chinese-Mistral-7B-v0.1": 0.9679499379283654, "HuggingFaceH4/zephyr-7b-beta": 0.9994815195329807, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9783791657090335, "meta-llama/Meta-Llama-3-8B": 0.972908822524745, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9959151316795523}}, {"question": "\u4f53\u73b0\u7ec4\u7ec7\u4e2d\u6a2a\u8de8\u8fc7\u6743\u529b\u6267\u884c\u8def\u7ebf\u800c\u76f4\u63a5\u8054\u7cfb\u7684\u539f\u5219\u662f\nA. \u79e9\u5e8f\u539f\u5219\nB. \u8df3\u677f\u539f\u5219\nC. \u516c\u5e73\u539f\u5219\nD. \u9002\u5f53\u7684\u96c6\u6743\u548c\u5206\u6743\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.43607054367772935, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6580557820767508, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6cd5\u5f8b\u5236\u5b9a\u7684\u539f\u5219\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u6709\nA. \u9a6c\u514b\u601d\u6df1\u523b\u5730\u6307\u51fa\uff0c\u7acb\u6cd5\u8005\u5e94\u8be5\u628a\u81ea\u5df1\u770b\u4f5c\u4e00\u4e2a\u81ea\u7136\u79d1\u5b66\u5bb6\u3002\u4ed6\u4e0d\u662f\u5728\u5236\u9020\u6cd5\u5f8b\uff0c\u4e0d\u662f\u5728\u53d1\u660e\u6cd5\u5f8b\uff0c\u800c\u4ec5\u4ec5\u662f\u5728\u8868\u8ff0\u6cd5\u5f8b\u3002\u4ed6\u628a\u7cbe\u795e\u5173\u7cfb\u7684\u5185\u5728\u89c4\u5f8b\u8868\u73b0\u5728\u6709\u610f\u8bc6\u7684\u73b0\u884c\u6cd5\u5f8b\u4e4b\u4e2d\u3002\u8fd9\u8bf4\u660e\uff0c\u6cd5\u5f8b\u7684\u5236\u5b9a\u5e94\u5f53\u575a\u6301\u6c11\u4e3b\u6027\u539f\u5219\nB. \u53ea\u8981\u4e00\u5207\u6cd5\u5f8b\u5236\u5b9a\u90fd\u4ee5\u5baa\u6cd5\u4e3a\u6839\u636e\uff0c\u5219\u4e00\u5b9a\u4f1a\u5b9e\u73b0\u6cd5\u5236\u7684\u7edf\u4e00\nC. \u6cd5\u5236\u7edf\u4e00\u539f\u5219\u7684\u8981\u6c42\u662f\u6cd5\u5f8b\u4e4b\u95f4\u6ca1\u6709\u77db\u76fe\nD. \u5728\u6211\u56fd\uff0c\u7acb\u6cd5\u5185\u5bb9\u7684\u6c11\u4e3b\u6027\u662f\u6307\u6cd5\u5f8b\u5236\u5b9a\u5fc5\u987b\u4ece\u6700\u5927\u591a\u6570\u4eba\u7684\u6700\u6839\u672c\u5229\u76ca\u51fa\u53d1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u552f\u4e00\u8fde\u4e8e\u8111\u5e72\u80cc\u4fa7\u7684\u8111\u795e\u7ecf\u662f\nA. \u5c55\u795e\u7ecf\nB. \u52a8\u773c\u795e\u7ecf\nC. \u89c6\u795e\u7ecf\nD. \u6ed1\u8f66\u795e\u7ecf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7080951396257694, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u3001\u4e59\u7b7e\u8ba2\u4e70\u5356\u5408\u540c\uff0c\u7532\u5411\u4e59\u652f\u4ed8\u5168\u90e8\u4ef7\u6b3e\uff0c\u7ea6\u5b9a\u4e59\u5e94\u4e8e12\u670830\u65e5\u524d\u4ea4\u4ed8\u8d27\u7269\u300212\u670825\u65e5\uff0c\u7532\u5f97\u77e5\u4e59\u8fd1\u671f\u5c06\u51fa\u56fd\uff0c\u5e76\u5df2\u5c06\u5168\u90e8\u5e93\u5b58\u8d27\u7269\u53ca\u5176\u4ed6\u8d22\u4ea7\u5356\u7ed9\u4ed6\u4eba\u3002\u4e8e\u662f\uff0c\u7532\u8981\u6c42\u4e59\u627f\u62c5\u8fdd\u7ea6\u8d23\u4efb\uff0c\u4e59\u62d2\u7edd\u3002\u6839\u636e\u4e0a\u8ff0\u60c5\u5f62\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u672a\u5230\u4ea4\u4ed8\u671f\u9650\uff0c\u7532\u65e0\u6743\u8981\u6c42\u4e59\u627f\u62c5\u8fdd\u7ea6\u8d23\u4efb\nB. \u7532\u53ea\u80fd\u572812\u670830\u65e5\u540e\u8981\u6c42\u4e59\u627f\u62c5\u8fdd\u7ea6\u8d23\u4efb\nC. \u7532\u6709\u6743\u8981\u6c42\u4e59\u627f\u62c5\u8fdd\u7ea6\u8d23\u4efb\nD. \u7532\u6709\u6743\u64a4\u9500\u4e70\u5356\u5408\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9128145605864629, "meta-math/MetaMath-Mistral-7B": 0.9930348980311269, "itpossible/Chinese-Mistral-7B-v0.1": 0.6594839886530024, "HuggingFaceH4/zephyr-7b-beta": 0.9774607830668215, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9349998301532354, "meta-llama/Meta-Llama-3-8B": 0.6738682210004198, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.602437100362301}}, {"question": "1969\u5e742\u67089\u65e5\uff0c\u4e16\u754c\u7b2c\u4e00\u67b6\u5bbd\u673a\u8eab\u6c11\u822a\u673a\u9996\u6b21\u8bd5\u98de\u6210\u529f\uff0c\u5b83\u662f\nA. \u6ce2\u97f3757\nB. \u6ce2\u97f3747\nC. \u6ce2\u97f3727\nD. \u6ce2\u97f3767\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5356688086820646, "meta-math/MetaMath-Mistral-7B": 0.6146419960592125, "itpossible/Chinese-Mistral-7B-v0.1": 0.8912895654074275, "HuggingFaceH4/zephyr-7b-beta": 0.8210376457396169, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5149177965958717, "meta-llama/Meta-Llama-3-8B": 0.9729088249794547, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.999165112979948}}, {"question": "\u8d44\u672c\u662f\u4e00\u79cd\u8fd0\u52a8\uff0c\u8d44\u672c\u5faa\u73af\u662f\u4ece\nA. \u8d44\u672c\u8fd0\u52a8\u7684\u5b9e\u73b0\u6761\u4ef6\u65b9\u9762\u7814\u7a76\u8d44\u672c\u7684\u8fd0\u52a8\nB. \u8d44\u672c\u8fd0\u52a8\u7684\u77db\u76fe\u6027\u65b9\u9762\u7814\u7a76\u8d44\u672c\u7684\u8fd0\u52a8\nC. \u8d44\u672c\u8fd0\u52a8\u7684\u901f\u5ea6\u65b9\u9762\u7814\u7a76\u8d44\u672c\u7684\u8fd0\u52a8\nD. \u8d44\u672c\u8fd0\u52a8\u7684\u5f62\u5f0f\u548c\u6761\u4ef6\u65b9\u9762\u7814\u7a76\u8d44\u672c\u7684\u8fd0\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37887215961900156, "meta-math/MetaMath-Mistral-7B": 0.5200940293519974, "itpossible/Chinese-Mistral-7B-v0.1": 0.43219643717792317, "HuggingFaceH4/zephyr-7b-beta": 0.9562740382174748, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.710165099487527, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7028138421957405}}, {"question": "2018\u5e74\uff0c\u56fd\u5bb6\u7edf\u4e00\u6cd5\u5f8b\u804c\u4e1a\u8d44\u683c\u8003\u8bd5\u5236\u5ea6\u5728\u6211\u56fd\u6b63\u5f0f\u5b9e\u65bd\u3002\u8be5\u5236\u5ea6\u96c6\u4e2d\u4f53\u73b0\u7684\u6cd5\u5f8b\u804c\u4e1a\u7279\u5f81\u662f\nA. \u6cd5\u5f8b\u804c\u4e1a\u5177\u6709\u76f8\u5f53\u5927\u7684\u81ea\u6cbb\u6027\nB. \u6cd5\u5f8b\u804c\u4e1a\u8981\u6c42\u8bbe\u7f6e\u4e25\u683c\u7684\u51c6\u5165\u95e8\u69db\nC. \u6cd5\u5f8b\u804c\u4e1a\u5fc5\u987b\u5177\u5907\u7279\u5b9a\u7684\u804c\u4e1a\u4f26\u7406\nD. \u4ece\u4e8b\u6cd5\u5f8b\u804c\u4e1a\u610f\u5473\u7740\u80a9\u8d1f\u66f4\u591a\u7684\u793e\u4f1a\u8d23\u4efb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3374067140235278, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7310637709224282, "HuggingFaceH4/zephyr-7b-beta": 0.9554332092811917, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5290611352457788}}, {"question": "\u5728\u4e2d\u56fd\u7b2c\u4e00\u4e2a\u6bd4\u8f83\u5b8c\u6574\u7684\u7ffb\u8bd1\u4ecb\u7ecd\u897f\u65b9\u7684\u6c11\u65cf\u5b9a\u4e49\u7684\u662f\u8c01\nA. \u6881\u542f\u8d85\nB. \u6c6a\u5146\u94ed\nC. \u738b\u660e\nD. \u674e\u5927\u948a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5749918830660065, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5896373551947481}}, {"question": "\u5bf9\u4e8b\u7269\u5b58\u5728\u65b9\u5f0f\u548c\u8fd0\u52a8\u72b6\u6001\u52a0\u4ee5\u8ba4\u8bc6\uff0c\u5e76\u7528\u6587\u5b57\u7b49\u7b26\u53f7\u8868\u8ff0\u51fa\u6765\u5c31\u5f62\u6210\u4e86\nA. \u7269\u7406\u4fe1\u606f\nB. \u81ea\u7136\u4fe1\u606f\nC. \u4eba\u5de5\u4fe1\u606f\nD. \u7535\u5b50\u4fe1\u606f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5272843097944059, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.40322669517609055, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.43921757655972987, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8571698571384596}}, {"question": "\u6c11\u65cf\u5173\u7cfb\u7684\u57fa\u672c\u8868\u73b0\u5f62\u5f0f\u662f\nA. \u6c11\u65cf\u7fa4\u4f53\u4e4b\u95f4\u7684\u5173\u7cfb\nB. \u6c11\u65cf\u81ea\u6cbb\u5730\u65b9\u4e0e\u56fd\u5bb6\u4e4b\u95f4\u7684\u5173\u7cfb\nC. \u6c11\u65cf\u81ea\u6cbb\u5730\u65b9\u4e0e\u4e2a\u4eba\u4e4b\u95f4\u7684\u5173\u7cfb\nD. \u6c11\u65cf\u4e2a\u4f53\u4e4b\u95f4\u7684\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.431575164490715, "meta-math/MetaMath-Mistral-7B": 0.7684312406803052, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7923509257479551, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7345665397981752, "meta-llama/Meta-Llama-3-8B": 0.6631241167476103, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9ec4\u4f53\u7684\u5f62\u6210\u3001\u53d1\u80b2\u548c\u529f\u80fd\uff0c\u63cf\u8ff0\u6070\u5f53\u7684\u662f\nA. \u5206\u6ccc\u5b55\u6fc0\u7d20\nB. \u7ef4\u630114\u5929\u5de6\u53f3\u5747\u9000\u5316\nC. \u6392\u5375\u540e\u7531\u5375\u6ce1\u5185\u819c\u548c\u5375\u6ce1\u9897\u7c92\u7ec6\u80de\u5f62\u6210\nD. \u6392\u5375\u540e\u7531\u5375\u6ce1\u819c\u5f62\u6210\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4542690839788951, "meta-math/MetaMath-Mistral-7B": 0.7403994618040227, "itpossible/Chinese-Mistral-7B-v0.1": 0.563482664571169, "HuggingFaceH4/zephyr-7b-beta": 0.9941655463255212, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8440335363937806, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9508793473215733}}, {"question": "\u4e0b\u5217\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f\nA. \u8db3\u91cf\u7684NaHSO3\u6eb6\u6db2\u548cNa2CO3\u6eb6\u6db2\u6df7\u5408\u540e\u80fd\u4ea7\u751fCO2\u6c14\u4f53\uff0c\u82e5\u5411Na2CO3\u6eb6\u6db2\u4e2d\u901a\u5165\u8db3\u91cfSO2\u6c14\u4f53\uff0c\u4e5f\u53ef\u5f97\u5230CO2\nB. pH\uff1d5\u7684H2S\u6eb6\u6db2\u4e2d\uff0cc(H\uff0b)\uff1dc(HS\uff0d)\uff1d1\u00d710\uff0d5mol\u00b7L^\uff0d1\nC. \u5ba4\u6e29\u4e0b\uff0c\u5c060.05 mol Na2CO3\u56fa\u4f53\u6eb6\u4e8e\u6c34\u914d\u6210100 mL\u6eb6\u6db2\uff0c\u5411\u6eb6\u6db2\u4e2d\u52a0\u51650.05 mol CaO\uff0c\u5219\u6eb6\u6db2\u4e2d\u589e\u5927\nD. 25 \u00b0C\u548c100 \u00b0C\u65f6H2O\u7684Kw\uff0c\u524d\u8005\u6570\u503c\u5c0f\u4e8e\u540e\u8005\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4078864031777637, "HuggingFaceH4/zephyr-7b-beta": 0.34171283410793735, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "RNA\u7f16\u8f91\u6240\u6d89\u53ca\u7684\u53cd\u5e94\u8fc7\u7a0b\u662f\nA. RNA\u5408\u6210\u540e\u7684\u52a0\u5de5\u8fc7\u7a0b\nB. RNA\u805a\u5408\u9176\u8bc6\u522b\u6a21\u677f\u7684\u8fc7\u7a0b\nC. tRNA\u53cd\u5bc6\u7801\u5bf9\u5bc6\u7801\u7684\u8bc6\u522b\u8fc7\u7a0b\nD. DNA\u6307\u5bfc\u7684RNA\u5408\u6210\u8fc7\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5744698148409803, "meta-math/MetaMath-Mistral-7B": 0.9928961453731352, "itpossible/Chinese-Mistral-7B-v0.1": 0.4785320406655144, "HuggingFaceH4/zephyr-7b-beta": 0.9975145275061454, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8439357515597133, "meta-llama/Meta-Llama-3-8B": 0.5074099558576348, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2016\u5e7411\u6708\uff0c\u4e2d\u56fd\u82af\u7ea7\u76f4\u5f84\u4e3a5\u7c73\u7684\u5927\u578b\u8fd0\u8f7d\u706b\u7bad\u201c\u957f\u5f81\u4e94\u53f7\u201d\u4ece\u54ea\u4e00\u4e2a\u536b\u661f\u53d1\u5c04\u4e2d\u5fc3\u9996\u98de\u6210\u529f\uff0c\u540c\u65f6\u8fd8\u5c06\u201c\u5b9e\u8df5\u5341\u4e03\u53f7\u536b\u661f\u201d\u76f4\u63a5\u9001\u5165\u4e86\u5730\u7403\u9759\u6b62\u8f68\u9053\nA. \u897f\u660c\nB. \u592a\u539f\nC. \u9152\u6cc9\nD. \u6587\u660c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9176\u6d3b\u6027\u4e2d\u5fc3\u7684\u67d0\u4e9b\u57fa\u56e2\u53ef\u4ee5\u53c2\u4e0e\u8d28\u5b50\u7684\u8f6c\u79fb\uff0c\u8fd9\u79cd\u50ac\u5316\u4f5c\u7528\u79f0\u4e3a\nA. \u9178\u78b1\u50ac\u5316\nB. \u4eb2\u6838\u50ac\u5316\nC. \u591a\u5143\u50ac\u5316\nD. \u5171\u4ef7\u50ac\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8528123625948566, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53ef\u5c06\u56fe\u7247\u8f93\u5165\u8ba1\u7b97\u673a\u7684\u662f\nA. \u952e\u76d8\nB. \u7ed8\u56fe\u4eea\nC. \u6570\u7801\u7167\u76f8\u673a\nD. \u9f20\u6807\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6827212181648147, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8385640497654522, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8842005480850411, "meta-llama/Meta-Llama-3-8B": 0.607357758869277, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.674794741809705}}, {"question": "\u5728\u8d44\u6e90\u7ba1\u7406\u5668\u4e2d\uff0c\u5355\u51fb\u6587\u4ef6\u5939\u5de6\u8fb9\u7684\u201c+\u201d\nA. \u5728\u53f3\u7a97\u53e3\u4e2d\u663e\u793a\u8be5\u6587\u4ef6\u5939\u4e2d\u7684\u6587\u4ef6\u5939\u548c\u6587\u4ef6\nB. \u5728\u5de6\u7a97\u53e3\u4e2d\u5c55\u5f00\u8be5\u6587\u4ef6\u5939\u7684\u5b50\u6587\u4ef6\u5939\nC. \u5728\u53f3\u7a97\u53e3\u4e2d\u5c55\u5f00\u8be5\u6587\u4ef6\u5939\u7684\u5b50\u6587\u4ef6\u5939\nD. \u5728\u5de6\u7a97\u53e3\u4e2d\u663e\u793a\u6587\u4ef6\u5939\u4e2d\u7684\u5b50\u6587\u4ef6\u5939\u548c\u6587\u4ef6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36115819042206615, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7366333870473986, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4350764598732043, "meta-llama/Meta-Llama-3-8B": 0.31649016643535544, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5e38\u6e29\u5e38\u538b\u4e0b\uff0c\u67d0\u70e7\u78b1\u6eb6\u6db2\u4e0e0.05 mol\u6c2f\u6c14\u6070\u597d\u5b8c\u5168\u53cd\u5e94\uff0c\u5f97\u5230pH\uff1d9\u7684\u6df7\u5408\u6eb6\u6db2(\u6eb6\u8d28\u4e3aNaCl\u4e0eNaClO)\u3002\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f(NA\u4ee3\u8868\u963f\u4f0f\u52a0\u5fb7\u7f57\u5e38\u6570\u7684\u503c\nA. \u6c2f\u6c14\u7684\u4f53\u79ef\u4e3a1.12 L\nB. \u6240\u5f97\u6eb6\u6db2\u4e2dClO\uff0d\u7684\u6570\u76ee\u4e3a0.05NA\nC. \u6240\u5f97\u6eb6\u6db2\u4e2d\u542bOH\uff0d\u7684\u6570\u76ee\u4e3a1\u00d710^\uff0d5NA\nD. \u539f\u70e7\u78b1\u6eb6\u6db2\u4e2d\u542b\u6eb6\u8d28\u79bb\u5b50\u6570\u4e3a0.2NA\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5236\u5b9a\u300a\u4e2d\u56fd\u65b0\u95fb\u5de5\u4f5c\u8005\u804c\u4e1a\u9053\u5fb7\u51c6\u5219\u300b\u7684\u662f\u3002\nA. \u4e2d\u5171\u4e2d\u592e\u5ba3\u4f20\u90e8\nB. \u5404\u65b0\u95fb\u5355\u4f4d\nC. \u4e2d\u534e\u5168\u56fd\u8bb0\u8005\u534f\u4f1a\nD. \u4e2d\u534e\u5168\u56fd\u65b0\u95fb\u5de5\u4f5c\u8005\u534f\u4f1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5457169450634906, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.515255490938943, "HuggingFaceH4/zephyr-7b-beta": 0.817486175454862, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.515684394694087, "meta-llama/Meta-Llama-3-8B": 0.5321077706795383, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u8fd1\u671f\u5728\u6d77\u5e95\u53d1\u73b0\u4e86\u4e00\u79cd\u4fd7\u79f0\u201c\u53ef\u71c3\u51b0\u201d\u7684\u51b0\u5757\u72b6\u5929\u7136\u6c14\u6c34\u5408\u7269\u201d\u80fd\u6e90\u603b\u91cf\u53ef\u8fbe\u5168\u56fd\u77f3\u6cb9\u603b\u91cf\u7684_\u534a\uff0c\u71c3\u70e71t\u201c\u53ef\u71c3\u51b0\u201d\u91ca\u653e\u5c80\u7684\u80fd\u91cf\u4e0e164t\u5929\u7136\u6c14\u76f8\u5f53\uff0c\u7531\u6b64\u53ef\u5224\u65ad\u201c\u53ef\u71c3\u51b0\u201d\nA. \u53ea\u6709\u71c3\u70e7\u65f6\u624d\u5177\u6709\u5185\u80fd\nB. \u5177\u6709\u8f83\u9ad8\u7684\u5316\u5b66\u80fd\nC. \u6ca1\u6709\u71c3\u70e7\u65f6\u53ea\u5177\u6709\u5316\u5b66\u80fd\nD. \u5177\u6709\u8f83\u9ad8\u7684\u5185\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.667921593907727}}, {"question": "\u4eba\u6c11\u5f53\u5bb6\u4f5c\u4e3b\u662f\u793e\u4f1a\u4e3b\u4e49\u6c11\u4e3b\u653f\u6cbb\u7684\u672c\u8d28\u548c\u6838\u5fc3\u3002\u4eba\u6c11\u5f53\u5bb6\u4f5c\u4e3b\u7684\u6839\u672c\u4fdd\u8bc1\u662f\nA. \u670d\u52a1\u5927\u5c40\nB. \u6539\u9769\u521b\u65b0\nC. \u6267\u6cd5\u4e3a\u6c11\nD. \u515a\u7684\u9886\u5bfc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5303556252961568, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.9104979727430658, "HuggingFaceH4/zephyr-7b-beta": 0.9319948950237193, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9561056247639697, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8fdd\u80cc\u89c4\u5219\u201c\u6240\u6709\u524d\u63d0\u90fd\u5fc5\u987b\u662f\u771f\u7684\u201d\u7684\u8c2c\u8bef\u5305\u62ec\nA. \u4ee5\u4e0a\u90fd\u5bf9\nB. \u524d\u63d0\u865a\u5047\u8c2c\u8bef\nC. \u4e0d\u4e00\u81f4\u8c2c\u8bef\nD. \u9884\u671f\u7406\u7531\u8c2c\u8bef\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7289340962297768, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7193708958356646, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.46522741395200123}}, {"question": "\u5728\u5173\u4e8e\u504f\u5dee\u884c\u4e3a\u7684\u7406\u8bba\u89e3\u91ca\u4e2d\uff0c\u91cd\u89c6\u4e2a\u4eba\u7684\u7cbe\u795e\u3001\u60c5\u7eea\u3001\u6027\u683c\u7b49\u5fc3\u7406\u56e0\u7d20\u5bf9\u884c\u4e3a\u7684\u5f71\u54cd\u7684\u89c2\u70b9\u5c5e\u4e8e\nA. \u751f\u7269\u5b66\u89e3\u91ca\nB. \u793e\u4f1a\u5b66\u89e3\u91ca\nC. \u6587\u5316\u5b66\u89e3\u91ca\nD. \u5fc3\u7406\u5b66\u89e3\u91ca\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5303554031410938, "meta-math/MetaMath-Mistral-7B": 0.5980100100004178, "itpossible/Chinese-Mistral-7B-v0.1": 0.8705403677212601, "HuggingFaceH4/zephyr-7b-beta": 0.9805420957739548, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.93989335666815, "meta-llama/Meta-Llama-3-8B": 0.7330553619756993, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9194256507200173}}, {"question": "\u4e0b\u5217\u673a\u68b0\u4e2d\u3001\u5c5e\u4e8e\u9759\u538b\u78be\u538b\u673a\u68b0\u7684\u662f\nA. \u5c65\u5e26\u63a8\u571f\u673a\nB. \u7f8a\u811a\u78be\nC. \u592f\u677f\nD. \u632f\u52a8\u78be\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5314572424818412}}, {"question": "20\u4e16\u7eaa50\uff0d60\u5e74\u4ee3\uff0c\u897f\u65b9\u65b0\u95fb\u5b66\u548c\u4f20\u64ad\u5b66\u7814\u7a76\u4e2d\uff0c\u5bf9\u65b0\u95fb\u4ef7\u503c\u7684\u7814\u7a76\u9887\u4e3a\u7a81\u51fa\u7684\u662f\u76d6\u5c14\u987f\u548c\u9c81\u5947\u7684\nA. \u201c\u9009\u62e9\u6027\u4f20\u64ad\u6a21\u5f0f\u201d\nB. \u201c\u8f90\u5c04\u6027\u4f20\u64ad\u6a21\u5f0f\u201d\nC. \u201c\u9009\u62e9\u6027\u5b88\u95e8\u6a21\u5f0f\u201d\nD. \u201c\u975e\u7fa4\u4f53\u5316\u4f20\u64ad\u6a21\u5f0f\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.538821479308809, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4302342859755254, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8bcd\u8bed\u4e2d\u6709\u9519\u522b\u5b57\u7684\u2f00\u9879\u662f\nA. \u7981\u9522 \u5e37\u5e55 \u5439\u2f51\u6c42\u75b5 \u901a\u5bb5\u8fbe\u65e6\nB. \u555c\u6ce3 \u5591\u54d1 \u2eec\u2f3c\u534f\u2f12 \u8fe5\u4e4e\u4e0d\u540c\nC. \u60b2\u6006 \u7765\u7768 \u7c97\u5236\u70c2\u6e23 \u5473\u540c\u56bc\u814a\nD. \u64ba\u6387 \u5d14\u5dcd \u6b47\u65af\u5e95\u2fa5 \u9500\u58f0\u533f\u8ff9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2904324311152723, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3017509138664272, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2906893535433972, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u5916\u4ea4\u653f\u7b56\u7684\u5b97\u65e8\u6216\u57fa\u672c\u76ee\u6807\u662f\nA. \u5b9e\u73b0\u7766\u90bb\u53cb\u597d\nB. \u4e0e\u8d44\u672c\u4e3b\u4e49\u56fd\u5bb6\u548c\u5e73\u76f8\u5904\nC. \u4e0e\u53d1\u5c55\u4e2d\u56fd\u5bb6\u56e2\u7ed3\u5408\u4f5c\nD. \u7ef4\u62a4\u4e16\u754c\u548c\u5e73\uff0c\u4fc3\u8fdb\u5171\u540c\u53d1\u5c55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.951016280181213, "meta-math/MetaMath-Mistral-7B": 0.9892685530576822, "itpossible/Chinese-Mistral-7B-v0.1": 0.7977136660082318, "HuggingFaceH4/zephyr-7b-beta": 0.9998965513576895, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9659861158390087, "meta-llama/Meta-Llama-3-8B": 0.930092287999431, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9505501649417805}}, {"question": "\u9ad8\u4e00\u5b66\u751f\u5c0f\u5cf0\u7684\u7236\u6bcd\u4e0d\u5c65\u884c\u76d1\u62a4\u804c\u8d23\uff0c\u653e\u4efb\u5c0f\u5cf0\u5f3a\u884c\u7d22\u8981\u4ed6\u4eba\u8d22\u7269\uff0c\u4f9d\u636e\u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u9884\u9632\u672a\u6210\u5e74\u4eba\u72af\u7f6a\u6cd5\u300b\uff0c\u6709\u6743\u5bf9\u5c0f\u5cf0\u7236\u6bcd\u7ed9\u4e88\u8bad\u8beb\u7684\u662f\nA. \u6559\u80b2\u884c\u653f\u90e8\u95e8\nB. \u5b66\u6821\nC. \u516c\u5b89\u673a\u5173\nD. \u4eba\u6c11\u6cd5\u9662\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.40870135873933694, "meta-math/MetaMath-Mistral-7B": 0.5804388013254669, "itpossible/Chinese-Mistral-7B-v0.1": 0.5113041555810103, "HuggingFaceH4/zephyr-7b-beta": 0.7983649603588004, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.785986362372982, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9397800010804204}}, {"question": "\u81b3\u98df\u8c03\u67e5\u548c\u8bc4\u4ef7\u4e2d\u7ecf\u5e38\u4f7f\u7528\u7684\u8bb0\u8d26\u6cd5\u7684\u57fa\u7840\u662f\nA. \u81b3\u98df\u8d26\u76ee\u7684\u8bb0\u5f55\u4eba\u3002\nB. \u81b3\u98df\u8d26\u76ee\u3002\nC. \u96c6\u4f53\u81b3\u98df\u5185\u5bb9\u3002\nD. \u4e2a\u4f53\u81b3\u98df\u5185\u5bb9\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6574844165819318, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6109320230194251, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7448792443867791}}, {"question": "1896\u5e74\u5728\u300a\u4ec1\u5b66\u300b\u4e00\u4e66\u4e2d\u63d0\u51fa\u201c\u793e\u4f1a\u5b66\u201d\u540d\u79f0\u7684\u5b66\u8005\u662f\nA. \u6881\u542f\u8d85\nB. \u5eb7\u6709\u4e3a\nC. \u8c2d\u55e3\u540c\nD. \u4e25\u590d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.48691815367426367, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.28966338381871215, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u2f00\u4ef6\u4e0a\u2f90\u6253\u2f0b\u6298\u51fa\u552e \u5c31\u662f\u6309\u7167\u539f\u4ef7\u7684\nA. \u767e\u5206\u4e4b80\nB. \u767e\u5206\u4e4b120 \nC. \u767e\u5206\u4e4b100\nD. \u767e\u5206\u4e4b20 \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.28850952576306876, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6085790041501776}}, {"question": "\u6cbf\u76f4\u7ebf\u8fd0\u52a8\u7684\u7269\u4f53\uff0c\u5176\u901f\u5ea6\u4e0e\u65f6\u95f4\u6210\u53cd\u6bd4\uff0c\u5219\u5176\u52a0\u901f\u5ea6\u4e0e\u901f\u5ea6\u7684\u5173\u7cfb\u662f\nA. \u4e0e\u901f\u5ea6\u6210\u53cd\u6bd4\nB. \u4e0e\u901f\u5ea6\u6210\u6b63\u6bd4\nC. \u4e0e\u901f\u5ea6\u5e73\u65b9\u6210\u6b63\u6bd4\nD. \u4e0e\u901f\u5ea6\u5e73\u65b9\u6210\u53cd\u6bd4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.32871459371036227, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4101046003501243, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.42876490286251856}}, {"question": "\u5173\u4e8e\u4f1a\u8ba1\u8d26\u7c3f\u7684\u8bb0\u8d26\u89c4\u5219\uff0c\u4e0b\u5217\u8868\u8ff0\u4e0d\u6b63\u786e\u7684\u662f\nA. \u8bb0\u8d26\u65f6\u5e94\u4f7f\u7528\u84dd\u9ed1\u58a8\u6c34\u6216\u78b3\u7d20\u58a8\u6c34\u7684\u94a2\u7b14\u4e66\u5199\uff0c\u4e0d\u5f97\u4f7f\u7528\u5706\u73e0\u7b14(\u94f6\u884c\u7684\u590d\u5199\u8d26\u7c3f\u9664\u5916)\u6216\u94c5\u7b14\nB. \u5728\u4e0d\u8bbe\u501f\u8d37\u7b49\u680f\u7684\u591a\u680f\u5f0f\u8d26\u9875\u4e2d\uff0c\u767b\u8bb0\u51cf\u5c11\u6570\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u7ea2\u8272\u58a8\u6c34\u8bb0\u8d26\nC. \u8d26\u9875\u767b\u8bb0\u6ee1\u65f6\uff0c\u5e94\u529e\u7406\u8f6c\u9875\u624b\u7eed\nD. \u4f7f\u7528\u6d3b\u9875\u5f0f\u8d26\u7c3f\u65f6\uff0c\u5e94\u5148\u5c06\u5176\u88c5\u8ba2\u6210\u518c\uff0c\u4ee5\u9632\u6b62\u6563\u5931\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u4e0b\u5217\u5b9e\u9a8c\u64cd\u4f5c\u548c\u73b0\u8c61\uff0c\u6240\u5f97\u51fa\u7684\u7ed3\u8bba\u4e0d\u6b63\u786e\u7684\u662f\nA. \u5b9e\u9a8c\u64cd\u4f5c\uff1a\u5728\u9152\u7cbe\u706f\u4e0a\u52a0\u70ed\u94dd\u7b94\u3002\u5b9e\u9a8c\u73b0\u8c61:\u94dd\u7b94\u7194\u5316\u4f46\u4e0d\u6ef4\u843d\u3002\u7ed3\u8bba:\u7194\u70b9\uff1a\u6c27\u5316\u94dd>\u94dd\u3002\nB. \u5b9e\u9a8c\u64cd\u4f5c\uff1a\u5e38\u6e29\u4e0b\uff0c\u5411\u6d53\u786b\u9178\u4e2d\u6295\u5165\u94c1\u7247\u3002\u5b9e\u9a8c\u73b0\u8c61:\u94c1\u7247\u4e0d\u6eb6\u89e3\u3002\u7ed3\u8bba:\u5e38\u6e29\u4e0b\uff0c\u94c1\u4e0d\u4e0e\u6d53\u786b\u9178\u53cd\u5e94\u3002\nC. \u5b9e\u9a8c\u64cd\u4f5c\uff1a\u5411\u7845\u9178\u94a0\u6eb6\u6db2\u4e2d\u6ef4\u52a01\u6ef4\u915a\u915e\uff0c\u7136\u540e\u9010\u6ef4\u52a0\u5165\u7a00\u76d0\u9178\u81f3\u7ea2\u8272\u892a\u53bb\u3002\u5b9e\u9a8c\u73b0\u8c61:2 min\u540e\uff0c\u8bd5\u7ba1\u91cc\u51fa\u73b0\u51dd\u80f6\u3002\u7ed3\u8bba:\u9178\u6027\uff1a\u76d0\u9178>\u7845\u9178\u3002\nD. \u5b9e\u9a8c\u64cd\u4f5c\uff1a\u5411\u67d0\u6eb6\u6db2\u4e2d\u5148\u6ef4\u52a0KSCN\u6eb6\u6db2\uff0c\u518d\u6ef4\u52a0\u5c11\u91cf\u6c2f\u6c34\u3002\u5b9e\u9a8c\u73b0\u8c61:\u5148\u65e0\u660e\u663e\u73b0\u8c61\uff0c\u540e\u6eb6\u6db2\u53d8\u6210\u7ea2\u8272\u3002\u7ed3\u8bba:\u6eb6\u6db2\u4e2d\u542b\u6709Fe2\uff0b\uff0c\u6ca1\u6709Fe3\uff0b\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.34611314921428016, "itpossible/Chinese-Mistral-7B-v0.1": 0.3756998283262494, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3499320087587727, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5673062651753994}}, {"question": "\u201c\u751f\u4ea7\u5173\u7cfb\u662f\u4eba\u4eec\u5728\u751f\u4ea7\u4e2d\u7ed3\u6210\u7684\u4eba\u4e0e\u4eba\u4e4b\u95f4\u7684\u5173\u7cfb\u201d\uff0c\u8fd9\u4e2a\u5b9a\u4e49\u7684\u79cd\u5dee\u662f\nA. \u4eba\u4e0e\u4eba\u4e4b\u95f4\u7684\u5173\u7cfb\nB. \u5728\u751f\u4ea7\u4e2d\u7ed3\u6210\u7684\u5173\u7cfb\nC. \u4eba\u4eec\u5728\u751f\u4ea7\u4e2d\u7ed3\u6210\u7684\u4eba\u4e0e\u4eba\u4e4b\u95f4\u7684\nD. \u4eba\u4eec\u5728\u751f\u4ea7\u4e2d\u7ed3\u6210\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33110093121618506, "meta-math/MetaMath-Mistral-7B": 0.3615085256056106, "itpossible/Chinese-Mistral-7B-v0.1": 0.2766930999728216, "HuggingFaceH4/zephyr-7b-beta": 0.730344984173382, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5174255370910192, "meta-llama/Meta-Llama-3-8B": 0.6109588524261916, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.49309197311599057}}, {"question": "\u878d\u8d44\u79df\u5165\u56fa\u5b9a\u8d44\u4ea7\u89c6\u4e3a\u627f\u79df\u4f01\u4e1a\u7684\u8d44\u4ea7\uff0c\u4f53\u73b0\u4e86\u4f1a\u8ba1\u7684\nA. \u6743\u8d23\u53d1\u751f\u5236\u539f\u5219\nB. \u914d\u6bd4\u539f\u5219\nC. \u8c28\u614e\u539f\u5219\nD. \u5b9e\u8d28\u91cd\u4e8e\u5f62\u5f0f\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9570310485899679, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3376787962540697, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5\u67d0\u751f\u5b58\u7fa4\u4f5350\u5c81\u7684\u751f\u5b58\u4eba\u6570\u4e3a89509\u4eba\uff0c\u5f80\u540e5\u5e74\u7684\u6b7b\u4ea1\u7387\u5206\u522b\u4e3a0.006\uff0c0.007\uff0c0.009\uff0c0.012\u548c0.015\uff0c\u5219\u8be5\u7fa4\u4f5355\u5c81\u65f6\u7684\u751f\u5b58\u4eba\u6570\u4e3a( )\u3002\nA. 85206\nB. 87206\nC. 87509\nD. 86206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u6570\u662f7777776/7777777\uff0c\u4e59\u6570\u662f9999998/9999999\uff0c\u90a3\u4e48\u7532\u4e59\u4e24\u6570\u7684\u5173\u7cfb\u662f\nA. \u7532<\u4e59\nB. \u7532>\u4e59\nC. \u4e0d\u786e\u5b9a\nD. \u7532=\u4e59\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5303554031410936, "meta-llama/Meta-Llama-3-8B": 0.30308918294772175, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f8e\u56fd\u524d\u56fd\u52a1\u537f\u57fa\u8f9b\u683c\u66fe\u7ecf\u8bf4\u8fc7\uff1a\u201c\u4e8c\u5341\u4e00\u4e16\u7eaa\u7684\u56fd\u9645\u79e9\u5e8f\u4f1a\u51fa\u73b0\u4e00\u4e2a\u4f3c\u4e4e\u81ea\u76f8\u77db\u76fe\u7684\u7279\u70b9\uff1a\u4e00\u65b9\u9762\u6108\u6765\u6108\u5206\u6563\uff0c\u4e00\u65b9\u9762\u6108\u6765\u6108\u5168\u7403\u5316\u3002\u201d\u4e0b\u5217\u6700\u63a5\u8fd1\u6b64\u89c2\u70b9\u7684\u662f\nA. \u5357\u5317\u7684\u8d2b\u5bcc\u5dee\u8ddd\u52a0\u5267\nB. \u56fd\u9645\u653f\u6cbb\u7ecf\u6d4e\u65b0\u79e9\u5e8f\u5df2\u7ecf\u5efa\u7acb\nC. \u4e16\u754c\u591a\u6781\u5316\u4e0d\u53ef\u9006\u8f6c\nD. \u79f0\u9738\u4e0e\u53cd\u9738\u7684\u6597\u4e89\u5c06\u957f\u671f\u5b58\u5728\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36087106371472144, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.47617855622393623, "HuggingFaceH4/zephyr-7b-beta": 0.9948824313713501, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7457521552726173, "meta-llama/Meta-Llama-3-8B": 0.7100999106160798, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9421408507496896}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u4eea\u5668\u4f7f\u7528\u7684\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u632f\u8361\u5206\u6db2\u6f0f\u6597\u65f6\u5e94\u5173\u95ed\u5176\u73bb\u7483\u585e\u548c\u6d3b\u585e\nB. \u9525\u5f62\u74f6\u7528\u4f5c\u53cd\u5e94\u5bb9\u5668\u65f6\u4e00\u5b9a\u4e0d\u80fd\u52a0\u70ed\nC. \u94f6\u955c\u53cd\u5e94\u540e\u7684\u8bd5\u7ba1\u7528\u6d53\u785d\u9178\u6d17\u6da4\uff0c\u4ee5\u52a0\u5feb\u94f6\u7684\u6eb6\u89e3\nD. \u6ef4\u5b9a\u7ba1\u88c5\u6ef4\u5b9a\u6db2\u65f6\u5e94\u5148\u7528\u84b8\u998f\u6c34\u6da6\u6d17\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3454901351726419, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e0b\u5217\u6240\u8ff0\u5b9e\u4f8b\u4e2d\uff0c\u82e5\u4e0d\u8ba1\u7a7a\u6c14\u963b\u529b\uff0c\u673a\u68b0\u80fd\u5b88\u6052\u7684\u662f\nA. \u77f3\u5757\u81ea\u7531\u4e0b\u843d\u7684\u8fc7\u7a0b\nB. \u7535\u68af\u52a0\u901f\u4e0a\u5347\u7684\u8fc7\u7a0b\nC. \u5728\u7ad6\u76f4\u9762\u5185\u505a\u5300\u901f\u5706\u5468\u8fd0\u52a8\u7684\u7269\u4f53\nD. \u6728\u7bb1\u6cbf\u7c97\u7cd9\u659c\u9762\u5300\u901f\u4e0b\u6ed1\u7684\u8fc7\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4391498016136454, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6df7\u5408\u6240\u6709\u5236\u7ecf\u6d4ea\u662f\u4e0d\u540c\u6240\u6709\u5236\u7ecf\u6d4e\u6309\u7167\u4e00\u5b9a\u539f\u5219\u5b9e\u884c\u8054\u5408\u751f\u4ea7\u6216\u7ecf\u8425\u7684\u7ecf\u6d4e\u5f62\u5f0f b\u5176\u6027\u8d28\u662f\u516c\u6709\u5236\u7ecf\u6d4e c\u5176\u4e2d\u7684\u56fd\u6709\u3001\u96c6\u4f53\u6210\u5206\u90fd\u662f\u516c\u6709\u5236\u7ecf\u6d4e\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206 d\u5176\u6027\u8d28\u662f\u975e\u516c\u6709\u5236\u7ecf\u6d4e\nA. ad\nB. acd\nC. bc\nD. ac\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.354490488342194, "meta-math/MetaMath-Mistral-7B": 0.4961947569148644, "itpossible/Chinese-Mistral-7B-v0.1": 0.3051050775177134, "HuggingFaceH4/zephyr-7b-beta": 0.9761366690008606, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5011905291740351, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.37380586698049156}}, {"question": "\u968f\u7740\u5e02\u573a\u7ecf\u6d4e\u7684\u53d1\u8fbe\uff0c\u5c45\u7edf\u6cbb\u5730\u4f4d\u7684\u662f\nA. \u503a\u6743\u5173\u7cfb\nB. \u5951\u7ea6\u5173\u7cfb\nC. \u7269\u6743\u5173\u7cfb\nD. \u8d22\u4ea7\u4ea4\u6362\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4324113728502249, "HuggingFaceH4/zephyr-7b-beta": 0.7736291837432915, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4916237397704288, "meta-llama/Meta-Llama-3-8B": 0.3710689062049661, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5817880611366385}}, {"question": "1950\u5e74\uff0c\u4e1c\u5317\u4eba\u6c11\u653f\u5e9c\u89c4\u5b9a\u5728\u5927\u533a\u8303\u56f4\u5185\u5bf9\u7164\u70ad\u3001\u94a2\u6750\u7b49\u591a\u79cd\u751f\u4ea7\u8d44\u6599\u7edf\u4e00\u5206\u914d\uff0c\u968f\u540e\uff0c\u4e1c\u5317\u5730\u533a\u8ba1\u5212\u5206\u914d\u7684\u7269\u8d44\u79cd\u7c7b\u9010\u5e74\u589e\u52a0\u3002\u4ece1953\u5e74\u8d77\uff0c\u8ba1\u5212\u5206\u914d\u8c03\u62e8\u4f53\u5236\u5f00\u59cb\u5728\u5168\u56fd\u94fa\u5f00\u3002\u8fd9\u53cd\u6620\u4e86\u4e2d\u56fd\u8ba1\u5212\u7ecf\u6d4e\u4f53\u5236\nA. \u662f\u5728\u5baa\u6cd5\u539f\u5219\u4e0b\u5efa\u7acb\u7684\nB. \u662f\u65b0\u751f\u56fd\u5bb6\u653f\u6743\u7684\u57fa\u7840\nC. \u968f\u7740\u5de5\u4e1a\u5316\u5efa\u8bbe\u7684\u8fdb\u884c\u800c\u5efa\u7acb\nD. \u968f\u7740\u884c\u653f\u533a\u57df\u7684\u6269\u5927\u9010\u6b65\u5efa\u7acb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.391819045858275, "meta-math/MetaMath-Mistral-7B": 0.5789938192052226, "itpossible/Chinese-Mistral-7B-v0.1": 0.5246735360764785, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5801904574077725, "meta-llama/Meta-Llama-3-8B": 0.6040086498540921, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u8ff0\u4e0e\u75c5\u6bd2\u86cb\u767d\u8d28\u65e0\u5173\u7684\u4f5c\u7528\u662f\nA. \u5438\u9644\u4f5c\u7528\nB. \u5bf9\u8102\u6eb6\u5242\u654f\u611f\u6027\nC. \u75c5\u6bd2\u5305\u819c\u7684\u6210\u5206\nD. \u4fdd\u62a4\u6838\u9178\u4f5c\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9814302662877749, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8308947823799065}}, {"question": "\u73b0\u60a3\u7814\u7a76\u4e3b\u8981\u5206\u6790\u6307\u6807\u662f\nA. \u6b7b\u4ea1\u6784\u6210\u6bd4\nB. \u60a3\u75c5\u7387\nC. \u53d1\u75c5\u7387\nD. \u7eed\u53d1\u7387\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5751823872434219, "meta-llama/Meta-Llama-3-8B": 0.40870098779248626, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u636e\u300a\u5468\u793c\u00b7\u5929\u5b98\u5bb0\u300b\u4e2d\u8bb0\u8f7d\uff0c\u5468\u4ee3\u5df2\u51fa\u73b0\u4e86\u8bb8\u591a\u79cd\u5173\u4e8e\u917f\u9152\u65b9\u9762\u7684\u4e13\u804c\u5b98\u540f\uff0c\u5982\u638c\u63e1\u5168\u56fd\u201c\u9152\u4e4b\u653f\u4ee4\u201d\u7684\u5b98\u79f0\u505a\nA. \u9152\u4eba\nB. \u5927\u914b\nC. \u9152\u6b63\nD. \u91af\u4eba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.321937374146456, "meta-math/MetaMath-Mistral-7B": 0.3681585634507254, "itpossible/Chinese-Mistral-7B-v0.1": 0.6372738118257989, "HuggingFaceH4/zephyr-7b-beta": 0.6933321213205803, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5326190697185917, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8203397611212824}}, {"question": "\u4e00\u822c\u800c\u8a00\uff0c\u670d\u88c5\u3001\u978b\u5e3d\u6700\u9002\u5b9c\u91c7\u7528\u7684\u5206\u9500\u7b56\u7565\u662f\nA. \u72ec\u5bb6\u5206\u9500\nB. \u9009\u62e9\u5206\u9500\nC. \u6df7\u5408\u5206\u9500\nD. \u5bc6\u96c6\u5206\u9500\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5510\u8bd7\u6240\u63cf\u5199\u7684\u5185\u5bb9\u4e0e\u5bf9\u5e94\u7684\u4f53\u80b2\u9879\u76ee\u4e0d\u76f8\u7b26\u7684\u662f\nA. \u4e0a\u5f26\u660e\u6708\u534a\uff0c\u6fc0\u7bad\u6d41\u661f\u8fdc\u2014\u2014\u5c04\u7bad\nB. \u5fa1\u9a6c\u7275\u6765\u4eb2\u81ea\u8bd5\uff0c\u73e0\u7403\u5230\u5904\u7389\u8e44\u77e5\u2014\u2014\u9a6c\u7403\nC. \u6768\u6874\u51fb\u8282\u96f7\u9617\u9617\uff0c\u4e71\u6d41\u9f50\u8fdb\u58f0\u8f70\u7136\u2014\u2014\u6e38\u6cf3\nD. \u58ee\u5f92\u6052\u8d3e\u52c7\uff0c\u62d4\u62d2\u62b5\u957f\u6cb3\u2014\u2014\u62d4\u6cb3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.336112069202162, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4310339459413379, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.877286590289694, "meta-llama/Meta-Llama-3-8B": 0.47995611658487325, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.990156751880803}}, {"question": "\u5973\uff0c30\u5c81\u3002\u53d1\u73b0\u9888\u90e8\u5305\u57572\u5e74\uff0c\u5305\u5757\u9010\u6e10\u589e\u5927\uff0c\u65e0\u7532\u4ea2\u8868\u73b0\uff0c\u76ee\u524d\u6709\u618b\u95f7\u611f\u3002\u67e5\u4f53\uff1a\u53f3\u4fa7\u7532\u72b6\u817a\u53ef\u89e6\u53ca4cm\u00d73cm\u5305\u5757\uff0c\u5149\u6ed1\uff0c\u8d28\u97e7\uff0c\u968f\u541e\u54bd\u4e0a\u4e0b\u79fb\u52a8\uff0c\u65e0\u538b\u75db\uff0c\u672a\u89e6\u53ca\u80bf\u5927\u6dcb\u5df4\u7ed3\u3002\u6838\u7d20\u626b\u63cf\uff1a\u7532\u72b6\u817a\u53f3\u53f6\u6e29\u7ed3\u8282\u3002\u5efa\u8bae\u60a3\u8005\u624b\u672f\u6cbb\u7597\u6700\u4e3b\u8981\u7684\u4f9d\u636e\u662f\nA. \u53ef\u80fd\u7ee7\u53d1\u7532\u4ea2\nB. \u6709\u538b\u8feb\u75c7\u72b6\nC. \u6613\u53d1\u751f\u7ee7\u53d1\u611f\u67d3\nD. \u5305\u5757\u53ef\u80fd\u53d1\u751f\u7834\u88c2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4950129409306276, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4265880698013488, "meta-llama/Meta-Llama-3-8B": 0.4743223188344243, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u201c\u575a\u6301\u8ba1\u5212\u2f63\u80b2\u57fa\u672c\u56fd\u7b56\u4e0d\u52a8\u6447\u201d\u7684\u6839\u672c\u539f\u56e0\u662f\nA. \u6211\u56fd\u2f08\u2f1d\u4e1c\u591a\u2ec4\u5c11\nB. \u6211\u56fd\u2f08\u2f1d\u57fa\u6570\u2f24\nC. \u6211\u56fd\u2f08\u2f1d\u51fa\u73b0\u8d1f\u589e\u2ed3\nD. \u6211\u56fd\u2f08\u2f1d\u57fa\u6570\u2f29\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4716472028482283, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3932534368794713, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4438166905608482}}, {"question": "\u67d0\u60a3\u8005\u56e0\u8033\u9e23\u3001\u7729\u6655\u3001\u8033\u804b\u5230\u533b\u9662\u5c31\u8bca\uff0c\u4e34\u5e8a\u4e0a\u8bca\u65ad\u4e3a\u819c\u8ff7\u8def\u708e(\u7f8e\u5c3c\u5c14\u6c0f\u75c5)\uff0c\u5176\u539f\u56e0\u53ef\u80fd\u662f\u523a\u6fc0\u4ec0\u4e48\u7ed3\u6784?\nA. \u4f4d\u89c9\u611f\u53d7\u5668\nB. \u524d\u5ead\u795e\u7ecf\nC. \u8717\u795e\u7ecf\nD. \u9f13\u819c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u636e\u8bb0\u8f7d\uff0c\u4e3a\u4f7f\u63a2\u9669\u66f4\u6709\u6210\u6548\uff0c\u897f\u73ed\u7259\u56fd\u738b\u66fe\u7b54\u5e94\u54e5\u4f26\u5e03\u4eab\u6709\u65b0\u5927\u96461/10\u7684\u6536\u5165\uff0c\u5141\u8bb8\u9ea6\u54f2\u4f26\u8239\u961f\u62e5\u6709\u6240\u53d1\u73b0\u7684\u5c9b\u5c7f\u548c\u5927\u9646\u6536\u5165\u76841/20\uff0c\u540e\u53c8\u589e\u81f31/15\u3002\u8fd9\u4e00\u8bb0\u8f7d\u8bf4\u660e\nA. \u822a\u6d77\u4eba\u624d\u53d7\u5230\u897f\u73ed\u7259\u7684\u91cd\u89c6\nB. \u826f\u597d\u7ade\u4e89\u673a\u5236\u4fc3\u4f7f\u63a2\u9669\u6210\u529f\nC. \u65b0\u822a\u8def\u5f00\u8f9f\u5f97\u76ca\u4e8e\u738b\u6743\u652f\u6301\nD. \u897f\u73ed\u7259\u6700\u65e9\u5f00\u5c55\u4e86\u822a\u6d77\u63a2\u9669\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4903710114151317, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.32205625344145955, "HuggingFaceH4/zephyr-7b-beta": 0.9362085700131372, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6846434400007576, "meta-llama/Meta-Llama-3-8B": 0.594135424635511, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5400172004354575}}, {"question": "\u94c1\u8def\u516c\u53f8\u4e0e\u822a\u7a7a\u516c\u53f8\u90fd\u63d0\u4f9b\u5ba2\u8fd0\u670d\u52a1\uff0c\u4e24\u8005\u7684\u7ade\u4e89\u5173\u7cfb\u5c5e\u4e8e\nA. \u54c1\u724c\u7ade\u4e89\u8005\nB. \u4e00\u822c\u7ade\u4e89\u8005\nC. \u4ea7\u54c1\u5f62\u5f0f\u7ade\u4e89\u8005\nD. \u613f\u671b\u7ade\u4e89\u8005\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3785916682018518, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.37354500251984707, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6381112667598758}}, {"question": "\u518d\u7535\u5b50\u5de5\u7a0b\u4e2d\uff0c\u6548\u7387\u603b\u662f____\nA. \u7b49\u4e8e1\nB. \u5c0f\u4e8e1\nC. \u4e0d\u7b49\u4e8e1\nD. \u5927\u4e8e1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5613325662731318, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6941790996803653, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4fe1\u606f\u8fdb\u884c\u5747\u8861\u3001\u5168\u9762\u7684\u9632\u62a4\uff0c\u63d0\u9ad8\u6574\u4e2a\u7cfb\u7edf\u201c\u5b89\u5168\u6700\u4f4e\u70b9\u201d\u7684\u5b89\u5168\u6027\u80fd\uff0c\u8fd9\u79cd\u5b89\u5168\u539f\u5219\u88ab\u79f0\u4e3a\nA. \u6700\u5c0f\u7279\u6743\u539f\u5219\nB. \u6728\u6876\u539f\u5219\nC. \u7b49\u7ea7\u5316\u539f\u5219\nD. \u6700\u5c0f\u6cc4\u9732\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3837616204620929, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f57\u9a6c\u5e1d\u56fd\u5bf9\u57fa\u7763\u5f92\u8fdb\u884c\u7684\u6700\u540e\u4e00\u6b21\u5927\u89c4\u6a21\u9547\u538b\u53d1\u751f\u5728\u8c01\u7684\u6267\u653f\u65f6\u671f\u3002\nA. \u6234\u514b\u91cc\u5148\nB. \u56fe\u62c9\u771f\nC. \u5c3c\u7984\nD. \u72c4\u5965\u591a\u897f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.794616502031425, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5224\u65ad\u4e0b\u5217\u6709\u5173\u5316\u5b66\u57fa\u672c\u6982\u5ff5\u7684\u4f9d\u636e\u4e0d\u6b63\u786e\u7684\u662f\nA. \u5171\u4ef7\u5316\u5408\u7269\uff1a\u662f\u5426\u542b\u6709\u5171\u4ef7\u952e\nB. \u6c27\u5316\u8fd8\u539f\u53cd\u5e94\uff1a\u5143\u7d20\u5316\u5408\u4ef7\u662f\u5426\u53d8\u5316\nC. \u6eb6\u6db2\u4e0e\u80f6\u4f53\uff1a\u80fd\u5426\u53d1\u751f\u4e01\u8fbe\u5c14\u6548\u5e94\nD. \u5f3a\u5f31\u7535\u89e3\u8d28\uff1a\u5728\u6c34\u6eb6\u6db2\u6216\u7194\u878d\u72b6\u6001\u4e0b\u80fd\u5426\u5b8c\u5168\u7535\u79bb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u57fa\u91d1\u4f1a\u5c5e\u4e8e\nA. \u79c1\u4eba\u90e8\u95e8\nB. \u516c\u5171\u90e8\u95e8\nC. \u7b2c\u4e09\u90e8\u95e8\nD. \u7b2c\u4e8c\u90e8\u95e8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44326039420220215, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.37427443058792187, "HuggingFaceH4/zephyr-7b-beta": 0.9497820847250367, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7466987442793002, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8736398323866343}}, {"question": "\u5973\uff0c40\u5c81\u3002\u8fd1\u4e09\u5e74\u7ecf\u5e38\u4e8e\u6e05\u6668\u7a81\u53d1\u6655\u53a5\uff0c\u51fa\u51b7\u6c57\uff0c\u996e\u7cd6\u6c34\u540e\u75c7\u72b6\u7f13\u89e3\u3002B\u8d85\u793a\u80f0\u817a\u5360\u4f4d\uff0c\u7ea61.5cm\u3002\u8be5\u80bf\u7624\u597d\u53d1\u90e8\u4f4d\u4f9d\u6b21\u662f\nA. \u80f0\u5c3e\u3001\u80f0\u4f53\u3001\u80f0\u5934\nB. \u80f0\u5934\u4f53\u5c3e\u51e0\u7387\u57fa\u672c\u76f8\u7b49\nC. \u80f0\u5934\u3001\u80f0\u9888\u3001\u80f0\u4f53\nD. \u80f0\u5934\u3001\u80f0\u4f53\u3001\u80f0\u5c3e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0d\u6b63\u5f53\u5730\u5229\u7528\u653f\u5e9c\u548c\u516c\u4f17\u6240\u8d4b\u4e88\u7684\u6743\u5229\u548c\u6743\u5a01\u6765\u4e3a\u4e2a\u4eba\u6216\u4e2a\u4eba\u6240\u6548\u5fe0\u7684\u67d0\u4e2a\u96c6\u56e2\u8c0b\u53d6\u5229\u76ca\uff0c\u8fd9\u88ab\u79f0\u4e3a\nA. \u6050\u6016\u4e3b\u4e49\nB. \u8d3f\u8d42\nC. \u8150\u8d25\nD. \u4e0d\u5f53\u5f97\u5229\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8515981845153842, "meta-math/MetaMath-Mistral-7B": 0.958092113250578, "itpossible/Chinese-Mistral-7B-v0.1": 0.7480299734000285, "HuggingFaceH4/zephyr-7b-beta": 0.8637079125217458, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9232615763549183, "meta-llama/Meta-Llama-3-8B": 0.474820744722625, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.800737833722171}}, {"question": "\u7f8e\u56fd\u7387\u591a\u56fd\u90e8\u961f\u4e0e\u4f0a\u62c9\u514b\u4e4b\u95f4\u7684\u7b2c\u4e00\u6b21\u6d77\u6e7e\u6218\u4e89\u662f\u5728\u54ea\u4e00\u5e74\u53d1\u751f\u7684\nA. 1999\nB. 2003\nC. 1974\nD. 1990\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6057922852644291, "meta-math/MetaMath-Mistral-7B": 0.7386320631956763, "itpossible/Chinese-Mistral-7B-v0.1": 0.9007943634892108, "HuggingFaceH4/zephyr-7b-beta": 0.9741456289758003, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47331598501055683, "meta-llama/Meta-Llama-3-8B": 0.9633202994164457, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9705570371247297}}, {"question": "\u4eba\u4eec\u7684\u884c\u4e3a\u6240\u8981\u8fbe\u5230\u7684\u9053\u5fb7\u7ed3\u679c\u7684\u4e3b\u89c2\u610f\u56fe\u662f\nA. \u9053\u5fb7\u76ee\u7684\nB. \u9053\u5fb7\u8d23\u4efb\nC. \u9053\u5fb7\u624b\u6bb5\nD. \u9053\u5fb7\u54c1\u8d28\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6788682334187924, "meta-math/MetaMath-Mistral-7B": 0.8143911970126779, "itpossible/Chinese-Mistral-7B-v0.1": 0.41675004792424064, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5015219506515166, "meta-llama/Meta-Llama-3-8B": 0.8788403699899062, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.810165194192992}}, {"question": "\u4e8c\u6218\u540e\u671f\uff0c\u7f8e\u56fd\u79ef\u6781\u63a8\u52a8\u8054\u5408\u56fd\u548c\u5e03\u96f7\u987f\u68ee\u6797\u4f53\u7cfb\u7684\u5efa\u7acb\uff1b1949\u5e74\u7ec4\u5efa\u5317\u5927\u897f\u6d0b\u516c\u7ea6\u7ec4\u7ec7\uff1b1951\u5e74\u7b7e\u8ba2\u300a\u65e5\u7f8e\u5b89\u5168\u4fdd\u969c\u6761\u7ea6\u300b\u3002\u8fd9\u53cd\u6620\u4e86\u7f8e\u56fd\u4f01\u56fe\nA. \u5efa\u7acb\u7f8e\u56fd\u4e3b\u5bfc\u7684\u56fd\u9645\u79e9\u5e8f\nB. \u5c06\u672c\u56fd\u5229\u76ca\u51cc\u9a7e\u4e8e\u4ed6\u56fd\u5229\u76ca\u4e4b\u4e0a\nC. \u4ee5\u5408\u6cd5\u7684\u6b66\u529b\u5e72\u6d89\u522b\u56fd\u5185\u653f\nD. \u5168\u9762\u63a7\u5236\u897f\u6b27\u4e0e\u65e5\u672c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8826260286294263, "meta-math/MetaMath-Mistral-7B": 0.9937256317663556, "itpossible/Chinese-Mistral-7B-v0.1": 0.9051743588744859, "HuggingFaceH4/zephyr-7b-beta": 0.9996280972300312, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.983505687238546, "meta-llama/Meta-Llama-3-8B": 0.8830036094536751, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.99445942647299}}, {"question": "\u6cb3\u59c6\u6e21\u9057\u5740\u4f4d\u4e8e\nA. \u957f\u6c5f\u6d41\u57df\nB. \u96c5\u9c81\u85cf\u5e03\u6c5f\u6d41\u57df\nC. \u6dee\u6cb3\u6d41\u57df\nD. \u9ec4\u6cb3\u6d41\u57df\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8477927589078708, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4907843077095061}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u505a\u76f4\u7ebf\u8fd0\u52a8\u7684\u7269\u4f53\u7684\u52a0\u901f\u5ea6\u63cf\u8ff0\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u52a0\u901f\u5ea6\u5728\u6570\u503c\u4e0a\u7b49\u4e8e\u5355\u4f4d\u65f6\u95f4\u5185\u901f\u5ea6\u7684\u53d8\u5316\u91cf\nB. \u901f\u5ea6\u53d8\u5316\u8d8a\u6765\u8d8a\u5feb\uff0c\u52a0\u901f\u5ea6\u8d8a\u6765\u8d8a\u5c0f\nC. \u52a0\u901f\u5ea6\u4e3a\u6b63\uff0c\u7269\u4f53\u4e00\u5b9a\u52a0\u901f\nD. \u5f53\u52a0\u901f\u5ea6\u4e0e\u901f\u5ea6\u65b9\u5411\u76f8\u540c\u4e14\u53c8\u51cf\u5c0f\u65f6\uff0c\u7269\u4f53\u505a\u51cf\u901f\u8fd0\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.535729589125995, "meta-math/MetaMath-Mistral-7B": 0.8208620247589148, "itpossible/Chinese-Mistral-7B-v0.1": 0.4934439933154051, "HuggingFaceH4/zephyr-7b-beta": 0.8763613962834574, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6485356577060631, "meta-llama/Meta-Llama-3-8B": 0.44670212079699295, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8278185136898578}}, {"question": "\u5173\u4e8e\u52a8\u80fd\u3001\u673a\u68b0\u80fd\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u9519\u8bef\u7684\u662f\nA. \u5728\u5e73\u76f4\u516c\u8def\u4e0a\u884c\u9a76\u7684\u6c7d\u8f66\uff0c\u901f\u5ea6\u53d8\u5316\u65f6\u5b83\u7684\u673a\u68b0\u80fd\u4e5f\u53d8\u5316\nB. \u4e24\u8f86\u6c7d\u8f66\u4ee5\u76f8\u540c\u7684\u901f\u5ea6\u884c\u9a76\u65f6\uff0c\u8d28\u91cf\u5927\u7684\u52a8\u80fd\u5927\nC. \u4e00\u8f86\u6c7d\u8f66\u5177\u6709\u52a8\u80fd\uff0c\u5b83\u4e00\u5b9a\u5177\u6709\u673a\u68b0\u80fd\nD. \u67d0\u6c7d\u8f66\u7684\u673a\u68b0\u80fd\u589e\u52a0\u4e86\uff0c\u5b83\u7684\u8fd0\u52a8\u901f\u5ea6\u4e00\u5b9a\u662f\u52a0\u5927\u4e86\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3516042863233241, "HuggingFaceH4/zephyr-7b-beta": 0.7032264248002785, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5473230844577356, "meta-llama/Meta-Llama-3-8B": 0.371068906204966, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4871416572550292}}, {"question": "\u628a\u5fc3\u7406\u6d3b\u52a8\u6307\u5411\u4e8e\u5185\u5fc3\u4e16\u754c\u7684\u4eba\u683c\u7279\u5f81\u79f0\u4e3a\nA. \u5185\u5411\u4eba\u683c\nB. \u5916\u5411\u4eba\u683c\nC. \u5747\u8861\u6027\u4eba\u683c\nD. T\u578b\u4eba\u683c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6315211128289333, "meta-math/MetaMath-Mistral-7B": 0.9881241101759528, "itpossible/Chinese-Mistral-7B-v0.1": 0.6197193441312318, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.819087880994439, "meta-llama/Meta-Llama-3-8B": 0.9485278738910006, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9146771865710536}}, {"question": "\u65b0\u95fb\u4f20\u64ad\u8fc7\u7a0b\u7684\u4e3b\u8981\u73af\u8282\u662f\nA. \u9009\u62e9\u4e8b\u5b9e\u2014\u8bc4\u4ef7\u4e8b\u5b9e\u2014\u64ad\u62a5\u4f20\u9012\u2014\u53d7\u4f17\u63a5\u53d7\u2014\u4fe1\u606f\u53cd\u9988\nB. \u9009\u62e9\u4e8b\u5b9e\u2014\u52a0\u5de5\u5236\u4f5c\u2014\u64ad\u62a5\u4f20\u9012\u2014\u53d7\u4f17\u63a5\u53d7\u2014\u4fe1\u606f\u53cd\u9988\nC. \u9009\u62e9\u4e8b\u5b9e\u2014\u52a0\u5de5\u5236\u4f5c\u2014\u5206\u6790\u8bc4\u8bba\u2014\u53d7\u4f17\u63a5\u53d7\u2014\u4fe1\u606f\u590d\u5236\nD. \u9009\u62e9\u4e8b\u5b9e\u2014\u52a0\u5de5\u5236\u4f5c\u2014\u5206\u6790\u8bc4\u8bba\u2014\u53d7\u4f17\u63a5\u53d7\u2014\u4fe1\u606f\u53cd\u9988\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4839880091148901, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.417305741279239, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e0d\u5c5e\u4e8e6\u5927\u8425\u517b\u7d20\u7684\u662f\nA. \u77ff\u7269\u8d28\nB. \u8102\u8d28\nC. \u6c34\nD. \u81b3\u98df\u7ea4\u7ef4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u6d41\u96e8\u7684\u964d\u96e8\u7279\u6027\u662f[ ]\u3002\nA. \u964d\u96e8\u5f3a\u5ea6\u5c0f\uff0c\u96e8\u533a\u8303\u56f4\u5c0f\uff0c\u964d\u96e8\u5386\u65f6\u77ed\nB. \u964d\u96e8\u5f3a\u5ea6\u5c0f\uff0c\u96e8\u533a\u8303\u56f4\u5927\uff0c\u964d\u96e8\u5386\u65f6\u957f\nC. \u964d\u96e8\u5f3a\u5ea6\u5927\uff0c\u96e8\u533a\u8303\u56f4\u5927\uff0c\u964d\u96e8\u5386\u65f6\u957f\nD. \u964d\u96e8\u5f3a\u5ea6\u5927\uff0c\u96e8\u533a\u8303\u56f4\u5c0f\uff0c\u964d\u96e8\u5386\u65f6\u77ed\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53\u67d0\u6761\u7ea6\u7684\u82e5\u5e72\u6761\u6b3e\u4f20\u51fa\u540e\uff0c\u5149\u7eea\u7687\u5e1d\u53d1\u51fa\u201c\u53f0\u6e7e\u5272\u5219\u5929\u4e0b\u4eba\u5fc3\u7686\u53bb\uff0c\u6715\u4f55\u4ee5\u4e3a\u5929\u4e0b\u4e3b!\u201d\u7684\u54c0\u53f9\uff0c\u53f0\u7c4d\u4eac\u5b98\u53ca\u4e3e\u4eba\u7eb7\u7eb7\u58f0\u8a00\u201c\u4e0e\u5176\u751f\u4e3a\u964d\u864f\uff0c\u4e0d\u5982\u6b7b\u4e3a\u4e49\u6c11!\u201d\u8be5\u6761\u7ea6\u5e94\u662f\nA. \u300a\u5929\u6d25\u6761\u7ea6\u300b\nB. \u300a\u5357\u4eac\u6761\u7ea6\u300b\nC. \u300a\u5317\u4eac\u6761\u7ea6\u300b\nD. \u300a\u9a6c\u5173\u6761\u7ea6\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.44229881712550045, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3301226516819503, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7427531708856862}}, {"question": "\u636e\u300a\u5357\u6d77\u53bf\u5fd7\u300b\u8bb0\u8f7d\uff0c\u73e0\u6c5f\u4e09\u89d2\u6d32\u4e00\u5e26\u673a\u5668\u7f2b\u4e1d\u4e1a\u8d77\u6b65\u4e8e\u5341\u4e5d\u4e16\u7eaa\u4e03\u5341\u5e74\u4ee3\uff0c\u201c\u4e09\u56db\u5e74\u95f4\uff0c\u5357\uff08\u6d77\uff09\u3001\u987a\uff08\u5fb7\uff09\u4e24\u9091\u7ee7\u4e4b\u800c\u8d77\u8005\uff0c\u591a\u81f3\u767e\u6570\u5341\u5bb6\u3002\u201d\u4e0e\u6b64\u60c5\u5f62\u76f4\u63a5\u76f8\u5173\u7684\u662f\nA. \u65b0\u5f0f\u4f01\u4e1a\u7684\u5f15\u9886\u4f5c\u7528\nB. \u6e05\u653f\u5e9c\u9f13\u52b1\u6c11\u95f4\u8bbe\u5382\nC. \u5916\u56fd\u8d44\u672c\u8f93\u5165\u7684\u523a\u6fc0\nD. \u5e7f\u5dde\u88ab\u5217\u4e3a\u901a\u5546\u53e3\u5cb8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u4f4d\u7f8e\u56fd\u6559\u80b2\u5bb6\u5728\u5176\u8457\u4f5c\u300a\u6559\u80b2\u53f2\u6559\u79d1\u4e66\u300b\u4e2d\u5199\u9053:\u201c\u539f\u59cb\u793e\u4f1a\u7684\u6559\u80b2\u666e\u904d\u91c7\u7528\u7684\u65b9\u6cd5\u662f\u7b80) \u7684\u89c2\u70b9\u3002\u5355\u7684\u65e0\u610f\u8bc6\u7684\u6a21\u4eff\u3002\u201d\u8fd9\u4f53\u73b0\u7684\u662f()\nA. \u793e\u4f1a\u8d77\u6e90\u8bf4\nB. \u5fc3\u7406\u8d77\u6e90\u8bf4\nC. \u751f\u7269\u8d77\u6e90\u8bf4\nD. \u795e\u8bdd\u8d77\u6e90\u8bf4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44329471636561074, "meta-math/MetaMath-Mistral-7B": 0.7016211407371974, "itpossible/Chinese-Mistral-7B-v0.1": 0.5463494462366163, "HuggingFaceH4/zephyr-7b-beta": 0.885134711024336, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.784797440964822, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6d88\u9632\u8f66\u548c\u6d88\u9632\u6813\u7684\u989c\u8272\u662f\nA. \u9ec4\u8272\nB. \u7ea2\u8272\nC. \u767d\u8272\nD. \u5168\u5bf9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6146051360514011, "itpossible/Chinese-Mistral-7B-v0.1": 0.7252514621043089, "HuggingFaceH4/zephyr-7b-beta": 0.621319259804883, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7177912677867222, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u8ba1\u7b97\u65e5\u964d\u6c34\u91cf\u7684\u65e5\u5206\u754c\u662f\u4ece()\u65f6\u81f3( ) \u65f6\u3002\nA. 12~12\nB. 20~20\nC. 0~24\nD. 08~08\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ea4\u7ef4\u4f5c\u7269\u4e2d\u7684\u9ebb\u7c7b\uff0c\u5982\u82ce\u9ebb\u3001\u4e9a\u9ebb\u3001\u5927\u9ebb\u3001\u9ec4\u9ebb\u3001\u7ea2\u9ebb\u7684\u4ea7\u54c1\u662f\u5229\u7528\nA. \u79cd\u5b50\u8868\u9762\u7740\u751f\u7684\u7ea4\u7ef4\nB. \u53f6\u7247\u7684\u53f6\u7ea4\u7ef4\nC. \u6839\u7684\u5bfc\u7ba1\u7ea4\u7ef4\nD. \u830e\u6746\u7684\u97e7\u76ae\u7ea4\u7ef4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7990973025014861, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9775988597337479}}, {"question": "1\u5206\u5b50\u786c\u8102\u9170\u8f85\u9176A\u7ecf1\u6b21\u03b2\u6c27\u5316\u540e\uff0c\u5176\u4ea7\u7269(\u966416\u78b3\u8102\u9170\u8f85\u9176A\u5916\u7684\u5176\u4ed6\u4ea7\u7269)\u5f7b\u5e95\u6c27\u5316\u53ef\u51c0\u751f\u6210\u591a\u5c11\u5206\u5b50ATP\nA. 12\u5206\u5b50\nB. 9\u5206\u5b50\nC. 14\u5206\u5b50\nD. 5\u5206\u5b50\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4fdd\u8bc1\u7acb\u6cd5\u7684\u793e\u4f1a\u4e3b\u4e49\u65b9\u5411\u548c\u6027\u8d28\u7684\u91cd\u8981\u539f\u5219\u662f\nA. \u5408\u5baa\u6027\u4e0e\u6cd5\u5236\u7edf\u4e00\u7684\u539f\u5219\nB. \u603b\u7ed3\u56fd\u5185\u5b9e\u8df5\u548c\u501f\u9274\u5916\u56fd\u7ecf\u9a8c\u76f8\u7ed3\u5408\u7684\u539f\u5219\nC. \u7acb\u8db3\u5168\u5c40\u3001\u7edf\u7b79\u517c\u987e\u3001\u9002\u5f53\u5b89\u6392\u7684\u539f\u5219\nD. \u7ef4\u62a4\u6cd5\u7684\u7a33\u5b9a\u6027\u3001\u8fde\u7eed\u6027\u4e0e\u53ca\u65f6\u5e9f\u3001\u6539\u3001\u7acb\u76f8\u7ed3\u5408\u7684\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u7535\u89c6\u5267\u4e2d\u6709\u8fd9\u6837\u4e00\u4e2a\u60c5\u8282\uff1a1948\u5e74\u7684\u4e0a\u6d77\uff0c\u6709\u4eba\u5229\u7528\u7f8e\u5143\u548c\u9ec4\u91d1\u4e4b\u95f4\u7684\u4ef7\u683c\u6ce2\u52a8\u725f\u53d6\u5229\u6da6\u3002\u6709\u5b66\u8005\u6307\u51fa\u5176\u5b58\u5728\u53f2\u5b9e\u9519\u8bef\uff0c\u7406\u7531\u662f\u5f53\u65f6\nA. \u4e2d\u56fd\u52a0\u5165\u4e86\u300a\u5173\u7a0e\u4e0e\u8d38\u6613\u603b\u534f\u5b9a\u300b\nB. \u5904\u4e8e\u5e03\u96f7\u987f\u68ee\u6797\u4f53\u7cfb\u7684\u6846\u67b6\u4e0b\nC. \u4e2d\u56fd\u6b63\u9762\u4e34\u5185\u6218\u5168\u9762\u7206\u53d1\nD. \u4e0a\u6d77\u5df2\u7ecf\u6ca6\u9677\u4e3a\u201c\u5b64\u5c9b\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u8fd0\u52a8\u7684\u5408\u6210\u548c\u5206\u89e3\uff0c\u4ee5\u4e0b\u8bf4\u6cd5\u6b63\u786e\u7684\u9009\u9879\u662f\nA. \u5300\u53d8\u901f\u8fd0\u52a8\u7684\u8f68\u8ff9\u53ef\u4ee5\u662f\u76f4\u7ebf\uff0c\u4e5f\u53ef\u4ee5\u662f\u66f2\u7ebf\nB. \u5206\u8fd0\u52a8\u662f\u76f4\u7ebf\u8fd0\u52a8\uff0c\u90a3\u4e48\u5408\u8fd0\u52a8\u5fc5\u662f\u76f4\u7ebf\u8fd0\u52a8\nC. \u5408\u8fd0\u52a8\u7684\u65f6\u95f4\u7b49\u4e8e\u4e24\u4e2a\u5206\u8fd0\u52a8\u7684\u65f6\u95f4\u4e4b\u548c\nD. \u66f2\u7ebf\u8fd0\u52a8\u7684\u52a0\u901f\u5ea6\u65b9\u5411\u53ef\u80fd\u4e0e\u901f\u5ea6\u5728\u540c\u4e00\u76f4\u7ebf\u4e0a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.34624084333667404, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8432056577281237}}, {"question": "\u9886\u4e8b\u5a5a\u59fb\uff0c\u662f\u6307\u5728\u9a7b\u5728\u56fd\u4e0d\u53cd\u5bf9\u7684\u60c5\u51b5\u4e0b\uff0c\u4e00\u56fd\u6388\u6743\u5176\u9a7b\u5916\u9886\u4e8b\u6216\u5916\u4ea4\u4ee3\u8868\u4f9d\u5176\u672c\u56fd\u6cd5\u5f8b\u89c4\u5b9a\u7684\u65b9\u5f0f\u529e\u7406\u7ed3\u5a5a\u624b\u7eed\uff0c\u6210\u7acb\u5a5a\u59fb\u7684\u5236\u5ea6\uff0c\u5b83\u4e00\u822c\u9002\u7528\u4e8e\nA. \u9886\u4e8b\u5b98\u5458\u4e0e\u5916\u4ea4\u5b98\u5458\u7ed3\u5a5a\nB. \u5728\u9a7b\u5728\u56fd\u7684\u672c\u56fd\u4fa8\u6c11\u4e0e\u672c\u56fd\u4fa8\u6c11\u7ed3\u5a5a\nC. \u9a7b\u5728\u56fd\u516c\u6c11\u4e0e\u4efb\u4f55\u7b2c\u4e09\u56fd\u516c\u6c11\u7ed3\u5a5a\nD. \u9a7b\u5728\u56fd\u516c\u6c11\u4e0e\u672c\u56fd\u516c\u6c11\u7ed3\u5a5a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.374447994289981, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4220412342074068, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4862642968747983}}, {"question": "\u4e13\u95e8\u6559\u80b2\u662f\u4ee5\u4e0b\u5217\u54ea\u4e00\u9879\u4e3a\u57fa\u7840\u7684\uff0c\u80fd\u591f\u5728\u4eba\u7684\u4e00\u822c\u53d1\u5c55\u7684\u57fa\u7840\u4e0a\u4fc3\u8fdb\u4eba\u7684\u7279\u6b8a\u53d1\u5c55\nA. \u57fa\u7840\u6559\u80b2\nB. \u666e\u901a\u6559\u80b2\nC. \u9ad8\u7b49\u6559\u80b2\nD. \u804c\u4e1a\u6559\u80b2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6ce2\u901f\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u6ce2\u901f\u4e0e\u6ce2\u6e90\u7684\u9891\u7387\u6210\u6b63\u2f50\nB. \u6ce2\u901f\u7531\u4ecb\u8d28\u548c\u6ce2\u6e90\u5171\u540c\u51b3\u5b9a\nC. \u53cd\u6620\u4e86\u632f\u52a8\u5728\u4ecb\u8d28\u4e2d\u4f20\u64ad\u7684\u5feb\u6162\nD. \u53cd\u6620\u4e86\u8d28\u70b9\u632f\u52a8\u7684\u5feb\u6162\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9313017473347958, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8538473570995275, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6871317239661109}}, {"question": "\u5728\u5206\u5b50\u6d41\u884c\u75c5\u5b66\u7814\u7a76\u4e2d\uff0c\u9009\u62e9\u504f\u501a\u9664\u6837\u672c\u9009\u62e9\u504f\u501a\u5916\uff0c\u8fd8\u53ef\u80fd\u51fa\u73b0\nA. \u6df7\u6742\u504f\u501a\nB. \u68c0\u6d4b\u504f\u501a\nC. \u6807\u672c\u50a8\u5b58\u504f\u501a\nD. \u6807\u672c\u91c7\u96c6\u504f\u501a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f9d\u7167\u82f1\u56fd\u6cd5\uff0c\u4e0b\u5217\u5404\u9879\u4e2d\u4f1a\u4f7f\u5408\u540c\u65e0\u6548\u7684\u662f\nA. \u5728\u8ba4\u5b9a\u5f53\u4e8b\u4eba\u4e0a\u53d1\u751f\u9519\u8bef\nB. \u4e00\u65b9\u5f53\u4e8b\u4eba\u5224\u65ad\u4e0a\u7684\u9519\u8bef\nC. \u4e00\u65b9\u5f53\u4e8b\u4eba\u5bf9\u81ea\u5df1\u5c65\u7ea6\u80fd\u529b\u7684\u4f30\u8ba1\u9519\u8bef\nD. \u4e00\u65b9\u5f53\u4e8b\u4eba\u610f\u601d\u8868\u793a\u7684\u9519\u8bef\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.26463422059185693, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9093\u6069\u94ed\u662f\u4e2d\u56fd\u5171\u4ea7\u515a\u7b2c\u4e00\u6b21\u5168\u56fd\u4ee3\u8868\u5927\u4f1a13\u4f4d\u4ee3\u8868\u4e4b\u4e00\uff0c\u4ed6\u662f\nA. \u82d7\u65cf\nB. \u6c34\u65cf\nC. \u571f\u5bb6\u65cf\nD. \u5e03\u4f9d\u65cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5e74\u9f84\u572860\u5468\u5c81\u4ee5\u4e0a\u7684\u9a7e\u9a76\u4eba\u591a\u957f\u65f6\u95f4\u63d0\u4ea4\u4e00\u6b21\u8eab\u4f53\u6761\u4ef6\u8bc1\u660e\nA. \u6bcf1\u5e74\nB. \u6bcf6\u4e2a\u6708\nC. \u6bcf2\u5e74\nD. \u6bcf3\u5e74\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.30817677628857404, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u539f\u59cb\u793e\u4f1a\u662f\u7ba1\u7406\u601d\u60f3\u7684\u840c\u82bd\u9636\u6bb5\uff0c\u5176\u95f4\u51fa\u73b0\u4e86\u6309\u4eba\u7684\u81ea\u7136\u5c5e\u6027\u548c\u5404\u6210\u5458\u7684\u7279\u6b8a\u5174\u8da3\u4e0e\u80fd\u529b\u8fdb\u884c\u5206\u5de5\u7684\u5373\nA. \u7b80\u5355\u7684\u81ea\u7136\u52b3\u52a8\u5206\u5de5\u601d\u60f3\nB. \u539f\u59cb\u7684\u79d1\u5b66\u7ba1\u7406\u601d\u60f3\nC. \u6734\u7d20\u7684\u52b3\u52a8\u534f\u4f5c\u601d\u60f3\nD. \u539f\u59cb\u7684\u7ec4\u7ec7\u601d\u60f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7418232907263073, "meta-math/MetaMath-Mistral-7B": 0.8449501073546062, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9999893439398192, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9614257931598449, "meta-llama/Meta-Llama-3-8B": 0.8436447073733137, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6188849974258479}}, {"question": "\u8003\u8bd5\u662f\u6559\u5e08\u8bc4\u4ef7\u5b66\u751f\u5b66\u4e60\u6210\u6548\u7684\u91cd\u8981\u624b\u6bb5\uff0c\u4f46\u82e5\u6559\u5e08\u4ec5\u51ed\u8003\u8bd5\u6210\u7ee9\u6765\u8bc4\u4ef7\u5b66\u751f\uff0c\u5c31\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5b66\u751f\u4ea7\u751f\u201c\u4e66\u5446\u5b50\u578b\u201d\u6210\u5c31\u4e2d\u5fc3\u7684\u504f\u5411\u3002\u8fd9\u8bf4\u660e\u6559\u80b2\u5177\u6709 ()\nA. \u6b63\u5411\u9690\u6027\u529f\u80fd\nB. \u6b63\u5411\u663e\u6027\u529f\u80fd\nC. \u8d1f\u5411\u663e\u6027\u529f\u80fd\nD. \u8d1f\u5411\u9690\u6027\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3607860911977634, "HuggingFaceH4/zephyr-7b-beta": 0.46740694784045017, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3756998283262494}}, {"question": "\u5546\u4eba\u4e2d\u95f4\u5546\u548c\u4ee3\u7406\u4e2d\u95f4\u5546\u7684\u6839\u672c\u533a\u522b\u5728\u4e8e\nA. \u662f\u5426\u50a8\u5b58\u5546\u54c1\nB. \u662f\u5426\u62e5\u6709\u5546\u54c1\u6240\u6709\u6743\nC. \u662f\u5426\u8fd0\u9001\u5546\u54c1\nD. \u662f\u5426\u6279\u53d1\u5546\u54c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8527606787876142, "meta-math/MetaMath-Mistral-7B": 0.9647014701318731, "itpossible/Chinese-Mistral-7B-v0.1": 0.7804282248822639, "HuggingFaceH4/zephyr-7b-beta": 0.999929004072793, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9799662710100279, "meta-llama/Meta-Llama-3-8B": 0.48398800911489004, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.732240313586814}}, {"question": "\u98df\u7269\u5728\u80c3\u4e2d\u6392\u7a7a\u901f\u5ea6\u7531\u5feb\u5230\u6162\u7684\u6392\u5217\u987a\u5e8f\u4e3a\nA. \u7cd6\u3001\u86cb\u767d\u8d28\u3001\u8102\u80aa\nB. \u7cd6\u3001\u8102\u80aa\u3001\u86cb\u767d\u8d28\nC. \u86cb\u767d\u8d28\u3001\u8102\u80aa\u3001\u7cd6\nD. \u86cb\u767d\u8d28\u3001\u7cd6\u3001\u8102\u80aa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ee7\u53d1\u6027\u9897\u7c92\u6027\u56fa\u7f29\u80be\u5e38\u89c1\u4e8e\nA. \u6162\u6027\u80be\u76c2\u80be\u708e\nB. \u6025\u8fdb\u578b\u9ad8\u8840\u538b\nC. \u6162\u6027\u80be\u5c0f\u7403\u80be\u708e\nD. \u7f13\u8fdb\u578b\u9ad8\u8840\u538b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3023937904843768, "meta-math/MetaMath-Mistral-7B": 0.35686329776860143, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7888975091072902, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5303553913963677, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f7f\u6c11\u65cf\u5b9a\u4e49\u5efa\u7acb\u5728\u8fa9\u8bc1\u552f\u7269\u4e3b\u4e49\u548c\u5386\u53f2\u552f\u7269\u4e3b\u4e49\u7684\u57fa\u7840\u4e0a\uff0c\u4ece\u800c\u5f7b\u5e95\u5426\u5b9a\u4e86\u201c\u6c11\u65cf\u6587\u5316\u81ea\u6cbb\u201d\u8bba\u7684\u662f\nA. \u6069\u683c\u65af\nB. \u65af\u5927\u6797\nC. \u9a6c\u514b\u601d\nD. \u5217\u5b81\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4209626922472216, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9092386455420851}}, {"question": "\u5b97\u6559\u6539\u9769\u65f6\u51fa\u73b0\u7684\u65b0\u6559\uff0c\u82f1\u6587\u4e3aProtestantism\uff0c\u76f4\u8bd1\u201c\u6297\u8bae\u5b97\u201d\u3002\u8fd9\u91cc\u7684\u201c\u6297\u8bae\u201d\u662f\u6307\nA. \u6297\u8bae\u8d35\u65cf\u7684\u6743\u5a01\nB. \u6297\u8bae\u56fd\u738b\u7684\u6743\u5a01\nC. \u6297\u8bae\u300a\u5723\u7ecf\u300b\u7684\u6743\u5a01\nD. \u6297\u8bae\u5929\u4e3b\u6559\u4f1a\u7684\u6743\u5a01\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.45162374288183804, "itpossible/Chinese-Mistral-7B-v0.1": 0.6118235774105757, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5906171404968016, "meta-llama/Meta-Llama-3-8B": 0.7905366156252286, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8682049914633773}}, {"question": "\u4e0b\u5217\u5c5e\u4e8e\u201c\u7528\u5bd2\u8fdc\u5bd2\u201d\u6cbb\u6cd5\u7684\u662f\nA. \u9634\u76db\u614e\u7528\u5bd2\u836f\nB. \u5047\u5bd2\u614e\u7528\u5bd2\u836f\nC. \u51ac\u5b63\u614e\u7528\u5bd2\u836f\nD. \u9633\u865a\u614e\u7528\u5bd2\u836f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5047\u8a00\u5224\u65ad\u662f\u65ad\u5b9a\u4e00\u4e2a\u4e8b\u7269\u60c5\u51b5\u7684\u5b58\u5728\u662f\u53e6\u4e00\u4e2a\u4e8b\u7269\u60c5\u51b5\u5b58\u5728\u7684\u6761\u4ef6\u7684\u547d\u9898\uff0c\u5047\u8a00\u63a8\u7406\u662f\u6307\u5927\u524d\u63d0\u662f\u5047\u8a00\u5224\u65ad\u7684\u6f14\u7ece\u63a8\u7406\u3002\u4e0b\u5217\u4e0d\u5c5e\u4e8e\u5047\u8a00\u63a8\u7406\u7684\u662f\nA. \u8003\u8bd5\u6210\u7ee9\u4e0d\u597d\uff0c\u6216\u662f\u7531\u4e8e\u590d\u4e60\u65b9\u6cd5\u4e0d\u5bf9\uff0c\u6216\u662f\u7531\u4e8e\u4e34\u573a\u53d1\u6325\u4e0d\u597d\uff0c\u738b\u65b0\u540c\u5b66\u8fd9\u6b21\u8003\u5f97\u4e0d\u597d\u3002\u4e0d\u662f\u590d\u4e60\u65b9\u6cd5\u4e0d\u5bf9\uff0c\u6240\u4ee5\u4e00\u5b9a\u662f\u4e34\u573a\u53d1\u6325\u4e0d\u597d\nB. \u5982\u679c\u592a\u9633\u6652\u5f97\u5389\u5bb3\uff0c\u674e\u660e\u5c31\u4e0d\u4f1a\u53bb\u6e38\u6cf3\uff0c\u4eca\u5929\u592a\u9633\u6652\u5f97\u5f88\u5389\u5bb3\uff0c\u7531\u6b64\u53ef\u4ee5\u65ad\u5b9a\uff0c\u674e\u660e\u6ca1\u6709\u53bb\u6e38\u6cf3\nC. \u60f3\u5f53\u7ffb\u8bd1\u5c31\u8981\u5b66\u5916\u8bed\uff0c\u6211\u53c8\u4e0d\u60f3\u5f53\u7ffb\u8bd1\uff0c\u4f55\u5fc5\u8d39\u529b\u5b66\u5916\u8bed\nD. \u5982\u679c\u6ca1\u6709\u7279\u522b\u7684\u539f\u56e0\uff0c\u516c\u53f8\u4e00\u822c\u4e0d\u4f1a\u6279\u51c6\u804c\u5458\u4eec\u7684\u4e8b\u5047\u3002\u516c\u53f8\u6279\u51c6\u4e86\u804c\u5458\u9648\u5217\u7684\u4e8b\u5047\u7533\u8bf7\uff0c\u770b\u6765\u4e00\u5b9a\u6709\u7279\u522b\u7684\u539f\u56e0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "S2126G\u4ea4\u6362\u673a\u5982\u4f55\u5c06\u63a5\u2f1d\u8bbe\u7f6e\u4e3aTAG VLAN\u6a21\u5f0f\nA. switchport mode trunk\nB. switchport mode tag\nC. set port trunk on\nD. trunk on\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.377908886269584, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.577057409603595}}, {"question": "\u5173\u4e8e\u2f12\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u2f12\u2f00\u5b9a\u4ea7\u2f63\u5728\u4e24\u4e2a\u76f4\u63a5\u63a5\u89e6\u7684\u7269\u4f53\u4e4b\u95f4\nB. \u6ed1\u52a8\u6469\u64e6\u2f12\u7684\u2f45\u5411\u2f00\u5b9a\u4e0e\u7269\u4f53\u7684\u8fd0\u52a8\u2f45\u5411\u76f8\u53cd\nC. \u4f5c\u2f64\u2f12\u548c\u53cd\u4f5c\u2f64\u2f12\u7684\u5408\u2f12\u4e3a\u96f6\nD. \u2f12\u662f\u4f7f\u7269\u4f53\u4ea7\u2f63\u52a0\u901f\u5ea6\u7684\u539f\u56e0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7071163040943401, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7c7b\u57df\u754c\u9762\u65b9\u7a0b\u6cd5\u4e2d\uff0c\u4e0d\u80fd\u6c42\u7ebf\u6027\u4e0d\u53ef\u5206\u60c5\u51b5\u4e0b\u5206\u7c7b\u95ee\u9898\u8fd1\u4f3c\u6216\u7cbe\u786e\u89e3\u7684\u65b9\u6cd5\u662f\nA. \u57fa\u4e8e\u4e8c\u6b21\u51c6\u5219\u7684H-K\u7b97\u6cd5\nB. \u611f\u77e5\u5668\u7b97\u6cd5 \nC. \u52bf\u51fd\u6570\u6cd5\nD. \u4f2a\u9006\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6766365072118347, "itpossible/Chinese-Mistral-7B-v0.1": 0.3403147063768956, "HuggingFaceH4/zephyr-7b-beta": 0.8366075518026724, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4660103713187301, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.33065623127838456}}, {"question": "\u4e0b\u5217\u73b0\u8c61\u4e2d\uff0c\u5c5e\u4e8e\u6559\u80b2\u73b0\u8c61\u7684\u662f ()\nA. \u873b\u8713\u70b9\u6c34\nB. \u5ba3\u4f20\u90e8\u95e8\u5728\u5730\u94c1\u7ad9\u60ac\u6302\u793e\u4f1a\u4e3b\u4e49\u6838\u5fc3\u4ef7\u503c\u89c2\u6807\u8bed\nC. \u5076\u7136\u95f4\u5b66\u4f1a\u73e0\u5fc3\u7b97\nD. \u7329\u7329\u642c\u7bb1\u53d6\u98df\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43634032475656825, "meta-math/MetaMath-Mistral-7B": 0.7673920467061109, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4310339313236754, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8479447241481692}}, {"question": "\u300a\u6d45\u6790\u4e2d\u897f\u65b9\u4f20\u7edf\u6587\u5316\u7684\u5dee\u5f02\u4e0e\u4ea4\u878d\u300b\u4e00\u6587\u6307\u51fa\uff1a\u201c\u897f\u65b9\u6587\u5316\u5bf9\u4e2d\u56fd\u6587\u5316\u7684\u5438\u6536\u5b8c\u5168\u53d6\u51b3\u4e8e\u4e2d\u56fd\u6587\u5316\u7684\u9b45\u529b\u548c\u897f\u65b9\u6587\u5316\u53d1\u5c55\u7684\u5185\u5728\u8981\u6c42\u3002\u2026\u2026\u4e2d\u56fd\u6587\u5316\u5728\u66b4\u529b\u51b2\u649e\u4e0b\u88ab\u8feb\u5438\u6536\u897f\u65b9\u8fd1\u4ee3\u6587\u5316\uff0c\u4e0d\u5f97\u4e0d\u8fdb\u884c\u75db\u82e6\u7684\u8f6c\u578b\u4e0e\u5b17\u53d8\u3002\u201d\u8fd9\u8bf4\u660e\nA. \u6587\u660e\u7684\u4ea4\u878d\u548c\u6c72\u53d6\u5177\u6709\u660e\u663e\u7684\u5dee\u5f02\nB. \u4e2d\u56fd\u7684\u8fd1\u4ee3\u6587\u660e\u5bf9\u897f\u65b9\u5177\u6709\u5438\u5f15\u529b\nC. \u4e2d\u897f\u6587\u5316\u4ea4\u878d\u548c\u6c72\u53d6\u7684\u8fc7\u7a0b\u8f83\u76f8\u4f3c\nD. \u6587\u5316\u7684\u81ea\u8eab\u9b45\u529b\u51b3\u5b9a\u5176\u4f20\u64ad\u7684\u7a0b\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3893866359093139, "meta-math/MetaMath-Mistral-7B": 0.47853461901651106, "itpossible/Chinese-Mistral-7B-v0.1": 0.47340443864112025, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.719835198448391, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.46178020407749665}}, {"question": "\u5173\u4e8e\u6cd5\u5f8b\u5173\u7cfb\u7684\u6f14\u53d8\uff0c\u4e0b\u5217\u8868\u8ff0\u4e0d\u6b63\u786e\u7684\u6709\nA. \u5ba2\u89c2\u4e8b\u5b9e\u548c\u6cd5\u5f8b\u4e8b\u5b9e\u7684\u540e\u679c\u90fd\u662f\u786e\u5b9a\u7684\nB. \u5ba2\u89c2\u4e8b\u5b9e\u4e0d\u4ee5\u4eba\u7684\u610f\u5fd7\u4e3a\u8f6c\u79fb\uff0c\u7531\u4e8e\u6cd5\u5f8b\u4e8b\u5b9e\u662f\u5bf9\u5ba2\u89c2\u4e8b\u5b9e\u7684\u6cd5\u5f8b\u5316\uff0c\u56e0\u6b64\u6cd5\u5f8b\u4e8b\u5b9e\u4e5f\u662f\u6cd5\u5f8b\u7684\u660e\u786e\u89c4\u5b9a\uff0c\u4e0e\u4eba\u7684\u610f\u5fd7\u65e0\u5173\nC. \u5982\u679c\u6ca1\u6709\u76f8\u5e94\u6cd5\u5f8b\u89c4\u8303\u7684\u5b58\u5728\uff0c\u6cd5\u5f8b\u5173\u7cfb\u5c31\u4e0d\u53ef\u80fd\u53d1\u751f\u6f14\u53d8\nD. \u6ca1\u6709\u6cd5\u5f8b\u4e8b\u5b9e\u5c31\u4e0d\u4f1a\u5f62\u6210\u6cd5\u5f8b\u5173\u7cfb\uff0c\u6cd5\u5f8b\u4e5f\u5c31\u65e0\u6cd5\u4f5c\u7528\u4e8e\u4eba\u4eec\u7684\u884c\u4e3a\u548c\u793e\u4f1a\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.737942647044436, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7040161003735658}}, {"question": "\u5904\u65b9\u540d\u76ca\u667a\u4ec1\uff0c\u8c03\u5242\u65f6\u5e94\u4ed8\nA. \u871c\u7099\u54c1\nB. \u918b\u5236\u54c1\nC. \u7145\u5236\u54c1\nD. \u76d0\u7099\u54c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6211\u56fd\u7684\u6574\u4e2a\u80fd\u6e90\u7ed3\u6784\u4e2d\uff0c\u6211\u56fd\u7684\u4e3b\u8981\u80fd\u6e90\u662f\nA. \u6c34\u7535\nB. \u77f3\u6cb9\nC. \u5929\u7136\u6c14\nD. \u7164\u70ad\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9657881246851455, "meta-math/MetaMath-Mistral-7B": 0.9927929781656722, "itpossible/Chinese-Mistral-7B-v0.1": 0.9548643659925767, "HuggingFaceH4/zephyr-7b-beta": 0.9999544977180077, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9257098591208844, "meta-llama/Meta-Llama-3-8B": 0.9528292600159641, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9391863512787503}}, {"question": "\u94f6\u884c\u662f\u73b0\u4ee3\u7ecf\u6d4e\u6d3b\u52a8\u7684\u67a2\u7ebd\uff0c\u76ee\u524d\u4e2d\u56fd\u5546\u4e1a\u94f6\u884c\u7684\u4e3b\u8981\u4e1a\u52a1\u662f\nA. \u8bc1\u5238\u6295\u8d44\u3001\u53d1\u653e\u8d37\u6b3e\u548c\u529e\u7406\u7ed3\u7b97\nB. \u5438\u6536\u5b58\u6b3e\u3001\u53d1\u653e\u8d37\u6b3e\u548c\u529e\u7406\u7ed3\u7b97\nC. \u4fe1\u6258\u6295\u8d44\u3001\u5438\u6536\u5b58\u6b3e\u548c\u53d1\u653e\u8d37\u6b3e\nD. \u5546\u4e1a\u4fdd\u9669\u3001\u5438\u6536\u5b58\u6b3e\u548c\u53d1\u653e\u8d37\u6b3e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8927053025424027, "meta-math/MetaMath-Mistral-7B": 0.9778836933215996, "itpossible/Chinese-Mistral-7B-v0.1": 0.8791944100201964, "HuggingFaceH4/zephyr-7b-beta": 0.9959259421870239, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.930264268414507, "meta-llama/Meta-Llama-3-8B": 0.9735334621984364, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9902688197705157}}, {"question": "\u6309\u7167\u4f20\u7edf\u7684\u201c\u516d\u4e66\u201d\u4f53\u4f8b\uff0c\u201c\u4fdd\u201d\u5b57\u5e94\u5c5e\nA. \u4f1a\u610f\nB. \u5f62\u58f0\nC. \u6307\u4e8b\nD. \u8c61\u5f62\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u54ea\u4e2a\u4e0d\u5c5e\u4e8e\u6839\u636e\u601d\u7ef4\u51ed\u501f\u7269\u5206\u7c7b\u7684\nA. \u76f4\u89c2\u52a8\u4f5c\u601d\u7ef4\nB. \u903b\u8f91\u601d\u7ef4\nC. \u5e38\u89c4\u601d\u7ef4\nD. \u5f62\u8c61\u601d\u7ef4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u6cd5\u5f8b\u6548\u529b\u7684\u5f3a\u5f31\u7a0b\u5ea6\u4e0d\u540c\uff0c\u6cd5\u5f8b\u89c4\u5219\u53ef\u4ee5\u5206\u4e3a\nA. \u5f3a\u884c\u6027\u89c4\u5219\u548c\u4efb\u610f\u6027\u89c4\u5219\nB. \u8c03\u6574\u6027\u89c4\u5219\u548c\u6784\u6210\u6027\u89c4\u5219\nC. \u786e\u5b9a\u6027\u89c4\u5219\u548c\u51c6\u7528\u6027\u89c4\u5219\nD. \u6388\u6743\u6027\u89c4\u5219\u548c\u4e49\u52a1\u6027\u89c4\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u673a\u52a8\u8f66\u9a7e\u9a76\u4eba\u521d\u6b21\u7533\u9886\u9a7e\u9a76\u8bc1\u540e\u7684\u5b9e\u4e60\u671f\u662f\u591a\u957f\u65f6\u95f4\nA. 18\u4e2a\u6708\nB. 6\u4e2a\u6708\nC. 12\u4e2a\u6708\nD. 16\u4e2a\u6708\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34993237149116996, "meta-math/MetaMath-Mistral-7B": 0.48454909990184813, "itpossible/Chinese-Mistral-7B-v0.1": 0.41803714176624135, "HuggingFaceH4/zephyr-7b-beta": 0.8464282553027991, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47369843406030043, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bd7\u53e5\u201c\u611f\u65f6\u82b1\u6e85\u6cea\uff0c\u6068\u522b\u9e1f\u60ca\u5fc3\u201d\u7684\u4f5c\u8005\u662f\nA. \u674e\u5546\u9690\nB. \u674e\u767d\nC. \u675c\u7267\nD. \u675c\u752b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5896717830564916, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u521b\u7acb\u56fd\u9645\u793c\u8ba9\u8bf4\u7684\u662f\nA. \u610f\u5927\u5229\u7684\u5df4\u6258\u9c81\u65af\nB. \u82f1\u56fd\u7684\u6234\u897f\nC. \u7f8e\u56fd\u7684\u65af\u6258\u96f7\nD. \u8377\u5170\u7684\u80e1\u4f2f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.3872012688759771, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.453609445271565}}, {"question": "\u5173\u4e8e\u7ed5\u7ebf\u578b\u5f02\u6b65\u7535\u52a8\u673a\u8f6c\u5b50\u4e32\u9891\u654f\u53d8\u963b\u5668\u542f\u52a8\u65b9\u5f0f\u7684\u7279\u70b9\uff0c\u4e0b\u5217\u53d9\u8ff0\u9519\u8bef\u7684\u662f\nA. \u968f\u7740\u7535\u52a8\u673a\u8f6c\u901f\u7684\u589e\u9ad8\uff0c\u9891\u654f\u53d8\u963b\u5668\u81ea\u52a8\u5e73\u6ed1\u5730\u51cf\u5c0f\u963b\u6297\u503c\uff0c\u4ece\u800c\u53ef\u4ee5\u9650\u5236\u542f\u52a8\u7535\u6d41\nB. \u9891\u654f\u53d8\u963b\u5668\u76f8\u5f53\u4e8e\u4e00\u4e2a\u94c1\u5fc3\u635f\u8017\u5f88\u5927\u7684\u4e09\u76f8\u7535\u6297\u5668\nC. \u5728\u542f\u52a8\u8fc7\u7a0b\u4e2d\uff0c\u9891\u654f\u53d8\u963b\u5668\u7684\u7535\u6297\u503c\u548c\u5bf9\u5e94\u4e8e\u94c1\u5fc3\u6da1\u6d41\u635f\u8017\u7684\u7b49\u6548\u7535\u963b\u503c\u968f\u7740\u8f6c\u5b50\u7535\u6d41\u9891\u7387\u7684\u51cf\u5c0f\u800c\u81ea\u52a8\u589e\u52a0\nD. \u9891\u654f\u53d8\u963b\u5668\u4e0d\u9700\u7ecf\u8fc7\u5206\u7ea7\u5207\u6362\u7535\u963b\u5c31\u53ef\u4ee5\u4f7f\u7535\u673a\u5e73\u7a33\u542f\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1947\u5e74\uff0c\u6210\u7acb\u7b2c\u4e00\u6240\u516c\u5171\u5173\u7cfb\u5b66\u9662\u7684\u662f\nA. \u6ce2\u58eb\u987f\u5927\u5b66\nB. \u5251\u6865\u5927\u5b66\nC. \u8036\u9c81\u5927\u5b66\nD. \u54c8\u4f5b\u5927\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.34104602156755776}}, {"question": "\u7b97\u673a\u75c5\u6bd2\u662f\u4e00\u79cd\nA. \u7535\u5b50\u4eea\u5668\nB. \u751f\u7269\nC. \u786c\u4ef6\u635f\u574f\u4ea7\u751f\u7684\u7a0b\u5e8f\nD. \u7a0b\u5e8f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9332965566794694, "meta-math/MetaMath-Mistral-7B": 0.9975362755806265, "itpossible/Chinese-Mistral-7B-v0.1": 0.9085224907706837, "HuggingFaceH4/zephyr-7b-beta": 0.9995876964999931, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6451414200530219, "meta-llama/Meta-Llama-3-8B": 0.9537521044067989, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u81ea1962\u5e74\u4e16\u754c\u4e0a\u7b2c\u4e00\u4e2a\u673a\u5668\u4eba\u7814\u5236\u6210\u529f\u81f3\u4eca\u3002\u673a\u5668\u4eba\u7684\u53d1\u5c55\u7ecf\u5386\u4e01\u4e09\u4ee3\u3002\u5370\u64cd\u7eb5\u578b\u673a\u5668\u4eba\u3001\u81ea\u52a8\u578b\u673a\u5668\u4eba\u548c\u667a\u80fd\u578b\u673a\u5668\u4eba\u3002\u673a\u5668\u4eba\u7684\u4f7f\u7528\nA. \u63a8\u52a8\u4e86\u7ecf\u6d4e\u5168\u7403\u5316\u8fdb\u7a0b\nB. \u4f53\u73b0\u4e86\u79d1\u5b66\u6280\u672f\u7684\u7efc\u5408\u8fd0\u7528\nC. \u4fc3\u8fdb\u4e86\u7ecf\u6d4e\u6301\u7eed\u7e41\u8363\nD. \u7f13\u89e3\u4e86\u4eba\u53e3\u538b\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5198970006913297, "meta-math/MetaMath-Mistral-7B": 0.7475944922512405, "itpossible/Chinese-Mistral-7B-v0.1": 0.7360700625713289, "HuggingFaceH4/zephyr-7b-beta": 0.9837879098603962, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6906256778733588, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9657267726253275}}, {"question": "\u4e0b\u9762\u54ea\u4e2a\u5bb6\u5ead\u53ef\u80fd\u4ea7\u751fO\u8840\u578b\u7684\u5c0f\u5b69\nA. AB\u578b\u7236\u4eb2\u548cA\u578b\u6bcd\u4eb2\nB. AB\u8840\u578b\u7236\u4eb2\u548cO\u578b\u6bcd\u4eb2\nC. A\u8840\u578b\u592b\u5987\nD. AB\u578b\u7236\u4eb2\u548cB\u578b\u6bcd\u4eb2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u68c0\u9a8c\u8ba1\u6570\u8d44\u6599\u7684\u4e24\u79cd\u5c5e\u6027\u6216\u7279\u5f81\u4e4b\u95f4\u6709\u65e0\u5173\u8054\u65f6\uff0c\u5e38\u7528\u7684\u65b9\u6cd5\u4e3a\nA. t\u68c0\u9a8c\nB. u\u68c0\u9a8c\nC. $x^2$ \u68c0\u9a8c\nD. \u79e9\u548c\u68c0\u9a8c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9480156040369211, "meta-math/MetaMath-Mistral-7B": 0.9938129014782111, "itpossible/Chinese-Mistral-7B-v0.1": 0.6249131718570649, "HuggingFaceH4/zephyr-7b-beta": 0.9626424558151889, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9880871612284395, "meta-llama/Meta-Llama-3-8B": 0.8414862681367555, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.953883414657128}}, {"question": "\u7f50\u88c5\u98df\u54c1\u4e2d\u6700\u8010\u70ed\u6027\u6700\u5f3a\u7684\u5fae\u751f\u7269\u662f\nA. \u5927\u80a0\u6746\u83cc\nB. \u8089\u6bd2\u68ad\u72b6\u82bd\u5b62\u6746\u83cc\nC. \u6c99\u95e8\u6c0f\u83cc\nD. \u4e73\u9178\u83cc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5326190993940749, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4006796757977684, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5325311454773345}}, {"question": "\u516d\u5ea6\u7684\u6b63\u786e\u6b21\u5e8f\u662f\nA. \u6301\u6212\uff0c\u5e03\u65bd\uff0c\u5fcd\u8fb1\uff0c\u7cbe\u8fdb\uff0c\u7985\u5b9a\uff0c\u822c\u82e5\nB. \u5e03\u65bd\uff0c\u6301\u6212\uff0c\u5fcd\u8fb1\uff0c\u7cbe\u8fdb\uff0c\u7985\u5b9a\uff0c\u822c\u82e5\nC. \u7cbe\u8fdb\uff0c\u5fcd\u8fb1\u3001\u7985\u5b9a\uff0c\u5e03\u65bd\uff0c\u6301\u6212\uff0c\u822c\u82e5\nD. \u5fcd\u8fb1\uff0c\u6301\u6212\uff0c\u5e03\u65bd\uff0c\u7cbe\u8fdb\uff0c\u7985\u5b9a\uff0c\u822c\u82e5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.36692100238588393, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.42439545122303324, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e92\u60e0\u610f\u8bc6\u662f\u6307\u5728\u7ec4\u7ec7\u4e0e\u516c\u4f17\u7684\u4ea4\u5f80\u548c\u5408\u4f5c\u4e2d\uff0c\u5c06\u5e73\u7b49\u4e92\u5229\u3001\u8ffd\u6c42\uff08\uff09\u4f5c\u4e3a\u7ec4\u7ec7\u5904\u7406\u5404\u79cd\u60c5\u51b5\u7684\u884c\u4e3a\u51c6\u5219\uff0c\u5c06\u7ec4\u7ec7\u81ea\u8eab\u7684\u53d1\u5c55\u4e0e\u516c\u4f17\u7684\u53d1\u5c55\u8054\u7cfb\u8d77\u6765\uff0c\u4e89\u53d6\u65e2\u6709\u5229\u4e8e\u81ea\u5df2\u53c8\u6709\u5229\u4e8e\u5bf9\u65b9\uff0c\u4f7f\u7ec4\u7ec7\u548c\u516c\u4f17\u5171\u540c\u534f\u8c03\u53d1\u5c55\u3002\nA. \u793e\u4f1a\nB. \u76ee\u6807\nC. \u516c\u4f17\nD. \u53cc\u8d62\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9239358532556299, "meta-math/MetaMath-Mistral-7B": 0.9875127645312539, "itpossible/Chinese-Mistral-7B-v0.1": 0.8658084798228344, "HuggingFaceH4/zephyr-7b-beta": 0.9998034788941111, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9769455437510144, "meta-llama/Meta-Llama-3-8B": 0.9272221494090679, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9211661653440382}}, {"question": "\u5728\u65e5\u76ca\u6fc0\u70c8\u7684\u7efc\u5408\u56fd\u529b\u7ade\u4e89\u4e2d\uff0c\u6700\u4e3b\u8981\u7684\u7ade\u4e89\u662f\nA. \u519b\u4e8b\u529b\u91cf\nB. \u7ecf\u6d4e\u6280\u672f\nC. \u6587\u5316\u5e95\u8574\nD. \u653f\u6cbb\u6784\u67b6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8054766261870168, "meta-math/MetaMath-Mistral-7B": 0.9628112860603174, "itpossible/Chinese-Mistral-7B-v0.1": 0.877050020996413, "HuggingFaceH4/zephyr-7b-beta": 0.9953200951129888, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8997095211385184, "meta-llama/Meta-Llama-3-8B": 0.9375145338045845, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.980009193509161}}, {"question": "\u98df\u54c1\u751f\u4ea7\u7ecf\u8425\u4eba\u5458\u5e94\u5f53\u8fdb\u884c\u5065\u5eb7\u68c0\u67e5\u7684\u9891\u7387\u662f\nA. \u4e00\u5e742\u6b21\nB. \u4e00\u5e741\u6b21\nC. \u4e24\u5e741\u6b21\nD. \u4e09\u5e741\u6b21\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5013271136257224, "itpossible/Chinese-Mistral-7B-v0.1": 0.34040633907578005, "HuggingFaceH4/zephyr-7b-beta": 0.6654469912262677, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u56db\u4f4d\u90fd\u662f\u6211\u56fd\u5510\u671d\u6770\u51fa\u7684\u8bd7\u4eba\uff0c\u5176\u4e2d\u53f7\u79f0\u201c\u8bd7\u5723\u201d\u7684\u662f\nA. \u674e\u767d\nB. \u675c\u752b\nC. \u738b\u7ef4\nD. \u767d\u5c45\u6613\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.42694255809450027, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5907723563441586, "HuggingFaceH4/zephyr-7b-beta": 0.4134293444466014, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.33619507220286804, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53ea\u8981\u7ed9\u8db3\u4e86\u65f6\u95f4\u548c\u9002\u5f53\u7684\u6559\u5b66\uff0c\u51e0\u4e4e\u6240\u6709\u7684\u5b66\u751f\u5bf9\u51e0\u4e4e\u6240\u6709\u7684\u5185\u5bb9\u90fd\u80fd\u5230\u8fbe\u638c\u63e1\u7684\u7a0b\u5ea6\u3002\u8fd9\u662f\u3002\nA. \u53d1\u73b0\u5b66\u4e60\nB. \u610f\u4e49\u5b66\u4e60\nC. \u673a\u68b0\u5b66\u4e60\nD. \u638c\u63e1\u5b66\u4e60\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2947319625574089, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5751869733187936, "HuggingFaceH4/zephyr-7b-beta": 0.9865001382924412, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4101046147696096, "meta-llama/Meta-Llama-3-8B": 0.6409876728766498, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9480598586811116}}, {"question": "\u7537\u6027\uff0c55 \u5c81\u3002\u8170\u75db 3 \u4e2a\u6708\u3002\u67e5\u4f53\uff1aL4\u30015 \u6c34\u5e73\u68d8\u7a81\u95f4\u538b\u75db\u3002\u53cc\u4e0b\u80a2\u65e0\u7578\u5f62\uff0c\u808c\u529b\u6b63\u5e38\uff0c\u53cc\u4fa7\u819d\u53cd\u5c04\u6b63\u5e38\uff0c\u8ddf\u8171\u53cd\u5c04\u672a\u5f15\u51fa\uff1b\u53cc\u4e0b\u80a2\u80a2\u4f53\u6df1\u6d45\u611f\u89c9\u5bf9\u79f0\u3001\u6b63\u5e38\uff0c\u978d\u533a\u75db\u3001\u89e6\u89c9\u51cf\u9000\u3002\u4e3a\u660e\u786e\u8bca\u65ad\uff0c\u5e94\u9996\u9009\u7684\u68c0\u67e5\u662f\nA. MRI\nB. \u808c\u7535\u56fe\nC. \u810a\u9ad3\u9020\u5f71\nD. X\u7ebf\u7247\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5644026867267952, "meta-math/MetaMath-Mistral-7B": 0.979239314111536, "itpossible/Chinese-Mistral-7B-v0.1": 0.42207143316759105, "HuggingFaceH4/zephyr-7b-beta": 0.9991653755181504, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6278604581678042, "meta-llama/Meta-Llama-3-8B": 0.8286861778803402, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8674726534213979}}, {"question": "\u52a0\u5f3a\u5168\u793e\u4f1a\u7f51\u7edc\u5b89\u5168\u610f\u8bc6\u6559\u80b2\u57f9\u8bad\u5c31\u662f\u5efa\u8bbe\u7f51\u7edc\u5f3a\u56fd\u7684\u91cd\u8981\u4e3e\u63aa\uff0c\u5c31\u662f\u7ef4\u62a4\u7f51\u7edc\u7a7a\u95f4\u79e9\u5e8f\u3001\u4fdd\u969c\u7f51\u7edc\u5b89\u5168\u7684()\u5de5\u4f5c\u3002\nA. \u51b3\u5b9a\u6027\nB. \u6709\u6548\u6027\nC. \u57fa\u7840\u6027\nD. \u6839\u672c\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5690243407597597, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5710154657568272, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8099553944707734}}, {"question": "\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\uff0c\u8ba1\u7b97\u673a\u4e0d\u80fd\u591f\nA. \u5e2e\u7528\u6237\u4fdd\u5b58\u4e00\u4e9b\u91cd\u8981\u7684\u8d44\u6599\nB. \u5e2e\u7528\u6237\u5904\u7406\u6570\u636e\nC. \u5e2e\u7528\u6237\u7f8e\u5316\u7167\u7247\nD. \u81ea\u6211\u5f00\u53d1\u8f6f\u4ef6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8609457827785557, "meta-math/MetaMath-Mistral-7B": 0.8685861401818687, "itpossible/Chinese-Mistral-7B-v0.1": 0.8297599100053539, "HuggingFaceH4/zephyr-7b-beta": 0.9999669821969103, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9835123357192936, "meta-llama/Meta-Llama-3-8B": 0.9119843173754846, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6388\u6743\u6027\u89c4\u8303\u548c\u4e49\u52a1\u6027\u89c4\u8303\u7684\u5212\u5206\u4e3b\u8981\u662f\u4ece\uff08\uff09\u89d2\u5ea6\u51fa\u53d1\u7684\u3002\nA. \u6cd5\u5f8b\u89c4\u5219\u5185\u5bb9\u662f\u5426\u786e\u5b9a\nB. \u4fdd\u62a4\u6743\u76ca\nC. \u4e0d\u540c\u884c\u4e3a\u6a21\u5f0f\nD. \u6cd5\u5f8b\u6548\u529b\u7684\u5f3a\u5f31\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4733159701518363, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.759154392107288, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u7f3a\u9677\u4e2d\u80fd\u591f\u7531\u5de5\u9891\u8010\u538b\u8bd5\u9a8c\u8003\u6838\u7684\u662f\nA. \u9ad8\u538b\u7ed5\u7ec4\u548c\u4f4e\u538b\u7ed5\u7ec4\u5f15\u7ebf\u4e4b\u95f4\u7684\u7edd\u7f18\u8584\u5f31\nB. \u9ad8\u538b\u7ed5\u7ec4\u4e0e\u9ad8\u538b\u5206\u63a5\u5f15\u7ebf\u4e4b\u95f4\u7684\u7edd\u7f18\u8584\u5f31\nC. \u5916\u7ed5\u7ec4\u76f8\u95f4\u7edd\u7f18\u8ddd\u79bb\u8fc7\u5c0f\nD. \u7ed5\u7ec4\u531d\u95f4\u7edd\u7f18\u635f\u4f24\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5e38\u89c1\u7684\u6076\u610f\u4ee3\u7801\u7c7b\u578b\u6709\uff1a\u7279\u6d1b\u4f0a\u6728\u9a6c\u3001\u8815\u866b\u3001\u75c5\u6bd2\u3001\u540e\u95e8\u3001Rootkit\u3001\u50f5\u5c38\u7a0b\u5e8f\u3001\u5e7f\u544a\u8f6f\u4ef6\u30022017\u5e745\u6708\u7206\u53d1\u7684\u6076\u610f\u4ee3\u7801WannaCry\u52d2\u7d22\u8f6f\u4ef6\u5c5e\u4e8e\nA. \u8815\u866b\nB. Rootkit\nC. \u7279\u6d1b\u4f0a\u6728\u9a6c\nD. \u540e\u95e8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b0\u7eb9\u72b6\u4f53\u662f\u6307\nA. \u5c3e\u72b6\u6838\nB. \u8c46\u72b6\u6838\nC. \u5c3e\u72b6\u6838\u548c\u8c46\u72b6\u6838\nD. \u5c3e\u72b6\u6838\u548c\u58f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4e8e\u7ec4\u7ec7\u53d8\u9769\u7684\u76ee\u6807\u63cf\u8ff0\u4e0d\u6b63\u786e\u7684\u662f\nA. \u4f7f\u5458\u5de5\u66f4\u5177\u73af\u5883\u9002\u5e94\u6027\nB. \u4f7f\u8463\u4e8b\u4f1a\u66f4\u5177\u73af\u5883\u9002\u5e94\u6027\nC. \u4f7f\u7ec4\u7ec7\u66f4\u5177\u73af\u5883\u9002\u5e94\u6027\nD. \u4f7f\u7ba1\u7406\u8005\u66f4\u5177\u73af\u5883\u9002\u5e94\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.49784669541621124, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7779238078442172, "meta-llama/Meta-Llama-3-8B": 0.40101791712441026, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6920565185612898}}, {"question": "\u793e\u4f1a\u540c\u751f\u7269\u4e00\u6837\u4e5f\u662f\u4e00\u4e2a\u6709\u673a\u4f53\uff0c\u4f46\u793e\u4f1a\u4e0d\u662f\u7b80\u5355\u7684\u6709\u673a\u4f53\u800c\u662f\u201c\u8d85\u6709\u673a\u4f53\u201d\uff0c\u6301\u8fd9\u79cd\u89c2\u70b9\u7684\u793e\u4f1a\u5b66\u8005\u662f\nA. \u97e6\u4f2f\nB. \u6d82\u5c14\u5e72\nC. \u65af\u5bbe\u585e\nD. \u5b54\u5fb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4430448139737191, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53\u4ee3\u6559\u80b2\u7cfb\u7edf\u4e2d\uff0c\u6838\u5fc3\u662f()\u3002\nA. \u6c11\u4fd7\u6559\u80b2\nB. \u793e\u4f1a\u6559\u80b2\nC. \u5b66\u6821\u6559\u80b2\nD. \u5bb6\u5ead\u6559\u80b2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5096420131943612, "meta-math/MetaMath-Mistral-7B": 0.74424784661452, "itpossible/Chinese-Mistral-7B-v0.1": 0.7990972809712856, "HuggingFaceH4/zephyr-7b-beta": 0.8214054796385718, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7030650230876017, "meta-llama/Meta-Llama-3-8B": 0.8015779778707995, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8181482976597639}}, {"question": "\u6700\u65e9\u5230\u4e2d\u56fd\u4f20\u6559\u7684\u8036\u7a23\u6559\u4f1a\u4eba\u5458\u662f\nA. \u6c99\u52ff\u7565\nB. \u7f57\u8000\u62c9\nC. \u5229\u739b\u7aa6\nD. \u7f57\u660e\u575a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2906893535433972, "HuggingFaceH4/zephyr-7b-beta": 0.5499200190222969, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.644645461392185, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.501170368616006}}, {"question": "\u5728\u7ec4\u7ec7\u51b3\u7b56\u4e2d\uff0c\u6ce8\u91cd\u56de\u6eaf\u5206\u6790\u7684\u662f\nA. \u98ce\u9669\u578b\u51b3\u7b56\nB. \u6218\u7565\u51b3\u7b56\nC. \u8ffd\u8e2a\u51b3\u7b56\nD. \u5371\u673a\u51b3\u7b56\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.490778006785068, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5540052805157856, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9741416840812187}}, {"question": "\u4ee5\u4e0b\u54ea\u4f4d\u5973\u6027\u7684\u540d\u5b57\u88ab\u7528\u6765\u547d\u540d\u5916\u592a\u7a7a\u73af\u5f62\u5c71\nA. \u73ed\u662d\nB. \u8521\u6587\u59ec\nC. \u738b\u662d\u541b\nD. \u674e\u6e05\u7167\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6f5c\u6c34\u5458\u4ece\u6df1\u6c34\u4e2d\u8fc7\u5feb\u5730\u5347\u5411\u6c34\u9762\uff0c\u5bb9\u6613\u53d1\u751f\nA. \u80ba\u6c34\u80bf\nB. \u4e8c\u6c27\u5316\u78b3\u6813\u585e\nC. \u80ba\u4e0d\u5f20\nD. \u6c2e\u6c14\u6813\u585e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6070726822542748, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e3a\u4e86\u89e3\u4e24\u79cd\u6cbb\u7597\u65b9\u6cd5\u5bf9\u539f\u53d1\u6027\u809d\u764c\u7684\u7597\u6548\uff0c\u5c06\u75c5\u4eba\u968f\u673a\u5206\u6210\u4e24\u7ec4\uff0c\u4e00\u7ec4\u4f7f\u75285-\u6c1f\u5c3f\u5627\u5576\u5341\u8f85\u52a9\u7597\u6cd5(\u7b80\u79f05-\u6c1f\u7ec4)\uff0c\u53e6\u4e00\u7ec4\u4f7f\u7528\u5b89\u6170\u5242\u5341\u8f85\u52a9\u7597\u6cd5(\u7b80\u79f0\u5b89\u6170\u7ec4)\u3002\u6cbb\u7597\u7ed3\u679c\u6309\u7f13\u89e3\u3001\u6b7b\u4ea1\u5212\u5206\u30025-\u6c1f\u7ec412\u4eba\uff0c\u5176\u4e2d7\u4eba\u7f13\u89e3\uff0c5\u4eba\u6b7b\u4ea1;\u5b89\u6170\u7ec411\u4eba\uff0c\u5176\u4e2d4\u4eba\u7f13\u89e3\uff0c7\u4eba\u6b7b\u4ea1\u3002\u5728\u5206\u6790\u4e24\u79cd\u7597\u6cd5\u7684\u7597\u6548\u5dee\u5f02\u6709\u65e0\u7edf\u8ba1\u5b66\u610f\u4e49\u65f6\uff0c\u5e94\u9009\u7528\u7684\u7edf\u8ba1\u5b66\u5206\u6790\u65b9\u6cd5\u662f\nA. logistic\u56de\u5f52\u5206\u6790\nB. Fisher\u7cbe\u786e\u68c0\u9a8c\nC. $\\chi^2$\u68c0\u9a8c\nD. Ridit\u5206\u6790\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9393116493690101, "meta-math/MetaMath-Mistral-7B": 0.9981672630348212, "itpossible/Chinese-Mistral-7B-v0.1": 0.8583109303407713, "HuggingFaceH4/zephyr-7b-beta": 0.9999622655975114, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9645737404172353, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5602443268860681}}, {"question": "\u4e0b\u5217\u4e0d\u7b26\u5408\u809d\u786c\u5316\u5e76\u53d1\u809d\u80be\u7efc\u5408\u5f81\u63cf\u8ff0\u7684\u662f\nA. \u5c3f\u86cb\u767d\u5e38\uff1e500mg/d\nB. \u51fa\u73b0\u5728\u5408\u5e76\u8179\u6c34\u7684\u60a3\u8005\nC. \u8840\u94a0\u548c\u5c3f\u94a0\u5747\u964d\u4f4e\nD. \u6269\u8840\u7ba1\u7269\u8d28\u4e0d\u80fd\u88ab\u706d\u6d3b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u79e6\u6c49\u65f6\u671f\uff0c\u4e2d\u56fd\u54f2\u5b66\u79f0\u4e3a\uff1a\nA. \u54f2\u5b66\nB. \u9053\u672f\u4e4b\u5b66\nC. \u4ee5\u4e0a\u90fd\u4e0d\u5bf9\nD. \u4e49\u7406\u4e4b\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5188673511055544, "meta-math/MetaMath-Mistral-7B": 0.8015779968311225, "itpossible/Chinese-Mistral-7B-v0.1": 0.5076856194232978, "HuggingFaceH4/zephyr-7b-beta": 0.9918375060336904, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8596323298023958, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6322402621594003}}, {"question": "\u81ea\u7136\u754c\u7684\u6c34\u6587\u5faa\u73af\u4f7f\u6c34\u8d44\u6e90\u5177\u6709[ ]\u3002\nA. \u975e\u518d\u751f\u6027\nB. \u968f\u673a\u6027\nC. \u5730\u533a\u6027\nD. \u518d\u751f\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7325785732368395, "meta-math/MetaMath-Mistral-7B": 0.9746190360492364, "itpossible/Chinese-Mistral-7B-v0.1": 0.8009320978906399, "HuggingFaceH4/zephyr-7b-beta": 0.9671538732622329, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8303532741518754, "meta-llama/Meta-Llama-3-8B": 0.8989788905757718, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u4eec\u77e5\u89c9\u5230\u6708\u4eae\u5728\u52a8\uff0c\u800c\u6d6e\u4e91\u662f\u9759\u6b62\u7684\uff0c\u662f\u56e0\u4e3a\nA. \u771f\u5b9e\u8fd0\u52a8\nB. \u8bf1\u53d1\u8fd0\u52a8\nC. \u81ea\u4e3b\u8fd0\u52a8\nD. \u52a8\u666f\u8fd0\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3157387487413704, "meta-math/MetaMath-Mistral-7B": 0.4336343088283542, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u54e6\uff0c\u9999\u96ea\u300b\u4e2d\u70ed\u60c5\u5730\u529d\u9999\u96ea\u7559\u5728\u8bdd\u5c71\u53e3\u7ad9\u81ea\u5df1\u4eb2\u621a\u5bb6\u4f4f\u4e00\u591c\u518d\u56de\u53f0\u513f\u6c9f\u7684\u4eba\u662f\nA. \u5973\u5927\u5b66\u751f\nB. \u51e4\u5a07\nC. \u201c\u5317\u4eac\u8bdd\u201d\nD. \u4e2d\u5e74\u5973\u670d\u52a1\u5458\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u4f8b\u7532\u72b6\u817a\u80bf\u7624\uff0c\u5448\u6d78\u6da6\u6027\u751f\u957f\u3002\u955c\u4e0b\u89c1\u764c\u7ec6\u80de\u5448\u6ee4\u6ce1\u72b6\u6392\u5217\uff0c\u7ec6\u80de\u6838\u5448\u6bdb\u73bb\u7483\u72b6\uff0c\u6838\u91cd\u53e0\u6838\u6c9f\u660e\u663e\u3002\u5e94\u8bca\u65ad\u4e3a\nA. \u6ee4\u6ce1\u72b6\u764c\nB. \u4e73\u5934\u72b6\u764c\nC. \u9ad3\u6837\u764c\nD. \u672a\u5206\u5316\u764c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u808c\u7275\u5f20\u53cd\u5c04\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u662f\u7ef4\u6301\u59ff\u52bf\u7684\u57fa\u672c\u53cd\u5c04\nB. \u808c\u68ad\u662f\u808c\u7275\u5f20\u53cd\u5c04\u7684\u611f\u53d7\u5668\nC. \u810a\u9ad3\u6a2a\u65ad\u540e\uff0c\u808c\u7275\u5f20\u53cd\u5c04\u6c38\u4e45\u6d88\u5931\nD. \u53cd\u5c04\u7684\u57fa\u672c\u4e2d\u67a2\u4f4d\u4e8e\u810a\u9ad3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44590204181482285, "meta-math/MetaMath-Mistral-7B": 0.837053632477708, "itpossible/Chinese-Mistral-7B-v0.1": 0.4025265304015081, "HuggingFaceH4/zephyr-7b-beta": 0.981182098154418, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9022400857861662, "meta-llama/Meta-Llama-3-8B": 0.8383749224227797, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9766522982550808}}, {"question": "\u5173\u4e8e\u5168\u9762\u4f9d\u6cd5\u6cbb\u56fd\u5fc5\u987b\u575a\u6301\u7684\u57fa\u672c\u539f\u5219\uff0c\u4e0b\u5217\u7406\u89e3\u6b63\u786e\u7684\u662f\nA. \u4ece\u4e2d\u56fd\u5b9e\u9645\u51fa\u53d1\u539f\u5219\u5c31\u662f\u8981\u62d2\u7edd\u79fb\u690d\u548c\u501f\u9274\u5176\u4ed6\u56fd\u5bb6\u7684\u5236\u5ea6\u548c\u7ecf\u9a8c\nB. \u4eba\u6c11\u4e3b\u4f53\u5730\u4f4d\u539f\u5219\u662f\u6307\u4e00\u5207\u6cd5\u5f8b\u6d3b\u52a8\u90fd\u5e94\u5f53\u4ea4\u7ed9\u5168\u4f53\u516c\u6c11\u6765\u5b8c\u6210\nC. \u6cd5\u6cbb\u4e0e\u5fb7\u6cbb\u76f8\u7ed3\u5408\u8981\u6c42\u5c06\u9053\u5fb7\u7edf\u4e00\u5230\u6cd5\u5f8b\u4e2d\u6765\nD. \u6cd5\u5f8b\u9762\u524d\u4eba\u4eba\u5e73\u7b49\u662f\u5168\u9762\u4f9d\u6cd5\u6cbb\u56fd\u7684\u4e00\u9879\u57fa\u672c\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6584492140177101, "meta-math/MetaMath-Mistral-7B": 0.9581704025877732, "itpossible/Chinese-Mistral-7B-v0.1": 0.6220906703189746, "HuggingFaceH4/zephyr-7b-beta": 0.9993325491763057, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9206880841454284, "meta-llama/Meta-Llama-3-8B": 0.8721140635183127, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7154514051661259}}, {"question": "\u4e0b\u5217\u53d8\u5316\u4e2d\uff0c\u54ea\u79cd\u53d8\u5316\u4e0d\u4f1a\u5bfc\u81f4\u9700\u6c42\u66f2\u7ebf\u7684\u4f4d\u79fb\nA. \u4ea7\u54c1\u7684\u4ef7\u683c\nB. \u4eba\u4eec\u7684\u504f\u597d\u548c\u7231\u597d\nC. \u76f8\u5173\u4ea7\u54c1\u7684\u4ef7\u683c\nD. \u6d88\u8d39\u8005\u7684\u6536\u5165\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c06\u51fa\u53e3\u5546\u54c1\u4ee5\u4f4e\u4e8e\u56fd\u5e02\u573a\u4ef7\u683c\uff0c\u751a\u81f3\u4f4e\u4e8e\u5546\u54c1\u751f\u4ea7\u6210\u672c\u7684\u4ef7\u683c\uff0c\u5728\u56fd\u5916\u5e02\u573a\u629b\u552e\u5546\u54c1\u4ee5\u6253\u51fb\u7ade\u4e89\u5bf9\u624b\u7684\u524a\u4ef7\u7ade\u9500\u884c\u4e3a\uff0c\u88ab\u79f0\u4e3a\nA. \u5546\u54c1\u524a\u4ef7\nB. \u5546\u54c1\u503e\u9500\nC. \u5546\u54c1\u629b\u552e\nD. \u5546\u54c1\u7ade\u9500\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217IP\u5730\u5740\u5c5e\u4e8e\u6807\u51c6B\u7c7bIP\u5730\u5740\u7684\u662f\nA. 190.168.12.7/16\nB. 120.10.1.1/16\nC. 172.19.3.245/24\nD. 10\uff0e0\uff0e0\uff0e1/16\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39368751373426225, "meta-math/MetaMath-Mistral-7B": 0.8184704531849069, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.44296667015562063, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5fc3\u808c\u7ec6\u80de\u4e0d\u53d1\u751f\u5b8c\u5168\u5f3a\u76f4\u6536\u7f29\u7684\u539f\u56e0\u662f\nA. \u623f\u5ba4\u5ef6\u6401\nB. \u201c\u5168\u6216\u65e0\u201d\u5f0f\u6536\u7f29\nC. \u808c\u8d28\u7f51 Ca2+\u50a8\u5b58\u5c11\nD. \u6709\u6548\u4e0d\u5e94\u671f\u7279\u522b\u957f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.35823796793174434, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5927\u6750\u5c0f\u7528\u53e4\u6240\u53f9\uff0c\u7ba1\u4ef2\u3001\u8427\u4f55\u5b9e\u6d41\u4e9a\u201d\u662f\u9646\u6e38\u7684\u4e00\u53e5\u540d\u8bd7\uff0c\u5176\u4e2d\u201c\u5927\u6750\u5c0f\u7528\u201d\u5f62\u5bb9\u7684\u662f\u4e0b\u9762\u54ea\u4f4d\u4eba\u7269\nA. \u8f9b\u5f03\u75be\nB. \u97e9\u6108\nC. \u5b8b\u7389\nD. \u5e9e\u7edf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3534716122147515}}, {"question": "\u7ef4\u751f\u7d20\u662f\u7ef4\u6301\u808c\u4f53\u5065\u5eb7\u6240\u5fc5\u9700\u7684\u4e00\u7c7b\u4f4e\u5206\u5b50\u6709\u673a\u5316\u5408\u7269\uff0c\u901a\u5e38\u5206\u4e3a\u8102\u6eb6\u6027\u548c\nA. \u6cb9\u6eb6\u6027\nB. \u8102\u80aa\nC. \u6c34\u6eb6\u6027\nD. \u6709\u673a\u6eb6\u5242\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6928657250490642, "meta-math/MetaMath-Mistral-7B": 0.8598227719717234, "itpossible/Chinese-Mistral-7B-v0.1": 0.6679341663842396, "HuggingFaceH4/zephyr-7b-beta": 0.9201206656778738, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45764002707587026, "meta-llama/Meta-Llama-3-8B": 0.7950521584123491, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9413205009901179}}, {"question": "\u53d8\u91cf\u9009\u62e9\u662f\u7528\u6765\u9009\u62e9\u6700\u597d\u7684\u5224\u522b\u5668\u5b50\u96c6\uff0c \u5982\u679c\u8981\u8003\u8651\u6a21\u578b\u6548\u7387\uff0c\u6211\u4eec\u5e94\u8be5\u505a\u9664\u4e86\u4e0b\u5217\u54ea\u9879\u7684\u53d8\u91cf\u9009\u62e9\u7684\u8003\u8651\nA. \u4ea4\u53c9\u9a8c\u8bc1\nB. \u53d8\u91cf\u5bf9\u4e8e\u6a21\u578b\u7684\u89e3\u91ca\u6709\u591a\u5927\u4f5c\u7528\nC. \u7279\u5f81\u643a\u5e26\u7684\u4fe1\u606f\nD. \u591a\u4e2a\u53d8\u91cf\u5176\u5b9e\u6709\u76f8\u540c\u7684\u7528\u5904\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5663881986703987, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5e02\u573a\u578b\u7ec4\u7ec7\u5177\u6709\u7684\u4f18\u70b9\u662f\nA. \u80fd\u591f\u6709\u6548\u5730\u534f\u8c03\u5404\u79cd\u5e02\u573a\u8425\u9500\u804c\u80fd\nB. \u80fd\u52a0\u5f3a\u4f01\u4e1a\u90e8\u95e8\u95f4\u7684\u534f\u4f5c\uff0c\u9002\u5e94\u6027\u5f3a\uff0c\u6709\u5229\u4e8e\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\nC. \u53ef\u4ee5\u6309\u7167\u6ee1\u8db3\u4e0d\u540c\u987e\u5ba2\u7684\u9700\u6c42\u6765\u7ec4\u7ec7\u5e02\u573a\u8425\u9500\u6d3b\u52a8\uff0c\u6709\u5229\u4e8e\u5e02\u573a\u5f00\u62d3\nD. \u4e0a\u4e0b\u7ea7\u6743\u8d23\u660e\u786e\u3001\u6c9f\u901a\u8fc5\u901f\u3001\u7ba1\u7406\u6548\u7387\u8f83\u9ad8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33223263850354773, "meta-math/MetaMath-Mistral-7B": 0.47995611658487325, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6781\u9650 $\\lim _{\\substack{x \\rightarrow 0 \\\\ y \\rightarrow 0}} \\frac{3 x-y}{x+y}$ ( ).\nA. \u5b58\u5728\uff0c \u4f46\u4e0d\u7b49\u4e8e $\\frac{1}{2}$ \u4e5f\u4e0d\u7b49\u4e8e 0\nB. \u7b49\u4e8e $\\frac{1}{2}$\nC. \u7b49\u4e8e 0\nD. \u4e0d\u5b58\u5728\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.33065623127838456, "itpossible/Chinese-Mistral-7B-v0.1": 0.3331832353540621, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e1969\u5e74\u4fee\u8ba2\u76841954\u5e74\u300a\u56fd\u9645\u9632\u6b62\u6d77\u4e0a\u6cb9\u6c61\u516c\u7ea6\u300b\u7684\u89c4\u5b9a\uff0c\u7981\u6392\u533a\u662f\nA. \u8ddd\u6d77\u5cb850\u6d77\u91cc\u4ee5\u5185\u6d77\u57df\nB. \u6240\u6709\u6d77\u57df\nC. \u8ddd\u6d77\u5cb8100\u6d77\u91cc\u4ee5\u5185\u6d77\u57df\nD. \u8ddd\u6d77\u5cb8150\u6d77\u91cc\u4ee5\u5185\u6d77\u57df\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u662f\u751f\u7269\u79d1\u5b66\u53f2\u4e0a\u4e00\u4e9b\u7ecf\u5178\u5b9e\u9a8c\u7684\u201c\u65b9\u6cd5\u4e0e\u7ed3\u679c\u201d\u548c\u201c\u7ed3\u8bba\u6216\u89c2\u70b9\u201d\uff0c\u5176\u4e2d\u76f8\u5339\u914d\u7684\u662f\nA. \u65b9\u6cd5\u4e0e\u7ed3\u679c\uff1a\u5148\u5c06\u7ec6\u83cc\u57f9\u517b\u5728\u542b^15 N\u7684\u57f9\u517b\u57fa\u4e2d\uff0c\u57f9\u517b\u591a\u4ee3\u540e\uff0c\u518d\u5c06\u7ec6\u83cc\u653e\u5728\u542b^14 N\u7684\u57f9\u517b\u57fa\u4e2d\u7e41\u6b96\u4e00\u4ee3\uff0c\u79bb\u5fc3\u540e\u53ea\u51fa\u73b0\u4e2d\u5bc6\u5ea6\u5e26\u3002\u7ed3\u8bba\u6216\u89c2\u70b9\uff1aDNA\u7684\u590d\u5236\u4e0d\u662f\u5168\u4fdd\u7559\u590d\u5236\u3002\nB. \u65b9\u6cd5\u4e0e\u7ed3\u679c\uff1a\u5355\u4fa7\u5149\u7167\u5c04\u4e0b\uff0c\u80da\u82bd\u9798\u5411\u5149\u5f2f\u66f2\u751f\u957f\uff0c\u53bb\u5c16\u7aef\u7684\u80da\u82bd\u9798\u4e0d\u751f\u957f\u4e5f\u4e0d\u5f2f\u66f2\u3002\u7ed3\u8bba\u6216\u89c2\u70b9\uff1a\u751f\u957f\u7d20\u5177\u6709\u6781\u6027\u8fd0\u8f93\u7684\u7279\u70b9\u3002\nC. \u65b9\u6cd5\u4e0e\u7ed3\u679c\uff1a\u5c06\u5929\u7afa\u8475\u7684\u53f6\u7247\u9ed1\u6697\u5904\u7406\u540e\uff0c\u4e00\u534a\u906e\u5149\uff0c\u4e00\u534a\u66dd\u5149\u3002\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u7528\u9152\u7cbe\u8131\u8272\uff0c\u5206\u522b\u7528\u7898\u6db2\u5904\u7406\uff0c\u66dd\u5149\u7ec4\u84dd\u8272\uff0c\u906e\u5149\u7ec4\u65e0\u84dd\u8272\u3002\u7ed3\u8bba\u6216\u89c2\u70b9\uff1a\u7eff\u8272\u690d\u7269\u5149\u5408\u4f5c\u7528\u7684\u573a\u6240\u662f\u53f6\u7eff\u4f53\u3002\nD. \u65b9\u6cd5\u4e0e\u7ed3\u679c\uff1a\u5c06\u6d3b\u7684R\u578b\u80ba\u708e\u53cc\u7403\u83cc\u4e0e\u52a0\u70ed\u6740\u6b7b\u7684S\u578b\u80ba\u708e\u53cc\u7403\u83cc\u6df7\u5408\u540e\u6ce8\u5165\u5c0f\u9f20\u4f53\u5185\uff0c\u5c0f\u9f20\u6b7b\u4ea1\u3002\u7ed3\u8bba\u6216\u89c2\u70b9\uff1aDNA\u662f\u80ba\u708e\u53cc\u7403\u83cc\u7684\u9057\u4f20\u7269\u8d28\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4880530544278662, "meta-math/MetaMath-Mistral-7B": 0.3387394555782919, "itpossible/Chinese-Mistral-7B-v0.1": 0.30702421322472195, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.561703873687499, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u7ecf\u6d4e\u793e\u4f1a\uff0c\u7ecf\u6d4e\u81ea\u7531\u4e3b\u8981\u662f\u8868\u73b0\u5728\u8d22\u4ea7\u6743\u7684\uff08\uff09\u4e0a\nA. \u6240\u6709\u6743\nB. \u652f\u914d\nC. \u6536\u76ca\u6743\nD. \u8f6c\u8ba9\u6743\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u98df\u4e0d\u538c\u7cbe\uff0c\u810d\u4e0d\u538c\u7ec6\u201d\u662f\uff08\uff09\u83dc\u7684\u996e\u98df\u539f\u5219\nA. \u5b54\u5e9c\u83dc\nB. \u968f\u56ed\u83dc\nC. \u8c2d\u5bb6\u83dc\nD. \u7ea2\u697c\u83dc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36762417554868215, "meta-math/MetaMath-Mistral-7B": 0.49309197311599057, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.48783852322368093, "meta-llama/Meta-Llama-3-8B": 0.46683854452590934, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7533382014607205}}, {"question": "\u624b\u90e8\u5f00\u653e\u6027\u635f\u4f24\u540e\uff0c\u65e9\u671f\u6e05\u521b\u7f1d\u5408\u4e0d\u5e94\u8d85\u8fc7\u7684\u65f6\u95f4\u662f\nA. 8 \u5c0f\u65f6\nB. 12 \u5c0f\u65f6\nC. 16 \u5c0f\u65f6\nD. 4 \u5c0f\u65f6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3878985698342042, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.45487344101533467, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3626062801770276, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u81ea\u6211\u610f\u8bc6\u6027\u522b\u4e0e\u751f\u7269\u5b66\u6027\u522b\u4e0d\u4e00\u81f4\uff0c\u79f0\u4e3a\nA. \u540c\u6027\u604b\nB. \u6027\u53d6\u5411\u969c\u788d\nC. \u6027\u504f\u597d\u969c\u788d\nD. \u6027\u8ba4\u540c\u969c\u788d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.46087095962334906, "meta-math/MetaMath-Mistral-7B": 0.6390235759365617, "itpossible/Chinese-Mistral-7B-v0.1": 0.6803512891784421, "HuggingFaceH4/zephyr-7b-beta": 0.9810168425232816, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5443805060178588, "meta-llama/Meta-Llama-3-8B": 0.9410161459597921, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.996864955728492}}, {"question": "\u51fd\u6570 $f(x\uff0c y)=\\left\\{\\begin{array}{ll}x \\arctan \\frac{y}{x}\uff0c & x \\neq 0\uff0c \\\\ 0\uff0c & x=0\\end{array}\\right.$ \u4e0d\u8fde\u7eed\u7684\u70b9\u96c6\u4e3a\nA. $x=0\uff0c y \\geq 0$ \u7684\u70b9\u96c6\nB. \u7a7a\u96c6\nC. $y$ \u8f74\u4e0a\u7684\u6240\u6709\u70b9\nD. $x=0\uff0c y \\leq 0$ \u7684\u70b9\u96c6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4372938845847469, "itpossible/Chinese-Mistral-7B-v0.1": 0.31712010892822357, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u865a\u5730\u5740\u662f\nA. \u4e0d\u5b58\u5728\u7684\u5730\u5740\nB. \u78c1\u76d8\u5730\u5740\nC. \u2f64\u6237\u7f16\u7a0b\u53ef\u4f7f\u2f64\u7684\u5730\u5740\nD. \u4e3b\u5b58\u5730\u5740\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7969637885137754, "meta-math/MetaMath-Mistral-7B": 0.9820805443349626, "itpossible/Chinese-Mistral-7B-v0.1": 0.8435313025698021, "HuggingFaceH4/zephyr-7b-beta": 0.9946438094930674, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6359144869370226, "meta-llama/Meta-Llama-3-8B": 0.9303363312611964, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9980507458150081}}, {"question": "\u5fc3\u5ba4\u808c\u7ec6\u80de\u52a8\u4f5c\u7535\u4f4d\u5feb\u901f\u590d\u6781\u672b\u671f\u7684\u539f\u56e0\nA. Ca2+\u5185\u6d41\nB. Na+\u5185\u6d41\nC. Cl-\u5185\u6d41\nD. K+\u5916\u6d41\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6172712184250093, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u56fa\u5b9a\u5316\u9176\u548c\u56fa\u5b9a\u5316\u7ec6\u80de\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u56fa\u5b9a\u5316\u7ec6\u80de\u7528\u4e8e\u751f\u4ea7\u80fd\u5206\u6ccc\u5230\u7ec6\u80de\u5916\u7684\u4ea7\u7269\nB. \u51dd\u80f6\u4e0e\u88ab\u5305\u57cb\u7ec6\u80de\u4e4b\u95f4\u4e0d\u662f\u901a\u8fc7\u5171\u4ef7\u952e\u7ed3\u5408\nC. \u56fa\u5b9a\u5316\u9176\u7684\u4e3b\u8981\u76ee\u7684\u662f\u5b9e\u73b0\u9176\u7684\u91cd\u590d\u5229\u7528\nD. \u6eb6\u89e3\u6c27\u4ea4\u6362\u53d7\u963b\u662f\u56fa\u5b9a\u5316\u9176\u5e94\u7528\u7684\u91cd\u8981\u9650\u5236\u56e0\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3127242143872847, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.40287366758214765, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4482780905009404}}, {"question": "\u6c83\u5c14\u592b\u58f0\u79f0\uff0c\u6700\u7ec8\uff0c\u89c4\u8303\u95ee\u9898\u5fc5\u987b\u4ece\u4ee5\u4e0b\u65b9\u9762\u8fdb\u884c\u8bc4\u4f30\uff1a\nA. \u9053\u5fb7\u89c2\u70b9\u3002\nB. \u4e00\u79cd\u4e0e\u5bf9\u4efb\u4f55\u6709\u5e8f\u7684\u4ef7\u503c\u4f53\u7cfb\u7684\u627f\u8bfa\u65e0\u5173\u7684\u89c2\u70b9\u3002 \nC. \u4e2a\u4eba\u5b8c\u7f8e\u7684\u89c2\u70b9\u3002\nD. \u7f8e\u5fb7\u7684\u89c2\u70b9\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6127595154423698, "meta-math/MetaMath-Mistral-7B": 0.9445424438603722, "itpossible/Chinese-Mistral-7B-v0.1": 0.36562478238057156, "HuggingFaceH4/zephyr-7b-beta": 0.997253571453944, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7536287392063516, "meta-llama/Meta-Llama-3-8B": 0.4645499814021883, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9279399046539731}}, {"question": "\u4ee5\u4e0b\u90a3\u4e00\u4f4d\u8d5e\u666e\u5728\u81ea\u5df1\u767b\u4f4d\u540e\u5f00\u59cb\u5b9e\u884c\u5927\u89c4\u6a21\u706d\u4f5b\u8fd0\u52a8\nA. \u6717\u8fbe\u739b\nB. \u8d64\u677e\u5fb7\u8d5e\nC. \u8d64\u5fb7\u7956\u8d5e\nD. \u8d64\u7956\u5fb7\u8d5e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3689108554330874, "itpossible/Chinese-Mistral-7B-v0.1": 0.3354456100429363, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.40252653040150815, "meta-llama/Meta-Llama-3-8B": 0.882741452464558, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u4e07\u6709\u5f15\u529b\u5b9a\u5f8b\u7684\u9002\u7528\u8303\u56f4\uff0c\u4ee5\u4e0b\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u9009\u9879\u662f\nA. \u53ea\u9002\u7528\u4e8e\u7403\u5f62\u7269\u4f53\uff0c\u4e0d\u9002\u7528\u4e8e\u5176\u4ed6\u5f62\u72b6\u7684\u7269\u4f53\nB. \u53ea\u9002\u7528\u4e8e\u8d28\u70b9\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u7269\u4f53\nC. \u9002\u7528\u4e8e\u81ea\u7136\u754c\u4e2d\u4efb\u610f\u4e24\u4e2a\u7269\u4f53\u4e4b\u95f4\nD. \u53ea\u9002\u7528\u4e8e\u5929\u4f53\uff0c\u4e0d\u9002\u7528\u4e8e\u5730\u9762\u7269\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7913905759669554, "meta-math/MetaMath-Mistral-7B": 0.9873678503781023, "itpossible/Chinese-Mistral-7B-v0.1": 0.4221216124438063, "HuggingFaceH4/zephyr-7b-beta": 0.9881644253235164, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8462206194002585, "meta-llama/Meta-Llama-3-8B": 0.8994694096793241, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9927260594805226}}, {"question": "\u201c\u5927\u5b9e\u6709\u7fb8\u72b6\u201d\u7684\u75c5\u673a\u5e94\u5c5e\u4e8e\nA. \u771f\u865a\u5047\u5b9e\nB. \u865a\u5b9e\u5939\u6742\nC. \u771f\u5b9e\u5047\u865a\nD. \u865a\u4e2d\u5939\u5b9e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u968f\u7740\u793e\u4f1a\u4e3b\u4e49\u5e02\u573a\u7ecf\u6d4e\u4f53\u5236\u7684\u5efa\u7acb\uff0c\u76ee\u524d\u6211\u56fd\u65b0\u95fb\u673a\u6784\u7684\u4e3b\u8981\u8fd0\u4f5c\u65b9\u5f0f\u662f\nA. \u4e8b\u4e1a\u5355\u4f4d\uff0c\u884c\u653f\u7ba1\u7406\nB. \u4e8b\u4e1a\u5355\u4f4d\uff0c\u4f01\u4e1a\u5316\u7ba1\u7406\nC. \u4e8b\u4e1a\u5355\u4f4d\uff0c\u4e8b\u4e1a\u7ba1\u7406\nD. \u4f01\u4e1a\u5355\u4f4d\uff0c\u4f01\u5316\u7ba1\u7406\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5125252811999867, "meta-math/MetaMath-Mistral-7B": 0.8571698461923917, "itpossible/Chinese-Mistral-7B-v0.1": 0.4899701589245879, "HuggingFaceH4/zephyr-7b-beta": 0.7105480563175093, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.546041442987357, "meta-llama/Meta-Llama-3-8B": 0.5424536252481797, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6146419890003175}}, {"question": "\u6211\u4eec\u6240\u5c45\u7684\u5a11\u5a46\u4e16\u754c\uff0c\u7ecf\u4e2d\u79f0\u4e3a\nA. \u4e03\u6d4a\u6076\u4e16\nB. \u4e09\u6d4a\u6076\u4e16\nC. \u5341\u6d4a\u6076\u4e16\nD. \u4e94\u6d4a\u6076\u4e16\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.45106954872746413, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8457\u540d\u7684\u57c3\u83f2\u5c14\u94c1\u5854\u4f4d\u4e8e\nA. \u6cd5\u56fd\nB. \u82f1\u56fd\nC. \u5fb7\u56fd\nD. \u7f8e\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39076155178455163, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.9125565587295745, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9862814577389696, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9995497516926545}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\u6a2a\u7ebf\u4e0a\uff0c\u4f9d\u6b21\u586b\u5165\u8bcd\u8bed\u6700\u6070\u5f53\u7684\u4e00\u9879\u662f\uff081\uff09\u5305\u5170\u94c1\u8def\u901a\u8f66\u4ee5\u6765\uff0c\u706b\u8f66\u5728\u6c99\u6f20\u4e0a\u884c\u9a76\uff0c\u4ece\u6765\u6ca1\u6709\u56e0\u4e3a\u98ce\u6c99\u7684\u800c\u53d1\u751f\u4e8b\u6545\u3002\uff082\uff09\u519c\u6c11\u5144\u5f1f\u8981\u505a\u597d\u51c6\u5907\uff0c\u9632\u6b62\u5bb3\u866b\u519c\u4f5c\u7269\u3002\uff083\uff09\u7531\u4e8e\u957f\u57ce\u5916\u7684\u98ce\u6c99\uff0c\u6986\u6797\u57ce\u4e5f\u53d7\u5230\u88ad\u51fb\u3002\uff084\uff09\u8fd9\u4e2a\u533a\u57df\u4e0d\u65ad\u53d7\u5230\u98ce\u6c99\u7684\uff0c\u6709\u4e9b\u90e8\u5206\u9010\u6e10\u53d8\u6210\u8352\u6f20\u4e86\u3002\nA. \u4fb5\u5bb3 \u4fb5\u5360 \u4fb5\u5165 \u4fb5\u88ad\nB. \u4fb5\u5165 \u4fb5\u5bb3 \u4fb5\u88ad \u4fb5\u5360\nC. \u4fb5\u5360 \u4fb5\u5165 \u4fb5\u88ad \u4fb5\u5bb3\nD. \u4fb5\u88ad \u4fb5\u5bb3 \u4fb5\u5165 \u4fb5\u5360\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3550009601731296, "meta-math/MetaMath-Mistral-7B": 0.3589842189651603, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9940687527611488, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u751f\u4ea7\u5546\u54c1\u7684\u52b3\u52a8\u5206\u5177\u4f53\u52b3\u52a8\u548c\u62bd\u8c61\u52b3\u52a8\uff0c\u5176\u4e2d\u5177\u4f53\u52b3\u52a8\u7684\u4f5c\u7528\u662f\nA. \u521b\u9020\u65b0\u4ef7\u503c\nB. \u521b\u9020\u5fc5\u8981\u4ef7\u503c\nC. \u521b\u9020\u4f7f\u7528\u4ef7\u503c\nD. \u521b\u9020\u5269\u4f59\u4ef7\u503c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3331832353540621, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.35370309353919205, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u76d1\u62a4\u4eba\u6709\u6743\u5904\u7406\u88ab\u76d1\u62a4\u4eba\u8d22\u4ea7\u7684\u6cd5\u5b9a\u60c5\u5f62\u662f\nA. \u7ecf\u88ab\u76d1\u62a4\u4eba\u6240\u5728\u5730\u7684\u57fa\u5c42\u7ec4\u7ec7\u540c\u610f\nB. \u4e3a\u88ab\u76d1\u62a4\u4eba\u7684\u5229\u76ca\nC. \u7ecf\u88ab\u76d1\u62a4\u4eba\u540c\u610f\nD. \u53d1\u751f\u7ecf\u6d4e\u60c5\u51b5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.530522231772415, "meta-math/MetaMath-Mistral-7B": 0.9298610644122084, "itpossible/Chinese-Mistral-7B-v0.1": 0.8042364090697628, "HuggingFaceH4/zephyr-7b-beta": 0.9982329899422256, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.44391759676048864, "meta-llama/Meta-Llama-3-8B": 0.7149975779889578, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9338381352956064}}, {"question": "\u5bb6\u8695\u4e2d\uff0c\u8327\u8272\u4e0e\u8695\u8840\u6db2\u7684\u989c\u8272\u6709\u5173\uff0c\u5373\u767d\u8272\u8840\u6db2\u7684\u8695\u7ed3\u767d\u8272\u8327\uff0c\u9ec4\u8272\u8840\u6db2\u7684\u8695\u7ed3\u9ec4\u8272\u8327\u3002\u9ec4\u8840\u57fa\u56e0\uff08Y\uff09\u5bf9\u767d\u8840\u57fa\u56e0\uff08y\uff09\u5b8c\u5168\u663e\u6027\uff0c\u4f4d\u4e8e\u7b2c2\u53f7\u5e38\u67d3\u8272\u4f53\u4e0a\u3002\u9ec4\u8840\u6291\u5236\u57fa\u56e0\uff08I\uff09\u80fd\u6291\u5236Y\u57fa\u56e0\u7684\u4f5c\u7528\uff0c\u4f4d\u4e8e\u7b2c9\u53f7\u5e38\u67d3\u8272\u4f53\u4e0a\u3002\u4e0b\u5217\u76f8\u5173\u53d9\u8ff0\u9519\u8bef\u7684\u662f\nA. \u9ec4\u8272\u8840\u8695\u7684\u57fa\u56e0\u578b\u67092\u79cd\nB. \u767d\u8272\u8840\u8695\u7684\u540e\u4ee3\u53ea\u7ed3\u767d\u8272\u8327\nC. \u9ec4\u8272\u8840\u8695\u7684\u540e\u4ee3\u53ef\u7ed3\u767d\u8272\u8327\nD. \u767d\u8272\u8840\u8695\u7684\u57fa\u56e0\u578b\u67097\u79cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u636e\u300a\u7d20\u95ee\u00b7\u4e94\u5e38\u653f\u5927\u8bba\u300b\uff0c\u80fd\u6bd2\u8005\u5f53\u4ee5\nA. \u8584\u836f\nB. \u65e0\u6bd2\u836f\nC. \u539a\u836f\nD. \u9488\u77f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.39380244720675023}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e0d\u5c5e\u4e8e\u4e2d\u56fd\u53e4\u4ee3\u6027\u6587\u5316\u7684\u5178\u578b\u7279\u5f81\u7684\u662f\nA. \u96f6\u6563\nB. \u591a\u5143\u5316\u3001\u788e\u7247\u5316\nC. \u975e\u4e2d\u5fc3\u5316\nD. \u7cfb\u7edf\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7255908553355673, "meta-math/MetaMath-Mistral-7B": 0.9428402059042893, "itpossible/Chinese-Mistral-7B-v0.1": 0.8190878898268293, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9616845942581186, "meta-llama/Meta-Llama-3-8B": 0.9099931331306653, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2016\u5e74\uff0c\u5e74\u4ec522\u5c81\u7684\u9b4f\u5219\u897f\u56e0\u764c\u75c7\u53bb\u4e16\u3002\u4ed6\u6b64\u524d\u66fe\u76f8\u4fe1\u767e\u5ea6\u4e0a\u5173\u4e8e\u201c\u514d\u75ab\u6cbb\u7597\u6cd5\u201d\u764c\u75c7\u6cbb\u6108\u7387\u9ad8\u8fbe90%\u7684\u5e7f\u544a\uff0c\u82b1\u4e86\u9ad8\u6602\u533b\u836f\u8d39\u7528\u5374\u6beb\u65e0\u6548\u679c\u3002\u8be5\u4e8b\u4ef6\u4f53\u73b0\u4e86\u4f01\u4e1a\u8fdd\u53cd\u4e86\u8425\u9500\u7684\u9053\u5fb7\u7684\nA. \u4e0d\u6b3a\u8d1f\u5f31\u5c0f\u3001\u4e0d\u5f3a\u4e70\u5f3a\u5356\nB. \u4e0d\u641e\u6076\u4fd7\u3001\u5f18\u626c\u6b63\u80fd\u91cf\nC. \u4e0d\u6295\u673a\u53d6\u5de7\u3001\u6545\u610f\u6df7\u6dc6\nD. \u4e0d\u865a\u5047\u3001\u5938\u5927\u5ba3\u4f20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7220298575107771, "HuggingFaceH4/zephyr-7b-beta": 0.9628418786988256, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9468821595365172, "meta-llama/Meta-Llama-3-8B": 0.5729373795740661, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8888664629306042}}, {"question": "\u4e0b\u5217\u5e7d\u95e8\u6897\u963b\u60a3\u8005\u672f\u524d\u51c6\u5907\u63aa\u65bd\u4e2d\uff0c\u4e0d\u5408\u7406\u7684\u662f\nA. \u7ea0\u6b63\u6c34\u7535\u89e3\u8d28\u5e73\u8861\nB. \u7981\u98df\u3001\u80c3\u80a0\u51cf\u538b\nC. \u6e29\u76d0\u6c34\u6d17\u80c3\nD. \u5e94\u7528\u5e7f\u8c31\u6297\u751f\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.46615075157627756, "meta-math/MetaMath-Mistral-7B": 0.41680227275373427, "itpossible/Chinese-Mistral-7B-v0.1": 0.7069593798351966, "HuggingFaceH4/zephyr-7b-beta": 0.4374729217509311, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6007825574604638, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9a6c\u514b\u601d\u4e3b\u4e49\u7684\u4e2d\u56fd\u6c11\u65cf\u7406\u8bba\u5177\u6709\uff08\uff09\u548c\u79d1\u5b66\u6027\u7edf\u4e00\u7684\u7279\u70b9\u3002\nA. \u7406\u8bba\u6027\nB. \u653f\u6cbb\u6027\nC. \u5b66\u672f\u6027\nD. \u5b9e\u7528\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6240\u8c13\u516c\u5171\u51b3\u7b56\u7684\u201c\u601d\u60f3\u5e93\u201d\u6216\u201c\u667a\u56ca\u56e2\u201d\u662f\u6307\u51b3\u7b56\u4f53\u5236\u4e2d\u7684\nA. \u54a8\u8be2\u7cfb\u7edf\nB. \u4e2d\u67a2\u7cfb\u7edf\nC. \u4fe1\u606f\u7cfb\u7edf\nD. \u76d1\u63a7\u7cfb\u7edf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9181559035788901, "meta-math/MetaMath-Mistral-7B": 0.9960302956000087, "itpossible/Chinese-Mistral-7B-v0.1": 0.8878864063523606, "HuggingFaceH4/zephyr-7b-beta": 0.9997097151369453, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7561007917417925, "meta-llama/Meta-Llama-3-8B": 0.7340171918446364, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9026665879552607}}, {"question": "\u4e0b\u5217\u62a5\u8868\u4e2d\uff0c\u5c5e\u4e8e\u9759\u6001\u62a5\u8868\u7684\u662f\nA. \u5229\u6da6\u5206\u914d\u8868\nB. \u5229\u6da6\u8868\nC. \u8d44\u4ea7\u8d1f\u503a\u8868\nD. \u73b0\u91d1\u6d41\u91cf\u8868\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3026903026816732, "meta-math/MetaMath-Mistral-7B": 0.39380243297780554, "itpossible/Chinese-Mistral-7B-v0.1": 0.32871459371036227, "HuggingFaceH4/zephyr-7b-beta": 0.988605241943637, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3500288944602904, "meta-llama/Meta-Llama-3-8B": 0.928663822481993, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9881062997280095}}, {"question": "\u5bbf\u4e3b\u72b6\u6001\u662f\u9057\u4f20\u56e0\u7d20\u4e0e\u4e0b\u5217\u54ea\u79cd\u56e0\u7d20\u76f8\u4e92\u4f5c\u7528\u7684\u7ed3\u679c\nA. \u751f\u7269\u56e0\u7d20\nB. \u73af\u5883\u56e0\u7d20\nC. \u5316\u5b66\u56e0\u7d20\nD. \u7269\u7406\u56e0\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3827386530989879, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6066598606455222, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9194818546150725, "meta-llama/Meta-Llama-3-8B": 0.47484966053224986, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5c5e\u4e8e\u8def\u7531\u8868\u7684\u4ea7\u2f63\u2f45\u5f0f\u7684\u662f\nA. \u8def\u7531\u5668\u7684\u76f4\u8fde\u2f79\u6bb5\u2f83\u52a8\u2f63\u6210\nB. \u4ee5\u4e0a\u90fd\u662f\nC. \u901a\u8fc7\u2f3f\u2f2f\u914d\u7f6e\u6dfb\u52a0\u8def\u7531\nD. \u901a\u8fc7\u8fd0\u2f8f\u52a8\u6001\u8def\u7531\u534f\u8bae\u2f83\u52a8\u5b66\u4e60\u4ea7\u2f63\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c35\u5c81\u3002\u7532\u72b6\u817a\u4e73\u5934\u72b6\u764c\u6839\u6cbb\u624b\u672f\u540e\u7b2c\u4e00\u5929\uff0c\u53d1\u73b0\u996e\u6c34\u65f6\u6709\u545b\u54b3\uff0c\u8bf4\u8bdd\u97f3\u8c03\u964d\u4f4e\u3002\u6700\u53ef\u80fd\u7684\u539f\u56e0\u662f\nA. \u5589\u4e0a\u795e\u7ecf\u635f\u4f24\nB. \u5589\u8fd4\u795e\u7ecf\u635f\u4f24\nC. \u63d2\u7ba1\u81f4\u58f0\u95e8\u6c34\u80bf\nD. \u58f0\u5e26\u635f\u4f24\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.30601362565976303, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4651962596334627}}, {"question": "\u4f20\u64ad\u6587\u5316\u77e5\u8bc6\uff0c\u662f\u5927\u4f17\u4f20\u64ad\u7684\u4e00\u4e2a\u91cd\u8981\u529f\u80fd\uff0c\u8fd9\u4e00\u529f\u80fd\u4e5f\u5c31\u662f\nA. \u5ba3\u4f20\u529f\u80fd\nB. \u5a31\u4e50\u529f\u80fd\nC. \u6559\u80b2\u529f\u80fd\nD. \u8ba4\u8bc6\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9579001236748093, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6585884681689727, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c72\u5c81\uff0c\u9ad8\u8840\u538b\u3001\u5fc3\u7ede\u75db\u75c5\u53f22\u5e74\u300220\u5c0f\u65f6\u524d\u7ecf\u80a1\u52a8\u8109\u9014\u5f84\u884c\u51a0\u72b6\u52a8\u8109\u9020\u5f71\u663e\u793a\uff1a\u5347\u4e3b\u52a8\u8109\u660e\u663e\u6269\u5f20\uff0c\u5de6\u524d\u964d\u652f95%\u963b\u585e\uff0c\u534a\u5c0f\u65f6\u524d\u60a3\u8005\u8d77\u5e8a\u540e\u7a81\u611f\u80f8\u95f7\u3001\u80f8\u75db\u3001\u547c\u5438\u56f0\u96be\uff0c\u8840\u538b70/50mmHg\uff0c\u9888\u9759\u8109\u660e\u663e\u5145\u76c8\uff0c\u8840\u6c14\u5206\u6790PaO245mmHg\uff0cPaCO235mmHg\u3002\u6700\u53ef\u80fd\u7684\u8bca\u65ad\u662f\nA. \u6025\u6027\u80ba\u6813\u585e\nB. \u5fc3\u810f\u538b\u585e\nC. \u6025\u6027\u5fc3\u808c\u6897\u6b7b\nD. \u4e3b\u52a8\u8109\u5939\u5c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.49516317235292706, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3017444864199176, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8ba1\u7b97\u673a\u786c\u4ef6\u80fd\u76f4\u63a5\u8bc6\u522b\u5e76\u6267\u884c\u7684\u8bed\u8a00\u662f\nA. \u7b97\u6cd5\u8bed\u8a00\nB. \u673a\u5668\u8bed\u8a00\nC. \u9ad8\u7ea7\u8bed\u8a00\nD. \u7b26\u53f7\u8bed\u8a00\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9500017242008959, "meta-math/MetaMath-Mistral-7B": 0.993718336305599, "itpossible/Chinese-Mistral-7B-v0.1": 0.9665199016813087, "HuggingFaceH4/zephyr-7b-beta": 0.9999661369825784, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9739881774827175, "meta-llama/Meta-Llama-3-8B": 0.9814191303116924, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9990899742895848}}, {"question": "\u4ec0\u4e48\u6837\u7684\u6c34\u4e0d\u80fd\u559d\nA. \u5168\u662f\nB. \u957f\u65f6\u95f4\u6ca1\u6709\u66f4\u6362\u6ee4\u82af\u7684\u51c0\u6c34\u5668\u6ee4\u51fa\u7684\u6c34D\nC. \u6bcf\u5929\u65e9\u4e0a\u6c34\u9f99\u5934\u6700\u521d\u6d41\u51fa\u7684\u6c34\nD. \u716e\u5f00\u65f6\u95f4\u592a\u957f\u7684\u6c34\u3001\u6ce1\u4e86\u5f88\u4e45\u7684\u8336\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u5706\u5468\u8fd0\u52a8\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u505a\u5706\u5468\u8fd0\u52a8\u7684\u7269\u4f53\u52a0\u901f\u5ea6\u2f00\u5b9a\u59cb\u7ec8\u6307\u5411\u5706\u2f3c\nB. \u5300\u901f\u5706\u5468\u8fd0\u52a8\u662f\u5300\u901f\u8fd0\u52a8\nC. \u5300\u901f\u5706\u5468\u8fd0\u52a8\u662f\u53d8\u901f\u8fd0\u52a8\nD. \u5300\u901f\u5706\u5468\u8fd0\u52a8\u662f\u6240\u53d7\u5408\u2f12\u4e3a\u96f6\u7684\u8fd0\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5673840724166924}}, {"question": "\u5173\u4e8e\u6cd5\u5f8b\u5173\u7cfb\u7684\u4e3b\u4f53\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u6709\nA. \u81ea\u7136\u4eba\u5177\u6709\u6743\u5229\u80fd\u529b\u4e0d\u4e00\u5b9a\u5177\u6709\u884c\u4e3a\u80fd\u529b\uff1b\u6cd5\u4eba\u5177\u6709\u6743\u5229\u80fd\u529b\u5fc5\u987b\u6709\u884c\u4e3a\u80fd\u529b\nB. \u6cd5\u5f8b\u5173\u7cfb\u4e3b\u4f53\u662f\u7531\u6cd5\u5f8b\u89c4\u8303\u6240\u89c4\u5b9a\u7684\uff0c\u56e0\u6b64\uff0c\u6cd5\u5f8b\u89c4\u8303\u662f\u786e\u5b9a\u6cd5\u5f8b\u5173\u7cfb\u4e3b\u4f53\u8d44\u683c\u7684\u6700\u7ec8\u6839\u6e90\nC. \u6839\u636e\u5e74\u9f84\u548c\u7cbe\u795e\u72b6\u51b5\u7684\u4e0d\u540c\uff0c\u5c06\u6743\u5229\u80fd\u529b\u548c\u884c\u4e3a\u80fd\u529b\u5212\u5206\u4e3a\u4e09\u79cd\nD. \u5177\u6709\u6743\u5229\u80fd\u529b\u5c31\u5177\u6709\u53c2\u52a0\u6cd5\u5f8b\u5173\u7cfb\u7684\u8d44\u683c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3341942123326676, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4812280501390452, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4fdd\u5b58\u6216\u6536\u96c6\u8bc1\u636e\u7684\u65f6\u5019\u5e94\u5f53\u6ce8\u610f\u8bc1\u636e\u7684\u7279\u6027\uff0c\u4e0d\u80fd\u201c\u6355\u98ce\u6349\u5f71\u201d\uff0c\u66f4\u4e0d\u80fd\u201c\u4e3b\u89c2\u81c6\u65ad\u201d\u3002\u8fd9\u5f3a\u8c03\u7684\u662f\u8bc1\u636e\u7684\nA. \u5b8c\u6574\u6027\nB. \u5ba2\u89c2\u6027\nC. \u5408\u6cd5\u6027\nD. \u5173\u8054\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8007222637975864, "meta-math/MetaMath-Mistral-7B": 0.985010882305483, "itpossible/Chinese-Mistral-7B-v0.1": 0.8430603683527276, "HuggingFaceH4/zephyr-7b-beta": 0.9978055314143707, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9504037214140254, "meta-llama/Meta-Llama-3-8B": 0.49143981487088245, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9212970099432487}}, {"question": "\u5de5\u4e1a\u53d1\u9175\u4e2d\u4e00\u822c\u90fd\u91c7\u7528\u54ea\u79cd\u706d\u83cc\u65b9\u6cd5\u6740\u706d\u57f9\u517b\u57fa\u4e2d\u7684\u6709\u673a\u7269\nA. \u9ad8\u538b\u84b8\u6c7d\u706d\u83cc\nB. \u5e72\u70ed\u706d\u83cc\nC. \u706b\u7130\u706d\u83cc\nD. \u5e38\u538b\u706d\u83cc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3423962339378881, "meta-math/MetaMath-Mistral-7B": 0.4668385576635872, "itpossible/Chinese-Mistral-7B-v0.1": 0.4957404895145016, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4084128997908917, "meta-llama/Meta-Llama-3-8B": 0.32479161563275305, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6004227213486911}}, {"question": "\u5bf9\u6cd5\u5f8b\u8fdb\u884c\u5206\u7c7b\uff0c\u53ef\u4ee5\u4ece\u4e0d\u540c\u7684\u6807\u51c6\u3001\u89d2\u5ea6\u51fa\u53d1\u3002\u4e00\u822c\u6765\u8bb2\uff0c\u6839\u636e\u4e0d\u540c\u7684\u6cd5\u5f8b\u5f62\u5f0f\uff0c\u53ef\u4ee5\u628a\u6cd5\u5212\u5206\u4e3a\nA. \u56fd\u9645\u6cd5\u4e0e\u56fd\u5185\u6cd5\nB. \u6210\u6587\u6cd5\u548c\u4e0d\u6210\u6587\u6cd5\nC. \u4e00\u822c\u6cd5\u548c\u7279\u522b\u6cd5\nD. \u6839\u672c\u6cd5\u4e0e\u666e\u901a\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.39855636281732093, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f7f\u7528\u4eba\u5de5\u70b9\u706b\u7684\u71c3\u6c14\u7076\u5177\uff0c\u5728\u70b9\u706b\u65f6\u8981\u575a\u6301\u7684\u539f\u5219\u662f\nA. \u201c\u706b\u7b49\u6c14\u201d\nB. \u5148\u653e\u6c14\nC. \u6c14\u706b\u540c\u65f6\nD. \u6c14\u7b49\u706b\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3190122270038759, "meta-math/MetaMath-Mistral-7B": 0.3583275689754825, "itpossible/Chinese-Mistral-7B-v0.1": 0.7632763025729123, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3624670647091065, "meta-llama/Meta-Llama-3-8B": 0.4336343234669925, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3748084897810236}}, {"question": "\u53d8\u538b\u5668\u5728\u989d\u5b9a\u7535\u538b\u4e0b\uff0c\u4e8c\u6b21\u5f00\u8def\u65f6\u5728\u94c1\u82af\u4e2d\u6d88\u8017\u7684\u529f\u7387\u4e3a\nA. \u70ed\u635f\nB. \u65e0\u529f\u635f\u8017\nC. \u94dc\u635f\nD. \u94c1\u635f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7237436154746775, "HuggingFaceH4/zephyr-7b-beta": 0.8473635448522788, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7834686125323809, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6559\u80b2\u6d3b\u52a8\u4e2d\uff0c\u6559\u5e08\u8d1f\u8d23\u7ec4\u7ec7\u3001\u5f15\u5bfc\u5b66\u751f\u6cbf\u7740\u6b63\u786e\u7684\u65b9\u5411\uff0c\u91c7\u7528\u79d1\u5b66\u7684\u65b9\u6cd5\uff0c\u83b7\u5f97\u826f\u597d\u7684\u53d1\u5c55\uff0c\u8fd9\u8868\u660e ()\nA. \u5b66\u751f\u5728\u6559\u80b2\u6d3b\u52a8\u4e2d\u7684\u4e3b\u4f53\u5730\u4f4d\u5f97\u4e0d\u5230\u4f53\u73b0\nB. \u6559\u5e08\u5b8c\u5168\u63a7\u5236\u6559\u80b2\u6d3b\u52a8\nC. \u5b66\u751f\u5728\u6559\u80b2\u6d3b\u52a8\u4e2d\u662f\u88ab\u52a8\u7684\u5ba2\u4f53\nD. \u8981\u5145\u5206\u53d1\u6325\u6559\u5e08\u5728\u6559\u80b2\u6d3b\u52a8\u4e2d\u7684\u4e3b\u5bfc\u4f5c\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7199737144133304, "meta-math/MetaMath-Mistral-7B": 0.8363640252234494, "itpossible/Chinese-Mistral-7B-v0.1": 0.8291409617018257, "HuggingFaceH4/zephyr-7b-beta": 0.9981936800768567, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9129924674409038, "meta-llama/Meta-Llama-3-8B": 0.8865703037136213, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9363686259427471}}, {"question": "\u7f8e\u56fd\u822a\u5929\u5c40\uff08NASA\uff09\u4e8e2019\u5e74\u5ba3\u5e03\u8d85\u671f\u670d\u5f79\u7684\u706b\u661f\u63a2\u6d4b\u5668\u7ed3\u675f\u4f7f\u547d\uff0c\u5b83\u7684\u540d\u5b57\u662f\nA. \u673a\u9047\u53f7\nB. \u6731\u96c0\u53f7\nC. \u52c7\u6c14\u53f7\nD. \u6c34\u624b1\u53f7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.344785902992216, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5708457676108529, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5266794454613537}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u7ed3\u80a0\u764c\u6b63\u786e\u7684\u662f\nA. \u80c3\u80a0\u9053\u80bf\u7624\u4e2d\u9884\u540e\u6700\u597d\nB. \u53f3\u4fa7\u7ed3\u80a0\u764c\u591a\u8868\u73b0\u4e3a\u80a0\u6897\u963b\nC. \u5de6\u4fa7\u7ed3\u80a0\u764c\u591a\u8868\u73b0\u4e3a\u8d2b\u8840\nD. \u65e9\u671f\u4ee5\u8840\u9053\u8f6c\u79fb\u4e3a\u4e3b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3134225987317779, "meta-math/MetaMath-Mistral-7B": 0.4084129141920775, "itpossible/Chinese-Mistral-7B-v0.1": 0.39196262255759057, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u652f\u8def\u7535\u538b\u7b49\u4e8e\u5176\u652f\u8def\u4e24\u7aef\u7535\u538b\u4e4b\u5dee\uff0c\u4e0e\uff08\uff09\u65e0\u5173\u3002\nA. \u7535\u6d41\nB. \u8def\u5f84\nC. \u8282\u70b9\nD. \u7535\u538b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.46027288520897, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5028667229653593, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4596151236092447}}, {"question": "\u804c\u4e1a\u6559\u80b2\u7ba1\u7406\u73b0\u4ee3\u5316\u7684\u6838\u5fc3\u662f\u5b9e\u73b0\u4eba\u7684\nA. \u5c01\u5efa\u5316\nB. \u79d1\u5b66\u5316\nC. \u7d20\u8d28\u5316\nD. \u73b0\u4ee3\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6248381646763812, "meta-math/MetaMath-Mistral-7B": 0.7308038370949873, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.862177948867773, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5219191844392185, "meta-llama/Meta-Llama-3-8B": 0.5161665863503903, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u52a0\u9176\u6d17\u8863\u7c89\u7684\u53d9\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u9ad8\u6e29\u6613\u4f7f\u9176\u5931\u6d3b\uff0c\u56e0\u6b64\u51b7\u6c34\u6d17\u6da4\u53bb\u6c61\u6548\u679c\u5e94\u8be5\u6bd4\u6e29\u6c34\u597d\nB. \u5728pH\u4f4e\u4e8e7.0\u7684\u81ea\u6765\u6c34\u4e2d\uff0c\u78b1\u6027\u86cb\u767d\u9176\u4f9d\u7136\u80fd\u8d77\u4f5c\u7528\nC. \u6d17\u8863\u7c89\u4e2d\u8868\u9762\u6d3b\u6027\u5242\u5bf9\u78b1\u6027\u86cb\u767d\u9176\u6d3b\u6027\u6709\u4e00\u5b9a\u7684\u4fc3\u8fdb\u4f5c\u7528\nD. \u6d17\u8863\u7c89\u4e2d\u9176\u4e3b\u8981\u662f\u901a\u8fc7\u5feb\u901f\u5206\u89e3\u6eb6\u5728\u6c34\u91cc\u7684\u6c61\u6e0d\u53d1\u6325\u4f5c\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u725b\u80be\u548c\u732a\u80be\uff08\uff09\u542b\u91cf\u6700\u9ad8\uff0c\u662f\u5176\u4ed6\u4e00\u822c\u98df\u54c1\u7684\u6570\u5341\u500d\u3002\nA. \u7852\nB. \u950c\nC. \u78f7\nD. \u94ec\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5661227599487898, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4576668889110178, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679c\u6ca1\u6709\u9ea6\u514b\u65af\u97e6\u5728\u7406\u8bba\u4e0a\u8bc1\u660e\u65e0\u7ebf\u7535\u6ce2\u7684\u5b58\u5728\u4ee5\u53ca\u8d6b\u5179\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u9664\u4e86\u5e7b\u60f3\u5bb6\u5916\uff0c\u8c01\u90fd\u4e0d\u4f1a\u60f3\u5230\u4e0d\u8981\u5bfc\u7ebf\u5c31\u53ef\u4ee5\u4f20\u9012\u4fe1\u53f7\u3002\u6750\u6599\u53ef\u7528\u4e8e\u8bf4\u660e\u7b2c\u4e8c\u6b21\u5de5\u4e1a\u9769\u547d\u4e2d\nA. \u79d1\u5b66\u4e0e\u6280\u672f\u771f\u6b63\u7ed3\u5408\nB. \u6280\u672f\u63a8\u52a8\u4e86\u79d1\u5b66\u7406\u8bba\u8fdb\u6b65\nC. \u7535\u62a5\u6210\u4e3a\u6807\u5fd7\u6027\u6210\u5c31\nD. \u7535\u62a5\u53d1\u660e\u5177\u6709\u5076\u7136\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.615862754112481, "HuggingFaceH4/zephyr-7b-beta": 0.8556189466498441, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5494763957420246, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5939296339303243}}, {"question": "\u4e3a\u4fdd\u8bc1\u6ca5\u9752\u6df7\u5408\u6599\u7684\u5f3a\u5ea6\uff0c\u5728\u9009\u62e9\u77f3\u6599\u65f6\u5e94\u4f18\u5148\u8003\u8651\nA. \u4ee5\u4e0a\u5747\u4e0d\u5bf9\nB. \u78b1\u6027\u77f3\u6599\nC. \u4e2d\u6027\u77f3\u6599\nD. \u9178\u6027\u77f3\u6599\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6709\u5173\u5b9e\u9a8c\u539f\u7406\u3001\u65b9\u6cd5\u548c\u7ed3\u8bba\u90fd\u6b63\u786e\u7684\u662f\nA. \u7528\u84b8\u998f\u6c34\u3001\u915a\u915e\u3001BaCl2\u6eb6\u6db2\u548c\u5df2\u77e5\u6d53\u5ea6\u76d0\u9178\u6807\u51c6\u6db2\u4f5c\u8bd5\u5242\uff0c\u53ef\u6d4b\u5b9aNaOH\u56fa\u4f53(\u6742\u8d28\u4ec5\u4e3aNa2CO3)\u7684\u7eaf\u5ea6\nB. \u5b9e\u9a8c\u5ba4\u4e2d\u7684CCl4\u542b\u6709\u5c11\u91cf\u6eb4\uff0c\u52a0\u9002\u91cf\u7684\u82ef\uff0c\u632f\u8361\u3001\u9759\u7f6e\u540e\u5206\u6db2\uff0c\u53ef\u9664\u53bbCCl4\u4e2d\u7684\u6eb4\nC. \u5df2\u77e5Cu2O\uff0b2H\uff0b===Cu2\uff0b\uff0bCu\uff0bH2O\uff0c\u6c22\u6c14\u8fd8\u539f\u6c27\u5316\u94dc\u540e\u6240\u5f97\u7ea2\u8272\u56fa\u4f53\u80fd\u5b8c\u5168\u6eb6\u4e8e\u7a00\u785d\u9178\uff0c\u8bf4\u660e\u8fd8\u539f\u4ea7\u7269\u662f\u94dc\nD. \u53d6\u4e00\u5b9a\u91cf\u6c34\u57a2\u52a0\u76d0\u9178\uff0c\u751f\u6210\u80fd\u4f7f\u6f84\u6e05\u77f3\u7070\u6c34\u53d8\u6d51\u6d4a\u7684\u6c14\u4f53\uff0c\u8bf4\u660e\u6c34\u57a2\u7684\u4e3b\u8981\u6210\u5206\u4e3aCaCO3\u3001MgCO3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3705770177416, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.35898422461669394, "meta-llama/Meta-Llama-3-8B": 0.30601362565976303, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c14\u5757\u4e2d\u7684\u6c34\u6c7d\u51dd\u7ed3\u540e\uff0c\u82e5\u51dd\u7ed3\u7269\u4f5c\u4e3a\u964d\u6c34\u968f\u65f6\u8131\u79bb\u6c14\u5757\u964d\u5230\u5730\u9762\uff0c\u5219\u79f0\u8fd9\u79cd\u60c5\u51b5\u4e0b\u7684\u7a7a\u6c14\u72b6\u6001\u53d8\u5316\u4e3a[]\u3002\nA. \u6e7f\u7edd\u70ed\u8fc7\u7a0b\nB. \u964d\u6c34\u8fc7\u7a0b\nC. \u5047\u7edd\u70ed\u8fc7\u7a0b\nD. \u5e72\u7edd\u70ed\u8fc7\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4310339313236754, "meta-math/MetaMath-Mistral-7B": 0.6691215180281416, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4885556172635421, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5\u51fd\u6570$f(x)=ln(x)+ln(4-x)$\uff0c\u5219\nA. f(x)\u5728(0,4)\u5355\u8c03\u9012\u51cf\nB. y=f(x)\u7684\u56fe\u50cf\u5173\u4e8e\u76f4\u7ebfx=2\u5bf9\u79f0\nC. f(x)\u5728(0,4)\u5355\u8c03\u9012\u589e\nD. y=f(x)\u7684\u56fe\u50cf\u5173\u4e8e\u70b9(2,0)\u5bf9\u79f0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2658638974902097, "meta-math/MetaMath-Mistral-7B": 0.4180371668839167, "itpossible/Chinese-Mistral-7B-v0.1": 0.3168502611649064, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4978740512106581}}, {"question": "\u4e2d\u56fd\u54f2\u5b66\u8087\u7aef\u7684\u5f62\u5f0f\u662f\uff1a\nA. \u5173\u4e4e\u5929\u6587\uff0c\u5173\u4e4e\u4eba\u6587\nB. \u4ee5\u4e0a\u90fd\u4e0d\u5bf9\nC. \u795e\u8bdd\u4f20\u5947\nD. \u6218\u4e89\u593a\u6743\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4597169471792587, "meta-math/MetaMath-Mistral-7B": 0.9610052471376866, "itpossible/Chinese-Mistral-7B-v0.1": 0.4671452929714372, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9128146996985621, "meta-llama/Meta-Llama-3-8B": 0.4218340661721447, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7002597892078051}}, {"question": "\u4e0b\u5217\u54ea\u79cd\u4e1c\u897f\u53ef\u4ee5\u5e2e\u4eba\u964d\u4f4e\u8840\u8102\nA. \u725b\u5976\nB. \u767d\u5f00\u6c34\nC. \u8c46\u6d46\nD. \u76d0\u6c34\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6456758885828096, "meta-math/MetaMath-Mistral-7B": 0.5906171585102242, "itpossible/Chinese-Mistral-7B-v0.1": 0.8500813569067628, "HuggingFaceH4/zephyr-7b-beta": 0.9993112163156427, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9337534255170642, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7987887346885183}}, {"question": "\u4eba\u7c7b\u767d\u5316\u75c7\u662f\u5e38\u67d3\u8272\u4f53\u5355\u57fa\u56e0\u9690\u6027\u9057\u4f20\u75c5\uff0c\u8fd9\u610f\u5473\u7740\u767d\u5316\u75c7\u60a3\u8005\u7684\u53cc\u4eb2\u5fc5\u987b\nA. \u53cc\u4eb2\u90fd\u662f\u767d\u5316\u75c7\u60a3\u8005\nB. \u53cc\u4eb2\u4e4b\u4e00\u662f\u643a\u643a\u5e26\u8005\nC. \u53cc\u4eb2\u90fd\u662f\u7eaf\u5408\u4f53\nD. \u53cc\u4eb2\u90fd\u662f\u81f4\u75c5\u57fa\u56e0\u643a\u5e26\u8005\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5481732800165057, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e0b\u5217\u5404\u9879\u63d0\u9ad8\u4f1a\u8ba1\u804c\u4e1a\u9053\u5fb7\u6c34\u5e73\u7684\u63aa\u65bd\u4e2d\uff0c\u5c5e\u4e8e\u6df1\u5316\u4f53\u5236\u6539\u9769\u3001\u8f6c\u6362\u653f\u5e9c\u804c\u80fd\u7684\u662f\nA. \u63d0\u9ad8\u4f1a\u8ba1\u4eba\u5458\u7684\u4e13\u4e1a\u6280\u80fd\nB. \u5efa\u7acb\u9053\u5fb7\u884c\u4e3a\u6863\u6848\nC. \u5b8c\u5584\u516c\u53f8\u6cbb\u7406\u5236\u5ea6\nD. \u52a0\u5f3a\u793e\u4f1a\u8206\u8bba\u76d1\u7763\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6252649065815391, "meta-math/MetaMath-Mistral-7B": 0.7413557914523254, "itpossible/Chinese-Mistral-7B-v0.1": 0.7135289698667105, "HuggingFaceH4/zephyr-7b-beta": 0.9920658782059758, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.620516108055024, "meta-llama/Meta-Llama-3-8B": 0.7480298241408806, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8446565574556271}}, {"question": "\u5305\u8fc7\u6ee4\u6280\u672f\u9632\u706b\u5899\u5728\u8fc7\u6ee4\u6570\u636e\u5305\u65f6\uff0c\u4e00\u822c\u4e0d\u5173\u5fc3\nA. \u6570\u636e\u5305\u7684\u534f\u8bae\u7c7b\u578b\nB. \u6570\u636e\u5305\u7684\u5185\u5bb9\nC. \u6570\u636e\u5305\u7684\u6e90\u5730\u5740\nD. \u6570\u636e\u5305\u7684\u76ee\u7684\u5730\u5740\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7527544482067055, "meta-math/MetaMath-Mistral-7B": 0.8620426185160264, "itpossible/Chinese-Mistral-7B-v0.1": 0.5174255073248947, "HuggingFaceH4/zephyr-7b-beta": 0.9981834773956353, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6695348947804207, "meta-llama/Meta-Llama-3-8B": 0.37480846975763354, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e00\u4e2a\u5c0f\u578b\u6e56\u6cca\u5185\u6295\u9975\u517b\u6b96\u8089\u98df\u6027\u9c7c\u7c7b\u30025\u5e74\u540e\uff0c\u85fb\u7c7b\u7206\u53d1\uff0c\u5f15\u8d77\u6c34\u8349\uff08\u6c89\u6c34\u690d\u7269\uff09\u6b7b\u4ea1\uff0c\u4e4b\u540e\u6d6e\u6e38\u52a8\u7269\u53ca\u9c7c\u7c7b\u5927\u91cf\u6b7b\u4ea1\uff0c\u5bfc\u81f4\u6c34\u4f53\u53d1\u81ed\u3002\u4e0b\u5217\u53d9\u8ff0\u9519\u8bef\u7684\nA. \u6c34\u751f\u751f\u7269\u6b7b\u4ea1\u52a0\u91cd\u4e86\u6c34\u4f53\u7684\u6c61\u67d3\uff0c\u5c5e\u4e8e\u6b63\u53cd\u9988\u8c03\u8282\nB. \u5bfc\u81f4\u6c34\u8349\u6b7b\u4ea1\u7684\u6700\u4e3b\u8981\u975e\u751f\u7269\u56e0\u7d20\u662f\u5149\u7167\nC. \u6d41\u7ecf\u8be5\u6e56\u6cca\u7684\u603b\u80fd\u91cf\u662f\u751f\u4ea7\u8005\u6240\u56fa\u5b9a\u7684\u592a\u9633\u80fd\u603b\u91cf\nD. \u6295\u653e\u4ee5\u6d6e\u6e38\u690d\u7269\u4e3a\u98df\u7684\u9c7c\u7c7b\u53ef\u9632\u6b62\u85fb\u7c7b\u7684\u7206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.30817677628857404, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ecf\u6d4e\u5168\u7403\u5316\u4e0e\u533a\u57df\u96c6\u56e2\u5316\u4e24\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u662f\u4ec0\u4e48\nA. \u6709\u4e00\u5b9a\u77db\u76fe\uff0c\u4f46\u4ece\u957f\u8fdc\u770b\u662f\u76f8\u4e92\u4fc3\u8fdb\u3001\u5e76\u884c\u4e0d\u6096\nB. \u6709\u4e00\u5b9a\u8054\u7cfb\uff0c\u4f46\u4ece\u957f\u8fdc\u770b\u662f\u76f8\u4e92\u5bf9\u7acb\u7684\nC. \u65e0\u77db\u76fe\uff0c\u5b8c\u5168\u4e00\u81f4\nD. \u65e0\u8054\u7cfb\uff0c\u5e73\u884c\u53d1\u5c55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8103378608164893, "meta-math/MetaMath-Mistral-7B": 0.9465299758436015, "itpossible/Chinese-Mistral-7B-v0.1": 0.9337904098124445, "HuggingFaceH4/zephyr-7b-beta": 0.9964605950879841, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9440829373971158, "meta-llama/Meta-Llama-3-8B": 0.8414861218488298, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9604586278208238}}, {"question": "\u4e0b\u5217\u60c5\u5f62\u4e2d\uff0c\u7b26\u5408\u6211\u56fd\u6cd5\u5f8b\u9002\u7528\u539f\u5219\u7684\u662f\nA. \u68c0\u5bdf\u5b98\u5b59\u67d0\u5728\u529e\u6848\u8fc7\u7a0b\u4e2d\u63a5\u5230\u9886\u5bfc\u6279\u793a\uff0c\u5e76\u6309\u6279\u793a\u8981\u6c42\u5904\u7406\u6848\u4ef6\nB. \u67d0\u76d1\u72f1\u4f9d\u7167\u6cd5\u5f8b\u89c4\u5b9a\uff0c\u6279\u51c6\u6b63\u5728\u670d\u5211\u7684\u8d75\u67d0\u4fdd\u5916\u5c31\u533b\nC. \u6cd5\u5b98\u94b1\u67d0\u4e3a\u529e\u597d\u6848\u4ef6\uff0c\u591a\u6b21\u4e0e\u539f\u3001\u88ab\u544a\u53cc\u65b9\u79c1\u4e0b\u63a5\u89e6\nD. \u6751\u957f\u674e\u67d0\u5bf9\u4e00\u8d77\u5f3a\u5978\u6848\u8fdb\u884c\u534f\u8c03\uff0c\u6700\u7ec8\u4fc3\u6210\u53cc\u65b9\u4ee5\u8d54\u507f5000\u5143\u79c1\u4e86\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.632924022231153, "meta-math/MetaMath-Mistral-7B": 0.9396887630048214, "itpossible/Chinese-Mistral-7B-v0.1": 0.7193708657537343, "HuggingFaceH4/zephyr-7b-beta": 0.9992155333963842, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8810373334205256, "meta-llama/Meta-Llama-3-8B": 0.7829176115470042, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9223915249764217}}, {"question": "\u51fa\u73b0\u5728\u79e6\u4ee3\u7684\u4e66\u672c\u662f\nA. \u7532\u9aa8\u6587\nB. \u5c0f\u7bc6\nC. \u6977\u4e66\nD. \u91d1\u6587\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3386364065941128, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3949756340836636, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c35 \u5c81\u3002\u4e4f\u529b\u3001\u5fc3\u60b81\u5e74\u4f59\uff0c\u8fd12\u4e2a\u6708\u75c7\u72b6\u52a0\u91cd\uff0c\u4f34\u538c\u98df\u3001\u6d88\u7626\u3001\u624b\u98a4\u3002\u67e5\u4f53:\u7532\u72b6\u817a\u5f25\u6f2b\u6027\u80bf\u5927\uff0c\u5fc3\u7387 126 \u6b21/\u5206\uff0c\u5fc3\u5f8b\u6574\u3002\u5b9e\u9a8c\u5ba4\u68c0\u67e5\u63d0\u793a FT3\u3001FT4 \u663e\u8457\u589e\u9ad8\uff0cTSH\u964d\u4f4e\u3002\u8be5\u60a3\u8005\u6700\u53ef\u80fd\u7684\u8bca\u65ad\u662f\nA. \u591a\u7ed3\u8282\u6027\u6bd2\u6027\u7532\u72b6\u817a\u80bf\nB. \u4e9a\u6025\u6027\u7532\u72b6\u817a\u708e\nC. \u81ea\u8eab\u514d\u75ab\u7532\u72b6\u817a\u708e\nD. Graves \u75c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6188861973947422, "meta-math/MetaMath-Mistral-7B": 0.9040042404531797, "itpossible/Chinese-Mistral-7B-v0.1": 0.8304074448545863, "HuggingFaceH4/zephyr-7b-beta": 0.999864151298219, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5019550131072141, "meta-llama/Meta-Llama-3-8B": 0.5853619126927211, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7646356934834896}}, {"question": "\u5728\u7528\u81ea\u5df1\u7684\u8ba1\u7b97\u673a\u6d4f\u89c8\u7f51\u9875\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5982\u679c\u53d1\u73b0\u559c\u6b22\u7684\u7f51\u9875\u5e76\u5e0c\u671b\u4ee5\u540e\u591a\u6b21\u8bbf\u95ee\uff0c\u6700\u597d\u7684\u65b9\u6cd5\u662f\u628a\u8fd9\u4e2a\u9875\u9762\u5730\u5740\nA. \u653e\u5230\u6536\u85cf\u5939\u4e2d\nB. \u7528\u7b14\u8bb0\u6284\u5199\u5230\u7b14\u8bb0\u672c\u4e0a\nC. \u7528\u6587\u672c\u6587\u4ef6\u4fdd\u5b58\u4e0b\u6765\nD. \u8bb0\u5fc6\u4e0b\u6765\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.987786419154329, "meta-math/MetaMath-Mistral-7B": 0.9996528379433396, "itpossible/Chinese-Mistral-7B-v0.1": 0.9871805530727898, "HuggingFaceH4/zephyr-7b-beta": 0.9999986603604953, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9991705263630243, "meta-llama/Meta-Llama-3-8B": 0.9882394842477583, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9995986376469228}}, {"question": "\u6709\u89c2\u70b9\u8ba4\u4e3a\uff0c\u4eba\u4eec\u5bf9\u5927\u4f17\u4f20\u5a92\u53d1\u51fa\u4fe1\u606f\u7684\u9009\u62e9\u4f1a\u53d7\u5230\u5bb6\u5ead\u3001\u670b\u53cb\u548c\u719f\u4eba\u7b49\u7684\u5f71\u54cd\u3002\u8be5\u89c2\u70b9\u5c5e\u4e8e\u53d7\u4f17\u7406\u8bba\u4e2d\u7684\nA. \u9009\u62e9\u6027\u56e0\u7d20\u7406\u8bba\nB. \u4e2a\u4eba\u5dee\u5f02\u8bba\nC. \u793e\u4f1a\u5206\u7c7b\u8bba\nD. \u793e\u4f1a\u5173\u7cfb\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.40650588840997465, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5971291049726765, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.44422988298886246}}, {"question": "\u68c0\u9a8c\u611f\u53d7\u6027\u5927\u5c0f\u7684\u57fa\u672c\u6307\u6807\u88ab\u79f0\u4e3a\u611f\u89c9\u9608\u9650\uff0c\u662f\u4e2a\u4e34\u754c\u503c\u3002\u5b83\u5305\u62ec\uff1a\u7edd\u5bf9\u611f \u89c9\u9608\u9650\u548c\nA. \u7edd\u5bf9\u611f\u53d7\u6027\nB. \u5dee\u522b\u611f\u89c9\u9608\u9650\nC. \u7edd\u5bf9\u523a\u6fc0\u91cf\nD. \u5dee\u522b\u523a\u6fc0\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35321825629781733, "meta-math/MetaMath-Mistral-7B": 0.4228468872210799, "itpossible/Chinese-Mistral-7B-v0.1": 0.6684857823634706, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5326190697185917, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9721144323838316}}, {"question": "\u4e0d\u7528\u900f\u660e\u5bb9\u5668\u800c\u91c7\u7528\u68d5\u8272\u725b\u5976\u74f6\u548c\u907f\u5149\u7eb8\u76d2\u5305\u88c5\u725b\u5976\u7684\u539f\u56e0\u662f\nA. \u725b\u5976\u80f6\u4f53\u5728\u8d2e\u5b58\u4e2d\u6613\u53d1\u751f\u6df7\u6d4a\u548c\u5206\u5c42\nB. \u725b\u5976\u5bcc\u542b\u6838\u9ec4\u7d20\uff0c\u4f46\u6838\u9ec4\u7d20\u5bf9\u5149\u975e\u5e38\u654f\u611f\nC. \u725b\u5976\u5bcc\u542b\u6297\u574f\u8840\u9178\uff0c\u4f46\u6297\u574f\u8840\u9178\u5bf9\u5149\u662f\u4e0d\u7a33\u5b9a\u7684\nD. \u725b\u5976\u9047\u5149\u6613\u53d1\u751f\u78f7\u9178\u9499\u6c89\u6dc0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9084480871545697, "meta-math/MetaMath-Mistral-7B": 0.9888008392212604, "itpossible/Chinese-Mistral-7B-v0.1": 0.8486143805021888, "HuggingFaceH4/zephyr-7b-beta": 0.9949048687066148, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9531280852907149, "meta-llama/Meta-Llama-3-8B": 0.8272616548822112, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7552047280299096}}, {"question": "\u5728\u201c\u6211\u7684\u6587\u6863\u201d\u4e2d\uff0c\u80af\u5b9a\u53ef\u4ee5\u5efa\u7acb\nA. \u4e24\u4e2a\u4e0d\u540c\u540d\u7684\u6587\u4ef6\u5939\nB. \u4e24\u4e2a\u540c\u540d\u7684\u6587\u4ef6\nC. \u4e24\u4e2a\u540c\u540d\u7684\u6587\u4ef6\u5939\nD. \u540c\u540d\u7684\u6587\u4ef6\u548c\u6587\u4ef6\u5939\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.973724967853727}}, {"question": "\u6839\u636e\u2f5c\u987f\u8fd0\u52a8\u5b9a\u5f8b\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u5916\u2f12\u505c\u2f4c\u4f5c\u2f64\u540e\uff0c\u7269\u4f53\u7531\u4e8e\u60ef\u6027\u4f1a\u505c\u2f4c\nB. \u2f12\u662f\u6539\u53d8\u7269\u4f53\u8fd0\u52a8\u72b6\u6001\u7684\u539f\u56e0\nC. \u7269\u4f53\u505a\u5300\u901f\u76f4\u7ebf\u8fd0\u52a8\u65f6\uff0c\u6240\u53d7\u5408\u5916\u2f12\u4e0d\u4e3a\u96f6\nD. \u2f12\u662f\u7ef4\u6301\u7269\u4f53\u8fd0\u52a8\u7684\u539f\u56e0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4488746704506832, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3976839562230386, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u76d0\u7684\u8bf4\u6cd5\u9519\u8bef\u7684\u662f\nA. \u76d0\u53c8\u79f0\u201c\u767e\u5473\u4e4b\u738b\u201d\uff0c\u662f\u54b8\u5473\u7684\u8f7d\u4f53\uff0c\u5177\u6709\u53bb\u8165\u589e\u9c9c\u4e4b\u7528\nB. \u4eba\u4f53\u5982\u679c\u6444\u5165\u8fc7\u591a\u7684\u76d0\u4efd\uff0c\u5bb9\u6613\u4ea7\u751f\u9ad8\u8840\u538b\u3001\u6c34\u80bf\u7b49\u95ee\u9898\nC. \u751f\u6d3b\u4e2d\u7684\u4f4e\u94a0\u76d0\u52a0\u5165\u4e86\u4e00\u5b9a\u6bd4\u4f8b\u7684\u6c30\u5316\u94be\uff0c\u5176\u54b8\u5473\u8f83\u6de1\nD. \u6309\u6765\u6e90\u53ca\u5f00\u91c7\u65b9\u5f0f\u5206\u7c7b\uff0c\u76d0\u53ef\u5206\u4e3a\uff1a\u4e95\u76d0\u3001\u6d77\u76d0\u3001\u6e56\u76d0\u7b49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28986940000819506, "meta-math/MetaMath-Mistral-7B": 0.4703069356993337, "itpossible/Chinese-Mistral-7B-v0.1": 0.6518747581734077, "HuggingFaceH4/zephyr-7b-beta": 0.997296891921326, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4420441925857273, "meta-llama/Meta-Llama-3-8B": 0.353703093539192, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6781788483616602}}, {"question": "\u7406\u89e3\u4eba\u7c7b\u793e\u4f1a\u53d1\u5c55\u7684\u94a5\u5319\u662f\nA. \u6587\u5316\u53d1\u5c55\u53f2\nB. \u601d\u60f3\u53d1\u5c55\u53f2\nC. \u9636\u7ea7\u6597\u4e89\u53f2\nD. \u52b3\u52a8\u53d1\u5c55\u53f2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4134293535394713, "meta-math/MetaMath-Mistral-7B": 0.4790513830107404, "itpossible/Chinese-Mistral-7B-v0.1": 0.6268009869966066, "HuggingFaceH4/zephyr-7b-beta": 0.7520705313886533, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.852710364625948, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.43094735845378906}}, {"question": "\u7279\u53d1\u6027\u80ba\u7ea4\u7ef4\u5316\u7684\u5178\u578b\u80ba\u5bb9\u91cf\u548c\u901a\u6c14\u529f\u80fd\u7684\u7279\u5f81\u6027\u53d8\u5316\u662f\nA. FEV1/FVC \u51cf\u4f4e\nB. TLC \u51cf\u4f4e\nC. RV/TLC \u5347\u9ad8\nD. FEV1 \u5360\u9884\u8ba1\u503c\u767e\u5206\u6bd4\u51cf\u4f4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u4ea7\u54c1\u6210\u957f\u671f\u7684\u63cf\u8ff0\u4e2d\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u7531\u4e8e\u5e02\u573a\u9010\u6b65\u6269\u5927\uff0c\u6240\u4ee5\u4ef7\u683c\u968f\u4e4b\u4e0a\u6da8\nB. \u4ea7\u54c1\u7684\u751f\u4ea7\u5de5\u827a\u8d8b\u4e8e\u7a33\u5b9a\nC. \u5927\u6279\u7ade\u4e89\u8005\u52a0\u5165\uff0c\u5e02\u573a\u7ade\u4e89\u52a0\u5267\nD. \u6d88\u8d39\u8005\u5bf9\u4ea7\u54c1\u5df2\u719f\u6089\uff0c\u9500\u552e\u91cf\u589e\u957f\u5f88\u5feb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31815811094261875, "meta-math/MetaMath-Mistral-7B": 0.6138479478070499, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.720048710383745, "meta-llama/Meta-Llama-3-8B": 0.5028667305872193, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9608848600454805}}, {"question": "\u8bbe\u6709 $\\mathrm{X} \u3001 \\mathrm{Y}$ \u4e24\u7ec4\u6570\u636e\uff0c\u6c42\u5f97 $\\hat{y}=a+b x$\uff0c\u7ecf\u7edf\u8ba1\u5b66\u68c0\u9a8c\uff0c\u5728 $\\alpha=0.05$ \u6c34\u5e73\u4e0a\u62d2\u7edd $\\mathrm{H}_0$ : $\\beta=0$\uff0c\u5219\u81f3\u5c11\u6709 95\\%\u7684\u628a\u63e1\u65ad\u8a00 $\\mathrm{y}$ \u4e0e $\\mathrm{x}$ \u4e4b\u95f4\u5728\u4e13\u4e1a\u4e0a\u6709\u76f4\u7ebf\u5173\u7cfb\u3002\u8fd9\u4e00\u7ed3\u8bba ()\u3002\nA. \u5f88\u6709\u79d1\u5b66\u6027\nB. \u8131\u79bb\u5b9e\u9645\nC. \u7565\u6709\u95ee\u9898\nD. \u6839\u636e\u5145\u5206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u76f4\u7ebf\u578b\u7ec4\u7ec7\u7ed3\u6784\u4e00\u822c\u53ea\u9002\u5e94\u4e8e\nA. \u8de8\u56fd\u4f01\u4e1a\nB. \u521b\u65b0\u4efb\u52a1\u8f83\u9ad8\uff0c\u751f\u4ea7\u7ecf\u8425\u591a\u53d8\u7684\u4f01\u4e1a\nC. \u5927\u578b\u4f01\u4e1a\nD. \u89c4\u6a21\u8f83\u5c0f\uff0c\u751f\u4ea7\u6280\u672f\u4e0e\u5de5\u827a\u8fc7\u7a0b\u6bd4\u8f83\u7b80\u5355\u3001\u4ea7\u54c1\u5355\u4e00\u7684\u4f01\u4e1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8015782051891479, "meta-math/MetaMath-Mistral-7B": 0.9759046162210511, "itpossible/Chinese-Mistral-7B-v0.1": 0.7035782127167138, "HuggingFaceH4/zephyr-7b-beta": 0.9998829103778845, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8546051232425658, "meta-llama/Meta-Llama-3-8B": 0.9245377846659028, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b0\u95fb\u4e8b\u4e1a\u4ea7\u751f\u548c\u53d1\u5c55\u7684\u6839\u672c\u52a8\u529b\u662f\nA. \u5e02\u573a\u7ecf\u6d4e\u548c\u8d44\u672c\u4e3b\u4e49\u751f\u4ea7\u65b9\u5f0f\nB. \u751f\u4ea7\u793e\u4f1a\u5316\u7a0b\u5ea6\u63d0\u9ad8\u5e26\u6765\u7684\u4fe1\u606f\u9700\u6c42\u589e\u957f\nC. \u6587\u5316\u53d1\u8fbe\u7a0b\u5ea6\nD. \u6280\u672f\u8fdb\u6b65\u548c\u653f\u6cbb\u6c11\u4e3b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6105384102133993, "itpossible/Chinese-Mistral-7B-v0.1": 0.8291409595908346, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5233758337295195, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8020356932423577}}, {"question": "\u4e0b\u5217\u5c5e\u4e8e\u9ad8\u5ea6\u4fb5\u88ad\u6027\u6dcb\u5df4\u7624\u7684\u662f\nA. \u5957\u7ec6\u80de\u6dcb\u5df4\u7624\nB. Burkitt \u6dcb\u5df4\u7624\nC. \u539f\u53d1\u6027\u76ae\u80a4\u95f4\u53d8\u5927\u7ec6\u80de\u6dcb\u5df4\u7624\nD. \u5f25\u6f2b\u5927B\u7ec6\u80de\u6dcb\u5df4\u7624\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5769652534920188, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.706584293692389}}, {"question": "\u9632\u62b1\u6b7b\u5236\u52a8\u7cfb\u7edf\uff08ABS\uff09\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\u53ef\u4ee5\u6700\u5927\u9650\u5ea6\u53d1\u6325\u5236\u52a8\u5668\u6548\u80fd\nA. \u7d27\u6025\u5236\u52a8\nB. \u95f4\u6b47\u5236\u52a8\nC. \u6301\u7eed\u5236\u52a8\nD. \u7f13\u8e0f\u5236\u52a8\u8e0f\u677f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8331649984263704, "meta-math/MetaMath-Mistral-7B": 0.9676168802853915, "itpossible/Chinese-Mistral-7B-v0.1": 0.7332870490925242, "HuggingFaceH4/zephyr-7b-beta": 0.9990683639102089, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8937346618721502, "meta-llama/Meta-Llama-3-8B": 0.33619507220286804, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9301821045869129}}, {"question": "\u80c6\u77f3\u75c7\u75c5\u4eba\u5207\u9664\u80c6\u56ca\u540e\u6700\u53ef\u80fd\u51fa\u73b0\u7684\u60c5\u51b5\u662f\nA. \u589e\u52a0\u6d88\u5316\u6027\u6e83\u75a1\u98ce\u9669\nB. \u4e0d\u4f1a\u518d\u53d1\u751f\u80c6\u9053\u7ed3\u77f3\nC. \u5f71\u54cd\u8102\u80aa\u6027\u98df\u7269\u7684\u6d88\u5316\nD. \u80c6\u6c41\u6392\u51fa\u91cf\u660e\u663e\u51cf\u5c11\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65e0\u56fd\u7c4d\u4eba\u7532\u5728\u7f8e\u56fd\u901a\u8fc7\u7f51\u7edc\u5bf9\u6b63\u5728\u4e2d\u56fd\u65c5\u6e38\u7684\u82f1\u56fd\u4eba\u5b9e\u65bd\u8bc8\u9a97\uff0c\u9a97\u5f97\u5de8\u989d\u94b1\u6b3e\uff0c\u5bf9\u7532\u7684\u884c\u4e3a\uff0c\u6211\u56fd\u53f8\u6cd5\u673a\u5173\nA. \u4f9d\u666e\u904d\u7ba1\u8f96\u539f\u5219\u4eab\u6709\u5211\u4e8b\u7ba1\u8f96\u6743\nB. \u4f9d\u4fdd\u62a4\u539f\u5219\u4eab\u6709\u5211\u4e8b\u7ba1\u8f96\u6743\nC. \u6ca1\u6709\u5211\u4e8b\u7ba1\u8f96\u6743\nD. \u4f9d\u5c5e\u5730\u539f\u5219\u4eab\u6709\u5211\u4e8b\u7ba1\u8f96\u6743\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3527809287490976}}, {"question": "20\u4e16\u7eaa\u521d\uff0c\u7b2c\u4e00\u4e2a\u5c06\u65b0\u95fb\u4ef7\u503c\u7406\u8bba\u5f15\u5165\u4e2d\u56fd\u7684\u662f\nA. \u90b5\u98d8\u840d\nB. \u6208\u516c\u632f\nC. \u8521\u5143\u57f9\nD. \u5f90\u5b9d\u749c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.34412869855849515, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u4e0e\u6709\u673a\u7269\u7684\u7ed3\u6784\u3001\u6027\u8d28\u6709\u5173\u7684\u53d9\u8ff0\u6b63\u786e\u7684\u662f\nA. \u8461\u8404\u7cd6\u3001\u679c\u7cd6\u7684\u5206\u5b50\u5f0f\u5747\u4e3aC6H12O6\uff0c\u4e8c\u8005\u4e92\u4e3a\u540c\u5206\u5f02\u6784\u4f53\nB. \u7532\u70f7\u548cCl2\u7684\u53cd\u5e94\u4e0e\u4e59\u70ef\u548cBr2\u7684\u53cd\u5e94\u5c5e\u4e8e\u540c\u4e00\u7c7b\u578b\u7684\u53cd\u5e94\nC. \u82ef\u3001\u6cb9\u8102\u5747\u4e0d\u80fd\u4f7f\u9178\u6027KMnO4\u6eb6\u6db2\u892a\u8272\nD. \u4e59\u9187\u3001\u4e59\u9178\u5747\u80fd\u4e0eNa\u53cd\u5e94\u653e\u51faH2\uff0c\u4e8c\u8005\u5206\u5b50\u4e2d\u5b98\u80fd\u56e2\u76f8\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2994509091275925, "meta-math/MetaMath-Mistral-7B": 0.41270331149012474, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8532321903456264}}, {"question": "\u201c\u4e03\u6708\u6d41\u706b\uff0c\u4e5d\u6708\u6388\u8863\u201d\uff0c\u5176\u4e2d\u201c\u4e03\u6708\u6d41\u706b\u201d\u6307\u7684\u662f\nA. \u5929\u6c14\u9010\u6e10\u5347\u6e29\nB. \u6d41\u661f\u5f02\u5e38\u51fa\u73b0\nC. \u5929\u6c14\u6e10\u6e10\u8f6c\u51c9\nD. \u5929\u6c14\u708e\u70ed\u4f3c\u706b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6587\u5316\u51b2\u7a81\u7684\u6839\u6e90\u9664\u4e86\u6c11\u65cf\u6027\u4e4b\u5916\uff0c\u66f4\u6709\nA. \u5730\u57df\u6027\nB. \u56fd\u5bb6\u6027\nC. \u9636\u7ea7\u6027\nD. \u9636\u5c42\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.32567743233302715, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5015219388431061}}, {"question": "\u75f0\u3001\u996e\u3001\u6c34\u3001\u6e7f\u540c\u6e90\u800c\u5f02\u6d41\uff0c\u5176\u4e2d\u6700\u6e05\u7a00\u7684\u662f\nA. \u6c34\nB. \u6e7f\nC. \u996e\nD. \u75f0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3148300531811561, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u2014\u4e1c\u76df\u81ea\u8d38\u533a\u5168\u9762\u5efa\u6210\u5e26\u52a8\u4e86\u53cc\u8fb9\u8d38\u6613\u5feb\u901f\u589e\u957f\u30022010\u5e741\u5b63\u5ea6\uff0c\u6211\u56fd\u5bf9\u4e1c\u76df\u51fa\u53e3292.3\u4ebf\u7f8e\u5143\uff0c\u589e\u957f46.7%\uff0c\u81ea\u4e1c\u76df\u8fdb\u53e3336.8\u4ebf\u7f8e\u5143\uff0c\u589e\u957f76.6%\u3002\u8fd9\u8868\u660e\nA. \u533a\u57df\u6027\u96c6\u56e2\u6709\u5229\u4e8e\u5de9\u56fa\u6269\u5927\u96c6\u56e2\u5185\u90e8\u5e02\u573a\nB. \u6211\u56fd\u5728\u4e0e\u4e1c\u76df\u7684\u8d38\u6613\u4e2d\u4e00\u76f4\u5904\u4e8e\u52a3\u52bf\nC. \u4e1c\u76df\u662f\u6211\u56fd\u6700\u5927\u7684\u8d38\u6613\u4f19\u4f34\nD. \u5bf9\u5916\u5f00\u653e\u662f\u6211\u56fd\u7684\u57fa\u672c\u56fd\u7b56\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3325697047103148, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5643389075185399, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4fe1\u7528\u8bc1\u662f\u94f6\u884c\u6839\u636e\u4e0b\u5217\u54ea\u4e00\u65b9\u7684\u8bf7\u6c42\uff0c\u5f00\u7ed9\u51fa\u53e3\u65b9\u7684\u4e00\u79cd\u4fdd\u8bc1\u627f\u62c5\u652f\u4ed8\u8d27\u6b3e\u8d23\u4efb\u7684\u4e66\u9762\u51ed\u8bc1\nA. \u4eba\nB. \u6258\u6536\u4eba\nC. \u8fdb\u53e3\u65b9\nD. \u4e2d\u4ecb\u884c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39855636281732093, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.41098419480502263, "HuggingFaceH4/zephyr-7b-beta": 0.8327914089981456, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5026135455643435, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8747134378973759}}, {"question": "\u809d\u786c\u5316\u4e0e\u7f29\u7a84\u6027\u5fc3\u5305\u708e\u7684\u6c34\u80bf\u9274\u522b\u70b9\u662f\nA. \u6709\u65e0\u8179\u6c34\nB. \u6709\u65e0\u4e0b\u80a2\u6c34\u80bf\nC. \u6709\u65e0\u9888\u9759\u8109\u6012\u5f20\nD. \u6709\u65e0\u809d\u80bf\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.40202865642572594, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.49516341576988154}}, {"question": "\u4ee5\u4e0b\u4e0d\u5c5e\u4e8e\u4fe1\u606f\u5b89\u5168\u98ce\u9669\u8bc4\u4f30\u4e2d\u9700\u8981\u8bc6\u522b\u7684\u5bf9\u8c61\u662f\nA. \u98ce\u9669\u8bc6\u522b\nB. \u8d44\u4ea7\u8bc6\u522b\nC. \u8106\u5f31\u6027\u8bc6\u522b\nD. \u5a01\u80c1\u8bc6\u522b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.37491963634251846, "itpossible/Chinese-Mistral-7B-v0.1": 0.42019965192620573, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5372592281073606, "meta-llama/Meta-Llama-3-8B": 0.4430448139737191, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.37463310290620944}}, {"question": "\u7ef4\u62a4\u793e\u4f1a\u4e3b\u4e49\u7acb\u6cd5\u6743\u5a01\u6027\u7684\u4e00\u9879\u91cd\u8981\u539f\u5219\u662f\nA. \u5b9e\u4e8b\u6c42\u662f\uff0c\u4e00\u5207\u4ece\u5b9e\u9645\u51fa\u53d1\nB. \u7ef4\u62a4\u6cd5\u7684\u7a33\u5b9a\u6027\u3001\u8fde\u7eed\u6027\u548c\u4e25\u8083\u6027\u4e0e\u53ca\u65f6\u521b\u3001\u6539\u3001\u5e9f\u76f8\u7ed3\u5408\nC. \u539f\u5219\u6027\u548c\u7075\u6d3b\u6027\u76f8\u7ed3\u5408\nD. \u7fa4\u4f17\u8def\u7ebf\u548c\u4e13\u95e8\u673a\u5173\u5de5\u4f5c\u76f8\u7ed3\u5408\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5127221638772507, "meta-math/MetaMath-Mistral-7B": 0.8039025005951921, "itpossible/Chinese-Mistral-7B-v0.1": 0.474069820068834, "HuggingFaceH4/zephyr-7b-beta": 0.9987625189538835, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6590894750898665, "meta-llama/Meta-Llama-3-8B": 0.8546051084301903, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7131773704190583}}, {"question": "\u5973\u6027\uff0c16\u5c81\u30026\u4e2a\u6708\u6765\u5de6\u5c0f\u817f\u4e0a\u6bb5\u80bf\u80c0\u75bc\u75db\uff0c\u8fd11\u4e2a\u6708\u6765\u80bf\u80c0\u660e\u663e\uff0c\u4ee5\u591c\u95f4\u75db\u4e3a\u8457\u3002\u67e5\u4f53:\u5de6\u5c0f\u817f\u4e0a\u6bb5\u80bf\u80c0\uff0c\u6d45\u9759\u8109\u6012\u5f20\uff0c\u538b\u75db\u660e\u663e\uff0c\u89e6\u53ca\u4e00\u76f4\u5f84\u7ea66cm\u5de6\u53f3\u80bf\u5757\uff0c\u8d28\u786c\uff0c\u56fa\u5b9a\uff0c\u8fb9\u754c\u4e0d\u6e05\u3002X\u7ebf\u68c0\u67e5\u793a\uff0c\u5de6\u80eb\u9aa8\u4e0a\u7aef\u5448\u866b\u8680\u72b6\u6eb6\u9aa8\u6027\u7834\u574f\uff0c\u9aa8\u819c\u53cd\u5e94\u660e\u663e\uff0c\u53ef\u89c1Codman\u4e09\u89d2\u3002\u5728\u624b\u672f\u6cbb\u7597\u524d\uff0c\u5fc5\u987b\u8981\u8fdb\u884c\u7684\u68c0\u67e5\u662f\nA. \u80f8\u90e8X\u7ebf\u6444\u7247\nB. \u5934\u9885CT\nC. \u8179\u80a1\u6c9f\u6dcb\u5df4\u7ed3\u6d3b\u68c0\nD. \u80bf\u5757\u7a7f\u523a\u7269\u7ec6\u83cc\u57f9\u517b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5c5e\u4e8e\u6cd5\u5f8b\u5173\u7cfb\u7684\u662f\u54ea\u4e00\u9879\uff1f\nA. \u88ab\u544a\u4eba\u8058\u8bf7\u67d0\u5f8b\u5e08\u8fdb\u884c\u8fa9\u62a4\nB. \u67d0\u5e02\u533a\u4eba\u6c11\u6cd5\u9662\u515a\u7ec4\u4e66\u8bb0\u6768\u67d0\u4e0e\u8be5\u9662\u5176\u4ed6\u515a\u5458\u7684\u5173\u7cfb\nC. \u67d0\u7532\u8d4c\u535a\u8f93\u7ed9\u67d0\u4e591000\u5143\u94b1\uff0c\u7acb\u4e0b\u5b57\u636e\u8868\u793a\u57283\u5929\u5185\u4ed8\u6e05\nD. \u5df2\u8ba2\u5a5a\u7684\u67d0\u5bf9\u604b\u4eba\u4e4b\u95f4\u7684\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9600\u63a7\u5f0f\u5bc6\u5c01\u514d\u7ef4\u62a4\u94c5\u9178\u84c4\u7535\u6c60\u6d6e\u5145\u7535\u5e94\u91c7\u7528\nA. \u6052\u538b\u5145\u7535\u6cd5\nB. \u6052\u538b\u9650\u6d41\u5145\u7535\u6cd5\nC. \u534a\u6052\u6d41\u5145\u7535\u6cd5\nD. \u6052\u6d41\u5145\u7535\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5873401646230193, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.31483005318115603, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.42284687770923757}}, {"question": "\u5c0f\u8bf4\u300a\u6e38\u56ed\u60ca\u68a6\u300b\u53cd\u6620\u7684\u662f\u53f0\u6e7e20\u4e16\u7eaa\nA. \u4e03\u5341\u5e74\u4ee3\u7684\u73b0\u5b9e\nB. \u516b\u5341\u5e74\u4ee3\u7684\u73b0\u5b9e\nC. \u4e5d\u5341\u5e74\u4ee3\u7684\u73b0\u5b9e\nD. \u4e94\u3001\u516d\u5341\u5e74\u4ee3\u7684\u73b0\u5b9e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.3790504848994995, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6366677129527611, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.44391749977029443}}, {"question": "\u4e2a\u6027\u5fc3\u7406\u7279\u5f81\u662f\u5728\uff08\uff09\u5b9e\u8df5\u7684\u57fa\u7840\u4e0a\u5f62\u6210\u548c\u53d1\u5c55\u8d77\u6765\u7684\nA. \u8ba4\u77e5\u8fc7\u7a0b\u3001\u60c5\u611f\u8fc7\u7a0b\u3001\u610f\u5fd7\u8fc7\u7a0b\nB. \u8ba4\u77e5\u8fc7\u7a0b\nC. \u610f\u5fd7\u8fc7\u7a0b\nD. \u60c5\u611f\u8fc7\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6119846816779351, "meta-math/MetaMath-Mistral-7B": 0.6720329014481599, "itpossible/Chinese-Mistral-7B-v0.1": 0.7224412967357962, "HuggingFaceH4/zephyr-7b-beta": 0.994038038812033, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.954098575533124, "meta-llama/Meta-Llama-3-8B": 0.8890055555740075, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9757607601916343}}, {"question": "\u6ccc\u5c3f\u7cfb\u80f1\u6c28\u9178\u7ed3\u77f3\u7684\u7279\u70b9\u662f\nA. \u6613\u788e\u7c97\u7cd9\u3001\u4e0d\u89c4\u5219\uff0c\u5448\u7070\u767d\u8272\u3001\u9ec4\u8272\u6216\u68d5\u8272\nB. \u5149\u6ed1\u3001\u6de1\u9ec4\u81f3\u9ec4\u68d5\u8272\u3001\u8721\u6837\u5916\u89c2\nC. \u8d28\u786c\u7c97\u7cd9\u3001\u4e0d\u89c4\u5219\u3001\u5e38\u5448\u6851\u6939\u6837\uff0c\u68d5\u8910\u8272\nD. X\u7ebf\u4e0d\u88ab\u663e\u793a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5752953335039324}}, {"question": "\u82f1\u56fd\u57ce\u5e02\u5b66\u5bb6\u970d\u534e\u5fb7\u63d0\u51fa\uff0c\u5e94\u8be5\u517c\u6709\u57ce\u5e02\u548c\u4e61\u6751\u5404\u81ea\u7684\u4f18\u70b9\uff0c\u57ce\u4e61\u4ea4\u878d\u548c\u7fa4\u4f53\u7ec4\u5408\u578b\u57ce\u5e02\uff0c\u8fd9\u79cd\u57ce\u5e02\u7406\u8bba\u88ab\u79f0\u4e3a\nA. \u533a\u57df\u6574\u4f53\u53d1\u5c55\u7406\u8bba\nB. \u7ec4\u5408\u57ce\u5e02\nC. \u7530\u56ed\u57ce\u5e02\nD. \u57ce\u5e02\u96c6\u4e2d\u53d1\u5c55\u7406\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6613\u53d1\u751f\u6f5c\u4f0f\u611f\u67d3\u7684\u75c5\u6bd2\u662f\nA. \u75b1\u75b9\u75c5\u6bd2\nB. \u9ebb\u75b9\u75c5\u6bd2\nC. \u8f6e\u72b6\u75c5\u6bd2\nD. \u6d41\u611f\u75c5\u6bd2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6109588368402582, "HuggingFaceH4/zephyr-7b-beta": 0.4736611336613071, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4269326861123067, "meta-llama/Meta-Llama-3-8B": 0.761513542083068, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8992166785566554}}, {"question": "\u4e0b\u9762\u54ea\u53e5\u8bdd\u51fa\u81ea\u300a\u5b5f\u5b50\u300b\nA. \u6c34\u80fd\u8f7d\u821f\uff0c\u4ea6\u80fd\u8986\u821f\nB. \u6c11\u60df\u90a6\u672c\uff0c\u672c\u56fa\u90a6\u5b81\nC. \u5148\u5929\u4e0b\u4e4b\u5fe7\u800c\u5fe7\uff0c\u540e\u5929\u4e0b\u4e4b\u4e50\u800c\u4e50\nD. \u72ec\u4e50\u4e50\uff0c\u4e0e\u4eba\u4e50\u4e50\uff0c\u719f\u4e50\uff1f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.41902321781598223, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7596912824565972, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5014154900182665, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u91c7\u7528\u8d1d\u514b\u66fc\u6881\u5bf9\u4e8c\u7ea7\u516c\u8def\u8fdb\u884c\u5f2f\u6c89\u6d4b\u8bd5\u65f6\uff0c\u6d4b\u8bd5\u8f66\u540e\u8f74\u8f74\u91cd\u5e94\u4e3a\nA. 120kN\nB. 100kN\nC. 60kN\nD. 80kN\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3615085256056106, "itpossible/Chinese-Mistral-7B-v0.1": 0.3403147063768956, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3954699049739945}}, {"question": "\u5bf9\u4e00\u79cd\u65b0\u4ea7\u54c1\u5b9a\u4ef7\uff0c\u5e94\u9075\u5faa\u7684\u51b3\u7b56\u89c4\u5219\u662f\nA. \u65b0\u4ea7\u54c1\u7684\u5229\u6da6\u7387\u5e94\u5f53\u4e0e\u5176\u4ed6\u4ea7\u54c1\u7684\u5229\u6da6\u7387\u5927\u81f4\u76f8\u540c\nB. \u9500\u552e\u6536\u5165\u81f3\u5c11\u5e94\u8db3\u4ee5\u8865\u507f\u65b0\u4ea7\u54c1\u7684\u589e\u91cf\u6210\u672c\nC. \u9500\u552e\u6536\u5165\u5e94\u8db3\u4ee5\u8865\u507f\u65b0\u4ea7\u54c1\u7684\u589e\u91cf\u6210\u672c\u548c\u516c\u53f8\u5206\u644a\u4e0b\u6765\u7684\u56fa\u5b9a\u6210\u672c\nD. \u9500\u552e\u6536\u5165\u8db3\u4ee5\u8865\u507f\u65b0\u4ea7\u54c1\u7684\u4f1a\u8ba1\u6210\u672c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.8734281263693184, "itpossible/Chinese-Mistral-7B-v0.1": 0.4741292874171258, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u7528Word\u7f16\u8f91\u6587\u6863\u65f6\uff0c\u9009\u62e9\u67d0\u4e00\u6bb5\u6587\u5b57\u540e\uff0c\u628a\u9f20\u6807\u7f6e\u4e8e\u9009\u4e2d\u6587\u672c\u7684\u4efb\u4e00\u4f4d\u7f6e\uff0c\u6309\u9f20\u6807\u5de6\u952e\u62d6\u66f3\u5230\u9009\u533a\u5916\u7684\u53e6\u4e00\u4f4d\u7f6e\u4e0a\u540e\u653e\u5f00\u9f20\u6807\u3002\u90a3\u4e48\u8be5\u64cd\u4f5c\u53ef\u4ee5\nA. \u79fb\u52a8\u6587\u672c \nB.  \u590d\u5236\u6587\u672c\nC. \u66ff\u6362\u6587\u672c\nD. \u5220\u9664\u6587\u672c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6043888940951063, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9788996464204677}}, {"question": "\u56de\u65cf\u559d\u8336\u7684\u4e00\u5927\u7279\u8272\u662f\u559d\u76d6\u7897\u8336\uff0c\u4fd7\u79f0\nA. \u5976\u8336\nB. \u4e09\u751f\u996e\nC. \u4e09\u70ae\u53f0\nD. \u9165\u6cb9\u8336\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4066554944184799, "meta-math/MetaMath-Mistral-7B": 0.4493538250963979, "itpossible/Chinese-Mistral-7B-v0.1": 0.3994861107250143, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u80c6\u78b1\u916f\u9176\u7684\u63cf\u8ff0\u4e0d\u6b63\u786e\u7684\u662f\nA. \u53ef\u53d7\u6709\u673a\u78f7\u5316\u5408\u7269\u6291\u5236\nB. \u662f\u50ac\u5316\u4e59\u9170\u80c6\u78b1\u6c34\u89e3\u7684\u9176\nC. \u5c5e\u5178\u578b\u7684\u5def\u57fa\u9176\nD. \u8be5\u9176\u5931\u6d3b\uff0c\u53ef\u9020\u6210\u4e59\u9170\u80c6\u78b1\u5728\u4f53\u5185\u8fc7\u591a\u5806\u79ef\u800c\u4e2d\u6bd2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36891869340976374, "meta-math/MetaMath-Mistral-7B": 0.47255280593273274, "itpossible/Chinese-Mistral-7B-v0.1": 0.3825325926376154, "HuggingFaceH4/zephyr-7b-beta": 0.6657263001885215, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4369797307586426, "meta-llama/Meta-Llama-3-8B": 0.34031470637689565, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.57862821033233}}, {"question": "\u88ab\u79f0\u4e3a\u201c\u62c9\u4e01\u7f8e\u6d32\u7684\u300a\u7ebd\u7ea6\u65f6\u62a5\u300b\u201d\u7684\u62a5\u7eb8\u662f\nA. \u300a\u81f3\u4e0a\u62a5\u300b\nB. \u300a\u5df4\u897f\u65e5\u62a5\u300b\nC. \u300a\u73af\u7403\u62a5\u300b\nD. \u300a\u5723\u4fdd\u7f57\u5dde\u62a5\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3591355542404201, "meta-math/MetaMath-Mistral-7B": 0.4218340781756456, "itpossible/Chinese-Mistral-7B-v0.1": 0.31428200102867304, "HuggingFaceH4/zephyr-7b-beta": 0.7916459178634873, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f9d\u82f1\u7f8e\u6cd5\uff0c\u4e00\u4e2a\u4eba\u4ee5\u4ed6\u7684\u8a00\u8f9e\u6216\u884c\u52a8\u4f7f\u53e6\u4e00\u4e2a\u4eba\u6709\u6743\u4ee5\u4ed6\u7684\u540d\u4e49\u7b7e\u8ba2\u5408\u540c\uff0c\u8fd9\u79cd\u4ee3\u7406\u6743\u4ea7\u751f\u7684\u539f\u56e0\u4e3a\nA. \u5ba2\u89c2\u5fc5\u9700\u7684\u4ee3\u7406\u6743\nB. \u9ed8\u793a\u7684\u6388\u6743\nC. \u8ffd\u8ba4\nD. \u660e\u793a\u7684\u6307\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4204771729049709, "meta-math/MetaMath-Mistral-7B": 0.7255908315999522, "itpossible/Chinese-Mistral-7B-v0.1": 0.4196767952843744, "HuggingFaceH4/zephyr-7b-beta": 0.9788884873164942, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4392175618787783, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e1c\u6c49\u90d1\u7384\u201c\u4e09\u79ae\u201d\u6ce8\u7684\u5409\u6ce8\u7c7b\u522b\u662f\nA. \u7ae0\u53e5\u7c7b\nB. \u97f3\u4e49\u7c7b\nC. \u4e49\u758f\u7c7b\nD. \u4f20\u6ce8\u7c7b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3460058095032025, "itpossible/Chinese-Mistral-7B-v0.1": 0.325455072595945, "HuggingFaceH4/zephyr-7b-beta": 0.612207940440551, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3728641892601242, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u6743\u4f9d\u636e\u4eba\u5927\u5e38\u59d4\u51b3\u5b9a\u6279\u51c6\u6211\u56fd\u540c\u5916\u56fd\u7f14\u7ed3\u7684\u6761\u7ea6\u548c\u91cd\u8981\u534f\u5b9a\u7684\u662f\nA. \u56fd\u5bb6\u4e3b\u5e2d\nB. \u5168\u56fd\u4eba\u5927\u5e38\u59d4\u4f1a\u59d4\u5458\u957f\nC. \u603b\u7406\nD. \u5916\u4ea4\u90e8\u957f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u57fa\u7763\u6559\u54f2\u5b66\u7684\u53d1\u5c55\u79d1\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff0c\u5373\u65e9\u671f\u7684\u6559\u7236\u54f2\u5b66\u548c\u540e\u671f\u7684\nA. \u795e\u79d8\u4e3b\u4e49\u54f2\u5b66\nB. \u7ecf\u9662\u54f2\u5b66\nC. \u6000\u7591\u4e3b\u4e49\u54f2\u5b66\nD. \u552f\u7075\u4e3b\u4e49\u54f2\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37908647056428213, "meta-math/MetaMath-Mistral-7B": 0.4527911590776764, "itpossible/Chinese-Mistral-7B-v0.1": 0.703578194070362, "HuggingFaceH4/zephyr-7b-beta": 0.961149754450845, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4745341032026644, "meta-llama/Meta-Llama-3-8B": 0.829671101021373, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9983186807165485}}, {"question": "\u4e0b\u5217\u2f0f\u4e2a\u547d\u9898\u4e2d\uff0c\u2460\u4e24\u4e2a\u2faf\u5e73\u2f8f\u4e14\u76f8\u4f3c\uff0c\u5176\u4f59\u5404\u2faf\u90fd\u662f\u68af\u5f62\u7684\u591a\u2faf\u4f53\u662f\u68f1\u53f0\uff1b\u2461\u6709\u4e24\u4e2a\u2faf\u4e92\u76f8\u5e73\u2f8f\uff0c\u5176\u4f59\u56db\u4e2a\u2faf\u90fd\u662f\u7b49\u8170\u68af\u5f62\u7684\u516d\u2faf\u4f53\u662f\u68f1\u53f0\uff1b\u2462\u5404\u4fa7\u2faf\u90fd\u662f\u6b63\u2f45\u5f62\u7684\u56db\u68f1\u67f1\u2f00\u5b9a\u662f\u6b63\u2f45\u4f53\uff1b\u2463\u5206\u522b\u4ee5\u77e9\u5f62\u4e24\u6761\u4e0d\u7b49\u7684\u8fb9\u6240\u5728\u76f4\u7ebf\u4e3a\u65cb\u8f6c\u8f74\uff0c\u5c06\u77e9\u5f62\u65cb\u8f6c\uff0c\u6240\u5f97\u5230\u7684\u4e24\u4e2a\u5706\u67f1\u662f\u4e24\u4e2a\u4e0d\u540c\u7684\u5706\u67f1\u3002\u5176\u4e2d\u6b63\u786e\u7684\u6709\u51e0\u4e2a\u3002\nA. 2\nB. 1\nC. 4\nD. 3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u516c\u53f8\u751f\u4ea7\u6905\u5b50\uff0c\u5355\u4f4d\u4ea7\u54c1\u6210\u672c1O\u5143\uff0c\u4ea7\u54c1\u552e\u4ef715\u5143\uff0c\u5219\u8be5\u4ea7\u54c1\u7684\u6210\u672c\u52a0\u6210\u7387\u662f\nA. 70%\nB. 100%\nC. 33.30%\nD. 50%\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "20\u4e16\u7eaa20\u5e74\u4ee3\uff0c\u4f5c\u4e3a\u4e00\u822c\u7cfb\u7edf\u8bba\u7684\u521b\u59cb\u4eba\uff0c\u63d0\u51fa\u7cfb\u7edf\u6982\u5ff5\u7684\u662f\u5965\u5730\u5229\u7406\u8bba\u751f\u7269\u5b66\u5bb6\nA. \u7ef4\u7eb3\nB. \u8d1d\u5854\u6717\u83f2\nC. \u7533\u519c\nD. \u97e6\u4f2f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u4e9b\u4f20\u67d3\u75c5\u5982\u83cc\u75e2\uff0c\u5728\u6211\u56fd\u7ec8\u5e74\u5747\u53ef\u53d1\u75c5\uff0c\u4f46\u6bcf\u5e748-9\u6708\u4efd\u5219\u51fa\u73b0\u4e00\u4e2a\u53d1\u75c5\u9ad8\u5cf0\uff0c\u6b64\u73b0\u8c61\u79f0\u4e3a\nA. \u957f\u671f\u53d8\u52a8\nB. \u5468\u671f\u6027\nC. \u77ed\u671f\u6ce2\u52a8\nD. \u5b63\u8282\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.949278558043464, "meta-math/MetaMath-Mistral-7B": 0.9973353037583939, "itpossible/Chinese-Mistral-7B-v0.1": 0.7290501294766689, "HuggingFaceH4/zephyr-7b-beta": 0.9999975340857922, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9887413189376795, "meta-llama/Meta-Llama-3-8B": 0.9541740475478008, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9308579240005045}}, {"question": "\u4e2d\u5b66\u5316\u5b66\u4e2d\u5f88\u591a\u201c\u89c4\u5f8b\u201d\u90fd\u6709\u5176\u4f7f\u7528\u8303\u56f4\uff0c\u4e0b\u5217\u6839\u636e\u6709\u5173\u201c\u89c4\u5f8b\u201d\u63a8\u51fa\u7684\u7ed3\u8bba\u5408\u7406\u7684\u662f\nA. \u6839\u636e\u6c27\u5316\u8fd8\u539f\u53cd\u5e94\u7684\u89c4\u5f8b\uff0c\u63a8\u51fa\u5f3a\u6c27\u5316\u5242\u9047\u5230\u5f3a\u8fd8\u539f\u5242\u4e00\u5b9a\u4f1a\u53d1\u751f\u6c27\u5316\u8fd8\u539f\u53cd\u5e94\nB. \u6839\u636eCO2\u901a\u5165Ba(NO3)2\u6eb6\u6db2\u4e2d\u65e0\u6c89\u6dc0\u7684\u89c4\u5f8b\u63a8\u6d4b\uff0cSO2\u901a\u5165Ba(NO3)2\u6eb6\u6db2\u4e2d\u4e5f\u65e0\u6c89\u6dc0\u4ea7\u751f\nC. \u6839\u636e\u5316\u5b66\u53cd\u5e94\u7684\u89c4\u5f8b\uff0c\u63a8\u51fa\u6c22\u6c27\u5316\u94c1\u53ef\u901a\u8fc7\u5316\u5408\u53cd\u5e94\u5f97\u5230\nD. \u6839\u636e\u5143\u7d20\u7684\u975e\u91d1\u5c5e\u6027\u8f83\u5f3a\uff0c\u5176\u5355\u8d28\u4e5f\u8d8a\u6d3b\u6cfc\u89c4\u5f8b\uff0c\u63a8\u51fa\u78f7\u5355\u8d28\u6bd4N2\u7a33\u5b9a\u5f97\u591a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3239937717691632, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5549236167492922}}, {"question": "\u5728\u73b0\u4ee3\u7ba1\u7406\u5b66\u4e2d\uff0c\u7ba1\u7406\u4eba\u5458\u5bf9\u5f53\u524d\u7684\u5b9e\u9645\u5de5\u4f5c\u662f\u5426\u7b26\u5408\u8ba1\u5212\u800c\u8fdb\u884c\u6d4b\u5b9a\u5e76\u4fc3\u4f7f\u7ec4\u7ec7\u76ee\u6807\u5b9e\u73b0\u7684\u8fc7\u7a0b\uff0c\u88ab\u79f0\u4e3a\nA. \u63a7\u5236\nB. \u7ec4\u7ec7\nC. \u521b\u65b0\nD. \u9886\u5bfc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9787524609064955, "meta-math/MetaMath-Mistral-7B": 0.9919981014735619, "itpossible/Chinese-Mistral-7B-v0.1": 0.9757330174406379, "HuggingFaceH4/zephyr-7b-beta": 0.9993833678686954, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.939122221809648, "meta-llama/Meta-Llama-3-8B": 0.9759046127608728, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9991194762415215}}, {"question": "\u300a\u62a5\u520a\u7684\u56db\u79cd\u7406\u8bba\u300b\u4e2d\u201c\u62a5\u520a\u201d\u4e00\u8bcd\u6cdb\u6307\nA. \u4e00\u5207\u5927\u4f17\u4f20\u64ad\u5a92\u4ecb\nB. \u51fa\u7248\u7269\nC. \u6587\u5b57\nD. \u6240\u6709\u62a5\u7eb8\u6742\u5fd7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8966120440850549}}, {"question": "\u8981\u8ba4\u8bc6\u5230\u793e\u4f1a\u95ee\u9898\u7684\u957f\u671f\u5b58\u5728\u548c\u53cd\u590d\u6027\uff0c\u8981\u6709\u6301\u4e45\u6218\u7684\u610f\u5fd7\u548c\u6bc5\u529b\uff0c\u8fd9\u662f\u793e\u4f1a\u95ee\u9898\u9632\u6cbb\u7684\nA. \u6574\u4f53\u6027\u601d\u60f3\nB. \u957f\u671f\u6027\u601d\u60f3\nC. \u5168\u7403\u6027\u601d\u60f3\nD. \u5168\u5c40\u6027\u601d\u60f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8057676682765629, "meta-math/MetaMath-Mistral-7B": 0.9087291879879297, "itpossible/Chinese-Mistral-7B-v0.1": 0.9233245842430761, "HuggingFaceH4/zephyr-7b-beta": 0.9847400191960844, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.925889209544556, "meta-llama/Meta-Llama-3-8B": 0.784548023803377, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8868068996747915}}, {"question": "\u4e0e\u65e9\u671f\u884c\u4e3a\u5348\u89c9\u7406\u8bba\u76f8\u6bd4\u8f83\uff0c\u53e4\u5178\u7ba1\u7406\u7406\u8bba\u7684\u91cd\u8981\u7279\u70b9\u4e4b\u4e00\u662f\nA. \u6ce8\u91cd\u5bf9\u4eba\u7684\u884c\u4e3a\u7684\u7814\u7a76\u3002\nB. \u5f3a\u8c03\u6c11\u4e3b\u53c2\u52a0\u7ba1\u7406\u3002\nC. \u6ce8\u91cd\u5bf9\u7eaa\u5f8b\u7684\u7814\u7a76\u3002\nD. \u5f3a\u8c03\u4eba\u6027\u6fc0\u53d1\u7684\u7ba1\u7406\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4359599206820779, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.553177123486441, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4412842091857809}}, {"question": "\u51fd\u6570 $f(x)=\\frac{x-x^3}{\\sin \\pi x}$ \u7684\u53ef\u53bb\u95f4\u65ad\u70b9\u4e2a\u6570\u4e3a\uff08\uff09\nA. 2\nB. \u65e0\u7a77\u591a\u4e2a\nC. 1\nD. 3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u51ac\u5b63\uff0c\u4eba\u4eec\u65e5\u5e38\u9009\u62e9\u98df\u7528\u6e29\u70ed\u52a9\u9633\u4e4b\u54c1\uff0c\u4ee5\u8fbe\u6276\u9633\u6563\u5bd2\u4e4b\u529f\u6548\uff0c\u8c13\u4e4b\nA. \u201c\u836f\u81b3\u201d\nB. \u201c\u98df\u7597\nC. \u201c\u836f\u8865\u201d\nD. \u201c\u98df\u8865\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6900280858625832, "meta-math/MetaMath-Mistral-7B": 0.9012559267733428, "itpossible/Chinese-Mistral-7B-v0.1": 0.8762794335169852, "HuggingFaceH4/zephyr-7b-beta": 0.999731260799177, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5537307067890508, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u793e\u533a\u662f\u4e00\u4e2a\u793e\u4f1a\u5b66\u6982\u5ff5\u3002\u4ece\u516c\u5171\u5173\u7cfb\u5b66\u6765\u770b\uff0c\u662f\u6307\u7ec4\u7ec7\u4e0e\u7ec4\u7ec7\u6240\u5904\u7684\u7279\u5b9a\u5730\u57df\u5185\u7684\u6743\u5229\u673a\u6784\u3001\u5730\u65b9\u56e2\u4f53\u3001\u76f8\u5173\u5229\u76ca\u56e2\u4f53\u3001\u5c45\u6c11\u767e\u59d3\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u53c8\u79f0\nA. \u6d88\u8d39\u8005\u5173\u7cfb\nB. \u7fa4\u4f17\u5173\u7cfb\nC. \u516c\u5171\u5173\u7cfb\nD. \u533a\u57df\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1898\u5e746\u670811\u65e5\uff0c\u6e05\u5ef7\u9881\u5e03\u300a\u660e\u5b9a\u56fd\u662f\u8bcf\u300b\uff0c\u5176\u4e2d\u8bf4\uff1a\u201c\u4ee5\u5723\u8d24\u4e49\u7406\u4e4b\u5b66\uff0c\u690d\u5176\u6839\u672c\uff0c\u53c8\u987b\u535a\u91c7\u897f\u5b66\u4e4b\u5207\u4e8e\u65f6\u52a1\u8005\uff0c\u5b9e\u529b\u8bb2\u6c42\uff0c\u4ee5\u6551\u7a7a\u758f\u8fc2\u8c2c\u4e4b\u5f0a\u3002\u201d\u8fd9\u8bf4\u660e\u620a\u620c\u53d8\u6cd5\nA. \u4fa7\u91cd\u63d0\u5021\u52a1\u5b9e\u4e4b\u98ce\nB. \u517c\u5177\u6539\u826f\u4e0e\u9769\u547d\u7684\u8272\u5f69\nC. \u4e0e\u6d0b\u52a1\u8fd0\u52a8\u7684\u5b97\u65e8\u5e76\u65e0\u4e0d\u540c\nD. \u8bd5\u56fe\u901a\u8fc7\u59a5\u534f\u51cf\u5c11\u53d8\u9769\u963b\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9c81\u8fc5\u7684\u4f5c\u54c1\u4e2d\u4e0d\u5c5e\u4e8e\u519c\u6c11\u9898\u6750\u7684\u662f\nA. \u300a\u795d\u798f\u300b\nB. \u300a\u767d\u5149\u300b\nC. \u300a\u660e\u5929\u300b\nD. \u300a\u79bb\u5a5a\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.32205625344145955}}, {"question": "\u6700\u57fa\u672c\u7684\u4f1a\u8ba1\u6838\u7b97\u5f62\u5f0f\u662f\nA. \u591a\u680f\u5f0f\u65e5\u8bb0\u8d26\u6838\u7b97\u5f62\u5f0f\nB. \u65e5\u8bb0\u603b\u8d26\u6838\u7b97\u5f62\u5f0f\nC. \u8bb0\u8d26\u51ed\u8bc1\u6838\u7b97\u5f62\u5f0f\nD. \u79d1\u76ee\u6c47\u603b\u8868\u6838\u7b97\u5f62\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31851505670204755, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3900852433273489, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3423962339378881, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.49516317235292706}}, {"question": "1923\u20141929\u5e74\uff0c\u7f8e\u56fd\u7684\u4f01\u4e1a\u666e\u904d\u4f7f\u7528\u6d41\u6c34\u7ebf\u7b49\u5148\u8fdb\u751f\u4ea7\u7ba1\u7406\u65b9\u5f0f\uff0c\u63d0\u9ad8\u4e86\u52b3\u52a8\u751f\u4ea7\u7387\uff0c\u540c\u65f6\u5728\u5c11\u6570\u4f01\u4e1a\u4e2d\u5de5\u4eba\u53ef\u4ee5\u9886\u53d6\u517b\u8001\u91d1\uff0c\u4eab\u53d7\u5e26\u85aa\u4f11\u5047\u3002\u8fd9\u53cd\u6620\u51fa\u5f53\u65f6\u5728\u7f8e\u56fd\nA. \u56fd\u5bb6\u5e72\u9884\u4fc3\u8fdb\u4e86\u7ecf\u6d4e\u53d1\u5c55\nB. \u79d1\u6280\u672a\u5bf9\u7ecf\u6d4e\u53d1\u5c55\u53d1\u6325\u91cd\u5927\u4f5c\u7528\nC. \u4f9b\u7ed9\u4e0e\u9700\u6c42\u4fdd\u6301\u57fa\u672c\u5e73\u8861\nD. \u5de5\u4eba\u5206\u4eab\u7684\u7ecf\u6d4e\u53d1\u5c55\u6210\u679c\u6709\u9650\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3556591469645176, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6883399000021778, "HuggingFaceH4/zephyr-7b-beta": 0.5850108387778975, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6980665689331521, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6822320176328713}}, {"question": "\u901a\u8fc7\u4e0b\u5217\u54ea\u79cd\u9014\u5f84\u4e0d\u80fd\u5f97\u5230\u540c\u6e90\u4e09\u500d\u4f53\nA. \u672a\u51cf\u6570\u7684\u4e8c\u500d\u6027\u96cc\u914d\u5b50\u548c\u6b63\u5e38\u96c4\u914d\u5b50\u7ed3\u5408\nB. \u4e24\u4e2a\u7cbe\u6838\u8fdb\u5165\u540c\u4e00\u80da\u56ca\u548c\u4e00\u4e2a\u5375\u6838\u7ed3\u5408\nC. \u7528\u79cb\u6c34\u4ed9\u7d20\u5904\u7406\u4e8c\u500d\u4f53\nD. \u7528\u540c\u6e90\u56db\u500d\u4f53\u4e3a\u6bcd\u672c\uff0c\u4e0e\u540c\u4e00\u7269\u79cd\u7684\u4e8c\u500d\u4f53\u6742\u4ea4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.33767879625406966, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.532429707444219}}, {"question": "\u7532\u72b6\u817a\u4e0a\u52a8\u8109\u4e00\u822c\u6765\u81ea\nA. \u7532\u72b6\u9888\u5e72\nB. \u9888\u5185\u52a8\u8109\nC. \u9888\u5916\u52a8\u8109\nD. \u9888\u603b\u52a8\u8109\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.27697441697632474, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5e7f\u5927\u4eba\u6c11\u7fa4\u4f17\u5728\u515a\u7684\u9886\u5bfc\u4e0b\uff0c\u4f9d\u7167\u5baa\u6cd5\u548c\u6cd5\u5f8b\u7684\u89c4\u5b9a\uff0c\u901a\u8fc7\u5404\u79cd\u9014\u5f84\u548c\u5f62\u5f0f\u7ba1\u7406\u56fd\u5bb6\u4e8b\u52a1\uff0c\u7ba1\u7406\u7ecf\u6d4e\u6587\u5316\u4e8b\u4e1a\uff0c\u7ba1\u7406\u793e\u4f1a\u4e8b\u52a1\uff0c\u4fdd\u8bc1\u56fd\u5bb6\u5404\u9879\u5de5\u4f5c\u90fd\u4f9d\u6cd5\u8fdb\u884c\uff0c\u9010\u6b65\u5b9e\u73b0\u793e\u4f1a\u4e3b\u4e49\u6c11\u4e3b\u7684\u5236\u5ea6\u5316\u3001\u6cd5\u5f8b\u5316\uff0c\u4f7f\u8fd9\u79cd\u5236\u5ea6\u548c\u6cd5\u5f8b\u4e0d\u56e0\u9886\u5bfc\u4eba\u7684\u6539\u53d8\u800c\u6539\u53d8\uff0c\u4e0d\u56e0\u9886\u5bfc\u4eba\u770b\u6cd5\u548c\u6ce8\u610f\u529b\u7684\u6539\u53d8\u800c\u6539\u53d8\u3002\u201d\u8fd9\u53e5\u8bdd\u9610\u8ff0\u7684\u662f\nA. \u4eba\u6c11\u5f53\u5bb6\u4f5c\u4e3b\u7684\u5185\u6db5\nB. \u4f9d\u6cd5\u6cbb\u56fd\u7684\u5185\u6db5\nC. \u793e\u4f1a\u4e3b\u4e49\u6c11\u4e3b\u653f\u6cbb\u7684\u5185\u6db5\nD. \u516c\u6c11\u5728\u6cd5\u5f8b\u9762\u524d\u4e00\u5f8b\u5e73\u7b49\u7684\u5185\u6db5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5124346109479069, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9940081039326948, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6831079385317719, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9628633980698714}}, {"question": "\u5bb6\u5ead\u6210\u5458\u3001\u90bb\u5c45\u3001\u540c\u4e8b\u90fd\u4f1a\u5f71\u54cd\u6d88\u8d39\u8005\u7684\u8d2d\u4e70\u884c\u4e3a\uff0c\u4ece\u5bf9\u6d88\u8d39\u8005\u5f71\u54cd\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u4ed6\u4eec\u5c5e\u4e8e\u54ea\u79cd\u53c2\u7167\u7fa4\u4f53\uff1f\nA. \u6bd4\u8f83\u7fa4\u4f53\nB. \u6b21\u8981\u7fa4\u4f53\nC. \u6e34\u671b\u7fa4\u4f53\nD. \u9996\u8981\u7fa4\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4852287030912825, "meta-math/MetaMath-Mistral-7B": 0.8884621078832383, "itpossible/Chinese-Mistral-7B-v0.1": 0.4288648686499542, "HuggingFaceH4/zephyr-7b-beta": 0.9782505507379474, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7086937361635616, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe $M=\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}} \\frac{(1+x)^2}{1+x^2} \\mathrm{~d} x\uff0c N=\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}} \\frac{1+x}{\\mathrm{e}^x} \\mathrm{~d} x\uff0c K=\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}(1+\\sqrt{\\cos x}) \\mathrm{d} x$\uff0c \u5219( )\nA. $M>N>K$.\nB. $M>K>N$.\nC. $K>M>N$.\nD. $K>N>M$.\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.27416108226793107, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u56ca\u5185\u97e7\u5e26\u7684\u5173\u8282\u662f\nA. \u8155\u5173\u8282\nB. \u9acb\u5173\u8282\nC. \u819d\u5173\u8282\nD. \u80a9\u5173\u8282\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.28966338381871215, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u56fd\u5bb6\u7684\u53d1\u5c55\u5c24\u5176\u7ecf\u6d4e\u53d1\u5c55\u662f\u4e00\u5207\u53d1\u5c55\u4e2d\u56fd\u5bb6\u7684\u9996\u8981\u4efb\u52a1\uff0c\u65b0\u95fb\u5a92\u4ecb\u5fc5\u987b\u670d\u4ece\u3001\u670d\u52a1\u3001\u4fc3\u8fdb\u56fd\u5bb6\u53d1\u5c55\u5c24\u5176\u7ecf\u6d4e\u53d1\u5c55\u3002\u201d\u8fd9\u662f\u54ea\u79cd\u5a92\u4ecb\u7406\u8bba\u7684\u6838\u5fc3\u5185\u5bb9\uff1f\nA. \u53d1\u5c55\u65b0\u95fb\u5b66\nB. \u6781\u6743\u4e3b\u4e49\u7406\u8bba\nC. \u81ea\u7531\u4e3b\u4e49\u7406\u8bba\nD. \u793e\u4f1a\u8d23\u4efb\u7406\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.41209692555709065, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.34412869855849515, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6784\u6210\u75c5\u6bd2\u6838\u5fc3\u7684\u5316\u5b66\u6210\u5206\u662f\nA. \u6838\u9178\nB. \u86cb\u767d\u8d28\nC. \u7c7b\u8102\nD. \u78f7\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5689793515535498, "meta-math/MetaMath-Mistral-7B": 0.8565648612001575, "itpossible/Chinese-Mistral-7B-v0.1": 0.5028734530018918, "HuggingFaceH4/zephyr-7b-beta": 0.9320221113656784, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7257950205065267, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9513272639766392}}, {"question": "\u60a3\u8005\uff0c\u5973\uff0c30 \u5c81\u3002\u7532\u4ea2\u884c\u7532\u72b6\u817a\u6b21\u5168\u5207\u9664\u672f\u3002\u8fd4\u56de\u75c5\u623f 2 \u5c0f\u65f6\u540e\u611f\u6c14\u618b\uff0c\u5fc3\u95f7\uff0c\u6025\u67e5\u89c1\u60a3\u8005\u9762\u8272\u53d1\u767d\uff0c\u547c\u5438\u6025\u4fc3\uff0c30 \u6b21/\u5206\uff0c\u8840\u538b 136/90mmHg\uff0c\u8109\u7387 120 \u6b21/\u5206\uff0c\u4f24\u53e3\u90e8\u9971\u6ee1\uff0c\u5f20\u529b\u9ad8\u3002\u8be5\u60a3\u8005\u6700\u53ef\u80fd\u53d1\u751f\u7684\u5e76\u53d1\u75c7\u662f\nA. \u521b\u9762\u51fa\u8840\u538b\u8feb\u6c14\u7ba1\nB. \u5589\u4e0a\u795e\u7ecf\u635f\u4f24\nC. \u5589\u8fd4\u795e\u7ecf\u635f\u4f24\nD. \u7532\u72b6\u65c1\u817a\u635f\u4f24\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2936423674828322, "meta-math/MetaMath-Mistral-7B": 0.5401621535330479, "itpossible/Chinese-Mistral-7B-v0.1": 0.6554679692123961, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.49501294744843477, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u673a\u4f53\u7f3a\u6c27\u65f6\uff0c\u80be\u810f\u4ea7\u751f\u7ea2\u7ec6\u80de\u751f\u6210\u9176\uff0c\u8be5\u9176\u4f5c\u7528\u4e8e\u809d\u810f\u6240\u751f\u6210\u7684\u4fc3\u7ea2\u7ec6\u80de\u751f\u6210\u7d20\u539f\uff0c\u4f7f\u5176\u8f6c\u53d8\u6210\u4fc3\u7ea2\u7ec6\u80de\u751f\u6210\u7d20\uff08ESF\uff09\u3002\u4fc3\u7ea2\u7ec6\u80de\u751f\u6210\u7d20\u4e00\u65b9\u9762\u523a\u6fc0\u9aa8\u9ad3\u9020\u8840\u7ec4\u7ec7\uff0c\u4f7f\u5468\u56f4\u8840\u6db2\u4e2d\u7ea2\u7ec6\u80de\u6570\u589e\u52a0\uff0c\u4ece\u800c\u6539\u5584\u7f3a\u6c27\uff1b\u53e6\u4e00\u65b9\u9762\u53c8\u53cd\u9988\u6027\u5730\u6291\u5236\u809d\u810f\u4e2d\u7684\u4fc3\u7ea2\u7ec6\u80de\u751f\u6210\u7d20\u539f\u7684\u751f\u6210.\u4ee5\u4e0b\u53d9\u8ff0\u9519\u8bef\u7684\u662f\nA. \u4fc3\u7ea2\u7ec6\u80de\u751f\u6210\u7d20\u4f5c\u7528\u7684\u9776\u7ec6\u80de\u662f\u7ea2\u7ec6\u80de\uff0c\u7ea2\u7ec6\u80de\u6570\u91cf\u589e\u52a0\u53ef\u4ee5\u589e\u52a0\u643a\u6c27\u80fd\u529b\uff0c\u6539\u5584\u7f3a\u6c27\nB. \u8840\u6d46\u4e2d\u542b\u6709\u8f83\u591a\u7684\u86cb\u767d\u8d28\uff0c\u8840\u6d46\u6e17\u900f\u538b\u7684\u5927\u5c0f\u4e3b\u8981\u4e0e\u65e0\u673a\u76d0\u3001\u86cb\u767d\u8d28\u542b\u91cf\u6709\u5173\nC. \u4fc3\u7ea2\u7ec6\u80de\u751f\u6210\u7d20\u6291\u5236\u809d\u810f\u4e2d\u7684\u4fc3\u7ea2\u7ec6\u80de\u751f\u6210\u7d20\u539f\u7684\u751f\u6210\uff0c\u8fd9\u79cd\u53cd\u9988\u5c5e\u4e8e\u8d1f\u53cd\u9988\u8c03\u8282\uff0c\u8fd9\u79cd\u673a\u5236\u4fdd\u8bc1\u751f\u7269\u4f53\u5185\u7269\u8d28\u542b\u91cf\u7684\u7a33\u5b9a\uff0c\u4e0d\u4f1a\u9020\u6210\u6d6a\u8d39\nD. \u9aa8\u9ad3\u4e2d\u7684\u9020\u8840\u5e72\u7ec6\u80de\u8fd8\u80fd\u4ea7\u751f\u6dcb\u5df4\u7ec6\u80de\uff0c\u53c2\u4e0e\u514d\u75ab\u8c03\u8282\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4823508448525685, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9496895935237466, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u7528\u8bcd\u9020\u53e5\u54ea\u4e00\u9879\u6709\u9519\u8bef\uff1f\u8bf7\u4f60\u628a\u5b83\u9009\u51fa\u6765\u3002\nA. \u62ee\u636e\uff1a\u5979\u7684\u6027\u683c\u5341\u5206\u5185\u5411\uff0c\u5728\u964c\u751f\u4eba\u9762\u524d\u5e38\u5e38\u62ee\u636e\u4e0d\u5b89\u3002\nB. \u7cbe\u81f4\uff1a\u6587\u5177\u5e97\u91cc\u6446\u653e\u7740\u51e0\u4e2a\u7cbe\u81f4\u7684\u7b14\u76d2\uff0c\u6211\u5f88\u60f3\u4e70\u4e00\u4e2a\u3002\nC. \u6002\u607f\uff1a\u5728\u670b\u53cb\u7684\u6781\u529b\u6002\u607f\u4e0b\uff0c\u6211\u51b3\u5b9a\u4eca\u5e74\u6691\u5047\u53bb\u897f\u85cf\u65c5\u6e38\u3002\nD. \u915d\u917f\uff1a\u8fd9\u6b21\u73ed\u4f1a\u7ecf\u8fc7\u5927\u5bb6\u7684\u915d\u917f\uff0c\u7ec4\u7ec7\u5f97\u975e\u5e38\u597d\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.295278727801358, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.36768522148204796, "meta-llama/Meta-Llama-3-8B": 0.30765667748869174, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3533992459382818}}, {"question": "1950\u5e746\u6708\uff0c\u671d\u9c9c\u6218\u4e89\u7206\u53d1\u3002\u4e2d\u56fd\u6218\u58eb\u9ad8\u5531\u201c\u96c4\u7ea0\u7ea0\uff0c\u6c14\u6602\u6602\uff0c\u8de8\u8fc7\u9e2d\u7eff\u6c5f\u3002\u4fdd\u548c\u5e73\uff0c\u536b\u7956\u56fd\uff0c\u5c31\u662f\u4fdd\u5bb6\u4e61\u2026\u2026\u201d\u5165\u671d\u53c2\u6218\u3002\u5165\u671d\u90e8\u961f\u7684\u79f0\u547c\u662f\nA. \u4e2d\u56fd\u4eba\u6c11\u89e3\u653e\u519b\nB. \u56fd\u6c11\u9769\u547d\u519b\u7b2c\u516b\u8def\u519b\nC. \u4e2d\u56fd\u4eba\u6c11\u5fd7\u613f\u519b\nD. \u4e1c\u5317\u91ce\u6218\u519b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7343978596607955, "meta-math/MetaMath-Mistral-7B": 0.8869870303164457, "itpossible/Chinese-Mistral-7B-v0.1": 0.945206633892675, "HuggingFaceH4/zephyr-7b-beta": 0.9212575424497088, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6927860170606646, "meta-llama/Meta-Llama-3-8B": 0.9741765202555714, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.935718694376729}}, {"question": "\u611f\u5192\u5fcc\u7528\u4e0b\u5217\u54ea\u4e00\u79cd\u98df\u7269\nA. \u9752\u83dc\nB. \u6d77\u9c7c\nC. \u751f\u59dc\nD. \u8c46\u6d46\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.33873922196938544, "HuggingFaceH4/zephyr-7b-beta": 0.5055448370977466, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6072888041776174, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6295\u8d44\u65b9\u6848\u7532\u5934\u4e09\u5e74\u7684\u51c0\u73b0\u91d1\u6548\u76ca\u91cf\u5206\u522b\u4e3al00\u5143\u3001100\u5143\u548c100\u5143\uff0c\u6295\u8d44\u65b9\u6848\u4e59\u5934\u4e09\u5e74\u7684\u73b0\u91d1\u6548\u76ca\u91cf\u5206\u522b\u4e3a80\u5143\u3001100\u5143\u548c120\u5143\nA. \u4e59\u65b9\u6848\u73b0\u503c\u5927\nB. \u7532\u65b9\u6848\u73b0\u503c\u5927\nC. \u4e24\u4e2a\u65b9\u6848\u7684\u73b0\u503c\u76f8\u7b49\nD. \u5fc5\u987b\u4f5c\u8d34\u73b0\u8ba1\u7b97\uff0c\u624d\u80fd\u786e\u5b9a\u54ea\u4e2a\u65b9\u6848\u73b0\u503c\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.29660173325630934, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e09\u5b9d\u662f\u6307\nA. \u5929\u5730\u4eba\nB. \u6212\u5b9a\u6167\nC. \u4f5b\u6cd5\u50e7\nD. \u65ad\u820d\u79bb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7436048852255914, "meta-math/MetaMath-Mistral-7B": 0.9676168846043988, "itpossible/Chinese-Mistral-7B-v0.1": 0.40650587703396857, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9764264582752543, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.996493844058582}}, {"question": "\u53ef\u660e\u663e\u51cf\u5c11\u7ef4\u751f\u7d20\u3001\u77ff\u7269\u8d28\u7b49\u6c34\u6eb6\u6027\u8425\u517b\u7d20\u635f\u5931\u7684\u70f9\u8c03\u65b9\u5f0f\u662f\nA. \u5fae\u6ce2\u52a0\u70ed\nB. \u84b8\u716e\nC. \u70e4\nD. \u70b8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5:$q_x^{\\prime(1)}=0.015\uff0cq_x^{\\prime(2)}=0.030$\u3002\u51cf\u56e0 1 (\u5de5\u4f5c\u4e2d\u9014\u9000\u804c) \u4e2d\u7ec8\u6b62\u529b\u670d\u4ece\u5747\u5300\u5206\u5e03\uff0c\u51cf\u56e0 2(\u5de5\u4f5c\u671f\u95f4\u4f24\u6b8b) \u5728\u5e74\u4e2d\u53d1\u751f\uff0c\u5219$p_x^{(r)}$\u548c$q_x^{(2)}$\u7684\u503c\u5206\u522b\u4e3a( )\u3002\nA. 0.01478\uff0c0.029775\nB. 0.04455\uff0c0.029775\nC. 0.95545\uff0c0.014775\nD. 0.95545\uff0c0.029775\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.42375374089051565, "meta-math/MetaMath-Mistral-7B": 0.7855059076640841, "itpossible/Chinese-Mistral-7B-v0.1": 0.3213637558640315, "HuggingFaceH4/zephyr-7b-beta": 0.987986384307119, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.49077802168116, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f57\u8001\u5e08\u8bb2\u89e3\u201d\u89c2\u6f6e\u201d\u8fd9\u7bc7\u8bfe\u6587\u65f6\uff0c\u901a\u8fc7\u64ad\u653e\u89c6\u9891\uff0c\u8ba9\u5b66\u751f\u771f\u5207\u611f\u53d7\u5230\u94b1\u5858\u6c5f\u5927\u6f6e\u7684\u96c4\u4f1f\u58ee\u89c2\u3002\u4ed6\u5728\u6559\u5b66\u4e2d\u8d2f\u5f7b\u4e86\nA. \u79d1\u5b66\u6027\u548c\u601d\u60f3\u6027\u76f8\u7ed3\u5408\u539f\u5219\nB. \u7a33\u56fa\u6027\u539f\u5219\nC. \u76f4\u89c2\u6027\u539f\u5219\nD. \u5faa\u5e8f\u6e10\u8fdb\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.73075011555565, "meta-math/MetaMath-Mistral-7B": 0.7802510800860601, "itpossible/Chinese-Mistral-7B-v0.1": 0.9387152736632789, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8784709297073945, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6162\u6027\u963b\u585e\u6027\u80ba\u75be\u75c5\u60a3\u8005\u5728\u7a33\u5b9a\u671f\u9700\u8981\u957f\u671f\u5438\u5165\u7cd6\u76ae\u8d28\u6fc0\u7d20\u7684\u4e3b\u8981\u6307\u6807\u662f\nA. \u53bb\u5e74\u53d1\u751f2\u6b21\u6216\u4ee5\u4e0a\u6025\u6027\u52a0\u91cd\nB. \u80ba\u529f\u80fd\u793aFEV1%\u9884\u8ba1\u503c\u4e3a70%\nC. \u60a3\u8005\u5267\u70c8\u6d3b\u52a8\u65f6\u51fa\u73b0\u547c\u5438\u56f0\u96be\nD. \u5e73\u5730\u8d70\u6570\u5206\u949f\u540e\u9700\u505c\u4e0b\u6765\u5598\u6c14\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4127033114901247, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u521d\u4e2d\u8bed\u6587\u8bfe\u672c\u4e2d\u7537\u5973\u89d2\u8272\u6bd4\u4f8b\u4e3a\nA. 3:1\nB. 2:1\nC. 5:1\nD. 4:1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u516c\u6cd5\u548c\u79c1\u6cd5\u7684\u5212\u5206\u6765\u6e90\u4e8e\nA. \u82f1\u56fd\nB. \u7f57\u9a6c\nC. \u7f8e\u56fd\nD. \u53e4\u5e0c\u814a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5443943063547284, "HuggingFaceH4/zephyr-7b-beta": 0.907197794195552, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7412062691087216, "meta-llama/Meta-Llama-3-8B": 0.7674683398084521, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9134439249909804}}, {"question": "\u201c\u4e94\u724c\u4e00\u56fe\u201d\u6307\u7684\u662f\u5de5\u7a0b\u6982\u51b5\u724c\u3001\u7ba1\u7406\u4eba\u5458\u540d\u5355\u53ca\u76d1\u7763\u7535\u8bdd\u724c\u3001\u6d88\u9632\u4fdd\u536b\u724c\u53ca\nA. \u5371\u9669\u6e90\u6807\u793a\u724c\u3001\u6587\u660e\u65bd\u5de5\u724c\u3001\u5efa\u7b51\u6548\u679c\u56fe\nB. \u65bd\u5de5\u4eba\u5458\u73b0\u573a\u51fa\u5165\u724c\u3001\u5371\u9669\u6e90\u6807\u793a\u724c\u3001\u65bd\u5de5\u73b0\u573a\u5e73\u9762\u56fe\nC. \u5b89\u5168\u6210\u4ea7\u724c\u3001\u65bd\u5de5\u4eba\u5458\u73b0\u573a\u51fa\u5165\u724c\u3001\u5efa\u7b51\u6548\u679c\u56fe\nD. \u5b89\u5168\u751f\u4ea7\u724c\u3001\u6587\u660e\u65bd\u5de5\u724c\u3001\u65bd\u5de5\u73b0\u573a\u5e73\u9762\u56fe\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4392178824349045, "meta-math/MetaMath-Mistral-7B": 0.4964212373588252, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9983537306653955, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5246735604054736, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5efa\u8bbe\u5de5\u7a0b\u9879\u76ee\u603b\u8fdb\u5ea6\u76ee\u6807\u8bba\u8bc1\u7684\u6838\u5fc3\u5de5\u4f5c\u662f\nA. \u7f16\u5236\u9879\u76ee\u603b\u8fdb\u5ea6\u7eb2\u8981\nB. \u901a\u8fc7\u7f16\u5236\u603b\u8fdb\u5ea6\u7eb2\u8981\u8bba\u8bc1\u603b\u8fdb\u5ea6\u76ee\u6807\u5b9e\u73b0\u7684\u53ef\u80fd\u6027\nC. \u901a\u8fc7\u7f16\u5236\u9879\u76ee\u603b\u8fdb\u5ea6\u8ba1\u5212\u8bba\u8bc1\u8fdb\u5ea6\u76ee\u6807\u63a7\u5236\u63aa\u65bd\u7684\u6709\u6548\u6027\nD. \u7f16\u5236\u9879\u76ee\u603b\u8fdb\u5ea6\u8ba1\u5212\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6879486512986889, "meta-math/MetaMath-Mistral-7B": 0.7395913885310503, "itpossible/Chinese-Mistral-7B-v0.1": 0.8858189312134914, "HuggingFaceH4/zephyr-7b-beta": 0.9996466129060714, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7061102076426384, "meta-llama/Meta-Llama-3-8B": 0.9071769052747256, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8058011938975206}}, {"question": "\u201c\u592a\u540e\u4e4b\u8272\u5c11\u89e3\u201d\u4e2d\u201c\u8272\u201d\u6307\nA. \u6012\u8272\nB. \u989c\u8272\nC. \u54c1\u79cd\nD. \u5987\u5973\u7f8e\u8c8c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6912943256836751, "meta-math/MetaMath-Mistral-7B": 0.8974167605217546, "itpossible/Chinese-Mistral-7B-v0.1": 0.8939342076214395, "HuggingFaceH4/zephyr-7b-beta": 0.9999829132611644, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9720185605521714, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6679085065861801}}, {"question": "\u4e0b\u5217\u6bcf\u7ec4\u4e2d\u76f8\u540c\u7684\u4e24\u4e2a\u5b57\u8bfb\u97f3\u4e5f\u76f8\u540c\u7684\u4e00\u7ec4\u662f\nA. \u843d\u82b1\u6d41\u6c34 \u4e22\u4e09\u843d\u56db\nB. \u53d1\u4eba\u6df1\u7701 \u6c5f\u82cf\u7701\nC. \u6012\u53d1\u51b2\u51a0 \u5343\u94a7\u4e00\u53d1\nD. \u6328\u6253 \u6328\u6328\u6324\u6324\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.30290163314062096, "HuggingFaceH4/zephyr-7b-beta": 0.7623390882770933, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3060136256597631, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6822320270686517}}, {"question": "\u4e2d\u56fd\u73b0\u4ee3\u7b2c\u4e00\u90e8\u65b0\u8bd7\u96c6\u300a\u5c1d\u8bd5\u96c6\u300b\u51fa\u7248\u4e8e\nA. 1915\u5e74\nB. 1920\u5e74\nC. 1921\u5e74\nD. 1919\u5e74\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u57fa\u7763\u6559\u548c\u72b9\u592a\u6559\u5f7b\u5e95\u5206\u9053\u626c\u9573\u7684\u6700\u91cd\u8981\u7684\u601d\u60f3\u539f\u56e0\u662f\nA. \u9053\u5fb7\u4fe1\u4ef0\u7684\u5dee\u5f02\nB. \u72b9\u592a\u6559\u6709\u5f3a\u70c8\u7684\u590d\u4ec7\u5fc3\u7406\uff0c\u57fa\u7763\u6559\u4e3b\u5f20\u5bbd\u5bb9\nC. \u65b0\u7ea6\u7684\u51fa\u73b0\nD. \u72b9\u592a\u6559\u662f\u6734\u7d20\u7684\u5f8b\u6cd5\u4e3b\u4e49\uff0c\u57fa\u7763\u6559\u662f\u5f62\u800c\u4e0a\u5b66\u7684\u552f\u7075\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4315751764427392, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7003099987390725, "HuggingFaceH4/zephyr-7b-beta": 0.9294052757843592, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.471395083850317, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9441687528637542}}, {"question": "\u5973\u6027\uff0c43\u5c81\u3002\u809d\u5916\u80c6\u7ba1\u7ed3\u77f3\u75c7\u53f23\u5e74\uff0c10\u5c0f\u65f6\u524d\u7a81\u7136\u53f3\u4e0a\u8179\u7ede\u75db\uff0c\u6076\u5fc3\u3001\u5455\u5410\uff0c\u7ee7\u800c\u51fa\u73b0\uff0c\u5bd2\u6218\u3001\u9ad8\u70ed\u3001\u795e\u5fd7\u6de1\u6f20\u3001\u55dc\u7761\uff0c\u67e5\u4f53\uff1aT\uff1a40\u2103\uff0cP120\u6b21\u6bcf\u5206\uff0cBP\uff1a85/60mmHg\uff0c\u4e0a\u8179\u8f7b\u538b\u75db\u3002\u9996\u9009\u7684\u8179\u90e8\u68c0\u67e5\u65b9\u6cd5\u662f\nA. \u589e\u5f3aCT\nB. \u5934\u9885MRI\nC. B\u8d85\nD. X\u7ebf\u5e73\u7247\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4826707829462383, "meta-math/MetaMath-Mistral-7B": 0.6597201121178594, "itpossible/Chinese-Mistral-7B-v0.1": 0.6315996883972728, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7389664249705181}}, {"question": "\u9886\u5bfc\u8005\u975e\u5f3a\u5236\u6027\u5f71\u54cd\u529b\u4ea7\u751f\u7684\u56e0\u7d20\u662f\nA. \u8d44\u5386\u56e0\u7d20\nB. \u54c1\u683c\u56e0\u7d20\nC. \u4f20\u7edf\u56e0\u7d20\nD. \u804c\u4f4d\u56e0\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.8878346241638417, "itpossible/Chinese-Mistral-7B-v0.1": 0.5321093548188088, "HuggingFaceH4/zephyr-7b-beta": 0.634892742346855, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6153421435700608, "meta-llama/Meta-Llama-3-8B": 0.45669490314078304, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6980665438074531}}, {"question": "\u5728\u8ba1\u7b97\u673a\u7f51\u7edc\u4e2d\uff0c\u4e3a\u4e86\u4f7f\u8ba1\u7b97\u673a\u4e4b\u95f4\u80fd\u591f\u6b63\u786e\u4f20\u8f93\u4fe1\u606f\uff0c\u5fc5\u987b\nA. \u9075\u5faa\u7f51\u7edc\u534f\u8bae\nB. \u83b7\u5f97\u7528\u6237\u540d\u5bc6\u7801\nC. \u62e5\u6709\u7535\u5b50\u90ae\u7bb1\nD. \u4f7f\u7528IE\u6d4f\u89c8\u5668\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9925208082516189, "meta-math/MetaMath-Mistral-7B": 0.9998677620289272, "itpossible/Chinese-Mistral-7B-v0.1": 0.9845151379255009, "HuggingFaceH4/zephyr-7b-beta": 0.9999966116626021, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9995941076487229, "meta-llama/Meta-Llama-3-8B": 0.9865565210810907, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9997167762628948}}, {"question": "\u9a6c\u514b\u601d\u8bf4\uff1a\u201c\u7f8e\u6d32\u91d1\u94f6\u4ea7\u5730\u7684\u53d1\u73b0\uff0c\u571f\u8457\u5c45\u6c11\u88ab\u6d88\u706d\u3001\u88ab\u5974\u5f79\u548c\u88ab\u57cb\u846c\u4e8e\u77ff\u4e95\u3002\u5bf9\u4e1c\u5370\u5ea6\u5f00\u59cb\u8fdb\u884c\u7684\u5f81\u670d\u548c\u63a0\u593a\uff0c\u628a\u975e\u6d32\u53d8\u6210\u5546\u4e1a\u6027\u7684\u63a0\u593a\u9ed1\u4eba\u7684\u573a\u6240\uff0c\u2026\u2026\u8fd9\u4e00\u5207\u6807\u5fd7\u7740\u8d44\u672c\u4e3b\u4e49\u65f6\u4ee3\u7684\u66d9\u5149\u3002\u201d\u6750\u6599\u8bf4\u660e\u4e86\nA. \u6b96\u6c11\u6269\u5f20\u4e0e\u63a0\u593a\u52a0\u5feb\u4e86\u8d44\u672c\u539f\u59cb\u79ef\u7d2f\nB. \u6b96\u6c11\u6269\u5f20\u4f20\u64ad\u7740\u8d44\u672c\u4e3b\u4e49\u7684\u751f\u4ea7\u65b9\u5f0f\nC. \u9ed1\u5974\u8d38\u6613\u4fc3\u8fdb\u7f8e\u6d32\u8d44\u672c\u4e3b\u4e49\u7ecf\u6d4e\u4ea7\u751f\u53d1\u5c55\nD. \u6b96\u6c11\u6269\u5f20\u4e3a\u6b96\u6c11\u5730\u9020\u6210\u4e86\u6c89\u91cd\u707e\u96be\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.43754082971278296}}, {"question": "\u91d1\u878d\u8d44\u672c\u662f\u7531\nA. \u5784\u65ad\u7684\u94f6\u884c\u8d44\u672c\u548c\u5784\u65ad\u7684\u5de5\u4e1a\u8d44\u672c\u878d\u5408\u6216\u6df7\u5408\u751f\u957f\u800c\u6210\u7684\nB. \u4ea7\u4e1a\u8d44\u672c\u548c\u5546\u4e1a\u8d44\u672c\u878d\u5408\u6216\u6df7\u5408\u751f\u957f\u800c\u6210\u7684\nC. \u5784\u65ad\u94f6\u884c\u8d44\u672c\u548c\u94f6\u884c\u8d44\u672c\u878d\u5408\u6216\u6df7\u5408\u751f\u957f\u800c\u6210\u7684\nD. \u94f6\u884c\u8d44\u672c\u7684\u5de5\u4e1a\u8d44\u672c\u878d\u5408\u6216\u6df7\u5408\u751f\u957f\u800c\u6210\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7750097092312159, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b66\u4f5b\u5e94\u4ee5\u4ec0\u4e48\u4e3a\u9996\u8981\uff1f\nA. \u6b63\u5b9a\nB. \u6301\u6212\nC. \u5e03\u65bd\nD. \u6b63\u89c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7093806520150666}}, {"question": "\u4e0b\u5217\u751f\u6d3b\u4e2d\u7684\u505a\u6cd5\u6b63\u786e\u7684\u662f\nA. \u7528\u5bb6\u7528\u201c84\u6d88\u6bd2\u6db2\u201d\u5bf9\u767d\u8272\u7ec7\u7269\u6d88\u6bd2\nB. \u7528\u805a\u6c2f\u4e59\u70ef\uff08PVC\uff09\u4fdd\u9c9c\u819c\u5305\u88f9\u4fdd\u5b58\u9ad8\u6e29\u6cb9\u70b8\u98df\u7269\nC. \u7528\u201c201\u4e0d\u9508\u94a2\u201d\u5236\u6210\u7684\u9505\u7092\u83dc\nD. \u5728\u7259\u9f88\u80bf\u75db\u65f6\uff0c\u75285\u6444\u6c0f\u5ea6\u7684\u51c9\u6c34\u5237\u7259\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b0\u95fb\u53cd\u6620\u793e\u4f1a\u751f\u6d3b\u7684\u624b\u6bb5\u662f\nA. \u865a\u5e7b\u51a5\u60f3\nB. \u903b\u8f91\u63a8\u7406\nC. \u827a\u672f\u518d\u73b0\nD. \u5fe0\u5b9e\u8bb0\u5f55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9564938296080456, "meta-math/MetaMath-Mistral-7B": 0.995902511886601, "itpossible/Chinese-Mistral-7B-v0.1": 0.812311461677878, "HuggingFaceH4/zephyr-7b-beta": 0.9999849983044304, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9880029605745194, "meta-llama/Meta-Llama-3-8B": 0.9346854638930552, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.98888581964185}}, {"question": "\u5fb7\u56fd\u8457\u540d\u8bd7\u4eba\u6d77\u6d85\u5728\u4ed6\u7684\u4f5c\u54c1\u300a\u897f\u91cc\u897f\u4e9a\u7eba\u7ec7\u5de5\u4eba\u300b\u4e2d\u5199\u9053\uff1a\u201c\u5fe7\u90c1\u7684\u773c\u91cc\u6ca1\u6709\u773c\u6cea\uff0c\u4ed6\u4eec\u5750\u5728\u7ec7\u673a\u65c1\uff0c\u54ac\u7259\u5207\u9f7f\uff1a\u5fb7\u610f\u5fd7\uff0c\u6211\u4eec\u5728\u7ec7\u4f60\u7684\u5c38\u5e03\uff0c\u6211\u4eec\u7ec7\u8fdb\u53bb\u4e09\u5c42\u8bc5\u5492\u2014\u2014\u6211\u4eec\u7ec7\uff0c\u6211\u4eec\u7ec7\uff01\u201d\u8fd9\u5e94\u5c5e\u4e8e\u54ea\u4e00\u6587\u5b66\u6d41\u6d3e\nA. \u73b0\u5b9e\u4e3b\u4e49\nB. \u8352\u8bde\u4e3b\u4e49\nC. \u6d6a\u6f2b\u4e3b\u4e49\nD. \u73b0\u4ee3\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.465706094888918, "meta-llama/Meta-Llama-3-8B": 0.66701837801439, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65e5\u964d\u6c34\u91cf 50~100mm \u7684\u964d\u6c34\u79f0\u4e3a[ ]\u3002\nA. \u66b4\u96e8\nB. \u4e2d\u96e8\nC. \u5927\u96e8\nD. \u5c0f\u96e8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.38529846523852773, "meta-math/MetaMath-Mistral-7B": 0.8486143671019303, "itpossible/Chinese-Mistral-7B-v0.1": 0.7687640740736549, "HuggingFaceH4/zephyr-7b-beta": 0.6906018166543682, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7602102705845538, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1933\u5e74\uff0c\u7f57\u65af\u798f\u603b\u7edf\u7b7e\u7f72\u7530\u7eb3\u897f\u6cb3\u6d41\u57df\u7ba1\u7406\u5c40\u6cd5\u6848\uff0c\u5b9e\u65bd\u5bf9\u8be5\u6d41\u57df\u7684\u7efc\u5408\u6cbb\u7406\u4e0e\u5168\u9762\u53d1\u5c55\u8ba1\u5212\u3002\u5176\u201c\u4ee3\u8868\u73b0\u4ee3\u653f\u5e9c\u4e2d\u4e00\u79cd\u771f\u6b63\u65b0\u9896\u800c\u5bcc\u4e8e\u60f3\u8c61\u529b\u7684\u8bbe\u8ba1\u3002\u2026\u2026\u5b83\u5c06\u4e0d\u53d7\u90a3\u4e9b\u4e0d\u76f8\u5e72\u7684\u56fd\u5bb6\u754c\u9650\u7684\u7981\u5236\uff0c\u800c\u4e14\u5c06\u662f\u72ec\u7acb\u7ecf\u8425\u7684\u3001\u653f\u5e9c\u6240\u6709\u7684\u516c\u53f8\u201d\u3002\u8fd9\u79cd\u7ecf\u8425\u65b9\u5f0f\nA. \u5177\u6709\u79c1\u8425\u4f01\u4e1a\u7684\u67d0\u4e9b\u7075\u6d3b\u6027\nB. \u6539\u53d8\u4e86\u7530\u7eb3\u897f\u6cb3\u6d41\u57df\u56fd\u6709\u5236\nC. \u6291\u5236\u5784\u65ad\u8d44\u672c\u4e3b\u4e49\u7684\u53d1\u5c55\nD. \u8868\u660e\u56fd\u5bb6\u653e\u5f03\u5bf9\u4f01\u4e1a\u5e72\u9884\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30472639527554196, "meta-math/MetaMath-Mistral-7B": 0.5751869878830048, "itpossible/Chinese-Mistral-7B-v0.1": 0.5364815755964616, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5152277915574198, "meta-llama/Meta-Llama-3-8B": 0.3779088778108667, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7905242122421262}}, {"question": "\u6211\u56fd\u7684\u7acb\u6cd5\u4f53\u5236\u662f\nA. \u4e09\u5143\u591a\u5c42\u6b21\u7684\nB. \u4e8c\u7ea7\u7acb\u6cd5\u4f53\u5236\nC. \u4e00\u5143\u591a\u5c42\u6b21\u7684\nD. \u4e8c\u5143\u591a\u5c42\u6b21\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8154195345935149, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.34668920612958265, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6b7b\u5211\u7f13\u671f\u6267\u884c\uff0c\u4e0b\u5217\u8bf4\u6cd5\u9519\u8bef\u7684\u6709\nA. \u5728\u6b7b\u5211\u7f13\u671f\u6267\u884c\u671f\u95f4\uff0c\u5982\u679c\u6ca1\u6709\u6545\u610f\u72af\u7f6a\uff0c\u4e8c\u5e74\u671f\u6ee1\u4ee5\u540e\uff0c\u51cf\u4e3a\u65e0\u671f\u5f92\u5211\nB. \u5728\u6b7b\u5211\u7f13\u671f\u6267\u884c\u671f\u95f4\uff0c\u5982\u679c\u6709\u6545\u610f\u72af\u7f6a\uff0c\u4f46\u4e0d\u5c5e\u4e8e\u60c5\u8282\u6076\u52a3\u7684\uff0c\u53ef\u4ee5\u4e0d\u6267\u884c\u6b7b\u5211\uff0c\u65b0\u5224\u5904\u7684\u5211\u7f5a\u4e0e\u539f\u6765\u7684\u6b7b\u7f13\u5211\u6570\u7f6a\u5e76\u7f5a\uff0c\u7ee7\u7eed\u6267\u884c\u6b7b\u7f13\u5211\uff0c\u539f\u6709\u7684\u6b7b\u5211\u7f13\u671f\u6267\u884c\u671f\u95f4\u7ee7\u7eed\u6709\u6548\u3002\u6b7b\u5211\u7f13\u671f\u6267\u884c\u7684\u671f\u95f4\u91cd\u65b0\u8ba1\u7b97\nC. \u5728\u6b7b\u5211\u7f13\u671f\u6267\u884c\u671f\u95f4\uff0c\u5982\u679c\u6ca1\u6709\u6545\u610f\u72af\u7f6a\uff0c\u4e14\u786e\u6709\u91cd\u5927\u7acb\u529f\u8868\u73b0\uff0c\u4e8c\u5e74\u671f\u6ee1\u4ee5\u540e\uff0c\u51cf\u4e3a25\u5e74\u6709\u671f\u5f92\u5211\nD. \u5728\u6b7b\u5211\u7f13\u671f\u6267\u884c\u671f\u95f4\uff0c\u5982\u679c\u6709\u6545\u610f\u72af\u7f6a\uff0c\u4e14\u72af\u7f6a\u60c5\u8282\u6076\u52a3\u7684\uff0c\u6267\u884c\u6b7b\u5211\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8840\u6e90\u6027\u80ba\u8113\u80bf\u6700\u5e38\u89c1\u7684\u81f4\u75c5\u83cc\u662f\nA. \u519b\u56e2\u83cc\nB. \u94dc\u7eff\u5047\u5355\u80de\u83cc\nC. \u91d1\u9ec4\u8272\u8461\u8404\u7403\u83cc\nD. \u538c\u6c27\u83cc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31272421438728476, "meta-math/MetaMath-Mistral-7B": 0.3220562534414596, "itpossible/Chinese-Mistral-7B-v0.1": 0.46532510758461076, "HuggingFaceH4/zephyr-7b-beta": 0.5614781264072256, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4716471952506262, "meta-llama/Meta-Llama-3-8B": 0.37887215961900156, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u57fa\u56e0\u7a81\u53d8\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u7a81\u53d8\u9891\u7387\u5f88\u4f4e\uff0c\u4f46\u662f\u591a\u65b9\u5411\u7684\nB. \u8868\u73b0\u51fa\u4eb2\u4ee3\u6240\u6ca1\u6709\u7684\u8868\u73b0\u578b\u53eb\u57fa\u56e0\u7a81\u53d8\nC. \u57fa\u56e0\u7a81\u53d8\u80fd\u591f\u4ea7\u751f\u65b0\u7684\u57fa\u56e0\nD. DNA\u5206\u5b50\u4e2d\u6709\u9057\u4f20\u6548\u5e94\u7684\u7247\u6bb5\u4e2d\u78b1\u57fa\u53d1\u751f\u53d8\u5316\u7684\u662f\u57fa\u56e0\u7a81\u53d8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.38887970689955315, "itpossible/Chinese-Mistral-7B-v0.1": 0.5816502116738543, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4181874395315597, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.44403405367985443}}, {"question": "\u201c\u4ee5\u5408\u6cd5\u7684\u5f62\u5f0f\u63a9\u76d6\u975e\u6cd5\u76ee\u7684\u7684\u6c11\u4e8b\u884c\u4e3a\u65e0\u6548\u201d\u8fd9\u4e00\u89c4\u5219\u662f\nA. \u4efb\u610f\u6027\u89c4\u5219\nB. \u5f3a\u884c\u6027\u89c4\u5219\nC. \u6388\u6743\u6027\u89c4\u5219\nD. \u547d\u4ee4\u6027\u89c4\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3588823130168717, "itpossible/Chinese-Mistral-7B-v0.1": 0.310313193127302, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.32545507259594497, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5404\u56fd\u4e3b\u5f20\u672c\u56fd\u8bc1\u5238\u7ba1\u5236\u7acb\u6cd5\u57df\u5916\u6548\u529b\u7684\u4f9d\u636e\u662f\nA. \u56fd\u6c11\u5f85\u9047\u539f\u5219\nB. \u5bf9\u7b49\u539f\u5219\nC. \u4fdd\u62a4\u4e3b\u4e49\u539f\u5219\nD. \u5408\u7406\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65af\u591a\u845b\u5b66\u6d3e\u7684\u4ee3\u8868\u4eba\u7269\u4e0d\u5305\u62ec\nA. \u829d\u8bfa\nB. \u6cf0\u52d2\u65af\nC. \u585e\u5185\u5361\nD. \u5965\u52d2\u7559\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.560245681011954, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u636e\u8d44\u6599\u7edf\u8ba1\uff1a\u5728\u660e\u540e\u671f\u81f3\u6e05\u524d\u671f200\u4f59\u5e74\u95f4\uff0c\u4e16\u754c\u767d\u94f6\u4ea7\u91cf\u7684\u4e00\u534a\u6d41\u5165\u4e2d\u56fd\uff0c\u62e5\u6709\u4e00\u6d41\u57ce\u5e02\u548c\u6700\u4e3a\u5bc6\u96c6\u3001\u5b8c\u5584\u7684\u5e02\u573a\u7f51\u7edc\u7684\u4e2d\u56fd\uff0c\u6210\u4e3a\u5f53\u65f6\u4e16\u754c\u7ecf\u6d4e\u548c\u8d38\u6613\u7684\u4e2d\u5fc3\u533a\u57df\u3002\u7136\u800c\u5f53\u65f6\u5b83\u5374\u6ca1\u6709\u5f62\u6210\u5f3a\u5927\u7684\u626b\u8361\u65e7\u7ecf\u6d4e\u57fa\u7840\u7684\u9769\u547d\u6027\u53d8\u5316\u3002\u5176\u4e2d\u5185\u5728\u7684\u548c\u4eba\u4e3a\u7684\u539f\u56e0\u662f\nA. \u7531\u4e8e\u9e26\u7247\u5927\u91cf\u6d41\u5165\u5bfc\u81f4\u767d\u94f6\u7684\u5927\u91cf\u5916\u6d41\nB. \u201c\u91cd\u519c\u6291\u5546\u201d\u548c\u201c\u95ed\u5173\u9501\u56fd\u201d\u7684\u653f\u7b56\u7684\u538b\u5236\nC. \u82f1\u56fd\u5de5\u4e1a\u9769\u547d\u540e\u5bf9\u4e2d\u56fd\u8fdb\u884c\u7684\u5546\u54c1\u8f93\u51fa\nD. \u5927\u6cb3\u6d41\u57df\u7684\u519c\u4e1a\u6587\u660e\u4e0d\u9002\u4e8e\u5de5\u5546\u4e1a\u53d1\u5c55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5078471448927719, "meta-math/MetaMath-Mistral-7B": 0.755895829001925, "itpossible/Chinese-Mistral-7B-v0.1": 0.6159439932867812, "HuggingFaceH4/zephyr-7b-beta": 0.9613258288580622, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6719167794805342, "meta-llama/Meta-Llama-3-8B": 0.7631855554540348, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8681998416390752}}, {"question": "1702\u5e74\u82f1\u56fd\u56fd\u738b\u5a01\u5ec9\u4e8c\u4e16\u53bb\u4e16\uff0c\u5b89\u59ae\u5973\u738b\u7ee7\u4f4d\uff0c\u5f53\u65f6\u8bae\u4f1a\u5185\u90e8\u5b58\u5728\u4e24\u4e2a\u515a\u6d3e\uff0c\u5b89\u59ae\u5ef6\u8bef\u5360\u591a\u6570\u5e2d\u4f4d\u7684\u8f89\u683c\u515a\uff0c\u4e8e\u662f\u89e3\u9664\u4e86\u8f89\u683c\u515a\u4eba\u7684\u884c\u653f\u8981\u804c\uff0c\u4ee3\u4e4b\u4ee5\u6258\u5229\u515a\u4eba\u3002\u8fd9\u8bf4\u660e\u5f53\u65f6\u5728\u82f1\u56fd\nA. \u8bae\u4f1a\u65e0\u6743\u5236\u88c1\u56fd\u738b\nB. \u541b\u4e3b\u7acb\u5baa\u5236\u5c1a\u672a\u5b8c\u5584\nC. \u300a\u6743\u5229\u6cd5\u6848\u300b\u906d\u5230\u7834\u574f\nD. \u5185\u9601\u5236\u5df2\u57fa\u672c\u786e\u7acb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43131843123220165, "meta-math/MetaMath-Mistral-7B": 0.5557113352174357, "itpossible/Chinese-Mistral-7B-v0.1": 0.5377748110289073, "HuggingFaceH4/zephyr-7b-beta": 0.993588377764013, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7559610160918779, "meta-llama/Meta-Llama-3-8B": 0.8488658061679497, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6874820389344692}}, {"question": "\u5728\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\u65f6\uff0c\u9644\u4ef6\u7684\u5927\u5c0f\u662f\u6709\u9650\u5236\u7684\u3002\u5047\u8bbe\u67d0\u7f51\u7ad9\u89c4\u5b9a\u9644\u4ef6\u7684\u6700\u5927\u503c\u4e3a1M\uff0c\u73b0\u6709\u4e00\u4f4d\u540c\u5b66\u9700\u53d1\u9001\u603b\u5bb9\u91cf\u4e3a2250KB\u7684\u591a\u4e2a\u6587\u4ef6\u3002\u5219\u4ed6\u5728\u8be5\u7f51\u7ad9\u4e2d\u81f3\u5c11\u53d1\u9001\u7684\u6b21\u6570\u662f\nA. 3\nB. 2\nC. 1\nD. 4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.2885095257630687, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.33627075818826824, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3273141701864991, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3498694077905133}}, {"question": "\u76f8\u4fe1\u6027\u522b\u662f\u6c38\u4e45\u7684\u800c\u4e0d\u7ba1\u5e74\u9f84\u3001\u884c\u4e3a\u6216\u5916\u8c8c\u53d1\u751f\u4ec0\u4e48\u53d8\u5316\u662f\uff08\uff09\u7684\u6982\u5ff5\nA. \u6027\u522b\u8ba4\u540c\nB. \u6027\u522b\u523b\u677f\u5b9a\u578b\nC. \u6027\u522b\u6052\u5e38\u6027\nD. \u6027\u5dee\u522b\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6393430782275081, "HuggingFaceH4/zephyr-7b-beta": 0.9981202267338697, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5935374702107573, "meta-llama/Meta-Llama-3-8B": 0.897244867054869, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9868110807857187}}, {"question": "\u7532 (15 \u5468\u5c81) \u76d7\u7a83\u4ed6\u4eba\u94b1\u5305\uff0c\u88ab\u9648\u67d0\u53d1\u73b0\uff0c\u4e3a\u88c1\u8d43\u7269\u800c\u5f53\u573a\u4f7f\u7528\u66b4\u529b\uff0c\u5931\u624b\u5c06\u9648\u67d0\u6253\u6b7b\u7532\u7684\u884c\u4e3a\u6784\u6210\nA. \u62a2\u52ab\u7f6a\nB. \u8fc7\u5931\u81f4\u4eba\u6b7b\u4e4b\u7f6a\nC. \u76d7\u7a83\u7f6a\nD. \u6210\u5fc3\u4f24\u5bb3\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u76f4\u63a5\u548c\u8bed\u8a00\u6d3b\u52a8\u6709\u5173\u7684\u4e2d\u67a2\nA. \u989d\u4e2d\u56de\u540e\u5206\nB. \u9876\u4e0a\u5c0f\u53f6\u7f18\u4e0a\u56de\nC. \u989d\u4e0b\u56de\u540e\u5206\nD. \u89d2\u56de\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8bcd\u8bed\u4e2d\uff0c\u662f\u5f62\u5bb9\u8bcd\u4e0d\u8fbe\u610f\u7684\u662f\nA. \u70ed\u95f9\nB. \u771f\u60c5\nC. \u706b\u7ea2\nD. \u706b\u70ed\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.34686667406054084, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679c\u628a\u5b66\u6821\u7684\u4f4d\u7f6e\u8bb0\u4f5c0m\uff0c\u4ece\u5b66\u6821\u51fa\u53d1\u5411\u4e1c\u2f9b300m\u6709\u2f00\u4e2a\u4e66\u5e97\uff0c\u4e66\u5e97\u7684\u4f4d\u7f6e\u8bb0\u4f5c+300m\uff0c\u4ece\u4e66\u5e97\u51fa\u53d1\u5411\u2ec4\u2f9b600m\u6709\u2f00\u4e2a\u62a5\u4ead\uff0c\u62a5\u4ead\u7684\u4f4d\u7f6e\u8bb0\u4f5c\uff08 \uff09m\nA.  +900 \nB. -600\nC. -300\nD. -900\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.3742744353722653, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53ef\u4ee5\u5bfc\u81f4\u539f\u764c\u57fa\u56e0\u6fc0\u6d3b\u7684\u673a\u5236\u662f\nA. p53\u86cb\u767d\u8bf1\u5bfc\u7ec6\u80de\u51cb\u4ea1\nB. \u83b7\u5f97\u542f\u52a8\u5b50\nC. \u8f6c\u5f55\u56e0\u5b50\u4e0eRNA\u7684\u7ed3\u5408\nD. \u6291\u764c\u57fa\u56e0\u7684\u8fc7\u8868\u8fbe\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4181874540337694, "meta-math/MetaMath-Mistral-7B": 0.34315473629358506, "itpossible/Chinese-Mistral-7B-v0.1": 0.5415245310397739, "HuggingFaceH4/zephyr-7b-beta": 0.6713118034589453, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4146700649619064, "meta-llama/Meta-Llama-3-8B": 0.3376787962540697, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7cbe\u795e\u5206\u6790\u5b66\u6d3e\u7684\u4ee3\u8868\u4eba\u7269\u662f\nA. \u8a79\u59c6\u65af\nB. \u827e\u5bbe\u6d69\u65af\nC. \u9a6c\u65af\u6d1b\nD. \u5f17\u6d1b\u4f0a\u5fb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9331463009873517, "meta-math/MetaMath-Mistral-7B": 0.9906185987181385, "itpossible/Chinese-Mistral-7B-v0.1": 0.9898447063901912, "HuggingFaceH4/zephyr-7b-beta": 0.9999580594725216, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9519995901767142, "meta-llama/Meta-Llama-3-8B": 0.9965632334828748, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.99821459128374}}, {"question": "\u4e2a\u4eba\u7684\u6c11\u65cf\u6210\u4efd\uff0c\u53ea\u80fd\u4f9d\u636e\uff08\uff09\u7684\u6c11\u65cf\u6210\u4efd\u786e\u5b9a\nA. \u7236\u4eb2\nB. \u4e0d\u80fd\u786e\u5b9a\nC. \u6bcd\u4eb2\nD. \u7236\u4eb2\u6216\u6bcd\u4eb2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u540c\u4e00\u5730\u6bb5\u4e0a\u65e2\u6709\u65e0\u6027\u66f4\u65b0\u65b9\u5f0f\u57f9\u80b2\u5c0f\u5f84\u6750\uff0c\u53c8\u57f9\u80b2\u5b9e\u751f\u6811\u751f\u4ea7\u5927\u5f84\u6750\u7684\u7ecf\u8425\u65b9\u6cd5\uff0c\u79f0\u4e3a\nA. \u77ee\u6797\u4f5c\u4e1a\nB. \u5929\u7136\u6797\u7ecf\u8425\nC. \u4e2d\u6797\u4f5c\u4e1a\nD. \u6b21\u751f\u6797\u7ecf\u8425\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3984643349883637, "meta-math/MetaMath-Mistral-7B": 0.513313093675059, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.48398799422901045, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u54ea\u4e00\u4e2a\u4e3a\u7c97\u5236\u54c1\uff0c\u542b\u975e\u7cd6\u7269\u8d28\u53ca\u6709\u673a\u9178\u8f83\u591a\uff0c\u7279\u522b\u662f\u6709\u673a\u9178\u8fbe\u5230\u4e00\u5b9a\u6d53\u5ea6\u65f6\uff0c\u53ef\u4f7f\u725b\u5976\u4e2d\u7684\u86cb\u767d\u8d28\u53d1\u751f\u51dd\u805a\u548c\u6c89\u6dc0\uff0c\u8425\u517b\u4ef7\u503c\u5927\u5927\u964d\u4f4e\nA. \u8702\u871c\nB. \u7ea2\u7cd6\nC. \u767d\u7cd6\nD. \u6728\u7cd6\u9187\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.431033945941338, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u5b50\uff0c\u5c5e\u4e8e\u5a49\u66f2\u8fd9\u4e00\u8868\u8fbe\u65b9\u5f0f\u7684\u4e00\u53e5\u662f\nA. \u4e00\u65e6\u5c71\u9675\u5d29\uff0c\u9577\u5b89\u541b\u4f55\u4ee5\u81ea\u8a17\u65bc\u8d99?\nB. \u8449\u4e0a\u96e8\u8072\u679d\u4e0a\u6708\uff0c\u4f55\u987b\u7126\u5c3e\u59cb\u76f8\u77e5\u3002\nC. \u5927\u592b\u4e0d\u5f97\u9020\u8eca\u99ac\u3002\nD. \u96f7\u9706\u4e0d\u4f5c\uff0c\u98a8\u96e8\u4e0d\u8208\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0e\u53f3\u4e3b\u652f\u6c14\u7ba1\u76f8\u6bd4\uff0c\u5de6\u4e3b\u652f\u6c14\u7ba1\nA. \u7ec6\u800c\u77ed\nB. \u7c97\u800c\u957f\nC. \u7c97\u800c\u77ed\nD. \u7ec6\u800c\u957f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u901a\u8fc7console\u2f1d\u7ba1\u7406\u4ea4\u6362\u673a\u5728\u8d85\u7ea7\u7ec8\u7aef\u2fa5\u5e94\u8bbe\u4e3a\nA. \u6ce2\u7279\u7387\uff1a57600\u6570\u636e\u4f4d\uff1a8\u505c\u2f4c\u4f4d\uff1a1\u5947\u5076\u6821\u9a8c\uff1a\u6709\nB. \u6ce2\u7279\u7387\uff1a9600\u6570\u636e\u4f4d\uff1a6\u505c\u2f4c\u4f4d\uff1a2\u5947\u5076\u6821\u9a8c\uff1a\u6709\nC. \u6ce2\u7279\u7387\uff1a57600\u6570\u636e\u4f4d\uff1a6\u505c\u2f4c\u4f4d\uff1a1\u5947\u5076\u6821\u9a8c\uff1a\u2f46\nD. \u6ce2\u7279\u7387\uff1a9600\u6570\u636e\u4f4d\uff1a8\u505c\u2f4c\u4f4d\uff1a1\u5947\u5076\u6821\u9a8c\uff1a\u2f46\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.2906893535433972, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u4e0e\u773c\u89c6\u8fd1\u7269\u8c03\u8282\u65e0\u5173\u7684\u53cd\u5c04\u6d3b\u52a8\u662f\nA. \u53cc\u773c\u4f1a\u805a\nB. \u77b3\u5b54\u8c03\u8282\u53cd\u5c04\nC. \u77b3\u5b54\u5bf9\u5149\u53cd\u5c04\nD. \u6676\u72b6\u4f53\u53d8\u51f8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3007752593564565, "meta-math/MetaMath-Mistral-7B": 0.3882259150151051, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.35347162922091135, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u9aa8\u5de8\u7ec6\u80de\u7624\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u597d\u53d1\u4e8e\u819d\u5173\u8282\u4e0a\u3001\u4e0b\u9aa8\u7aef\nB. \u597d\u53d1\u4e8e 20-40 \u5c81\nC. \u591a\u5c5e\u6076\u6027\nD. \u5c40\u90e8\u80bf\u80c0\u5305\u5757\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.541524531039774, "meta-math/MetaMath-Mistral-7B": 0.685602969860369, "itpossible/Chinese-Mistral-7B-v0.1": 0.3802392203343767, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5284271792238497, "meta-llama/Meta-Llama-3-8B": 0.6845414285092583, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9506935807380834}}, {"question": "\u5bfc\u81f4\u652f\u6c14\u7ba1\u6269\u5f20\u75c7\u7684\u4e3b\u8981\u75c5\u53d8\u57fa\u7840\u662f\nA. \u652f\u6c14\u7ba1\u58c1\u7684\u9ecf\u6db2\u817a\u5927\u91cf\u589e\u751f\nB. \u652f\u6c14\u7ba1\u7ea4\u6bdb\u67f1\u72b6\u4e0a\u76ae\u7684\u9cde\u72b6\u4e0a\u76ae\u5316\u751f\nC. \u80ba\u7ec4\u7ec7\u7ea4\u7ef4\u5316\nD. \u652f\u6c14\u7ba1\u58c1\u7ed3\u6784\u7834\u574f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.34412869855849515, "HuggingFaceH4/zephyr-7b-beta": 0.7567936094606832, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6069\u683c\u65af\u8bf4\uff1a\u201c\u4eba\u7684\u667a\u529b\u662f\u6309\u7167\u4eba\u5982\u4f55\u5b66\u4f1a\u6539\u9020\u81ea\u7136\u754c\u800c\u53d1\u5c55\u7684\u3002\u201d\u8fd9\u8bf4\u660e\nA. \u81ea\u7136\u754c\u662f\u8ba4\u8bc6\u53d1\u5c55\u7684\u52a8\u529b\nB. \u4eba\u7684\u8ba4\u8bc6\u5177\u6709\u4e3b\u89c2\u80fd\u52a8\u6027\nC. \u79d1\u5b66\u8fdb\u6b65\u662f\u5b9e\u8df5\u7684\u76ee\u7684\nD. \u4eba\u5177\u6709\u8ba4\u8bc6\u81ea\u7136\u7684\u80fd\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53d1\u5c55\u6709\u4e2d\u56fd\u7279\u8272\u7684\u516c\u5171\u5173\u7cfb\u5b66\u7684\u524d\u63d0\u662f\nA. \u7814\u7a76\u56fd\u60c5\nB. \u79ef\u6781\u5e94\u7528\nC. \u6269\u5927\u7ec4\u7ec7\nD. \u52a0\u5f3a\u5ba3\u4f20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.832720171664092, "meta-math/MetaMath-Mistral-7B": 0.9373846601086296, "itpossible/Chinese-Mistral-7B-v0.1": 0.5440775312194898, "HuggingFaceH4/zephyr-7b-beta": 0.9999400070658213, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9255890874649673, "meta-llama/Meta-Llama-3-8B": 0.8378245801238042, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8100943543895532}}, {"question": "\u5173\u4e8e\u7837\u4e2d\u6bd2\u7684\u63cf\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u8fdb\u5165\u4eba\u4f53\u540e\u591a\u4ee5\u539f\u6709\u5f62\u5f0f\u53d1\u6325\u5176\u6bd2\u6027\nB. \u6709\u673a\u7837\u6bd2\u6027\u5927\u4e8e\u65e0\u673a\u7837\nC. \u4e09\u4ef7\u7837\u6bd2\u6027\u5927\u4e8e\u4e94\u4ef7\u7837\nD. \u7837\u4e2d\u6bd2\u9996\u9009\u4e8c\u758f\u57fa\u4e19\u78fa\u9178\u7eb3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9426068984844226, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e7e\u9686\u65f6\u6c5f\u5357\u5730\u4e3b\u201c\u6240\u5c45\u5728\u57ce\u6216\u4ed6\u5dde\u5f02\u53bf\uff0c\u5730\u4ea9\u5c71\u573a\u7686\u59d4\u4e4b\u4f43\u6237\u201d\u3002\u82cf\u5dde\u751a\u81f3\u51fa\u73b0\u201c\u571f\u8457\u5b89\u4e1a\u8005\u7530\u4e0d\u6ee1\u767e\u4ea9\uff0c\u4f59\u7686\u4f43\u519c\u4e5f\u3002\u4e0a\u7530\u534a\u5f52\u4e8e\u90e1\u57ce\u4e4b\u5bcc\u6237\u201d\u3002\u7531\u6b64\u53ef\u77e5\uff0c\u5f53\u65f6\u6c5f\u5357\nA. \u4e2a\u4f53\u519c\u8015\u4e3a\u4e3b\u8981\u751f\u4ea7\u5f62\u5f0f\nB. \u519c\u4e1a\u4e2d\u5546\u54c1\u5316\u751f\u4ea7\u666e\u904d\nC. \u519c\u4e1a\u751f\u4ea7\u5229\u6da6\u5fae\u4e0d\u8db3\u9053\nD. \u571f\u5730\u6240\u6709\u6743\u53d8\u66f4\u6781\u4e3a\u9891\u7e41\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3689108554330874, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7a0b\u5e8f\u6cd5\u662f\u89c4\u5b9a\u4fdd\u969c\u6743\u5229\u548c\u4e49\u52a1\u5b9e\u73b0\u7684\u7a0b\u5e8f\u65b9\u9762\u7684\u6cd5\u5f8b\u89c4\u8303\u3002\u4e0b\u5217\u5c5e\u4e8e\u7a0b\u5e8f\u6cd5\u7684\u662f\nA. \u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u62c5\u4fdd\u6cd5\u300b\nB. \u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u516c\u53f8\u6cd5\u300b\nC. \u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u7acb\u6cd5\u6cd5\u300b\nD. \u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u4ef2\u88c1\u6cd5\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43836605249276467, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d3e\u5e73\u51f9\u5728\u300a\u79e6\u8154\u300b\u4e2d\u8bf4\uff0c\u516b\u767e\u91cc\u79e6\u5ddd\u519c\u6c11\u89c6\u4e3a\u751f\u547d\u201c\u4e94\u5927\u8981\u7d20\u201d\u7684\uff0c\u9664\u79e6\u8154\u5916\uff0c\u8fd8\u6709\nA. \u897f\u6cb3\u5927\u9f13\u3001\u201c\u897f\u51e4\u201d\u767d\u9152\u3001\u5927\u53f6\u5377\u70df\u3001\u725b\u8089\u6ce1\u998d\nB. \u9ec4\u571f\u9ad8\u539f\u3001\u201c\u897f\u51e4\u201d\u767d\u9152\u3001\u5927\u53f6\u5377\u70df\u3001\u7f8a\u8089\u6ce1\u998d\nC. \u201c\u897f\u98ce\u201d\u767d\u9152\u3001\u957f\u7ebf\u8fa3\u5b50\u3001\u5927\u53f6\u5377\u70df\u3001\u725b\u8089\u6ce1\u998d\nD. \u201c\u897f\u51e4\u201d\u767d\u9152\u3001\u957f\u7ebf\u8fa3\u5b50\u3001\u5927\u53f6\u5377\u70df\u3001\u7f8a\u8089\u6ce1\u998d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\uff0c80\u5c81\uff0c\u6454\u5012\u540e\u81f4\u53f3\u9acb\u90e8\u75bc\u75db2\u5929\uff0c\u5e73\u7d20\u4f53\u5065\u3002X\u7ebf\u68c0\u67e5\u89c1\u53f3\u80a1\u9aa8\u9888\u5934\u4e0b\u578b\u9aa8\u6298\uff0c\u6709\u79fb\u4f4d\u3002\u6700\u5408\u9002\u7684\u6cbb\u7597\u65b9\u6cd5\u662f\nA. \u5207\u5f00\u590d\u4f4d\u5185\u56fa\u5b9a\nB. \u76ae\u7275\u5f15\nC. \u5367\u5e8a\u4f11\u606f\uff0c\u7a7f\u9632\u65cb\u8f6c\u978b\nD. \u4eba\u5de5\u80a1\u9aa8\u5934\u7f6e\u6362\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5154994906291287, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9ebb\u9189\u4e2d\u53d1\u73b0 CO2 \u84c4\u79ef\uff0c\u5982\u6392\u51fa\u901f\u5ea6\u8fc7\u5feb\uff0c\u6700\u53ef\u80fd\u51fa\u73b0\u7684\u4e34\u5e8a\u8868\u73b0\u662f\nA. \u8840\u538b\u4e0a\u5347\uff0c\u547c\u5438\u53d8\u5feb\nB. \u8840\u538b\u4e0a\u5347\uff0c\u547c\u5438\u53d8\u6162\nC. \u8840\u538b\u4e0b\u964d\uff0c\u547c\u5438\u6682\u505c\nD. \u8840\u538b\u4e0b\u964d\uff0c\u547c\u5438\u53d8\u5feb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3256774323330271, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.31283638571410965, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u7ade\u4e89\u5bf9\u624b\u5728\u4efb\u4f55\u65b9\u9762\u7684\u8fdb\u653b\u90fd\u4f1a\u8fc5\u901f\u3001\u5f3a\u70c8\u5730\u4f5c\u51fa\u53cd\u5e94\u7684\u7ade\u4e89\u8005\u662f\nA. \u4ece\u5bb9\u4e0d\u8feb\u578b\u7ade\u4e89\u8005\nB. \u968f\u673a\u578b\u7ade\u4e89\u8005\nC. \u9009\u62e9\u578b\u7ade\u4e89\u8005\nD. \u51f6\u731b\u578b\u7ade\u4e89\u8005\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8043012146957618, "meta-math/MetaMath-Mistral-7B": 0.9883163032094244, "itpossible/Chinese-Mistral-7B-v0.1": 0.5977709820913253, "HuggingFaceH4/zephyr-7b-beta": 0.9974596932107184, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9547054279173631, "meta-llama/Meta-Llama-3-8B": 0.9064623992312696, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9890825954675255}}, {"question": "\u67d0\u79d1\u6280\u4f01\u4e1a\u9664\u5bf9\u79d1\u6280\u4eba\u5458\u652f\u4ed8\u5de5\u8d44\u5916\uff0c\u8fd8\u91c7\u53d6\u79d1\u6280\u6210\u679c\u5165\u80a1\u7684\u6fc0\u52b1\u65b9\u5f0f\uff0c\u8c03\u52a8\u79d1\u6280\u4eba\u5458\u79ef\u6781\u6027\uff0c\u4f01\u4e1a\u6548\u76ca\u4e0d\u65ad\u63d0\u9ad8\u3002\u8fd9\u8bf4\u660e\uff1aa\u6309\u751f\u4ea7\u8981\u7d20\u5206\u914d\u6709\u5229\u4e8e\u7f29\u5c0f\u6536\u5165\u5dee\u8ddd\uff1bb\u5206\u914d\u5173\u7cfb\u7684\u8c03\u6574\u6709\u5229\u4e8e\u63a8\u52a8\u751f\u4ea7\u529b\u7684\u53d1\u5c55\uff1bc\u79d1\u6280\u4eba\u5458\u7684\u8111\u529b\u52b3\u52a8\u80fd\u521b\u9020\u66f4\u5927\u7684\u4ef7\u503c\uff1bd\u79d1\u6280\u4eba\u5458\u7684\u6536\u5165\u53d6\u51b3\u4e8e\u79d1\u6280\u6210\u679c\u7684\u4f7f\u7528\u4ef7\u503c\nA. ad\nB. ac\nC. bc\nD. cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.48742725327957154, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u2f26\u6ca1\u6709\u8bed\u75c5\u7684\u2f00\u9879\u662f\nA. \u770b\u7740\u8fd9\u4e9b\u5728\u6e05\u6668\u7684\u51c9\u2edb\u4e2d\u95ea\u70c1\u6447\u66f3\u7684\u7cbe\u7075\uff0c\u2f00\u9635\u83ab\u540d\u7684\u6fc0\u52a8\u5728\u6211\u7684\u80f8\u4e2d\u6d8c\u52a8\u3002\nB. \u4e2d\u8003\u590d\u4e60\u4e2d\uff0c\u4e0d\u5c11\u5b66\u2f63\u5b58\u5728\u7740\u590d\u4e60\u91cd\u70b9\u4e0d\u7a81\u51fa\uff0c\u65f6\u95f4\u5b89\u6392\u4e0d\u5408\u7406\uff0c\u6709\u7684\u751a\u2f84\u8bb0\u4f4f\u4e86\u524d\u2faf\u7684\u77e5\u8bc6\uff0c \u2f1c\u5fd8\u8bb0\u4e86\u540e\u2faf\u7684\u77e5\u8bc6\u3002\nC. \u4f60\u53ef\u4ee5\u5316\u60b2\u6124\u4e3a\u2f12\u91cf\uff0c\u4f46\u4f60\u4e0d\u80fd\u6028\u6068\uff0c\u6240\u4ee5\u6028\u6068\u53ea\u53ef\u80fd\u4f7f\u4f60\u8ddf\u504f\u6fc0\uff0c\u751a\u2f84\u9020\u6210\u66f4\u2f24\u7684\u5931\u8d25\u3002\nD. \u6784\u5efa\u65b0\u578b\u2f24\u56fd\u4e2d\u7f8e\u5173\u7cfb\u662f\u524d\u2f46\u53e4\u2f08\u3001\u540e\u2f46\u6765\u8005\u7684\u2f00\u9879\u4e8b\u4e1a\uff0c\u6ca1\u6709\u73b0\u6210\u7ecf\u9a8c\u548c\u6a21\u5f0f\u53ef\u4ee5\u7167\u642c\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\u52a0\u70b9\u7684\u8bcd\u5728\u53e4\u4eca\u8bcd\u4e49\u7684\u6f14\u53d8\u4e2d\u5c5e\u4e8e\u8bcd\u4e49\u6269\u5927\u7684\u662f\nA. \u6211\u7a3c\u65e2\u540c\uff0c\u4e0a\u5165\u6267\u5bab\u529f\u3002\nB. \u7a7a\u8c37\u4f20\u54cd\uff0c\u54c0\u8f6c\u4e45\u7edd\u3002\nC. \u5938\u7236\u4e0e\u65e5\u9010\u8d70\u3002\nD. \u5c0f\u5927\u4e4b\u72f1\uff0c\u867d\u4e0d\u80fd\u5bdf\uff0c\u5fc5\u4ee5\u60c5\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28496529561720413, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2896636654578384, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636eIncoterms2000\uff0cFOB\u5356\u65b9\u7684\u4ea4\u8d27\u5730\u70b9\u662f\nA. \u88c5\u8fd0\u6e2f\nB. \u88c5\u8fd0\u6e2f\u4e70\u65b9\u6307\u5b9a\u7684\u8239\u4e0a\nC. \u76ee\u7684\u6e2f\nD. \u76ee\u7684\u6e2f\u8239\u4e0a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u77e5\u89c9\u8fc7\u7a0b\u4e2d\uff0c\u7531\u4e8e\u67d0\u4e9b\u5ba2\u89c2\u4e8b\u7269\u5728\u76f8\u4e92\u5bf9\u6bd4\u4e2d\u6709\u7684\u5448\u73b0\u51fa\u8f83\u660e\u663e\u7684\u76f8\u5bf9\u7279\u70b9\uff0c\u81f4\u4f7f\u6211\u4eec\u53bb\u77e5\u89c9\u5b83\u3002\u8fd9\u4fbf\u662f\nA. \u77e5\u89c9\u7684\u4e3b\u52a8\u9009\u62e9\u6027\nB. \u77e5\u89c9\u7684\u504f\u89c1\nC. \u77e5\u89c9\u7684\u88ab\u52a8\u9009\u62e9\u6027\nD. \u77e5\u89c9\u7684\u4e3b\u89c2\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4472552265764219, "meta-math/MetaMath-Mistral-7B": 0.4275872838609673, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u2f7c\u738b a \u5c81\uff0c\u2f29\u674e\uff08 a -18\uff09\u5c81\uff0c\u8fc7 c \u5e74\u540e\uff0c\u4ed6\u4eec\u76f8\u5dee\uff08 \uff09\u5c81\nA. c-18\nB. 18\nC. c\nD. c+18\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3030891783826951}}, {"question": "\u79e6\u7edf\u4e00\u540e\uff0c\u4e3a\u4e86\u5de9\u56fa\u5c01\u5efa\u7edf\u6cbb\uff0c\u201c\u5efa\u7acb\u4e86\u4e00\u5957\u6bd4\u8f83\u5b8c\u5907\u7684\u3001\u7cfb\u7edf\u7684\u56fd\u5bb6\u7ba1\u7406\u673a\u6784\u2014\u2014\u91d1\u5b57\u5854\u5f0f\u7684\u4e2d\u67a2\u884c\u653f\u7cfb\u7edf\u2026\u2026\u91c7\u7528\u5206\u5de5\u7684\u539f\u5219\uff0c\u4f7f\u4e2d\u592e\u653f\u5e9c\u80fd\u7ef4\u6301\u6709\u6548\u7684\u8fd0\u4f5c\uff0c\u4ee5\u7ba1\u7406\u5e9e\u5927\u7684\u5e1d\u56fd\u201d\u3002\u6750\u6599\u8bf4\u660e\nA. \u5fa1\u53f2\u5236\u5ea6\u4fc3\u8fdb\u4e86\u56fd\u5bb6\u884c\u653f\u7684\u8fd0\u884c\nB. \u90e1\u53bf\u5236\u5ea6\u63a8\u52a8\u4e86\u4e2d\u592e\u96c6\u6743\u52a0\u5f3a\nC. \u4e09\u516c\u4e5d\u537f\u5236\u5ea6\u8bbe\u8ba1\u5f97\u6bd4\u8f83\u5408\u7406\nD. \u7687\u5e1d\u5236\u5ea6\u4fdd\u8bc1\u4e86\u884c\u653f\u7684\u6709\u6548\u8fd0\u4f5c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u98df\u54c1\u6700\u4e3b\u8981\u7684\u7279\u70b9\u662f\nA. \u6ecb\u5473\u548c\u6c14\u5473\nB. \u5916\u89c2\nC. \u8272\u6cfd\nD. \u8425\u517b\u548c\u6613\u6d88\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7968181886071957, "meta-math/MetaMath-Mistral-7B": 0.982139099949661, "itpossible/Chinese-Mistral-7B-v0.1": 0.7783519088558272, "HuggingFaceH4/zephyr-7b-beta": 0.9666221239763914, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8491539133034574, "meta-llama/Meta-Llama-3-8B": 0.9093873591650609, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7760041088632559}}, {"question": "\u5206\u8fbe\u8d56\u548c\u73ed\u7985\u4e24\u5927\u7cfb\u7684\u662f\nA. \u65b0\u9ec4\u6559\nB. \u7ea2\u6559\nC. \u8001\u9ec4\u6559\nD. \u767d\u6559\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3941848005910481, "meta-llama/Meta-Llama-3-8B": 0.4181874395315596, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53\u4f60\u665a\u4e0a\u5728\u5f3a\u70c8\u7684\u8f66\u524d\u706f\u9762\u524d\u8981\u4e00\u6bb5\u65f6\u95f4\u7684\u9002\u5e94\u662f\u611f\u89c9\uff08\uff09\u7279\u5f81\u3002\nA. \u6697\u9002\u5e94\nB. \u75b2\u52b3\nC. \u5bf9\u6bd4\nD. \u660e\u9002\u5e94\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3990012430409695, "meta-math/MetaMath-Mistral-7B": 0.5631104437503613, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8bd7\u4eba\u4e2d\uff0c\u51fa\u7248\u4e86\u4e2d\u56fd\u7b2c\u4e00\u90e8\u5341\u56db\u884c\u8bd7\u96c6\u7684\u662f\nA. \u51af\u81f3\nB. \u5f90\u5fd7\u6469\nC. \u6234\u671b\u8212\nD. \u80e1\u9002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.48280109883123723, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67e5\u4f53\u5df4\u5bbe\u65af\u57fa\u5f81\u9633\u6027\u63d0\u793a\u76ae\u5c42\u810a\u9ad3\u4fa7\u675f\u635f\u4f24\u7684\u6761\u4ef6\u662f\nA. \u6210\u4eba\u5728\u719f\u7761\u72b6\u6001\u4e0b\nB. \u6210\u4eba\u5728\u6e05\u9192\u72b6\u6001\u4e0b\nC. \u5a74\u513f\u5728\u6e05\u9192\u72b6\u6001\u4e0b\nD. \u6210\u4eba\u5728\u9ebb\u9189\u72b6\u6001\u4e0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u70ed\u91cf\u3001\u6e29\u5ea6\u3001\u5185\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u7269\u4f53\u6e29\u5ea6\u5347\u9ad8\uff0c\u5185\u80fd\u4e00\u5b9a\u589e\u52a0\nB. \u7269\u4f53\u6e29\u5ea6\u5347\u9ad8\uff0c\u4e00\u5b9a\u5438\u6536\u4e86\u70ed\u91cf\nC. \u7269\u4f53\u6e29\u5ea6\u4e0d\u53d8\uff0c\u4e00\u5b9a\u6ca1\u6709\u5438\u6536\u70ed\u91cf\nD. \u7269\u4f53\u5438\u6536\u70ed\u91cf\uff0c\u6e29\u5ea6\u4e00\u8d70\u5347\u9ad8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5847188137259511, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4622060288716199, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5600824888702723}}, {"question": "\u989e\u6a2a\u56de\u662f\nA. \u89c6\u89c9\u4e2d\u67a2\nB. \u611f\u89c9\u6027\u8bed\u8a00\u4e2d\u67a2\nC. \u8fd0\u52a8\u6027\u8bed\u8a00\u4e2d\u67a2\nD. \u542c\u89c9\u4e2d\u67a2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f20\u8001\u5e08\u8bfe\u524d\u5ba3\u5e03\uff1a\u201c\u4eca\u5929\u8bb2\u7684\u8bfe\u975e\u5e38\u91cd\u8981\uff0c\u8bb2\u5b8c\u540e\u5f53\u5802\u8fdb\u884c\u6d4b\u9a8c\u3002\u201d\u968f\u540e\u5b66\u751f\u4eec\u7cbe\u795e\u6296\u64de\uff0c\u5168\u795e\u8d2f\u6ce8\u5730\u6295\u5165\u542c\u8bfe\uff0c\u8bfe\u5802\u79e9\u5e8f\u4e95\u7136\uff0c\u8fd9\u79cd\u60c5\u51b5\u4e0b\u5f62\u6210\u7684\u7eaa\u5f8b\u5c5e\u4e8e\nA. \u4efb\u52a1\u4fc3\u6210\u7684\u7eaa\u5f8b\nB. \u89c4\u5219\u4fc3\u6210\u7684\u7eaa\u5f8b\nC. \u81ea\u6211\u4fc3\u6210\u7684\u7eaa\u5f8b\nD. \u96c6\u4f53\u4fc3\u6210\u7684\u7eaa\u5f8b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4528218188937079, "meta-math/MetaMath-Mistral-7B": 0.7352500176476773, "itpossible/Chinese-Mistral-7B-v0.1": 0.3568632977686015, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5858957832727165, "meta-llama/Meta-Llama-3-8B": 0.3749196223738779, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u4e0d\u5c5e\u4e8e\u5b66\u751f\u5fc3\u7406\u75b2\u52b3\u7279\u70b9\u7684\u662f\nA. \u6ce8\u610f\u529b\u6da3\u6563\nB. \u5931\u7720\nC. \u7cbe\u795e\u4e0d\u632f\nD. \u8bb0\u5fc6\u529b\u4e0b\u964d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.46454998140218834, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53cd\u7ade\u4e89\u6027\u6291\u5236\u4f5c\u7528\u7684\u63cf\u8ff0\u6b63\u786e\u7684\u662f\nA. \u6291\u5236\u5242\u65e2\u4e0e\u9176\u76f8\u7ed3\u5408\u53c8\u4e0e\u9176-\u5e95\u7269\u590d\u5408\u7269\u76f8\u7ed3\u5408\nB. \u6291\u5236\u5242\u4f7f\u9176\u4fc3\u53cd\u5e94\u7684Km\u503c\u964d\u4f4e\uff0cVmax\u5347\u9ad8\nC. \u6291\u5236\u5242\u53ea\u4e0e\u9176-\u5e95\u7269\u590d\u5408\u7269\u7ed3\u5408\nD. \u6291\u5236\u5242\u4f7f\u9176\u4fc3\u53cd\u5e94\u7684Km\u503c\u5347\u9ad8\uff0cVmax\u964d\u4f4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.308176776288574, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4118436968089692, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8ba1\u7b97\u673a\u786c\u4ef6\u7cfb\u7edf\u4e2d\u6700\u6838\u5fc3\u7684\u90e8\u4ef6\u662f\nA. \u4e3b\u677f\nB.  CPU\nC.  I/O\u8bbe\u5907\nD. \u5185\u5b58\u50a8\u5668\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8892692249497931, "meta-math/MetaMath-Mistral-7B": 0.9952151878294243, "itpossible/Chinese-Mistral-7B-v0.1": 0.9621749685619568, "HuggingFaceH4/zephyr-7b-beta": 0.9932422507140232, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9621476756275313, "meta-llama/Meta-Llama-3-8B": 0.9841634279463986, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9985765764416616}}, {"question": "\u5982\u679c\u4e0d\u5e78\u9047\u7ed1\u67b6\uff0c\u4e0b\u9762\u7684\u54ea\u4e9b\u81ea\u6551\u65b9\u6cd5\u662f\u975e\u5e38\u6709\u6548\u7684\nA. \u4e0d\u8981\u6fc0\u6055\u6b79\u5f92\uff0c\u5c3d\u91cf\u4fdd\u62a4\u81ea\u5df1\u4e0d\u53d7\u4f24\u5bb3\nB. \u4e0d\u653e\u5f03\u5e0c\u671b\uff0c\u5bfb\u627e\u673a\u4f1a\u62a5\u8b66\u6216\u9003\u79bb\nC. \u7262\u8bb0\u4e0e\u6551\u63f4\u548c\u7834\u6848\u6709\u5173\u7684\u4fe1\u606f\uff1a\u5982\u6b79\u5f92\u7684\u4eba\u6570\u3001\u59d3\u540d\u3001\u8eab\u5f62\u3001\u53e3\u97f3\u7b49\u7279\u5f81\uff0c\u8fd8\u6709\u52ab\u6301\u7684\u65f6\u95f4\u3001\u5730\u70b9\u3001\u52ab\u6301\u7684\u65b9\u5411\uff0c\u52ab\u5f80\u5730\u70b9\uff0c\u81ea\u5df1\u6240\u5904\u73af\u5883\u7b49\nD. \u4ee5\u4e0a\u90fd\u662f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9774361145474739, "meta-math/MetaMath-Mistral-7B": 0.9951405055155561, "itpossible/Chinese-Mistral-7B-v0.1": 0.9517580710393023, "HuggingFaceH4/zephyr-7b-beta": 0.9999607046780754, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9941015845460515, "meta-llama/Meta-Llama-3-8B": 0.9401171444781261, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9927751353561673}}, {"question": "\u5510\u4ee3\u5426\u5b9a\u4e86\u6309\u95e8\u7b2c\u9009\u5b98\u7684\u4e5d\u54c1\u4e2d\u6b63\u5236\u53ca\u7b49\u989d\u63a8\u8350\u7684\u5bdf\u4e3e\u5236\uff0c\u5b9e\u884c\u5206\u79d1\u8003\u8bd5\u3001\u5dee\u989d\u5f55\u53d6\u7684\u79d1\u4e3e\u5236\u3002\u8fd9\u53cd\u6620\u51fa\u5510\u4ee3\nA. \u541b\u4e3b\u96c6\u6743\u5f97\u5230\u5f3a\u5316\nB. \u9009\u624d\u6ce8\u91cd\u77e5\u8bc6\u6c34\u5e73\nC. \u9009\u5b98\u91cd\u89c6\u601d\u60f3\u54c1\u5fb7\nD. \u5b98\u5458\u7279\u6743\u53d7\u5230\u5426\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5538017304362052, "meta-math/MetaMath-Mistral-7B": 0.9097613355791284, "itpossible/Chinese-Mistral-7B-v0.1": 0.7011920168998552, "HuggingFaceH4/zephyr-7b-beta": 0.9730723548693706, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7673920488419473, "meta-llama/Meta-Llama-3-8B": 0.8241587322016498, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9724523155924332}}, {"question": "\u6559\u80b2\u662f\u4e00\u79cd\nA. \u81ea\u7136\u73b0\u8c61\nB. \u751f\u7269\u73b0\u8c61\nC. \u5fc3\u7406\u73b0\u8c61\nD. \u793e\u4f1a\u73b0\u8c61\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9367454692470029, "meta-math/MetaMath-Mistral-7B": 0.9969883393837192, "itpossible/Chinese-Mistral-7B-v0.1": 0.5048865774557842, "HuggingFaceH4/zephyr-7b-beta": 0.9999306812425833, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9768184422975291, "meta-llama/Meta-Llama-3-8B": 0.933890533713186, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9969461485006139}}, {"question": "\u6211\u56fd\u7684\u5f69\u5851\u5230\u76db\u5510\u8fbe\u5230\u9876\u5cf0\uff0c\u6b64\u65f6\u7684\u4ee3\u8868\u4f5c\u54c1\u662f\nA. \u9ea6\u79ef\u5c71\u77f3\u7a9f\u50cf\nB. \u6566\u714c\u5851\u50cf\nC. \u5c71\u897f\u664b\u7960\u50cf\nD. \u4e91\u5188\u77f3\u7a9f\u50cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7782309188676427}}, {"question": "\u5973\u6027\uff0c35 \u5c81\u3002\u4e4f\u529b\u3001\u5fc3\u60b81\u5e74\u4f59\uff0c\u8fd12\u4e2a\u6708\u75c7\u72b6\u52a0\u91cd\uff0c\u4f34\u538c\u98df\u3001\u6d88\u7626\u3001\u624b\u98a4\u3002\u67e5\u4f53:\u7532\u72b6\u817a\u5f25\u6f2b\u6027\u80bf\u5927\uff0c\u5fc3\u7387 126 \u6b21/\u5206\uff0c\u5fc3\u5f8b\u6574\u3002\u5b9e\u9a8c\u5ba4\u68c0\u67e5\u63d0\u793a FT3\u3001FT4 \u663e\u8457\u589e\u9ad8\uff0cTSH\u964d\u4f4e\u3002\u4e3a\u8fdb\u4e00\u6b65\u786e\u8bca\uff0c\u4e0b\u5217\u68c0\u67e5\u9879\u76ee\u4e2d\u610f\u4e49\u6700\u5927\u7684\u662f\nA. \u7532\u72b6\u817a\u6838\u7d20\u663e\u50cf\nB. ${ }^{131} \\mathrm{I}$ \u6444\u53d6\u7387\nC. \u7532\u72b6\u817aB\u8d85\nD. \u4fc3\u7532\u72b6\u817a\u6fc0\u7d20\u53d7\u4f53\u6297\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4009512986139124, "meta-math/MetaMath-Mistral-7B": 0.7503974084802494, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9316183790363171, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6700\u4e0d\u7a33\u5b9a\u7684\u9aa8\u6298\u662f\nA. \u80a1\u9aa8\u5e72\u87ba\u65cb\u5f62\u9aa8\u6298\nB. \u80eb\u9aa8\u4e0a\u7aef\u6a2a\u5f62\u9aa8\u6298\nC. L2 \u690e\u4f53\u538b\u7f29\u6027\u9aa8\u6298\nD. \u9885\u9aa8\u88c2\u7f1d\u9aa8\u6298\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u662f\u54c1\u5fb7\u7684\u5145\u8981\u6761\u4ef6\uff0c\u662f\u54c1\u5fb7\u7684\u6700\u7ec8\u56e0\u7d20\u3001\u6700\u7ec8\u73af\u8282\u7684\u662f\nA. \u9053\u5fb7\u610f\u5fd7\nB. \u9053\u5fb7\u8ba4\u8bc6\nC. \u9053\u5fb7\u89c2\u5ff5\nD. \u9053\u5fb7\u60c5\u611f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.7522893455022969, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4629909589258886, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5777518135484443, "meta-llama/Meta-Llama-3-8B": 0.42981479256993493, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u5c0f\u5b66\u751f\u53c2\u52a0\u5546\u4e1a\u6027\u5e86\u5178\u6f14\u51fa\u6d3b\u52a8\nA. \u5b66\u6821\u6709\u7533\u8bf7\u62a5\u544a\u53ef\u4ee5\nB. \u5b66\u6821\u7ec4\u7ec7\u4e25\u5bc6\u53ef\nC. \u5728\u5f97\u5230\u4e3b\u7ba1\u5c40\u540c\u610f\u540e\u53ef\u4ee5\nD. \u4e0d\u53ef\u4ee5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4177423267920221, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.9362864959925115, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5434\u67d0\u539f\u662f\u4e00\u540d\u8fd4\u57ce\u77e5\u9752\uff0c\u540e\u8fdb\u5382\u5f53\u4e86\u5de5\u4eba\uff0c\u73b0\u4e0b\u5c97\u5f00\u4e86\u4e00\u5bb6\u996d\u9986\uff0c\u6536\u5165\u9887\u4e30\u3002\u5434\u67d0\u5b8c\u6210\u7684\u793e\u4f1a\u6d41\u52a8\u662f\nA. \u7ed3\u6784\u6d41\u52a8\nB. \u5782\u76f4\u6d41\u52a8\nC. \u6c34\u5e73\u6d41\u52a8\nD. \u4ee3\u9645\u6d41\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.33767879625406966, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.33490177288810025, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c49\u6b66\u5e1d\u65f6\uff0c\u671d\u5ef7\u5236\u4f5c\u51fa\u8bb8\u591a\u4e00\u5c3a\u89c1\u65b9\u7684\u767d\u9e7f\u76ae\uff0c\u79f0\u4e3a\u201c\u76ae\u5e01\u201d\uff0c\u5b9a\u4ef7\u4e3a40\u4e07\u94b1\u4e00\u5f20\u3002\u8bf8\u4faf\u738b\u53c2\u52a0\u732e\u793c\u65f6\uff0c\u5fc5\u987b\u8d2d\u76ae\u5e01\u7528\u6765\u7f6e\u653e\u793c\u7269\uff0c\u800c\u5f53\u65f6\u4e00\u4e2a\u201c\u5343\u6237\u4faf\u201d\u4e00\u5e74\u7684\u79df\u7a0e\u6536\u5165\u7ea6\u4e3a20\u4e07\u94b1\u3002\u671d\u5ef7\u8fd9\u79cd\u505a\u6cd5\nA. \u52a0\u5f3a\u4e86\u8d27\u5e01\u7ba1\u7406\nB. \u5b9e\u73b0\u4e86\u5bf9\u5730\u65b9\u7684\u63a7\u5236\nC. \u524a\u5f31\u4e86\u8bf8\u4faf\u5b9e\u529b\nD. \u786e\u7acb\u4e86\u601d\u60f3\u4e0a\u7684\u7edf\u4e00\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44403389651823827, "meta-math/MetaMath-Mistral-7B": 0.5850105517044356, "itpossible/Chinese-Mistral-7B-v0.1": 0.557260475494401, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6612446665381344, "meta-llama/Meta-Llama-3-8B": 0.6609139204065951, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4e8e\u6ee1\u8db3\u53c2\u6570\u68c0\u9a8c\u6761\u4ef6\u7684\u6570\u503c\u53d8\u91cf\u8d44\u6599\uff0c\u5982\u679c\u91c7\u7528\u79e9\u548c\u68c0\u9a8c\uff0c\u5219\nA. \u7b2c\u4e00\u7c7b\u9519\u8bef\u7387\u589e\u5927\nB. \u7b2c\u4e00\u7c7b\u9519\u8bef\u7387\u51cf\u5c0f\nC. \u7b2c\u4e8c\u7c7b\u9519\u8bef\u7387\u589e\u5927\nD. \u7b2c\u4e8c\u7c7b\u9519\u8bef\u7387\u51cf\u5c0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u72af\u7f6a\u5206\u5b50\u4e3a\u65e5\u540e\u5411\u7532\u52d2\u7d22\u8d22\u7269\uff0c\u7528\u67aa\u5a01\u903c\u7532\u6740\u6b7b\u4e00\u540d\u8def\u4eba\u5e76\u5f55\u50cf\u3002\u7532\u7684\u6740\u4eba\u884c\u4e3a\u5c5e\u4e8e\uff08 \uff09\u3002\nA. \u6b63\u5f53\u9632\u536b\nB. \u7d27\u6025\u907f\u9669\nC. \u81ea\u6551\u884c\u4e3a\nD. \u72af\u7f6a\u884c\u4e3a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.573435531129514, "meta-math/MetaMath-Mistral-7B": 0.8441054046205221, "itpossible/Chinese-Mistral-7B-v0.1": 0.6665489303745115, "HuggingFaceH4/zephyr-7b-beta": 0.9999780776221409, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9303704724077599, "meta-llama/Meta-Llama-3-8B": 0.6710875353003906, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9967298351655739}}, {"question": "\u592a\u9633\u7ed5\u94f6\u5fc3\u8fd0\u52a8\u7684\u8f68\u9053\u901f\u5ea6\u662f\nA. 210km/s\nB. 7.9km/s\nC. 52km/s\nD. 16.7km/s\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28496529561720413, "meta-math/MetaMath-Mistral-7B": 0.5114296322203268, "itpossible/Chinese-Mistral-7B-v0.1": 0.27416108226793107, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.43477595641789346, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f9d\u4e00\u822c\u6cd5\u7406\uff0c\u80a1\u4efd\u6709\u9650\u516c\u53f8\u4e2d\u53ef\u8c03\u6362\u7684\u4f18\u5148\u80a1\uff0c\u610f\u4e3a\u8be5\u79cd\u80a1\u4efd\u53ef\u4ee5\u8c03\u6362\u6210\nA. \u56de\u6536\u80a1\nB. \u666e\u901a\u80a1\nC. \u52a3\u540e\u80a1\nD. \u516c\u53f8\u503a\u5238\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5082897711505279, "meta-math/MetaMath-Mistral-7B": 0.7389930665152771, "itpossible/Chinese-Mistral-7B-v0.1": 0.5158824454265187, "HuggingFaceH4/zephyr-7b-beta": 0.5745186740374605, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.46191544313137656, "meta-llama/Meta-Llama-3-8B": 0.5646805314320397, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6246057980426666}}, {"question": "\u4e0b\u5217\u54ea\u4e00\u9879\u4e0d\u5c5e\u4e8e\u6216\u4e0d\u80fd\u6784\u6210\u6cd5\u5f8b\u5173\u7cfb\uff1f\nA. \u5f20\u4e09\u7ecf\u674e\u56db\uff08\u7cbe\u795e\u75c5\u4eba\uff09\u7684\u7236\u6bcd\u540c\u610f\u800c\u6536\u517b\u674e\u56db\nB. \u7532\u3001\u4e59\u6839\u636e\u5408\u540c\u6cd5\u5efa\u7acb\u5408\u540c\u5173\u7cfb\nC. \u5c0f\u5218\u4e0e\u8d75\u67d0\u7ea6\u5b9a\u9009\u62e9\u6cd5\u9662\u7ba1\u8f96\nD. \u7532\u5e74\u6ee110\u5468\u5c81\u65f6\u5411\u4e59\u8d2d\u4e70\u4e86\u4ef7\u503c1000\u5143\u7684\u81ea\u884c\u8f66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e9a\u2fa5\u2f20\u591a\u5fb7\u5bf9\u201c\u2f12\u5230\u5e95\u5982\u4f55\u51b3\u5b9a\u8fd0\u52a8\u201d\u8fd9\u4e2a\u95ee\u9898\u7684\u56de\u7b54\u662f\uff1a\u2f12\u51b3\u5b9a\u7269\u4f53\u7684\u8fd0\u52a8\u901f\u5ea6\u3002\u4ed6\u8ba4\u4e3a\uff0c\u8981\u2ee2\u2ecb\u8dd1\u5f97\u66f4\u5feb\uff0c\u5c31\u8981\u2f64\u66f4\u591a\u7684\u2ee2\u53bb\u62c9\uff0c\u6216\u66f4\u5f3a\u7684\u2ee2\u53bb\u62c9\u3002\u6240\u4ee5\uff0c\u2f12\u8d8a\u2f24\u901f\u5ea6\u8d8a\u2f24\uff0c\u2f12\u8d8a\u2f29\u901f\u5ea6\u8d8a\u2f29\uff0c\u6ca1\u6709\u2f12\u65f6\uff0c\u901f\u5ea6\u5c31\u4e3a\u96f6\uff08\u9759\u2f4c\u4e0d\u52a8\uff09\u3002\u5bf9\u4e9a\u2fa5\u2f20\u591a\u5fb7\u7684\u89c2\u70b9\u6709\u4ee5\u4e0b\u2f0f\u79cd\u770b\u6cd5\uff0c\u5176\u4e2d\u6b63\u786e\u7684\u662f\nA. \u4e9a\u2fa5\u2f20\u591a\u5fb7\u7684\u89c2\u70b9\u5b8c\u5168\u6b63\u786e\uff0c\u2f12\u51b3\u5b9a\u7269\u4f53\u7684\u8fd0\u52a8\u901f\u5ea6\nB. \u7269\u4f53\u901f\u5ea6\u2f24\u2f29\u4e0d\u662f\u7531\u2f12\u6765\u51b3\u5b9a\uff0c\u6ca1\u6709\u2f12\u7269\u4f53\u7167\u6837\u8fd0\u52a8\nC. \u7269\u4f53\u53d7\u5230\u7684\u2f12\u548c\u7269\u4f53\u7684\u8d28\u91cf\u662f\u51b3\u5b9a\u7269\u4f53\u901f\u5ea6\u2f24\u2f29\u7684\u4e24\u4e2a\u56e0\u7d20\nD. \u6ca1\u6709\u2f12\u7269\u4f53\u4e0d\u4f1a\u8fd0\u52a8\uff0c\u4f46\u8fd0\u52a8\u7269\u4f53\u7684\u901f\u5ea6\u2f24\u2f29\u4e0d\u662f\u7531\u2f12\u6765\u51b3\u5b9a\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u5b66\u8005\u8ba4\u4e3a\uff0c\u5f53\u4eca\u4e16\u754c\uff0c\u4e9a\u6d32\u548c\u975e\u6d32\u7684\u7ecf\u6d4e\u4f53\u8868\u73b0\u7a81\u51fa\uff0c\u4e2d\u56fd\u7ecf\u6d4e\u5065\u5eb7\u7a33\u5b9a\u7684\u53d1\u5c55\u5bf9\u4e8e\u63a8\u52a8\u4e9a\u975e\u56fd\u5bb6\u548c\u5730\u533a\u7684\u589e\u957f\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5171\u5efa\u201c\u4e00\u5e26\u4e00\u8def\u201d\u5021\u8bae\u7684\u4e0d\u65ad\u843d\u5b9e\uff0c\u4e3a\u5305\u62ec\u62c9\u7f8e\u5728\u5185\u7684\u4e16\u754c\u7ecf\u6d4e\u63d0\u4f9b\u91cd\u8981\u52a8\u529b\u3002\u8fd9\u8868\u660e a\u4e2d\u56fd\u575a\u5b9a\u5730\u8d70\u4e2d\u56fd\u7279\u8272\u793e\u4f1a\u4e3b\u4e49\u9053\u8def\uff1bb\u4e2d\u56fd\u7684\u56fd\u9645\u5f71\u54cd\u529b\u3001\u611f\u53ec\u529b\u8fdb\u4e00\u6b65\u63d0\u9ad8\uff1bc\u4e2d\u56fd\u4e3a\u4e16\u754c\u548c\u5e73\u4e0e\u53d1\u5c55\u4f5c\u51fa\u65b0\u7684\u66f4\u5927\u8d21\u732e d\u4e2d\u56fd\u6210\u4e3a\u4e16\u754c\u591a\u6781\u5316\u4e2d\u7684\u5173\u952e\u4e00\u6781\nA. ad\nB. cd\nC. ab\nD. bc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2947319625574089, "HuggingFaceH4/zephyr-7b-beta": 0.45611811419006476, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3403147063768956, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b66\u4e60\u3001\u7814\u7a76\u3001\u7406\u89e3\u3001\u6982\u62ec\u3001\u5206\u6790\u7684\u80fd\u529b\u662f\nA. \u64cd\u4f5c\u80fd\u529b\nB. \u521b\u9020\u80fd\u529b\nC. \u8ba4\u77e5\u80fd\u529b\nD. \u793e\u4f1a\u4ea4\u5f80\u80fd\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8709930392464327, "meta-math/MetaMath-Mistral-7B": 0.9668041512891936, "itpossible/Chinese-Mistral-7B-v0.1": 0.9139312267216667, "HuggingFaceH4/zephyr-7b-beta": 0.999527647689206, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9713128419068996, "meta-llama/Meta-Llama-3-8B": 0.9528292613554493, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9984100510932566}}, {"question": "\u98df\u7269\u7279\u6b8a\u52a8\u529b\u4f5c\u7528\u6700\u9ad8\u7684\u7269\u8d28\u662f\nA. \u86cb\u767d\u8d28\nB. \u7cd6\nC. \u8102\u80aa\nD. \u6df7\u5408\u98df\u7269\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3306121833698776, "meta-math/MetaMath-Mistral-7B": 0.7149976022809543, "itpossible/Chinese-Mistral-7B-v0.1": 0.3017444864199176, "HuggingFaceH4/zephyr-7b-beta": 0.6968905717836283, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47617856154650484, "meta-llama/Meta-Llama-3-8B": 0.43241138747909885, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u63d0\u51fa\u201c\u7ba1\u7406\uff0c\u5c31\u662f\u5b9e\u884c\u8ba1\u5212\u3001\u7ec4\u7ec7\u3001\u6307\u6325\u3001\u534f\u8c03\u548c\u63a7\u5236\u201d\u7684\u7ba1\u7406\u5b66\u5bb6\u662f\nA. \u6cd5\u7ea6\u5c14\nB. \u897f\u8499\nC. \u5df4\u7eb3\u5fb7\nD. \u5b54\u8328\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.31292413413889664, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3749196223738779, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c34\u6d52\u4e00\u767e\u5355\u516b\u5c06\u4e2d\u6709\u51e0\u4f4d\u5973\u6027\nA. \u4e24\u4f4d\nB. \u56db\u4f4d\nC. \u4e00\u4f4d\nD. \u4e09\u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u76ee\u524d\u5728\u5168\u4e16\u754c\u5df2\u6d88\u706d\u7684\u75be\u75c5\u662f\nA. \u5929\u82b1\nB. \u9ebb\u75b9\nC. \u970d\u4e71\nD. \u9f20\u75ab\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.739856789964887, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7780726313956559, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.866054770563689}}, {"question": "\u5bf9\u8d22\u4ea7\u6e05\u67e5\u7ed3\u679c\u8fdb\u884c\u8d26\u52a1\u5904\u7406\u7684\u4e3b\u8981\u76ee\u7684\u662f\u4fdd\u8bc1\nA. \u8d26\u8bc1\u76f8\u7b26\nB. \u8d26\u8868\u76f8\u7b26\nC. \u8d26\u8d26\u76f8\u7b26\nD. \u8d26\u5b9e\u76f8\u7b26\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3588823130168717, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6640013818467273, "HuggingFaceH4/zephyr-7b-beta": 0.8720096689649723, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3860362871322059, "meta-llama/Meta-Llama-3-8B": 0.8303497422261701, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5061803533767298}}, {"question": "20\u4e16\u7eaa50\u5e74\u4ee3\uff0c\u4e2d\u56fd\u63d0\u51fa\u548c\u5e73\u5171\u5904\u4e94\u9879\u539f\u5219\uff0c\u53c2\u52a0\u65e5\u5185\u74e6\u4f1a\u8bae\u548c\u4e07\u9686\u4f1a\u8bae\u5e76\u53d6\u5f97\u6210\u529f\u3002\u8fd9\u4e9b\u6210\u5c31\nA. \u53cd\u6620\u4e86\u4e2d\u7f8e\u5173\u7cfb\u4ece\u5bf9\u6297\u8d70\u5411\u7f13\u548c\nB. \u5b9e\u73b0\u4e86\u4e0d\u7ed3\u76df\u5916\u4ea4\u7684\u653f\u7b56\u8f6c\u53d8\nC. \u4f53\u73b0\u4e86\u5916\u4ea4\u653f\u7b56\u7684\u72ec\u7acb\u81ea\u4e3b\u7cbe\u795e\nD. \u6d88\u9664\u4e86\u610f\u8bc6\u5f62\u6001\u5bf9\u5916\u4ea4\u7684\u5f71\u54cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5291382018519859, "meta-math/MetaMath-Mistral-7B": 0.7708483736110475, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.889741952036432, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5140406481421705, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0d\u540c\u65f6\u4ee3\u6709\u4e0d\u540c\u65f6\u4ee3\u7684\u4efb\u52a1\uff0c\u8fd9\u4e00\u4efb\u52a1\u53cd\u6620\u5230\u7231\u56fd\u4e3b\u4e49\u7684\u5185\u5bb9\u4e0a\u5c31\u662f\u7279\u5b9a\u65f6\u4ee3\u6761\u4ef6\u4e0b\u7231\u56fd\u4e3b\u4e49\u7684\u4e3b\u9898\u3002\u6211\u56fd\u65b0\u65f6\u671f\u7231\u56fd\u4e3b\u4e49\u7684\u4e3b\u9898\u662f\nA. \u52a0\u5f3a\u56fd\u9632\u5efa\u8bbe\uff0c\u7ef4\u62a4\u4e16\u754c\u548c\u5e73\nB. \u52a0\u5f3a\u56fd\u9645\u4ea4\u6d41\uff0c\u63d0\u5347\u56fd\u9645\u5730\u4f4d\nC. \u5efa\u8bbe\u548c\u53d1\u5c55\u4e2d\u56fd\u7279\u8272\u793e\u4f1a\u4e3b\u4e49\nD. \u4e89\u53d6\u6c11\u65cf\u72ec\u7acb\uff0c\u5b9e\u73b0\u6551\u4ea1\u56fe\u5b58\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.734221131011661, "meta-math/MetaMath-Mistral-7B": 0.9841480046036086, "itpossible/Chinese-Mistral-7B-v0.1": 0.7587556255653239, "HuggingFaceH4/zephyr-7b-beta": 0.9575770506505383, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9345279460979229, "meta-llama/Meta-Llama-3-8B": 0.9708698784780506, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9846453926430845}}, {"question": "\u201c\u865a\u5fc3\u201d\u548c\u201c\u865a\u4f2a\u201d\u7684\u533a\u522b\u662f\nA. \u8303\u56f4\u5927\u5c0f\u4e0d\u540c\nB. \u611f\u60c5\u8272\u5f69\u4e0d\u540c\nC. \u8bed\u4e49\u8f7b\u91cd\u4e0d\u540c\nD. \u8bed\u4f53\u8272\u5f69\u4e0d\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3054272097659661, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u82e5\u56de\u5f52\u76f4\u7ebf\u7684\u65b9\u7a0b\u4e3a$\\hat{y}=2-1.5x$\uff0c\u5219\u53d8\u91cfx\u589e\u52a0\u4e00\u4e2a\u5355\u4f4d\u65f6\nA. y\u5e73\u5747\u51cf\u5c112\u4e2a\u5355\u4f4d\nB. y\u5e73\u5747\u589e\u52a01.5\u4e2a\u5355\u4f4d\nC. y\u5e73\u5747\u589e\u52a02\u4e2a\u5355\u4f4d\nD. y\u5e73\u5747\u51cf\u5c111.5\u4e2a\u5355\u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9482282474226514, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.31483005318115603}}, {"question": "\u5316\u5b66\u4e0e\u751f\u6d3b\u5bc6\u5207\u76f8\u5173\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u5927\u91cf\u4f7f\u7528\u85aa\u67f4\u4e3a\u71c3\u6599\uff0c\u5b9e\u73b0\u4f4e\u78b3\u751f\u6d3b\nB. \u7528K2FeO4\u4ee3\u66ffCl2\u5904\u7406\u996e\u7528\u6c34\uff0c\u65e2\u6709\u6740\u83cc\u6d88\u6bd2\u4f5c\u7528\uff0c\u53c8\u6709\u51c0\u6c34\u4f5c\u7528\nC. CO2\u3001NO2\u6216SO2\u7684\u6392\u653e\u662f\u5f62\u6210\u9178\u96e8\u7684\u4e3b\u8981\u539f\u56e0\nD. \u4f7f\u7528\u586b\u57cb\u6cd5\u5904\u7406\u672a\u7ecf\u5206\u7c7b\u7684\u751f\u6d3b\u5783\u573e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6283363119724437, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.583730822387901, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6926577868915399}}, {"question": "110KV\u7ebf\u8def\u7684\u81ea\u7136\u529f\u7387\u4e3a\nA. 5180MW\nB. 885MW\nC. 9940MW\nD. 2210MW\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8302695771565649, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u90e8\u4f5c\u54c1\u662f\u4ee520\u4e16\u7eaa70\u5e74\u4ee3\u77e5\u8bc6\u9752\u5e74\u201c\u4e0a\u5c71\u4e0b\u4e61\u201d\u4e3a\u5386\u53f2\u80cc\u666f\u7684\nA. \u300a\u978b\u300b\nB. \u300a\u5c11\u5973\u5c0f\u6e14\u300b\nC. \u300a\u7a7a\u7684\u7a97\u300b\nD. \u300a\u559c\u5bb4\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3171201089282236, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c55 \u5c81\uff0c2 \u5929\u524d\u996e\u9152\u540e\u51fa\u73b0\u4e0a\u8179\u75db\uff0c\u8fdb\u884c\u6027\u52a0\u91cd\uff0c\u5411\u8170\u80cc\u90e8\u653e\u5c04\uff0c\u66fe\u5455\u5410 1 \u6b21\uff0c\u4e3a\u80c3\u5185\u5bb9\u7269\uff0c\u5455\u5410\u540e\u8179\u75db\u672a\u51cf\u8f7b\uff0c\u7a00\u4fbf 2 \u6b21\u3002\u67e5\u4f53\uff1aT37.3 \u5ea6\uff0cP80 \u6b21/\u5206\uff0cR20 \u6b21/\u5206\uff0cBP 120/80mmHg\uff0c\u4e0a\u8179\u90e8\u8f7b\u538b\u75db\uff0c\u809d\u813e\u808b\u4e0b\u672a\u89e6\u53ca\u3002\u786e\u8bca\u9996\u9009\u7684\u68c0\u67e5\u662f\nA. \u5927\u4fbf\u5e38\u89c4\u548c\u9690\u8840\nB. \u8179\u90e8\u5f71\u50cf\u5b66\u68c0\u67e5\nC. \u80c3\u955c\nD. \u8840\u3001\u5c3f\u6dc0\u7c89\u9176\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8eWORD\u8868\u683c\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u5bf9\u8868\u683c\u4e2d\u5355\u5143\u683c\u4e0d\u53ef\u4ee5\u8fdb\u884c\u5408\u5e76\u548c\u62c6\u5206\nB. \u8868\u683c\u5efa\u597d\u540e\uff0c\u4e0d\u80fd\u6539\u53d8\u8868\u683c\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\nC. \u8868\u683c\u53ef\u4ee5\u8fdb\u51fa\u53e3\u884c\u8fb9\u6846\u548c\u5e95\u7eb9\u7684\u8bbe\u7f6e\nD. \u5728\u8868\u683c\u7684\u5355\u5143\u683c\u4e2d\u53ea\u80fd\u8f93\u5165\u6587\u5b57\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6451538592120146, "meta-math/MetaMath-Mistral-7B": 0.9339289865992385, "itpossible/Chinese-Mistral-7B-v0.1": 0.4394042117234178, "HuggingFaceH4/zephyr-7b-beta": 0.9986745091608473, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9071769142288049, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9922564917504941}}, {"question": "\u4e0b\u5217\u5c5e\u4e8e\u5bf9\u4ea7\u54c1\u53ca\u5176\u76f8\u5173\u4e8b\u9879\u7684\u6697\u793a\u6027\u63cf\u8ff0\u7684\u662f\nA. \u5411\u5ba2\u6237\u627f\u8bfa\u65e0\u6761\u4ef6\u9000\u6b3e\u4e14\u65e0\u5176\u4ed6\u7279\u522b\u8bf4\u660e\nB. \u76f8\u5173\u9274\u5b9a\u8bc1\u4e66\nC. \u4f7f\u7528\u6709\u6548\u671f\nD. \u8bd5\u7528\u671f\u9650\u548c\u65e0\u6761\u4ef6\u9000\u6b3e\u7684\u671f\u9650\u4e0e\u60c5\u5f62\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.29660173325630934, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "16\u4e16\u7eaa\u6cd5\u56fd\u4f5c\u5bb6\u62c9\u4f2f\u96f7\u7684\u4e00\u90e8\u5c0f\u8bf4\uff0c\u98ce\u884c\u4e00\u65f6\uff0c\u4e24\u4e2a\u6708\u5185\u7684\u9500\u91cf\uff0c\u5c31\u8d85\u8fc7\u4e86\u300a\u5723\u7ecf\u300b\u4e5d\u5e74\u7684\u9500\u91cf\uff0c\u8fd9\u4e00\u90e8\u8d5e\u9882\u4eba\u6587\u4e3b\u4e49\u7684\u4f1f\u5927\u6770\u4f5c\u662f\nA. \u300a\u795e\u66f2\u300b\nB. \u300a\u5510\u5409\u8bc3\u5fb7\u300b\nC. \u300a\u5de8\u4eba\u4f20\u300b\nD. \u300a\u5341\u65e5\u8c08\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41371897786587764, "meta-math/MetaMath-Mistral-7B": 0.4801919661069125, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4104839681328262, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4596151413217764, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6b27\u62c9\u56fe\u8868\u793a\u7684\u4e24\u4e2a\u5bf9\u8c61\u7c7b\u4e4b\u95f4\u7684\u5173\u7cfb\u4e0d\u5305\u62ec\nA. \u4ea4\u53c9\u5173\u7cfb\nB. \u5168\u540c\u5173\u7cfb\nC. \u79cd\u5c5e\u5173\u7cfb\nD. \u540c\u4e00\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.34031470637689565, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.371068906204966, "meta-llama/Meta-Llama-3-8B": 0.2801288226217134, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u56fd\u5bb6\u4fe1\u606f\u5316\u4f53\u7cfb\u516d\u8981\u7d20\u4e2d\uff0c\u8fdb\u884c\u4fe1\u606f\u5316\u5efa\u8bbe\u7684\u57fa\u7840\u662f\nA. \u4fe1\u606f\u4eba\u624d\nB. \u4fe1\u606f\u8d44\u6e90\u7684\u5f00\u53d1\u548c\u5229\u7528\nC. \u4fe1\u606f\u6280\u672f\u548c\u4ea7\u4e1a\nD. \u4fe1\u606f\u5316\u653f\u7b56\u6cd5\u89c4\u548c\u89c4\u8303\u6807\u51c6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u4eec\u719f\u6089\u7684\u300a\u767e\u5bb6\u59d3\u300b\u662f\u6309\u7167\u4ec0\u4e48\u65b9\u5f0f\u6392\u5217\u7684\nA. \u653f\u6cbb\u5730\u4f4d\nB. \u4eba\u53e3\u6570\u91cf\nC. \u59d3\u6c0f\u7b14\u753b\nD. \u51fa\u73b0\u987a\u5e8f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2008\u5e749\u67088\u65e5\uff0c\u5c71\u897f\u8944\u6c7e\u53d1\u751f\u7279\u5927\u5c3e\u77ff\u6e83\u575d\u4e8b\u6545\uff0c\u4e8b\u540e\u5c71\u897f\u7701\u957f\u5f15\u548e\u8f9e\u804c\uff0c\u4e00\u526f\u7701\u957f\u88ab\u514d\u804c\u3002\u8fd9\u91cc\u5e94\u7528\u7684\u63a7\u5236\u65b9\u6cd5\u662f\nA. \u5185\u90e8\u5ba1\u8ba1\u6cd5\nB. \u4eba\u5458\u7ba1\u7406\u63a7\u5236\u6cd5\nC. \u73b0\u573a\u89c2\u5bdf\u6cd5\nD. \u524d\u9988\u63a7\u5236\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.49803982458894736, "HuggingFaceH4/zephyr-7b-beta": 0.45724527673078114, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3908304606703761, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.48714170419590686}}, {"question": "\u5973\u6027\uff0c29\u5c81\u3002\u56e0\u53f3\u4fa7\u7532\u72b6\u817a\u7ed3\u8282\u624b\u672f\uff0c\u672f\u4e2d\u89c1\u7532\u72b6\u817a\u53f3\u53f6\u591a\u4e2a\u56ca\u5b9e\u6027\u7ed3\u8282\uff0c\u9888\u90e8\u65e0\u80bf\u5927\u6dcb\u5df4\u7ed3\uff0c\u884c\u53f3\u53f6\u5168\u5207\u9664\u672f\u3002\u672f\u540e\u75c5\u7406\u62a5\u544a\u63d0\u793a\uff0c\u7532\u72b6\u817a\u5185\u67095mm\u4e73\u5934\u72b6\u764c\u7076\u3002\u8fdb\u4e00\u6b65\u7684\u5904\u7406\u5e94\u662f\nA. \u53e3\u670d\u7532\u72b6\u817a\u7d20\nB. \u7532\u72b6\u817a\u5168\u5207\u53ca\u9888\u6dcb\u5df4\u7ed3\u6e05\u626b\u672f\nC. \u5ce1\u90e8\u53ca\u5de6\u53f6\u90e8\u5206\u5207\u9664\u672f\nD. \u7532\u72b6\u817a\u8fd1\u5168\u5207\u9664\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u5e73\u2faf\u76f4\u2ec6\u5750\u6807\u7cfb\u4e2d\uff0c\u82e5\u70b9M\uff081\uff0c3\uff09\u4e0e\u70b9N\uff08x\uff0c3\uff09\u4e4b\u95f4\u7684\u8ddd\u79bb\u662f5\uff0c\u5219x\u7684\u503c\u662f\nA. \u4e0d\u786e\u5b9a\nB. 6\u6216\uff0d4\nC. 6\u62164\nD. 6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u80fd\u4f7f\u4e0b\u5217\u8bed\u610f\u987a\u7545\u3001\u8fde\u8d2f\u7684\u4e00\u7ec4\u77ed\u8bed\u662f\uff1a()\u5b9e\u65bd\u7d20\u8d28\u6559\u80b2\u6709\u8d56\u4e8e\u3001\u6216\u8005\u8bf4\u6839\u672c\u53d7\u5236\u4e8e\u6559\u80b2\u5de5\u4f5c\u8005\u7684\u8d23\u4efb\u3002\u6709\u4eba\u8bf4\uff0c\u5b69\u5b50\u4eec\u7684\u5fc3\u5c31\u50cf\u4e00\u5757\u5947\u5999\u7684\u5927\u5730\uff0c\u64ad\u4e0b\u2460\u7684\u79cd\u5b50\uff0c\u5c31\u4f1a\u83b7\u5f97\u884c\u4e3a\u7684\u6536\u83b7\uff0c\u64ad\u4e0b\u2461\u7684\u79cd\u5b50\uff0c\u5c31\u4f1a\u83b7\u5f97\u4e60\u60ef\u7684\u6536\u83b7\uff0c\u64ad\u4e0b\u2462\u7684\u79cd\u5b50\uff0c\u5c31\u4f1a\u83b7\u5f97\u6027\u683c\u7684\u6536\u83b7\uff0c\u64ad\u4e0b\u2463\u7684\u79cd\u5b50\uff0c\u5c31\u4f1a\u83b7\u5f97\u547d\u8fd0\u7684\u6536\u83b7\u3002\u4ece\u8fd9\u4e2a\u610f\u4e49\u4e0a\u6765\u8bf4\uff0c\u6559\u80b2\u5de5\u4f5c\u8005\u786e\u5b9e\u662f\u4e3b\u5bb0\u6bcf\u4e00\u4e2a\u5b66\u751f\u547d\u8fd0\u7684\u4eba\u3002\nA. \u2460\u884c\u4e3a\u2461\u4e60\u60ef\u2462\u6027\u683c\u2463\u601d\u60f3\nB. \u2460\u601d\u60f3\u2461\u884c\u4e3a\u2462\u4e60\u60ef\u2463\u6027\u683c\nC. \u2460\u601d\u60f3\u2461\u6027\u683c\u2462\u884c\u4e3a\u2463\u4e60\u60ef\nD. \u2460\u6027\u683c\u2461\u884c\u4e3a\u2462\u4e60\u60ef\u2463\u601d\u60f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.29096334599293733, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.325455072595945, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.567384078484277}}, {"question": "\u5df1\u77e5$S(x)=1-\\frac{x}{100}\uff0c0 \\leqslant x \\leqslant 100$\u4e14$l_0=10000$\uff0c\u5219$q_{30}$\u548c$d_{35}$\u7684\u503c\u4e3a( )\u3002\nA. 1/70\uff0c100\nB. 1/65\uff0c110\nC. 1/70\uff0c110\nD. 1/65\uff0c100\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4efb\u4f55\u4e2a\u4eba\u548c\u7ec4\u7ec7\u4e0d\u5f97\u6709\u5371\u5bb3\u56fd\u5bb6\u5b89\u5168\u7684\u884c\u4e3a\uff0c\u4e0d\u5f97\u5411\u5371\u5bb3\u56fd\u5bb6\u5b89\u5168\u7684\u4e2a\u4eba\u6216\u8005\u7ec4\u7ec7\u63d0\u4f9b\u4efb\u4f55\nA. \u8d44\u52a9\u6216\u8005\u534f\u52a9\nB. \u8d44\u52a9\nC. \u534f\u803f\nD. \u4fbf\u5229\u6761\u4ef6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7222446358224054, "meta-math/MetaMath-Mistral-7B": 0.9191741417986583, "itpossible/Chinese-Mistral-7B-v0.1": 0.6529888858383928, "HuggingFaceH4/zephyr-7b-beta": 0.9923896502618836, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8873448777699968, "meta-llama/Meta-Llama-3-8B": 0.6147665946006643, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8642131195985794}}, {"question": "\u5c0f\u53f6\u6027\u80ba\u708e\u662f\nA. \u7ea4\u7ef4\u7d20\u6027\u708e\nB. \u5316\u8113\u6027\u708e\nC. \u6025\u6027\u6d46\u6db2\u6027\u708e\nD. \u95f4\u8d28\u6027\u80ba\u708e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u79cd\u82b1\u6709\u6bd2\u4f46\u53ef\u4ee5\u6b62\u54b3\nA. \u7261\u4e39\nB. \u91d1\u94f6\u82b1\nC. \u66fc\u9640\u7f57\nD. \u4e01\u9999\u82b1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.343887939254148, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6359116237466097, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4179598352764338, "meta-llama/Meta-Llama-3-8B": 0.38676893720936567, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u793e\u4f1a\u4e3b\u4e49\u6761\u4ef6\u4e0b\u8ffd\u6c42\u6b63\u5f53\u7684\u4e2a\u4eba\u5229\u76ca\u662f\u6307\nA. \u4e2a\u4eba\u5229\u76ca\u670d\u4ece\u96c6\u4f53\u5229\u76ca\nB. \u96c6\u4f53\u5229\u76ca\u4e0e\u4e2a\u4eba\u5229\u76ca\u7684\u517c\u987e\u6027\nC. \u5728\u5b88\u6cd5\u548c\u5408\u4e4e\u9053\u5fb7\u6761\u4ef6\u9760\u8f9b\u52e4\u52b3\u52a8\u800c\u83b7\u5f97\nD. \u4e0d\u4e00\u5b9a\u8003\u8651\u6cd5\u5f8b\u548c\u9053\u5fb7\u8981\u6c42\u83b7\u5f97\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.46780221541856365, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5031195404593246, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7646356778157664}}, {"question": "\u8bbe\u6709X\u3001Y\u4e24\u7ec4\u6570\u636e\uff0c\u6c42\u5f97$\\hat{y}=a+bx$\uff0c\u7ecf\u7edf\u8ba1\u5b66\u68c0\u9a8c\uff0c\u5728$\\alpha=0.05$\u6c34\u5e73\u4e0a\u62d2\u7edd$H_0: \\beta=0$\uff0c\u5219\u81f3\u5c11\u670995%\u7684\u628a\u63e1\u65ad\u8a00y\u4e0ex\u4e4b\u95f4\u5728\u4e13\u4e1a\u4e0a\u6709\u76f4\u7ebf\u5173\u7cfb\u3002\u8fd9\u4e00\u7ed3\u8bba\nA. \u6839\u636e\u5145\u5206\nB. \u5f88\u6709\u79d1\u5b66\u6027\nC. \u7565\u6709\u95ee\u9898\nD. \u8131\u79bb\u5b9e\u9645\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5916\u56fd\u6cd5\u7684\u9002\u7528\uff0c\u5982\u8fdd\u80cc\u5584\u826f\u98ce\u4fd7\u6216\u5fb7\u56fd\u6cd5\u76ee\u7684\u65f6\uff0c\u5219\u4e0d\u4e88\u9002\u7528\u201d\u3002\u8be5\u6761\u6b3e\u7684\u7acb\u6cd5\u65b9\u5f0f\u5c5e\u4e8e\u516c\u5171\u79e9\u5e8f\u7acb\u6cd5\u65b9\u5f0f\u4e2d\u7684\nA. \u76f4\u63a5\u9650\u5236\u65b9\u5f0f\nB. \u5408\u5e76\u5f0f\u9650\u5236\u65b9\u5f0f\nC. \u95f4\u63a5\u9650\u5236\u65b9\u5f0f\nD. \u6df7\u5408\u9650\u5236\u65b9\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u53cd\u6050\u6016\u4e3b\u4e49\u6cd5\u300b\u5c06\u4e8e\u54ea\u4e00\u5e74\u54ea\u4e00\u6708\u54ea\u4e00\u65e5\u65bd\u884c\nA. 27/12/2015\nB. 01/01/2016\nC. 27/12/2016\nD. 01/01/2015\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.777238605798732, "itpossible/Chinese-Mistral-7B-v0.1": 0.2986334267609957, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3589842189651603, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.772479677004738}}, {"question": "\u67d0\u4e2d\u5b66\u5316\u5b66\u8001\u5e08\u5b8b\u67d0\u6b63\u7ec4\u7ec7\u5b66\u751f\u4e0a\u5b9e\u9a8c\u8bfe\uff0c\u5b66\u751f\u674e\u67d0\u56e0\u501f\u7528\u5750\u5728\u5b9e\u9a8c\u684c\u5bf9\u9762\u7684\u540c\u5b66\u7684\u7684 \u94a2\u7b14\uff0c\u78b0\u5012\u4e86\u9152\u7cbe\u706f\uff0c\u9152\u7cbe\u6e85\u5728\u672c\u7ec4\u540c\u5b66\u97e9\u67d0\u7684\u624b\u4e0a\u5e76\u71c3\u70e7\uff0c\u81f4\u4f7f\u97e9\u67d0\u624b\u90e8\u76ae\u80a4\u88ab\u707c\u4f24\u3002\u5728\u8fd9\u8d77\u4e8b\u6545\u4e2d\uff0c\u5e94\u5f53\u627f\u62c5\u8d54\u507f\u8d23\u4efb\u7684\u662f\nA. \u5b66\u6821\u548c\u5b8b\u67d0\nB. \u5b8b\u67d0\u548c\u674e\u67d0\u7684\u76d1\u62a4\u4eba\nC. \u5b66\u6821\u548c\u674e\u67d0\u7684\u76d1\u62a4\u4eba\nD. \u674e\u67d0\u7684\u76d1\u62a4\u4eba\u548c\u97e9\u67d0\u7684\u76d1\u62a4\u4eba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.303006867405966, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5157888980022398, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5404\u9879\u4e2d\uff0c\u5c5e\u4e8e\u6536\u5165\u8981\u7d20\u5185\u5bb9\u7684\u662f\nA. \u9500\u552e\u5546\u54c1\u7684\u6536\u5165\nB. \u51fa\u552e\u65e0\u5f62\u8d44\u4ea7\u7684\u6536\u5165\nC. \u51fa\u552e\u56fa\u5b9a\u8d44\u4ea7\u7684\u6536\u5165\nD. \u63a5\u53d7\u6350\u8d60\u7684\u6536\u5165\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4116286087063091, "meta-math/MetaMath-Mistral-7B": 0.7238514378624719, "itpossible/Chinese-Mistral-7B-v0.1": 0.7032350144633741, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.483096291081096, "meta-llama/Meta-Llama-3-8B": 0.5778375384471712, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.873347692600808}}, {"question": "\u6027\u7279\u8d28\u6307\u6027\u522b\u95f4\u5c55\u9732\u51fa\u6765\u7684\u89d2\u8272\u7279\u6027\u5206\u5e03\uff0c\u662f\u4efb\u4f55\u4e24\u6027\u793e\u4f1a\u7684\u4e00\u4e2a\nA. \u91cd\u8981\u8bae\u9898\nB. \u7279\u6b8a\u8bae\u9898\nC. \u7a81\u51fa\u8bae\u9898\nD. \u57fa\u7840\u8bae\u9898\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3802392203343767, "HuggingFaceH4/zephyr-7b-beta": 0.9335883084108386, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5185\u5fc3\u8d77\u778b\u605a\uff0c\u8981\u4ee5\uff08\uff09\u5bf9\u6cbb?\nA. \u81ea\u5728\u89c2\nB. \u56e0\u7f18\u89c2\nC. \u4e0d\u51c0\u89c2\nD. \u6148\u60b2\u89c2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3857686229279515, "meta-math/MetaMath-Mistral-7B": 0.7098584162524385, "itpossible/Chinese-Mistral-7B-v0.1": 0.4493716360696794, "HuggingFaceH4/zephyr-7b-beta": 0.892837932150287, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3376787962540697, "meta-llama/Meta-Llama-3-8B": 0.40870097338816025, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9929218778463604}}, {"question": "\u56fd\u9645\u6cd5\u6548\u529b\u7684\u6839\u636e\u662f\nA. \u81ea\u7136\u6cd5\u7684\u6548\u529b\nB. \u4eba\u7c7b\u6cd5\u5f8b\u826f\u77e5\nC. \u56fd\u5bb6\u4e4b\u95f4\u7684\u534f\u8bae\nD. \u6700\u9ad8\u89c4\u8303\u7684\u6548\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7295662202398016, "meta-math/MetaMath-Mistral-7B": 0.9849175344856573, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9996906299127496, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9771144032448884, "meta-llama/Meta-Llama-3-8B": 0.8402786443819348, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9597176891915263}}, {"question": "\u5904\u4e8e\u56fd\u5bb6\u653f\u6cbb\u5236\u5ea6\u591a\u5c42\u6b21\u7ed3\u6784\u4e2d\u7684\u6838\u5fc3\uff0c\u51b3\u5b9a\u4e86\u56fd\u5bb6\u6027\u8d28\u7684\u662f\nA. \u653f\u4f53\nB. \u653f\u515a\u5236\u5ea6\nC. \u57fa\u672c\u7ecf\u6d4e\u5236\u5ea6\nD. \u56fd\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5746933157561458, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8261083443527516, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7934068156293508, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "i.\u5b83\u4e0e\u666e\u901a\u6253\u5370\u2f2f\u4f5c\u539f\u7406\u57fa\u672c\u76f8\u540c\uff0c\u6253\u5370\u673a\u5185\u88c5\u6709\u6db2\u4f53\u6216\u7c89\u672b\u7b49\u201c\u6253\u5370\u6750\u6599\u201d\u3002 ii.\u8fd9\u79cd\u6253\u5370\u6280\u672f\u79f0\u4e3a\u3002 iii.\u51fa\u73b0\u572820\u4e16\u7eaa90\u5e74\u4ee3\u4e2d\u671f\uff0c\u5b9e\u9645\u4e0a\u662f\u5229\u2f64\u548c\u7eb8\u5c42\u53e0\u7b49\u6280\u672f\u7684\u6700\u65b0\u5feb\u901f\u6210\u578b\u88c5\u7f6e\u3002 vi.\u8be5\u6280\u672f\u5728\u73e0\u5b9d\u3001\u3001\u5efa\u7b51\u3001\u3001\u3001\u4ee5\u53ca\u5176\u4ed6\u9886\u57df\u90fd\u6709\u6240\u5e94\u2f64\u3002 v.\u4e0e\u7535\u8111\u8fde\u63a5\u540e\uff0c\u901a\u8fc7\u7535\u8111\u63a7\u5236\u628a\u201c\u6253\u5370\u6750\u6599\u201d\u2f00\u5c42\u5c42\u53e0\u52a0\u8d77\u6765\uff0c\u6700\u7ec8\u628a\u8ba1\u7b97\u673a\u4e0a\u7684\u84dd\u56fe\u53d8\u6210\u5b9e\u7269\u3002\u5bf9\u4e0a\u8ff0\u53e5\u2f26\u7ec4\u6210\u8bed\u6bb5\u987a\u5e8f\u6392\u5217\u6b63\u786e\u7684\u662f\nA. iii.i.v.ii.vi. \nB. iii.vi.v.i.ii. \nC. ii.iii.i.v.vi.\nD. ii.vi.i.v.iii.\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5858957977341094, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.33544561004293627, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7171699773062002}}, {"question": "\u5982\u679c\u67d0\u6d88\u8d39\u8005\u6700\u521d\u5904\u4e8e\u5747\u8861\u72b6\u6001\uff0c\u5f53\u5176\u8d27\u5e01\u6536\u5165\u589e\u52a0\u65f6\uff0c\nA. \u4f1a\u4f7f\u5176\u65b0\u7684\u5747\u8861\u70b9\u79fb\u5411\u66f4\u9ad8\u6c34\u5e73\u7684\u65e0\u5dee\u5f02\u66f2\u7ebf\nB. \u4f1a\u4f7f\u5176\u65e0\u5dee\u5f02\u66f2\u7ebf\u53d8\u5f97\u66f4\u9661\uff0c\u4f46\u4e0d\u4f1a\u6539\u53d8\u5747\u8861\u70b9\u7684\u4f4d\u7f6e\nC. \u7531\u4e8e\u5546\u54c1\u4ef7\u683c\u672a\u53d1\u751f\u53d8\u5316\uff0c\u56e0\u6b64\u5bf9\u5747\u8861\u70b9\u7684\u4f4d\u7f6e\u6ca1\u5f71\u54cd\nD. \u4f1a\u4f7f\u5176\u65b0\u7684\u5747\u8861\u70b9\u79fb\u5411\u66f4\u4f4e\u6c34\u5e73\u7684\u65e0\u5dee\u5f02\u66f2\u7ebf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6266245792896397, "meta-math/MetaMath-Mistral-7B": 0.9783609063961625, "itpossible/Chinese-Mistral-7B-v0.1": 0.5190771588060047, "HuggingFaceH4/zephyr-7b-beta": 0.9994966155196766, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8906469223693416, "meta-llama/Meta-Llama-3-8B": 0.6803512826972293, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9621695961879082}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\uff0c\u542b\u6709\u4ecb\u8bcd\u5bbe\u8bed\u524d\u7f6e\u7684\u4e00\u53e5\u662f\nA. \u4eca\u4eba\u541b\u4e4b\u5de6\u53f3\uff0c\u51fa\u5247\u7232\u52bf\u91cd\u800c\u6536\u5229\u65bc\u6c11\uff0c\u5165\u5247\u6bd4\u5468\u800c\u853d\u6076\u65bc\u541b\u3002\nB. \u5b50\u7389\u4e4b\u6557\uff0c\u5b50\u4e4b\u8209\u4e5f\u3002\u8209\u4ee5\u6557\u570b\uff0c\u5c07\u4f55\u8cc0\u7109?\nC. \u6562\u554f\u592b\u5b50\u60e1\u4e4e\u9577?\nD. \u76d6\u6709\u4e4b\u77e3\uff0c\u6211\u672a\u4e4b\u898b\u4e5f\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6052\u661f\u6838\u53cd\u5e94\u8017\u5c3d\u5176\u5185\u90e8\u71c3\u6599\u6240\u9700\u7684\u65f6\u95f4\u53eb\u6838\u65f6\u6807\uff0c\u53d6\u51b3\u4e8e\u6052\u661f\u7684\u8d28\u91cf\u548c\u5149\u5ea6\u3002\u5df2\u77e5\u592a\u9633\u7684\u6838\u65f6\u6807\u7ea6\u4e3a2.5\u00d710^{10}\u5e74\uff0c\u4e00\u9897\u4e3b\u5e8f\u661f\u7684\u8d28\u91cf\u662f\u592a\u9633\u768410\u500d\uff0c\u5149\u5ea6\u662f\u592a\u9633\u7684300\u500d\uff0c\u5176\u6838\u65f6\u6807\u7ea6\u4e3a\nA. 7.5\u00d710^{8}\u5e74\nB. 7.5\u00d710^{10}\u5e74\nC. 8.3\u00d710^{8}\u5e74\nD. 8.3\u00d710^{10}\u5e74\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3056913219994301, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u56fd\u571f\u8fbd\u9614\uff0c\u5168\u56fd\u571f\u5730\u9762\u79ef\u5360\u4e16\u754c\u9646\u5730\u9762\u79ef\u76847.4%\uff0c\u8015\u5730\u9762\u79ef\u5360\u5168\u7403\u8015\u5730\u603b\u9762\u79ef\u7684\uff08\uff09\u5de6\u53f3\nA. 1/10\nB. 1/15\nC. 1/20\nD. 1/5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5215179990724123, "itpossible/Chinese-Mistral-7B-v0.1": 0.32332502336054764, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u516c\u5171\u805a\u96c6\u573a\u6240\u53d1\u751f\u7a81\u53d1\u4e8b\u4ef6\u88ab\u8e29\u5012\u4e0d\u80fd\u7ad9\u7acb\u65f6\uff0c\u5e94\u91c7\u53d6___\u59ff\u52bf\nA. \u8eab\u4f53\u8737\u7f29\u6210\u7403\u72b6\uff0c\u53cc\u624b\u62b1\u5934\nB. \u4ef0\u9762\u671d\u4e0a\nC. \u5e73\u722c\u5730\u4e0a\nD. \u5176\u4ed6\u9009\u9879\u5747\u53ef\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8853617933600076, "meta-math/MetaMath-Mistral-7B": 0.9861774892938421, "itpossible/Chinese-Mistral-7B-v0.1": 0.8758686469043683, "HuggingFaceH4/zephyr-7b-beta": 0.9975067987732971, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9588684830337955, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6cd5\u5f8b\u7684\u7279\u5f81\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u6cd5\u5f8b\u5177\u6709\u5f3a\u5236\u6027\uff0c\u53ea\u80fd\u901a\u8fc7\u53f8\u6cd5\u4e88\u4ee5\u5b9e\u65bd\u548c\u5b9e\u73b0\nB. \u6cd5\u5f8b\u7531\u7acb\u6cd5\u673a\u5173\u5236\u5b9a\u6216\u8ba4\u53ef\uff0c\u4f53\u73b0\u4e86\u56fd\u5bb6\u610f\u5fd7\nC. \u4ee5\u4e49\u52a1\u4e3a\u672c\u4f4d\uff0c\u662f\u6cd5\u5f8b\u7684\u672c\u8d28\u7279\u5f81\nD. \u6cd5\u5f8b\u7684\u666e\u904d\u6027\u610f\u5473\u7740\u5728\u4e00\u56fd\u4e4b\u5185\uff0c\u6240\u6709\u4eba\u90fd\u5e94\u4eab\u6709\u76f8\u540c\u7684\u6cd5\u5f8b\u6743\u5229\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39690055859988593, "meta-math/MetaMath-Mistral-7B": 0.8418793975730593, "itpossible/Chinese-Mistral-7B-v0.1": 0.32545507259594497, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5560862764781486, "meta-llama/Meta-Llama-3-8B": 0.5425782529570012, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6684858101044147}}, {"question": "\u4e0b\u5217\u51fd\u6570\u4e2d\uff0c\u5728 $x=0$ \u5904\u4e0d\u53ef\u5bfc\u7684\u662f\nA. $f(x)=|x| \\sin |x|$.\nB. $f(x)=|x| \\sin \\sqrt{|x|}$.\nC. $f(x)=\\cos |x|$.\nD. $f(x)=\\cos \\sqrt{|x|}$.\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u4e9a\u5f53\u65af\u7684\u516c\u5e73\u7406\u8bba\uff0c\u6b63\u786e\u7684\u8868\u8ff0\u662f\nA. \u804c\u5de5\u7684\u5de5\u4f5c\u6001\u5ea6\u4e0e\u751f\u4ea7\u79ef\u6781\u6027\u65e2\u53d6\u51b3\u4e8e\u4ed6\u4eec\u6240\u83b7\u62a5\u916c\u7684\u7edd\u5bf9\u91cf\uff0c\u53c8\u53d6\u51b3\u4e8e\u76f8\u5bf9\u91cf\nB. \u804c\u5de5\u7684\u5de5\u4f5c\u6001\u5ea6\u4e0e\u751f\u4ea7\u79ef\u6781\u6027\u4e0e\u4ed6\u4eec\u6240\u83b7\u62a5\u916c\u7684\u7edd\u5bf9\u91cf\u548c\u76f8\u5bf9\u91cf\u65e0\u5173\nC. \u804c\u5de5\u7684\u5de5\u4f5c\u6001\u5ea6\u4e0e\u751f\u4ea7\u79ef\u6781\u6027\u4e3b\u8981\u53d6\u51b3\u4e8e\u4ed6\u4eec\u6240\u83b7\u62a5\u916c\u7684\u7edd\u5bf9\u91cf\nD. \u804c\u5de5\u7684\u5de5\u4f5c\u6001\u5ea6\u4e0e\u751f\u4ea7\u79ef\u6781\u6027\u4e0d\u53d6\u51b3\u4e8e\u4ed6\u4eec\u6240\u83b7\u62a5\u916c\u7684\u76f8\u5bf9\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9360336013987272, "meta-math/MetaMath-Mistral-7B": 0.9773260493437473, "itpossible/Chinese-Mistral-7B-v0.1": 0.6760031369918537, "HuggingFaceH4/zephyr-7b-beta": 0.9996317535107292, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.913990081956555, "meta-llama/Meta-Llama-3-8B": 0.7760040985026732, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9156519879760114}}, {"question": "\u5973\u6027\uff0c55 \u5c81\u300210 \u5929\u524d\u884c\u80c3\u764c\u6839\u6cbb\u672f\uff0c\u672f\u540e3 \u5929\u62d4\u9664\u80c3\u7ba1\u5f00\u59cb\u8fdb\u6d41\u98df\u30023 \u5929\u6765\u8fdb\u98df\u540e\u534a\u5c0f\u65f6\u51fa\u73b0\u53f3\u4e0a\u8179\u80c0\u75db\uff0c\u4f34\u5455\u5410\uff0c\u5410\u51fa\u7269\u4e3a\u5927\u91cf\u80c6\u6c41\uff0c\u5410\u540e\u75c7\u72b6\u7f13\u89e3\u3002\u6b64\u60a3\u8005\u6700\u53ef\u80fd\u7684\u8bca\u65ad\u662f\nA. \u543b\u5408\u53e3\u6897\u963b\nB. \u6162\u6027\u8f93\u5165\u88a2\u6897\u963b\nC. \u6025\u6027\u8f93\u5165\u88a2\u6897\u963b\nD. \u8f93\u51fa\u88a2\u6897\u963b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u516c\u5173\u8c03\u67e5\u4e2d\u4f7f\u7528\u6700\u4e3a\u5e7f\u6cdb\u7684\u65b9\u6cd5\u662f\nA. \u8d44\u6599\u5206\u6790\u6cd5\nB. \u6c11\u610f\u6d4b\u9a8c\u6cd5\nC. \u7fa4\u4f53\u8ba8\u8bba\u6cd5\nD. \u516c\u4f17\u4ee3\u8868\u5ea7\u8c08\u4f1a\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3965254564392249, "meta-math/MetaMath-Mistral-7B": 0.5262386711124637, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6474756562217, "meta-llama/Meta-Llama-3-8B": 0.4551316031872074, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u57fa\u8f9b\u683c\u5728\u300a\u8bba\u4e2d\u56fd\u300b\u5199\u9053\uff0c\u201c1969\u5e74\uff0c\u6bdb\uff08\u4e2d\u56fd\uff09\u7684\u5916\u4ea4\u653f\u7b56\u8d70\u5230\u4e86\u4e00\u4e2a\u8f6c\u6298\u70b9\u3002\u6bdb\u6cfd\u4e1c\u7ed9\u5f53\u65f6\u7684\u56db\u4f4d\u89e3\u653e\u519b\u5927\u5143\u5e05\u5e03\u7f6e\u4e86\u4e00\u4e2a\u2018\u5bb6\u5ead\u4f5c\u4e1a\u2019\u2014\u2014\u5206\u6790\u5f53\u524d\u7684\u56fd\u9645\u5c40\u52bf\u548c\u4e2d\u56fd\u7684\u6218\u7565\u9009\u62e9\uff0c\u56db\u5927\u5143\u5e05\u5f15\u7528\u4e86\u300a\u4e09\u56fd\u6f14\u4e49\u300b\u4e2d\u2018\u4e09\u5206\u5929\u4e0b\u2019\u7684\u5178\u6545\uff0c\u5efa\u8bae\u4e0e\u5f53\u65f6\u7684\u5934\u53f7\u654c\u4eba\u7f8e\u56fd\u6253\u5f00\u5173\u7cfb\u201d\u3002\u4e0a\u8ff0\u6750\u6599\u53ef\u4ee5\u7528\u4e8e\u7814\u7a76\nA. \u56fd\u9645\u653f\u6cbb\u65b0\u79e9\u5e8f\u7684\u5efa\u7acb\nB. \u4e2d\u56fd\u5168\u65b9\u4f4d\u7684\u5916\u4ea4\u653f\u7b56\nC. \u4e2d\u56fd\u7279\u8272\u7684\u5927\u56fd\u5916\u4ea4\nD. \u5916\u4ea4\u653f\u7b56\u8c03\u6574\u7684\u56e0\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.47607611251690324, "itpossible/Chinese-Mistral-7B-v0.1": 0.500006602249443, "HuggingFaceH4/zephyr-7b-beta": 0.8995891082374712, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5789211381882495, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b0\u822a\u8def\u5f00\u8f9f\u4ee5\u540e\uff0c\u4e16\u754c\u8bb8\u591a\u5730\u533a\u7684\u690d\u7269\u79cd\u5b50\u88ab\u5e26\u56de\u6b27\u6d32\uff0c\u7ecf\u57f9\u80b2\u6539\u826f\u6210\u4e3a\u6b27\u6d32\u4eba\u7684\u98df\u7269\u539f\u6599\uff0c\u5982\u7389\u7c73\u3001\u571f\u8c46\u7b49\u3002\u6b27\u6d32\u4eba\u4e5f\u5c06\u6b27\u6d32\u7684\u4e00\u4e9b\u690d\u7269\u79cd\u5b50\u5e26\u5230\u6b96\u6c11\u5730\u8fdb\u884c\u57f9\u80b2\u751f\u4ea7\uff0c\u6700\u7ec8\u6210\u4e3a\u6b96\u6c11\u5730\u4eba\u6c11\u7684\u91cd\u8981\u98df\u6750\u3002\u8fd9\u8bf4\u660e\u65b0\u822a\u8def\u5f00\u8f9f\nA. \u6709\u5229\u4e8e\u4e16\u754c\u5404\u5730\u6587\u660e\u7684\u4ea4\u6d41\u4e0e\u878d\u5408\nB. \u672a\u7ed9\u6b96\u6c11\u5730\u9020\u6210\u4efb\u4f55\u7684\u6253\u51fb\u548c\u7834\u574f\nC. \u5e26\u6765\u7684\u7269\u79cd\u4ea4\u6d41\u52a9\u63a8\u65b0\u65e7\u5927\u9646\u53d1\u5c55\nD. \u4fc3\u8fdb\u65b0\u65e7\u5927\u9646\u7ecf\u6d4e\u4e92\u8865\u800c\u5171\u540c\u53d1\u5c55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.44784216527748344, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.890060932572525, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.44178089647797625, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e16\u754c\u4e09\u5927\u4e3b\u8981\u5efa\u7b51\u4f53\u7cfb\u4e0d\u5305\u542b\u54ea\u4e2a\nA. \u6b27\u6d32\u5efa\u7b51\nB. \u4f0a\u65af\u5170\u5efa\u7b51\nC. \u7f8e\u6d32\u5efa\u7b51\nD. \u4e2d\u56fd\u5efa\u7b51\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6732242902231355, "meta-math/MetaMath-Mistral-7B": 0.8988798689969498, "itpossible/Chinese-Mistral-7B-v0.1": 0.49151163360642475, "HuggingFaceH4/zephyr-7b-beta": 0.9961999039409462, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8635670162911996, "meta-llama/Meta-Llama-3-8B": 0.755895835171457, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9946055840951754}}, {"question": "\u5973\u6027\uff0c42\u5c81\u30022\u4e2a\u6708\u4ee5\u6765\u8fdb\u884c\u6027\u4e4f\u529b\uff0c\u5934\u6655\uff0c\u5fc3\u60b8\uff0c\u7eb3\u5dee\u3002\u67e5\u4f53\uff1a\u9762\u8272\u82cd\u767d\uff0c\u5fc3\u7387110\u6b21/\u5206\u3002\u8840\u5e38\u89c4\uff1aHb72g/L\uff0cMCV124fl\uff0cMCH40pg\uff0cMCHC330g/L\uff0cRet1.0%\uff0cWBC3.4\u00d7109/L\uff0cPlt85\u00d7109/L\u3002\u8be5\u60a3\u8005\u6700\u53ef\u80fd\u7684\u8bca\u65ad\u662f\nA. \u5de8\u5e7c\u7ec6\u80de\u6027\u8d2b\u8840\nB. \u518d\u751f\u969c\u788d\u6027\u8d2b\u8840\nC. \u7f3a\u94c1\u6027\u8d2b\u8840\nD. \u6eb6\u8840\u6027\u8d2b\u8840\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.31292412927280244, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u80c6\u7ea2\u7d20\u5728\u809d\u810f\u4e2d\u7684\u8f6c\u5316\u4e3b\u8981\u662f\nA. \u4e0e\u8461\u8404\u7cd6\u9178\u7ed3\u5408\nB. \u4e0eY\u86cb\u767d\u7ed3\u5408\nC. \u4e0e\u6e05\u86cb\u767d\u7ed3\u5408\nD. \u4e0e\u8461\u8404\u7cd6\u919b\u9178\u7ed3\u5408\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4175059711973426, "HuggingFaceH4/zephyr-7b-beta": 0.812660974903263, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u79cd\u7269\u8d28\u4e0d\u5c5e\u4e8e\u7c7b\u8102\nA. \u7cd6\u8102\nB. \u80c6\u56fa\u9187\nC. \u5375\u78f7\u8102\nD. \u4e09\u9170\u7518\u6cb9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6159436310973785, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5633285076546953}}, {"question": "\u793e\u4f1a\u5b66\u4ea7\u751f\u7684\u76f4\u63a5\u6839\u6e90\u548c\u5173\u952e\u56e0\u7d20\u662f\nA. \u81ea\u7136\u79d1\u5b66\u7684\u53d1\u5c55.\nB. \u793e\u4f1a\u7ecf\u9a8c\u7814\u7a76\u7684\u79ef\u7d2f\nC. \u793e\u4f1a\u53d8\u9769\u7684\u9700\u8981\nD. \u7a7a\u60f3\u793e\u4f1a\u4e3b\u4e49\u7684\u4ea7\u751f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5673840931147964, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8016725978487294}}, {"question": "\u7531\u4e8e\u5730\u7403\u81ea\u8f6c\u7684\u4e0d\u5747\u5300\u6027\u548c\u957f\u671f\u53d8\u6162\u6027\uff0c\u4e16\u754c\u65f6\u4e0e\u539f\u5b50\u65f6\u7684\u5dee\u9700\u8981\u4eba\u4e3a\u95f0\u79d2\u6765\u89e3\u51b3\uff0c\u6b63\u95f0\u79d2\u4e3a\u534f\u8c03\u4e16\u754c\u65f6\u52a01\u79d2\uff0c\u8d1f\u95f0\u79d2\u4e3a\u5176\u51cf1\u79d2\u30022017\u5e741\u67081\u65e5\u7684\u95f0\u79d2\u5df2\u7ecf\u662f\u672c\u4e16\u7eaa\u7b2c5\u6b21\u95f0\u79d2\u3002\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u5176\u4e2d\u67093\u6b21\u6b63\u95f0\u79d22\u6b21\u8d1f\u95f0\u79d2\nB. \u5176\u4e2d\u67092\u6b21\u6b63\u95f0\u79d23\u6b21\u8d1f\u95f0\u79d2\nC. 5\u6b21\u5747\u4e3a\u8d1f\u95f0\u79d2\nD. 5\u6b21\u5747\u4e3a\u6b63\u95f0\u79d2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7304556092360367, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u4eba\u95f4\u559c\u5267\u300b\u7684\u4f5c\u8005\u662f\nA. \u5df4\u5c14\u624e\u514b\nB. \u72c4\u66f4\u65af\nC. \u6258\u5c14\u65af\u6cf0\nD. \u96e8\u679c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3191152350487737, "itpossible/Chinese-Mistral-7B-v0.1": 0.3900852433273489, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6cd5\u4e0e\u653f\u6cbb\u76f8\u4e92\u4f5c\u7528\u7684\u5173\u7cfb\u4e2d\uff0c\u54ea\u4e00\u4e2a\u9009\u9879\u662f\u9519\u8bef\u7684\uff1f\nA. \u6cd5\u53ef\u4ee5\u5bf9\u653f\u6cbb\u7ec4\u7ec7\u3001\u5229\u76ca\u96c6\u56e2\u7b49\u653f\u6cbb\u89d2\u8272\u7684\u884c\u4e3a\u548c\u6d3b\u52a8\u8fdb\u884c\u7a0b\u5e8f\u6027\u548c\u89c4\u8303\u6027\u63a7\u5236\nB. \u7531\u4e8e\u5728\u4e0a\u5c42\u5efa\u7b51\u4e2d\u653f\u6cbb\u5c45\u4e8e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u56e0\u6b64\u6cd5\u4f9d\u9644\u4e8e\u653f\u6cbb\uff0c\u5e76\u5b8c\u5168\u670d\u52a1\u4e8e\u653f\u6cbb\nC. \u6cd5\u5f8b\u79bb\u4e0d\u5f00\u653f\u6cbb\uff0c\u653f\u6cbb\u4e5f\u79bb\u4e0d\u5f00\u6cd5\u5f8b\nD. \u653f\u6cbb\u8fd0\u884c\u7684\u89c4\u8303\u5316\u3001\u653f\u6cbb\u53d1\u5c55\u4e2d\u653f\u6cbb\u751f\u6d3b\u7684\u6c11\u4e3b\u5316\u79bb\u4e0d\u5f00\u6cd5\u5f8b\u7684\u8fd0\u4f5c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8971989134776832, "meta-math/MetaMath-Mistral-7B": 0.97774147117235, "itpossible/Chinese-Mistral-7B-v0.1": 0.9390199569309869, "HuggingFaceH4/zephyr-7b-beta": 0.9994849867493988, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9832349188980991, "meta-llama/Meta-Llama-3-8B": 0.9102215543852107, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.36891081345169807}}, {"question": "\u67d0\u79cd\u7269\u8d28\u53ef\u63d2\u5165DNA\u5206\u5b50\u4e24\u6761\u94fe\u7684\u78b1\u57fa\u5bf9\u4e4b\u95f4\uff0c\u4f7fDNA\u53cc\u94fe\u4e0d\u80fd\u89e3\u5f00\u3002\u82e5\u5728\u7ec6\u80de\u6b63\u5e38\u751f\u957f\u7684\u57f9\u517b\u6db2\u4e2d\u52a0\u5165\u9002\u91cf\u7684\u8be5\u7269\u8d28\uff0c\u4e0b\u5217\u76f8\u5173\u53d9\u8ff0\u9519\u8bef\u7684\u662f\nA. \u53ef\u63a8\u6d4b\u8be5\u7269\u8d28\u5bf9\u764c\u7ec6\u80de\u7684\u589e\u6b96\u6709\u6291\u5236\u4f5c\u7528\nB. \u8be5\u7269\u8d28\u53ef\u5c06\u7ec6\u80de\u5468\u671f\u963b\u65ad\u5728\u5206\u88c2\u4e2d\u671f\nC. \u968f\u540e\u7ec6\u80de\u4e2d\u7684RNA\u8f6c\u5f55\u53d1\u751f\u969c\u788d\nD. \u968f\u540e\u7ec6\u80de\u4e2d\u7684DNA\u590d\u5236\u53d1\u751f\u969c\u788d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\u4e0e\u7537\u6027\u5fc3\u7406\u7279\u70b9\u5177\u6709\u660e\u663e\u5dee\u5f02\u6027\uff0c\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u5973\u6027\u5fc3\u7406\u7279\u70b9\u4e0d\u5305\u62ec\nA. \u88ab\u4e86\u89e3\nB. \u88ab\u5c0a\u91cd\nC. \u88ab\u4f9d\u8d56\nD. \u88ab\u5173\u5fc3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44719401270104575, "meta-math/MetaMath-Mistral-7B": 0.35347162922091135, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9209751117893732, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8159028165931816, "meta-llama/Meta-Llama-3-8B": 0.4890223324161174, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u300a\u5546\u6807\u6cd5\u300b\u7684\u89c4\u5b9a\uff0c\u5546\u6807\u6743\u4eba\u5c06\u5546\u6807\u8f6c\u8ba9\u7ed9\u4ed6\u4eba\u7684\uff0c\u53d7\u8ba9\u4eba\u4eab\u6709\u5546\u6807\u4e13\u7528\u6743\u7684\u8d77\u59cb\u65e5\u4e3a\nA. \u8f6c\u8ba9\u7684\u6ce8\u518c\u5546\u6807\u81ea\u5546\u6807\u5c40\u6838\u51c6\u6ce8\u518c\u4e4b\u65e5\nB. \u8f6c\u8ba9\u7684\u6ce8\u518c\u5546\u6807\u81ea\u767b\u8bb0\u4e4b\u65e5\nC. \u8f6c\u8ba9\u7684\u6ce8\u518c\u5546\u6807\u81ea\u516c\u544a\u4e4b\u65e5\nD. \u8f6c\u8ba9\u7684\u6ce8\u518c\u5546\u6807\u81ea\u5546\u6807\u5c40\u5907\u6848\u4e4b\u65e5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3460058095032025, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4798430226595385}}, {"question": "\u539f\u5b50\u7ed3\u6784\u51b3\u5b9a\u5143\u7d20\u7684\u6027\u8d28\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u7b2c\u4e8c\u5468\u671f\u5143\u7d20\u7684\u6700\u9ad8\u6b63\u5316\u5408\u4ef7\u90fd\u7b49\u4e8e\u5176\u539f\u5b50\u7684\u6700\u5916\u5c42\u7535\u5b50\u6570\nB. \u975e\u91d1\u5c5e\u5143\u7d20\u7684\u975e\u91d1\u5c5e\u6027\u8d8a\u5f3a\uff0c\u5176\u6c27\u5316\u7269\u5bf9\u5e94\u6c34\u5316\u7269\u7684\u9178\u6027\u4e5f\u4e00\u5b9a\u8d8a\u5f3a\nC. Na\u3001Al\u3001Cl\u7684\u539f\u5b50\u534a\u5f84\u4f9d\u6b21\u51cf\u5c0f\uff0cNa\uff0b\u3001Al3\uff0b\u3001Cl\uff0d\u7684\u79bb\u5b50\u534a\u5f84\u4e5f\u4f9d\u6b21\u51cf\u5c0f\nD. \u5728\u7b2c\u2165A\u65cf\u5143\u7d20\u7684\u6c22\u5316\u7269(H2R)\u4e2d\uff0c\u70ed\u7a33\u5b9a\u6027\u6700\u5f3a\u7684\u5176\u6cb8\u70b9\u4e5f\u4e00\u5b9a\u6700\u9ad8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\uff0c\u52a8\u8bcd\u5bbe\u8bed\u7701\u7565\u7684\u4e00\u53e5\u662f\nA. \u6578\u65e5\u4e0d\u6b7b\uff0c\u5308\u5974\u4ee5\u7232\u795e\u3002\nB. \u4eba\u7686\u6709\u5144\u5f1f\uff0c\u6211\u7368\u7121\u3002\nC. \u8af8\u751f\u696d\u60a3\u4e0d\u80fd\u7cbe\uff0c\u7121\u60a3\u6709\u53f8\u4e4b\u4e0d\u660e\u3002\nD. \u53ef\u8f3f\u8a00\u800c\u4e0d\u8f3f\u4e4b\u8a00\uff0c\u5931\u4eba\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.36617639865290497, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2010\u5e745\u67081\u65e5\u81f310\u670831\u65e5\uff0c\u4e0a\u6d77\u4e16\u535a\u4f1a\u6210\u529f\u4e3e\u529e\u3002\u8fd9\u662f\u81ea1851\u5e74\u8bde\u751f\u8d77\u9996\u6b21\u5728\u53d1\u5c55\u4e2d\u56fd\u5bb6\u4e3e\u529e\u3002\u4e3e\u529e\u4e16\u535a\u4f1a\u6700\u76f4\u63a5\u7684\u6548\u5e94\u662f\u4fc3\u8fdb\u56fd\u9645\u7ecf\u6d4e\u8d38\u6613\u65c5\u6e38\u4e1a\u53d1\u5c55\u4ee5\u53ca\u5404\u56fd\u4e4b\u95f4\u7684\u6587\u5316\u4ea4\u6d41\u548c\u53cb\u597d\u5408\u4f5c\u3002\u7533\u529e\u4e16\u535a\u4f1a\u6210\u529f\u5bf9\u6211\u56fd\u7684\u7ecf\u6d4e\u610f\u4e49\u662f\uff1aa\u6709\u5229\u4e8e\u6269\u5927\u6295\u8d44\u548c\u6d88\u8d39\u9700\u6c42 b\u6709\u5229\u4e8e\u4fc3\u8fdb\u7ecf\u6d4e\u7ed3\u6784\u7684\u8c03\u6574 c\u6709\u5229\u4e8e\u589e\u52a0\u5c31\u4e1a d\u6709\u5229\u4e8e\u6269\u5927\u5f00\u653e\uff0c\u4fc3\u8fdb\u56fd\u5185\u4f01\u4e1a\u53c2\u4e0e\u56fd\u9645\u7ade\u4e89\nA. d\nB. abcd\nC. cd\nD. bcd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5212060695669816, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7760640337530503}}, {"question": "\u516c\u5171\u5173\u7cfb\u5851\u9020\u7684\u662f\u7ec4\u7ec7\u7684\nA. \u4eba\u5458\u5f62\u8c61\nB. \u4ea7\u54c1\u5f62\u8c61\nC. \u670d\u52a1\u5f62\u8c61\nD. \u6574\u4f53\u5f62\u8c61\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7811817745696295, "meta-math/MetaMath-Mistral-7B": 0.9034428081912741, "itpossible/Chinese-Mistral-7B-v0.1": 0.8977364579693169, "HuggingFaceH4/zephyr-7b-beta": 0.9992195998097858, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8183648941508872, "meta-llama/Meta-Llama-3-8B": 0.8472389687915888, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7708200279437787}}, {"question": "\u8bd5\u54c1\u7edd\u7f18\u8868\u9762\u810f\u6c61\u3001\u53d7\u6f6e\uff0c\u5728\u8bd5\u9a8c\u7535\u538b\u4e0b\u4ea7\u751f\u8868\u9762\u6cc4\u6f0f\u7535\u6d41\uff0c\u5bf9\u8bd5\u54c1\u7684tg\u03b4\u548cC\u7684\u6d4b\u91cf\u7ed3\u679c\u7684\u5f71\u54cd\u7a0b\u5ea6\u662f\nA. \u8bd5\u54c1\u7535\u5bb9\u91cf\u8d8a\u5927\uff0c\u5f71\u54cd\u8d8a\u5927\nB. \u8bd5\u54c1\u7535\u5bb9\u91cf\u8d8a\u5c0f\uff0c\u5f71\u54cd\u8d8a\u5c0f\nC. \u8bd5\u54c1\u7535\u5bb9\u91cf\u8d8a\u5c0f\uff0c\u5f71\u54cd\u8d8a\u5927\nD. \u4e0e\u8bd5\u54c1\u7535\u5bb9\u91cf\u7684\u5927\u5c0f\u65e0\u5173\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3611581904220662, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.28218339836013884, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c45 \u5c81\u3002\u95f4\u65ad\u53cc\u4e0b\u80a2\u6c34\u80bf\u4f34\u86cb\u767d\u5c3f 10 \u5e74\uff0c\u4e4f\u529b\u3001\u7eb3\u5dee\u3001\u6076\u5fc31\u5468\uff0c\u5237\u7259\u65f6\u7259\u9f88\u51fa\u8840\u4f34\u76ae\u80a4\u78b0\u540e\u53d1\u97523\u5929\u4eba\u9662\u3002\u4eba\u9662\u65f6\u6d4b\u8840\u538b 150/90 mmHg\uff0c\u5316\u9a8c\u8840Hb 80 g/L\uff0c WBC 6.4X10^9/L\uff0c Plt 192*10^9/L\u3002\u5c3f\u86cb\u767d(++)\uff0c\u5c3f\u6bd4\u91cd 1.010\uff0c\u5c3f\u7cd6(\u00b1)\uff0c\u5076\u89c1\u9897\u7c92\u7ba1\u578b\u3002\u8840 Cr 707 $\\mu$mol/L\u3002\u8be5\u60a3\u8005\u8d2b\u8840\u6700\u53ef\u80fd\u7684\u539f\u56e0\u662f\nA. \u5931\u8840\u56e0\u7d20\nB. \u8425\u517b\u6027\u9020\u8840\u539f\u6599\u4e0d\u8db3\nC. \u6162\u6027\u6eb6\u8840\nD. \u7ea2\u7ec6\u80de\u751f\u6210\u7d20\u51cf\u5c11\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6559\u80b2\u5b66\u770b\u6765\uff0c\u4e0b\u5217\u54ea\u4e2a\u9009\u9879\u4e0d\u4ec5\u662f\u8bfe\u5802\u7ba1\u7406\u7814\u7a76\u7684\u4e3b\u8981\u8303\u7574\uff0c\u4e5f\u662f\u5b66\u4e60\u8fc7\u7a0b\u7814\u7a76\u548c\u6559\u5b66\u8bbe\u8ba1\u7814\u7a76\u6240\u4e0d\u80fd\u5ffd\u89c6\u7684\u91cd\u8981\u5185\u5bb9\nA. \u6559\u5b66\u5a92\u4f53\nB. \u8bc4\u4ef7\uff0f\u53cd\u601d\u8fc7\u7a0b\nC. \u6559\u5b66\u73af\u5883\nD. \u6559\u5b66\u5185\u5bb9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.46382609490402926, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u4eec\u6709\u610f\u8bc6\u5730\u89c4\u5212\u5e76\u5728\u8f83\u77ed\u65f6\u95f4\u5185\u5b9e\u73b0\u7684\u793e\u4f1a\u5c40\u90e8\u8c03\u6574\u6216\u5168\u9762\u6539\u826f\u7684\u8fc7\u7a0b\uff0c\u88ab\u79f0\u4e3a\nA. \u793e\u4f1a\u9769\u547d\nB. \u793e\u4f1a\u6539\u826f\nC. \u793e\u4f1a\u8fdb\u6b65\nD. \u793e\u4f1a\u6539\u9769\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7306066663663676, "meta-math/MetaMath-Mistral-7B": 0.8982037285869671, "itpossible/Chinese-Mistral-7B-v0.1": 0.6091268351813598, "HuggingFaceH4/zephyr-7b-beta": 0.9339769257600852, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.640199836222594, "meta-llama/Meta-Llama-3-8B": 0.680488749883952, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7187224720363957}}, {"question": "\u5728\u5bf9\u4e24\u4e2a\u53d8\u91cfx\u4e0ey\u8fdb\u884c\u76f4\u7ebf\u76f8\u5173\u5206\u6790\u540e\u53d1\u73b0\uff0c\u76f8\u5173\u7cfb\u6570r\u7ea6\u7b49\u4e8e0 \uff0c\u7ecf\u68c0\u9a8c\uff0c\u5f97P>0.9\u3002\u5728\u4e0b\u4e13\u4e1a\u7ed3\u8bba\u65f6\uff0c\u6b63\u786e\u7684\u8868\u8ff0\u5e94\u8be5\u662f\nA. x\u4e0ey\u4e4b\u95f4\u6709\u65e0\u5173\u7cfb\u5c1a\u672b\u786e\u5b9a\nB. x\u4e0ey\u4e4b\u95f4\u5448\u66f2\u7ebf\u5173\u7cfb\nC. x\u4e0ey\u4e4b\u95f4\u5448\u6ca1\u6709\u5173\u7cfb\nD. x\u4e0ey\u4e4b\u95f4\u5448\u76f4\u7ebf\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.38797403658066004, "HuggingFaceH4/zephyr-7b-beta": 0.9993671986315914, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5804390543079335, "meta-llama/Meta-Llama-3-8B": 0.6411639186146778, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9318873622501196}}, {"question": "\u503c\u5f97\u5f53\u4ee3\u5927\u5b66\u751f\u7ec8\u8eab\u9075\u5949\u548c\u8df5\u884c\u7684\u79d1\u5b66\u3001\u9ad8\u5c1a\u7684\u4eba\u751f\u76ee\u7684\u662f\nA. \u91d1\u94b1\u81f3\u4e0a\u7684\u4eba\u751f\u76ee\u7684\nB. \u4e3a\u4eba\u6c11\u670d\u52a1\u7684\u4eba\u751f\u76ee\u7684\nC. \u4ee5\u4e2a\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4eba\u751f\u76ee\u7684\nD. \u6ee1\u8db3\u4e2a\u4f53\u611f\u5b98\u9700\u8981\u548c\u5feb\u4e50\u7684\u4eba\u751f\u76ee\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9247921691238996, "meta-math/MetaMath-Mistral-7B": 0.9951067895468751, "itpossible/Chinese-Mistral-7B-v0.1": 0.9773592041145046, "HuggingFaceH4/zephyr-7b-beta": 0.9942818792903922, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9840398586180313, "meta-llama/Meta-Llama-3-8B": 0.9604011578907413, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5645555968928726}}, {"question": "\u98df\u54c1\u6740\u83cc\u65f6\u95f4\u7684\u8ba1\u7b97\u65b9\u6cd5\u6709\nA. \u56fe\u89e3\u6cd5\nB. \u4ee5\u4e0a\u5168\u90e8\nC. \u6c42\u548c\u6cd5\nD. \u516c\u5f0f\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9014226760226941, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5667456970200658}}, {"question": "\u672a\u5a5a\u540c\u5c45\u662f\u5f53\u4e8b\u4eba\u7684\u79c1\u751f\u6d3b\u9009\u62e9\nA. \u4f1a\u53d7\u5230\u4e25\u5389\u60e9\u7f5a\nB. \u5f7c\u6b64\u4e0d\u9700\u627f\u62c5\u4e49\u52a1\nC. \u4e0d\u4f1a\u906d\u9047\u4fb5\u6743\nD. \u4e0d\u53d7\u6cd5\u5f8b\u4fdd\u62a4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2957129572682819, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4788991545175166, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4e8eY/Y\u63a5\u6cd5\u7684\u7535\u538b\u4e92\u611f\u5668\u6240\u63a5\u4eea\u8868\u80fd\u591f\u6d4b\u91cf\nA. \u7ebf\u7535\u538b\nB. \u76f8\u7535\u538b\nC. \u96f6\u5e8f\u7535\u538b\nD. \u5f53\u9ad8\u538b\u4fa7B\u76f8\u7194\u65ad\u5668\u7194\u65ad\u65f6\u6240\u63a5\u7684\u7535\u538b\u8868\u6307\u793a\u7684\u7535\u538bUab\u4e3a\u6b63\u5e38\u503c1/2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4945075263795443}}, {"question": "\u4f01\u4e1a\u4e0e\u5176\u4ea7\u54c1\u6216\u670d\u52a1\u7684\u8d2d\u4e70\u8005\u6216\u76f4\u63a5\u6d88\u8d39\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u662f\nA. \u5458\u5de5\u5173\u7cfb\nB. \u5ba2\u6237\u5173\u7cfb\nC. \u987e\u5ba2\u5173\u7cfb\nD. \u80a1\u4e1c\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5468\u6069\u6765\u603b\u7406\u5f3a\u8c03\uff0c\u4e3a\u4e86\u62b5\u6297\u7f8e\u56fd\u7684\u4fb5\u7565\u548c\u5a01\u80c1\uff0c\u540c\u82cf\u8054\u7ed3\u76df\u662f\u5fc5\u8981\u7684\uff0c\u4f46\u662f\u201c\u4e0d\u80fd\u628a\u81ea\u5df1\u7684\u515a\u548c\u56fd\u5bb6\u7684\u72ec\u7acb\u6027\u4e22\u6389\u201d\uff1b\u5728\u201c\u4e07\u9686\u4f1a\u8bae\u201d\u4e0a\uff0c\u5468\u6069\u6765\u5bf9\u4e8e\u5404\u56fd\u56e0\u793e\u4f1a\u5236\u5ea6\u548c\u610f\u8bc6\u5f62\u6001\u7684\u4e0d\u540c\u800c\u4ea7\u751f\u7684\u77db\u76fe\u4e0e\u5206\u6b67\uff0c\u9c9c\u660e\u5730\u63d0\u51fa\u4e86\u201c\u6c42\u540c\u5b58\u5f02\u201d\u7684\u65b9\u9488\uff0c\u8bf4\u201c\u4e2d\u56fd\u4ee3\u8868\u56e2\u662f\u6765\u6c42\u56e2\u7ed3\u800c\u4e0d\u662f\u6765\u5435\u67b6\u7684\u201d\u3002\u8fd9\u8bf4\u660e\nA. \u72ec\u7acb\u81ea\u4e3b\u7684\u548c\u5e73\u5916\u4ea4\u653f\u7b56\u662f\u65b0\u4e2d\u56fd\u5916\u4ea4\u7684\u6839\u672c\u4fdd\u8bc1\nB. \u201c\u6c42\u540c\u5b58\u5f02\u201d\u662f\u6211\u56fd\u5904\u7406\u4e0e\u4e9a\u975e\u56fd\u5bb6\u5173\u7cfb\u7684\u57fa\u672c\u539f\u5219\nC. \u4e0e\u82cf\u8054\u7ed3\u76df\uff0c\u662f\u62b5\u6297\u7f8e\u56fd\u5bf9\u4e2d\u56fd\u4fb5\u7565\u4e0e\u5a01\u80c1\u7684\u6839\u672c\u4fdd\u8bc1\nD. \u5728\u4e2d\u82cf\u7ed3\u76df\u65f6\u671f\uff0c\u6211\u56fd\u7684\u5916\u4ea4\u653f\u7b56\u4e0e\u82cf\u8054\u4fdd\u6301\u9ad8\u5ea6\u4e00\u81f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9352198004687693, "meta-math/MetaMath-Mistral-7B": 0.9904586802626014, "itpossible/Chinese-Mistral-7B-v0.1": 0.7972341987386318, "HuggingFaceH4/zephyr-7b-beta": 0.9221148322345277, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9693923097678149, "meta-llama/Meta-Llama-3-8B": 0.4834360564796363, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6162\u6027\u708e\u75c7\u75c5\u7076\u5185\u6d78\u6da6\u7684\u708e\u6027\u7ec6\u80de\u4e3b\u8981\u4e3a\nA. \u4e2d\u6027\u7c92\u7ec6\u80de\u548c\u6d46\u7ec6\u80de\nB. \u5355\u6838\u7ec6\u80de\u548c\u6dcb\u5df4\u7ec6\u80de\nC. \u55dc\u9178\u6027\u7c92\u7ec6\u80de\u548c\u55dc\u78b1\u6027\u7c92\u7ec6\u80de\nD. \u6dcb\u5df4\u7ec6\u80de\u548c\u55dc\u9178\u6027\u7c92\u7ec6\u80de\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5103692653014452, "meta-math/MetaMath-Mistral-7B": 0.4706646634564346, "itpossible/Chinese-Mistral-7B-v0.1": 0.514040618363349, "HuggingFaceH4/zephyr-7b-beta": 0.825043636532496, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5940327556981279, "meta-llama/Meta-Llama-3-8B": 0.7255908612694709, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u6d25\u6db2\u4e0d\u5177\u6709\u76f4\u63a5\u56fa\u6444\u4f5c\u7528\u7684\u662f\nA. \u809d\nB. \u80ba\nC. \u80be\nD. \u813e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2906893535433972, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2906893535433972, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4547847236170506}}, {"question": "\u5728Windows2000\u4e2d\uff0c\u7528\u201c\u521b\u5efa\u5feb\u6377\u65b9\u5f0f\u201d\u521b\u5efa\u7684\u56fe\u6807A\nA. \u53ea\u80fd\u662f\u7a0b\u5e8f\u6587\u4ef6\u548c\u6587\u6863\u6587\u4ef6\nB. \u53ef\u4ee5\u662f\u4efb\u4f55\u6587\u4ef6\u6216\u6587\u4ef6\u5939\nC. \u6587\u4ef6\u5939\u4e0d\u80fd\u521b\u5efa\u5feb\u6377\u65b9\u5f0f\nD. \u53ea\u80fd\u662f\u53ef\u6267\u884c\u7a0b\u5e8f\u6216\u7a0b\u5e8f\u7ec4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8437294242715767, "meta-math/MetaMath-Mistral-7B": 0.9035644072075134, "itpossible/Chinese-Mistral-7B-v0.1": 0.720157884343024, "HuggingFaceH4/zephyr-7b-beta": 0.9999101485023979, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9503675099460601, "meta-llama/Meta-Llama-3-8B": 0.7780726468340582, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9901639787937185}}, {"question": "\u7f8e\u82cf\u51b7\u6218\u5168\u9762\u5c55\u5f00\u7684\u6807\u5fd7\u662f\nA. \u4e18\u5409\u5c14\u7684\u94c1\u5e55\u6f14\u8bf4\nB. \u4e24\u5927\u9635\u8425\u7684\u5f62\u6210\nC. \u9a6c\u6b47\u5c14\u8ba1\u5212\nD. \u675c\u9c81\u95e8\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u5c0f\u9ea6\u7684\u6e29\u5149\u53cd\u5e94\u7279\u6027\uff0c\u5c0f\u9ea6\u5c5e\u4e8e\nA. \u4f4e\u6e29\u957f\u65e5\u4f5c\u7269\nB. \u4f4e\u6e29\u77ed\u65e5\u4f5c\u7269\nC. \u9ad8\u6e29\u957f\u65e5\u4f5c\u7269\nD. \u9ad8\u6e29\u77ed\u65e5\u4f5c\u7269\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5e38\u4eba\u65b9\u6cd5\u5b66\u7684\u521b\u59cb\u4eba\u662f\nA. \u6208\u592b\u66fc\nB. \u52a0\u82ac\u514b\u5c14\nC. \u5e93\u5229\nD. \u7c73\u5fb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3160424181481997, "HuggingFaceH4/zephyr-7b-beta": 0.27312720402870727, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f53\u5185\u552f\u4e00\u4e0d\u80fd\u5408\u6210\u7684\u8425\u517b\u7d20\u662f\nA. \u77ff\u7269\u8d28\nB. \u86cb\u767d\u8d28\nC. \u7ef4\u751f\u7d20\nD. \u78b3\u6c34\u5316\u5408\u7269\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e16\u754c\u4e0a\u6700\u65e9\u7684\u901a\u8baf\u793e\u662f\nA. \u54c8\u74e6\u65af\u901a\u8baf\u793e\nB. \u7f8e\u8054\u793e\nC. \u6cd5\u65b0\u793e\nD. \u8def\u900f\u793e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4010179288407139, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9099091623745171, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.37354501646787847, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ec7\u5973\u661f\u662f\u4e00\u9897\u4f4d\u4e8e\u4e3b\u5e8f\u5e26\u4e0a\u7684\u5178\u578b\u7684AO\u578b\u6052\u661f\uff0c\u5176\u7edd\u5bf9\u661f\u7b49\u7ea6\u4e3a0.7\u7b49\uff0c\u5728\u4e0d\u8003\u8651\u6d88\u5149\u7684\u524d\u63d0\u4e0b\uff0c\u4e00\u9897\u89c6\u661f\u7b49\u4e3a15\u7b49\u7684\u540c\u7c7b\u578b\u6052\u661f\u8ddd\u79bb\u6211\u4eec\u5927\u7ea6\u591a\u8fdc\uff1f\uff08\uff09\nA. 8000\u79d2\u5dee\u8ddd\nB. 20000\u79d2\u5dee\u8ddd\nC. 500\u79d2\u5dee\u8ddd\nD. 3000\u79d2\u5dee\u8ddd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2731272040287072, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8def\u7fce\u7684\u957f\u7bc7\u5c0f\u8bf4\u300a\u8d22\u4e3b\u5e95\u513f\u5973\u4eec\u300b\u7b2c\u4e8c\u90e8\u6700\u7740\u529b\u63cf\u5199\u7684\u4e3b\u4eba\u516c\u662f\nA. \u91d1\u7d20\u75d5\nB. \u848b\u6377\u4e09\nC. \u848b\u7eaf\u7956\nD. \u848b\u6170\u7956\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.34182412994643013, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5fc3\u529b\u8870\u7aed\u7ec6\u80de\u662f\u6307\nA. \u5fc3\u529b\u8870\u7aed\u65f6\u80ba\u6ce1\u5185\u542b\u6709\u542b\u94c1\u8840\u9ec4\u7d20\u7684\u5de8\u566c\u7ec6\u80de\nB. \u5fc3\u529b\u8870\u7aed\u65f6\u80ba\u5185\u541e\u566c\u70ad\u672b\u7684\u5de8\u566c\u7ec6\u80de\nC. \u5fc3\u529b\u8870\u7aed\u65f6\u80ba\u5185\u7684\u6ce1\u6cab\u7ec6\u80de\nD. \u5fc3\u529b\u8870\u7aed\u65f6\u51fa\u73b0\u7684\u542b\u8102\u8910\u7d20\u7684\u5fc3\u808c\u7ec6\u80de\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9187810803230596}}, {"question": "Internet\u91c7\u7528\u7684\u534f\u8bae\u7c7b\u578b\u4e3a\nA. TCP/IP\nB. FTP\nC. IPX/SPX\nD. HTTP\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9876376700298697, "meta-math/MetaMath-Mistral-7B": 0.9995771889190667, "itpossible/Chinese-Mistral-7B-v0.1": 0.9526864240120834, "HuggingFaceH4/zephyr-7b-beta": 0.9998553492139585, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9906318394714605, "meta-llama/Meta-Llama-3-8B": 0.9765230818598554, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9995204248363427}}, {"question": "\u6c22\u5143\u7d20\u5728\u5b87\u5b99\u4e2d\u4e0d\u53ef\u80fd\u5b58\u5728\u7684\u5f62\u6001\u5305\u62ec\nA. H2\nB. HII\nC. HI\nD. HIII\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7528427379583604, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b0\u95fb\u89c2\u6307\u7684\u662f\nA. \u5bf9\u65b0\u95fb\u7684\u4e00\u822c\u770b\nB. \u5bf9\u65b0\u95fb\u7684\u603b\u7684\u770b\u6cd5\nC. \u5bf9\u65b0\u95fb\u7684\u5177\u4f53\u770b\u6cd5\nD. \u5bf9\u65b0\u95fb\u7684\u6b63\u786e\u770b\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39855676920675376, "meta-math/MetaMath-Mistral-7B": 0.553076263398462, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9508907033400573, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38726019115386007, "meta-llama/Meta-Llama-3-8B": 0.8407569268919917, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.723743622709101}}, {"question": "\u4e0b\u5217\u63cf\u8ff0\u4e2d\u4e0d\u6b63\u786e\u7684\u662f\nA. \u4e32\u8054\u7535\u963b\u53ef\u4ee5\u5206\u538b\nB. \u5e76\u8054\u7535\u963b\u53ef\u4ee5\u5206\u6d41\nC. \u7535\u963b\u4e32\u8054\u8d8a\u591a\u963b\u503c\u8d8a\u5927\nD. \u7535\u8def\u5e76\u8054\u8d8a\u591a\u963b\u503c\u8d8a\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5080054710816758, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "0.375\u7684\u8ba1\u6570\u5355\u4f4d\u662f\nA. 0.01\nB. \u65e0\u6cd5\u786e\u5b9a\nC. 0.1\nD. 0.001\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.37354501646787847, "HuggingFaceH4/zephyr-7b-beta": 0.9611598547097628, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d44\u672c\u5faa\u73af\u7684\u4e09\u79cd\u804c\u80fd\u5f62\u5f0f\u662f\nA. \u4e0d\u53d8\u8d44\u672c\u3001\u53ef\u53d8\u8d44\u672c\u3001\u6d41\u901a\u8d44\u672c\nB. \u8d27\u5e01\u8d44\u672c\u3001\u751f\u4ea7\u8d44\u672c\u3001\u5546\u54c1\u8d44\u672c\nC. \u4ea7\u4e1a\u8d44\u672c\u3001\u5546\u4e1a\u8d44\u672c\u3001\u501f\u8d37\u8d44\u672c\nD. \u56fa\u5b9a\u8d44\u672c\u3001\u6d41\u52a8\u8d44\u672c\u3001\u751f\u4ea7\u8d44\u672c \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3204793169644583, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3376787962540697, "HuggingFaceH4/zephyr-7b-beta": 0.6988583168940703, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4703069298158966, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5306718982446057}}, {"question": "\u4e00\u4e2a\u56e0\u6570\u6269\u5927\u5230\u539f\u6765\u768410\u500d\uff0c\u53e6\u4e00\u4e2a\u56e0\u6570\u7f29\u5c0f\u7684\u539f\u6765\u76841/20\uff0c\u5b83\u4eec\u7684\u79ef\nA. \u7f29\u5c0f\u5230\u539f\u6765\u76841/2\nB. \u7f29\u5c0f\u5230\u539f\u6765\u76841/10\nC. \u6269\u5927\u5230\u539f\u6765\u768410\u500d\nD. \u6269\u5927\u5230\u539f\u6765\u76842\u500d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4109840110062891}}, {"question": "\u673a\u5668\u5b66\u4e60\u4e2d\u505a\u7279\u5f81\u9009\u62e9\u65f6\uff0c\u53ef\u80fd\u7528\u5230\u7684\u65b9\u6cd5\u6709\nA. \u4ee5\u4e0a\u90fd\u6709\nB. \u5361\u65b9\nC. \u4fe1\u606f\u589e\u76ca\nD. \u671f\u671b\u4ea4\u53c9\u71b5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8896151307717564, "meta-math/MetaMath-Mistral-7B": 0.9098020404265684, "itpossible/Chinese-Mistral-7B-v0.1": 0.8450001061142349, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7148718238090964, "meta-llama/Meta-Llama-3-8B": 0.9249418040408821, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.971171260510912}}, {"question": "\u5047\u5b9a\u4f60\u4f7f\u7528SVM\u5b66\u4e60\u6570\u636eX\uff0c\u6570\u636eX\u91cc\u9762\u6709\u4e9b\u70b9\u5b58\u5728\u9519\u8bef\u3002\u73b0\u5728\u5982\u679c\u4f60\u4f7f\u7528\u4e00\u4e2a\u4e8c\u6b21\u6838\u51fd\u6570\uff0c\u591a\u9879\u5f0f\u9636\u6570\u4e3a2\uff0c\u4f7f\u7528\u677e\u5f1b\u53d8\u91cfC\u4f5c\u4e3a\u8d85\u53c2\u4e4b\u4e00\u3002 \u5f53\u4f60\u4f7f\u7528\u8f83\u5927\u7684C\uff08C\u8d8b\u4e8e\u65e0\u7a77\uff09\uff0c\u5219\uff1a\nA. \u4ee5\u4e0a\u5747\u4e0d\u6b63\u786e\nB. \u4e0d\u786e\u5b9a\nC. \u4e0d\u80fd\u6b63\u786e\u5206\u7c7b\nD. \u4ecd\u7136\u80fd\u6b63\u786e\u5206\u7c7b\u6570\u636e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5988336996037611, "meta-math/MetaMath-Mistral-7B": 0.5987341392007919, "itpossible/Chinese-Mistral-7B-v0.1": 0.39196259414668827, "HuggingFaceH4/zephyr-7b-beta": 0.9940156823160657, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7783518882898143, "meta-llama/Meta-Llama-3-8B": 0.553076263398462, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6769303846680397}}, {"question": "\u6211\u4eec\u5403\u7684\u6700\u591a\u7684\u8f6c\u57fa\u56e0\u7684\u98df\u54c1\u662f\nA. \u6c34\u679c\nB. \u852c\u83dc\nC. \u82b1\u751f\nD. \u5927\u8c46\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7397811841942176, "meta-math/MetaMath-Mistral-7B": 0.8850509643171024, "itpossible/Chinese-Mistral-7B-v0.1": 0.709542591233837, "HuggingFaceH4/zephyr-7b-beta": 0.9897312071319699, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7319938466497047, "meta-llama/Meta-Llama-3-8B": 0.5771828251811251, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u4e0d\u5c5e\u4e8e\u5e02\u573a\u57fa\u672c\u8981\u7d20\u7684\u662f\nA. \u8d2d\u4e70\u80fd\u529b\nB. \u8d2d\u4e70\u6b32\u671b\nC. \u6d88\u8d39\u8005\nD. \u4ea7\u54c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5334623210199778, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.325455072595945, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8412864678984917}}, {"question": "\u795e\u7ecf\u5904\u4e8e\u9759\u606f\u7535\u4f4d\u65f6\nA. \u6ca1\u6709K\u5916\u6d41\u4e5f\u6ca1\u6709Na\u5185\u6d41\nB. \u65e2\u6709K\u5916\u6d41\u4e5f\u6709\u5c11\u91cfNa\u5185\u6d41\nC. \u4ec5\u6709\u5c11\u91cfK\u5916\u6d41\nD. \u4ec5\u6709\u5c11\u91cfNa\u5185\u6d41\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33065623127838456, "meta-math/MetaMath-Mistral-7B": 0.40544190707321726, "itpossible/Chinese-Mistral-7B-v0.1": 0.42948401419559634, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.37578678665765913, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5485293121364198}}, {"question": "\u53d4\u672c\u534e\u7684\u4eba\u751f\u6001\u5ea6\u662f\u4ec0\u4e48\uff1f\nA. \u4ed6\u76f8\u4fe1\u6765\u4e16\uff0c\u6240\u6709\u7684\u82e6\u96be\u90fd\u4f1a\u6d88\u5931\u3002\nB. \u4ed6\u8ba4\u4e3a\u751f\u6d3b\u4e0d\u786e\u5b9a\u4f46\u503c\u5f97\u3002\nC. \u4ed6\u8ba4\u4e3a\u751f\u6d3b\u5f88\u8270\u96be\uff0c\u4f46\u6700\u7ec8\u5145\u6ee1\u5e0c\u671b\u3002\nD. \u4ed6\u8ba4\u4e3a\u751f\u6d3b\u5931\u53bb\u4e86\u610f\u4e49\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.949719662452294, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c60\u5c81\uff0c\u95f4\u65ad\u6c34\u80bf3\u5e74\uff0c\u52a0\u91cd1\u4e2a\u6708\uff0c\u6c14\u77ed\u3001\u5c3f\u5c112\u5929\u3002\u65e2\u5f80\u7cd6\u5c3f\u75c5\u75c5\u53f22\u5e74\u3002\u67e5\u4f53\uff1a\u8840\u538b150/90mmHg\uff0c\u79fb\u52a8\u6027\u6d4a\u97f3\u9633\u6027\uff0c\u4e0b\u80a2\u660e\u663e\u6c34\u80bf\u3002\u8f85\u52a9\u68c0\u67e5\uff1a\u5c3f\u86cb\u767d(++++)\uff0c\u7ea2\u7ec6\u80de0-2/HP\uff0c\u8840\u6d46\u767d\u86cb\u767d20g/L\u3002\u8be5\u60a3\u8005\u6700\u4e3b\u8981\u7684\u6cbb\u7597\u836f\u7269\u662f\nA. \u7cd6\u76ae\u8d28\u6fc0\u7d20\nB. \u8840\u7ba1\u7d27\u5f20\u7d20\u8f6c\u6362\u9176\u6291\u5236\u5242\nC. \u5229\u5c3f\u5242\nD. \u80f0\u5c9b\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4965194293553776, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u5173\u8840\u7cd6\u751f\u6210\u6307\u6570\u7684\u63cf\u8ff0\u9519\u8bef\u7684\u662f\nA. \u8840\u7cd6\u751f\u6210\u6307\u6570\u8d8a\u9ad8\uff0c\u8840\u7cd6\u5347\u9ad8\u8d8a\u8d8b\u7f13\u548c\nB. \u8840\u7cd6\u751f\u6210\u6307\u6570\u662f\u8861\u91cf\u98df\u7269\u5f15\u8d77\u9910\u540e\u8840\u7cd6\u53cd\u5e94\u7684\u4e00\u9879\u6709\u6548\u6307\u6807\nC. \u8840\u7cd6\u751f\u6210\u6307\u6570\u5373GI\u503c\uff0c\u662f\u98df\u7269\u7684\u4e00\u79cd\u751f\u7406\u5b66\u53c2\u6570\nD. \u8840\u7cd6\u751f\u6210\u6307\u6570\u662f\u8868\u793a\u542b50\u514b\u6709\u4ef7\u503c\u7684\u78b3\u6c34\u5316\u5408\u7269\u7684\u98df\u7269\u548c\u76f8\u5f53\u91cf\u7684\u8461\u8404\u7cd6\u6216\u9762\u5305\u57282\u5c0f\u65f6\u5185\u4f53\u5185\u8840\u7cd6\u5e94\u7b54\u6c34\u5e73\u767e\u5206\u6bd4\u503c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3176135680841276, "meta-math/MetaMath-Mistral-7B": 0.5879875206886312, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7067123450468359, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0d\u540c\u65f6\u671f\u3001\u5730\u57df\u3001\u6c11\u65cf\u548c\u9636\u5c42\u4e2d\u751f\u6d3b\u7684\u4eba\u7684\u601d\u60f3\u3001\u54c1\u884c\u3001\u624d\u80fd\u548c\u4e60\u6027\uff0c\u65e0\u4e0d\u6253\u4e0a\u5386\u53f2\u3001\u5730\u57df\u3001\u6c11\u65cf\u548c\u9636\u5c42\u7684\u70d9\u5370\uff0c\u8868\u73b0\u51fa\u5f88\u5927\u7684\u5dee\u5f02\uff0c\u8fd9\u79cd\u73b0\u8c61\u8bf4\u660e\u7684\u5f71\u54cd\u4eba\u53d1\u5c55\u7684\u56e0\u7d20\u662f\nA. \u793e\u4f1a\u73af\u5883\nB. \u4e2a\u4f53\u5b9e\u8df5\nC. \u6559\u80b2\u5f71\u54cd\nD. \u9057\u4f20\u7d20\u8d28\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8618406349809237, "meta-math/MetaMath-Mistral-7B": 0.9934531226488662, "itpossible/Chinese-Mistral-7B-v0.1": 0.9087593056051356, "HuggingFaceH4/zephyr-7b-beta": 0.9996886779375016, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8786866218345162, "meta-llama/Meta-Llama-3-8B": 0.8337381116388408, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9744722433293682}}, {"question": "\u300a\u7ec4\u7ec7\u90e8\u6765\u4e86\u4e2a\u5e74\u8f7b\u4eba\u300b\u4e2d\uff0c\u5bf9\u6797\u9707\u7684\u5f62\u8c61\u5206\u6790\u4e0d\u6b63\u786e\u7684\u4e00\u9879\u662f\nA. \u9762\u5bf9\u73b0\u5b9e\u751f\u6d3b\u4e2d\u7684\u79cd\u79cd\u590d\u6742\u73b0\u8c61\uff0c\u4ed6\u611f\u5230\u8fc7\u56f0\u60d1\u3001\u5f77\u5fa8\uff0c\u5e76\u4e0d\u662f\u4e00\u4e2a\u6210\u719f\u7684\u9769\u547d\u8005\u3002\nB. \u4ed6\u6ee1\u6000\u70ed\u60c5\u5730\u8e0f\u5165\u793e\u4f1a\uff0c\u5bf9\u65e7\u89c2\u5ff5\u3001\u65e7\u4e8b\u7269\u62b1\u7740\u8d28\u7591\u3001\u6279\u5224\u7684\u6001\u5ea6\uff0c\u4e0d\u4e3a\u9648\u89c4\u964b\u4e60\u6240\u675f\u7f1a\u3002\nC. \u6797\u9707\u662f\u4e00\u4e2a\u6709\u7406\u60f3\u3001\u6709\u671d\u6c14\u3001\u5bcc\u4e8e\u539f\u5219\u6027\u548c\u6b63\u4e49\u611f\u7684\u9752\u5e74\u515a\u5458\u5e72\u90e8\u3002\nD. \u4ed6\u6709\u4e00\u5b9a\u7684\u9769\u547d\u7ecf\u5386\uff0c\u6709\u80fd\u529b\u3001\u6709\u9b44\u529b\uff0c\u61c2\u5f97\u201c\u9886\u5bfc\u827a\u672f\u201d\uff0c\u77e5\u9053\u5982\u4f55\u53bb\u6293\u91cd\u70b9\uff0c\u201c\u4e0b\u51b3\u5fc3\u201d\u5c31\u53ef\u4ee5\u628a\u5de5\u4f5c\u505a\u5f97\u5f88\u51fa\u8272\u3002\u4f46\u5374\u4e0d\u4e3b\u52a8\u53bb\u6293\u5de5\u4f5c\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4243954657834915, "meta-math/MetaMath-Mistral-7B": 0.6054070090922251, "itpossible/Chinese-Mistral-7B-v0.1": 0.48890764282820565, "HuggingFaceH4/zephyr-7b-beta": 0.7276887117443269, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8198160645334471, "meta-llama/Meta-Llama-3-8B": 0.4965193995544993, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u4f8b\u5b50\u4e2d\uff0c\u4e0d\u5168\u662f\u81ea\u4e3b\u52a8\u8bcd\u7684\u6709\nA. \u8d70\u3001\u7b11\nB. \u5403\u3001\u7761\nC. \u5199\u3001\u6d17\nD. \u77e5\u9053\u3001\u60f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.27416108226793107, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3908304748611699, "HuggingFaceH4/zephyr-7b-beta": 0.655467996133395, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4349598892232299, "meta-llama/Meta-Llama-3-8B": 0.33544561004293627, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u68c0\u67e54.2-2.65=1.65\u5bf9\u4e0d\u5bf9\uff0c\u53ef\u4ee5\u7528()\u9a8c\u7b97\nA. 2.65+1.65\nB. 2.65\u20141.65\nC. 4.2-1.65\nD. 4.2+1.65\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53\u547d\u9898\u65ad\u8a00\u4e86\u4e3b\u9879\u6216\u8c13\u9879\u6240\u6307\u79f0\u7684\u7c7b\u7684\u6bcf\u4e00\u6210\u5458\u65f6\uff0c\u6211\u4eec\u5c31\u8bf4\u8fd9\u4e2a\u8bcd\u9879\u662f\nA. \u4e0d\u5468\u5ef6\u7684\nB. \u5047\u7684\nC. \u5468\u5ef6\u7684\nD. \u771f\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7940807189934587, "meta-math/MetaMath-Mistral-7B": 0.9732209804221159, "itpossible/Chinese-Mistral-7B-v0.1": 0.7884983766594591, "HuggingFaceH4/zephyr-7b-beta": 0.8354405987312936, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6372738049368161, "meta-llama/Meta-Llama-3-8B": 0.7017516450373159, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7806449271346022}}, {"question": "\u89e3\u51b3\u6295\u8d44\u4e89\u7aef\u56fd\u9645\u4e2d\u5fc3\u7ba1\u8f96\u7684\u5bf9\u8c61\u4ec5\u9650\u4e8e\nA. \u4e0d\u540c\u56fd\u5bb6\u7684\u4e2a\u4eba\u6216\u6cd5\u4eba\u4e4b\u95f4\u7684\u6295\u8d44\u4e89\u8bae\nB. \u56fd\u5bb6\u4e0e\u56fd\u9645\u7ec4\u7ec7\u4e4b\u95f4\u7684\u6295\u8d44\u4e89\u8bae\nC. \u4e00\u65b9\u4e3a\u7f14\u7ea6\u56fd\uff0c\u53e6\u4e00\u65b9\u4e3a\u4ed6\u56fd\u56fd\u6c11\u4e4b\u95f4\u7684\u6295\u8d44\u4e89\u8bae\nD. \u4e0d\u540c\u56fd\u5bb6\u653f\u5e9c\u95f4\u7684\u6295\u8d44\u4e89\u8bae\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41014736605061436, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9072097644382978, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.567840840022051, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f51\u7edc\u871c\u7f50\u6280\u672f\u662f\u4e00\u79cd\u4e3b\u52a8\u9632\u5fa1\u6280\u672f\uff0c\u662f\u5165\u4fb5\u68c0\u6d4b\u6280\u672f\u7684\u4e00\u4e2a\u91cd\u8981\u53d1\u5c55\u65b9\u5411\uff0c\u4ee5\u4e0b\u6709\u5173\u871c\u7f50\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f\nA. \u871c\u7f50\u7cfb\u7edf\u662f\u4e00\u4e2a\u5305\u542b\u6f0f\u6d1e\u7684\u8bf1\u9a97\u7cfb\u7edf\uff0c\u5b83\u901a\u8fc7\u6a21\u62df\u4e00\u4e2a\u6216\u8005\u591a\u4e2a\u6613\u53d7\u653b\u51fb\u7684\u4e3b\u673a\u548c\u670d\u52a1\uff0c\u7ed9\u653b\u51fb\u8005\u63d0\u4f9b\u4e00\u4e2a\u5bb9\u6613\u653b\u51fb\u7684\u76ee\u6807\nB. \u5982\u679c\u6ca1\u4eba\u653b\u51fb\uff0c\u871c\u7f50\u7cfb\u7edf\u5c31\u53d8\u5f97\u6beb\u65e0\u610f\u4e49\nC. \u871c\u7f50\u7cfb\u7edf\u4f1a\u76f4\u63a5\u63d0\u9ad8\u8ba1\u7b97\u673a\u7f51\u7edc\u5b89\u5168\u7b49\u7ea7\uff0c\u662f\u5176\u4ed6\u5b89\u5168\u7b56\u7565\u4e0d\u53ef\u66ff\u4ee3\u7684\nD. \u4f7f\u7528\u871c\u7f50\u6280\u672f\uff0c\u53ef\u4ee5\u4f7f\u76ee\u6807\u7cfb\u7edf\u5f97\u4ee5\u4fdd\u62a4\uff0c\u4fbf\u4e8e\u7814\u7a76\u5165\u4fb5\u8005\u7684\u653b\u51fb\u884c\u4e3a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7935374179698319, "meta-math/MetaMath-Mistral-7B": 0.851711888723062, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9932124862076054, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.925334011949169, "meta-llama/Meta-Llama-3-8B": 0.8661722280049705, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8156002623691274}}, {"question": "\u7531\u6837\u672c\u8ba1\u7b97\u4e24\u4e2a\u968f\u673a\u53d8\u91cfx\u548cy\u4e4b\u95f4\u7684\u7b80\u5355\u76f8\u5173\u7cfb\u6570r\u7684\u503c\u8fd1\u4f3c\u7b49\u4e8e\u96f6\uff0c\u7ecf\u7edf\u8ba1\u68c0\u9a8c\u5f97\u5230p=0.90\u3002\u4f5c\u7ed3\u8bba\u65f6\uff0c\u6b63\u786e\u7684\u8868\u8ff0\u5e94\u8be5\u662f\nA. x\u4e0ey\u4e4b\u95f4\u6beb\u65e0\u5173\u7cfb\nB. x\u4e0ey\u4e4b\u95f4\u5448\u76f4\u7ebf\u5173\u7cfb\nC. x\u4e0ey\u4e4b\u95f4\u5448\u66f2\u7ebf\u5173\u7cfb\nD. x\u4e0ey\u4e4b\u95f4\u6ca1\u6709\u76f4\u7ebf\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe $f(x)$ \u8fde\u7eed\uff0c \u5219\u5728\u4e0b\u5217\u53d8\u4e0a\u9650\u79ef\u5206\u4e2d\uff0c \u5fc5\u4e3a\u5076\u51fd\u6570\u7684\u662f ( ).\nA. $\\int_0^x f\\left(t^2\\right) \\mathrm{d} t$\nB. $\\int_0^x t[f(t)-f(-t)] \\mathrm{d} t$\nC. $\\int_0^x f^2(t) \\mathrm{d} t$\nD. $\\int_0^x t[f(t)+f(-t)] \\mathrm{d} t$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.33065623127838456, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679c\u4e00\u9897\u5468\u5e74\u89c6\u5dee\u4e3a0.0111\u89d2\u79d2\u7684\u6052\u661f\u57285\u5e74\u5185\u5728\u661f\u7a7a\u4e2d\u79fb\u52a8\u4e860.1\u4e2a\u89d2\u79d2\u3002\u90a3\u4e48\u5b83\u7684\u5207\u5411\u901f\u5ea6\u6700\u53ef\u80fd\u4e3a\uff08\uff09km/s\nA. 2.8\nB. 8.5\nC. 6.1\nD. 4.3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.35888231301687173, "itpossible/Chinese-Mistral-7B-v0.1": 0.2801288226217134, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6483555050707083}}, {"question": "\u7537\u6027\uff0c48 \u5c81\u30021\u4e2a\u6708\u6765\u6c14\u77ed\u3001\u547c\u5438\u56f0\u96be\uff0c1\u5468\u6765\u53d1\u70ed\u3001\u54b3\u55fd\uff0cB\u8d85\u53d1\u73b0\u201c\u53f3\u4fa7\u5927\u91cf\u80f8\u8154\u79ef\u6db2\u201d\u3002\u8fd1\u534a\u5e74\u65e5\u6e10\u6d88\u7626\u3002\u67e5\u4f53:T37.5C\uff0cR 21\u6b21/\u5206\uff0c\u8f7b\u5ea6\u8d2b\u8840\u8c8c\uff0c\u9ad8\u6795\u53f3\u4fa7\u5367\u4f4d\u3002\u8be5\u60a3\u8005\u80f8\u90e8\u542c\u8bca\u4e0d\u53ef\u80fd\u51fa\u73b0\u7684\u4f53\u5f81\u662f\nA. \u53f3\u4e2d\u4e0b\u80ba\u8bed\u97f3\u5171\u632f\u6d88\u5931\nB. \u53f3\u4e0b\u814b\u524d\u7ebf\u90e8\u53ef\u95fb\u53ca\u80f8\u819c\u6469\u64e6\u97f3\nC. \u53f3\u4e0a\u80ba\u53ef\u95fb\u53ca\u652f\u6c14\u7ba1\u547c\u5438\u97f3\nD. \u5de6\u4e0a\u80ba\u53ef\u95fb\u53ca\u652f\u6c14\u7ba1\u80ba\u6ce1\u547c\u5438\u97f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9256739012569895, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c06\u6bcf\u4e00\u9879\u7ecf\u6d4e\u4e1a\u52a1\u6240\u6d89\u53ca\u7684\u5404\u4e2a\u4f1a\u8ba1\u79d1\u76ee\u5355\u72ec\u7f16\u5236\u8bb0\u8d26\u51ed\u8bc1\uff0c\u6bcf\u5f20\u51ed\u8bc1\u53ea\u586b\u5217\u4e00\u4e2a\u4f1a\u8ba1\u79d1\u76ee\u53ca\u5176\u91d1\u989d\u7684\u8bb0\u8d26\u51ed\u8bc1\uff0c\u79f0\u4e3a\nA. \u5355\u5f0f\u8bb0\u8d26\u51ed\u8bc1\nB. \u590d\u5f0f\u8bb0\u8d26\u51ed\u8bc1\nC. \u6c47\u603b\u8bb0\u8d26\u51ed\u8bc1\nD. \u8d37\u9879\u8bb0\u8d26\u51ed\u8bc1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6348161133439102, "meta-math/MetaMath-Mistral-7B": 0.9290676522380015, "itpossible/Chinese-Mistral-7B-v0.1": 0.7249409957969993, "HuggingFaceH4/zephyr-7b-beta": 0.997694721315527, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7834068142134449, "meta-llama/Meta-Llama-3-8B": 0.6605489565573717, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5416368554194418}}, {"question": "\u4e0b\u5217\u4e0d\u5c5e\u4e8e\u4fb5\u72af\u5546\u4e1a\u79d8\u5bc6\u7684\u662f\nA. \u4ee5\u76d7\u7a83\u3001\u8d3f\u8d42\u3001\u6b3a\u8bc8\u3001\u80c1\u8feb\u3001\u7535\u5b50\u4fb5\u5165\u6216\u8005\u5176\u4ed6\u4e0d\u6b63\u5f53\u624b\u6bb5\u83b7\u53d6\u6743\u5229\u4eba\u7684\u5546\u4e1a\u79d8\u5bc6\nB. \u62ab\u9732\u3001\u4f7f\u7528\u6216\u8005\u5141\u8bb8\u4ed6\u4eba\u4f7f\u7528\u4ee5\u524d\u9879\u624b\u6bb5\u83b7\u53d6\u7684\u6743\u5229\u4eba\u7684\u5546\u4e1a\u79d8\u5bc6\nC. \u901a\u8fc7\u7ec4\u7ec7\u865a\u5047\u4ea4\u6613\u3001\u7f16\u9020\u865a\u5047\u4fe1\u606f\u7b49\u65b9\u5f0f\uff0c\u5e2e\u52a9\u5176\u4ed6\u7ecf\u8425\u8005\u8fdb\u884c\u865a\u5047\u6216\u5f15\u4eba\u8bef\u89e3\u7684\u5546\u4e1a\u5ba3\u4f20\nD. \u8fdd\u53cd\u4fdd\u5bc6\u4e49\u52a1\u6216\u8005\u8fdd\u53cd\u6743\u5229\u4eba\u6709\u5173\u4fdd\u5b88\u5546\u4e1a\u79d8\u5bc6\u7684\u8981\u6c42\uff0c\u62ab\u9732\u3001\u4f7f\u7528\u6216\u8005\u5141\u8bb8\u4ed6\u4eba\u4f7f\u7528\u5176\u6240\u957f\u63e1\u7684\u5546\u4e1a\u79d8\u5bc6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.555119875540615, "meta-math/MetaMath-Mistral-7B": 0.7327609232120303, "itpossible/Chinese-Mistral-7B-v0.1": 0.474069820068834, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.595431924510928, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9256137974756671}}, {"question": "\u6839\u636e\u7ecf\u6d4e\u6cd5\u8c03\u6574\u7684\u7279\u5b9a\u7ecf\u6d4e\u5173\u7cfb\u7684\u8981\u6c42\uff0c\u7ecf\u6d4e\u6cd5\u4e3b\u4f53\u5305\u62ec\u54ea\u56db\u7c7b\nA. \u56fd\u5bb6\u673a\u5173\u3001\u4f01\u4e1a\u548c\u5176\u4ed6\u793e\u4f1a\u7ec4\u7ec7\u3001\u519c\u6237\u548c\u4e2a\u4f53\u7ecf\u8425\u6237\u53ca\u516c\u6c11\u3001\u516c\u53f8\nB. \u6cd5\u4eba\u3001\u516c\u53f8\u3001\u516c\u6c11\u3001\u8054\u8425\u4f01\u4e1a\nC. \u6709\u9650\u8d23\u4efb\u516c\u53f8\u3001\u80a1\u4efd\u6709\u9650\u516c\u53f8\u3001\u79c1\u4eba\u4f01\u4e1a\u3001\u4e2a\u4f53\u5de5\u5546\u6237\nD. \u56fd\u5bb6\u673a\u5173\u3001\u4f01\u4e1a\u548c\u5176\u4ed6\u793e\u4f1a\u7ec4\u7ec7\u3001\u4f01\u4e1a\u7684\u5185\u90e8\u7ec4\u7ec7\u548c\u6709\u5173\u4eba\u5458\u3001\u519c\u6237\u548c\u4e2a\u4f53\u7ecf\u8425\u6237\u53ca\u516c\u6c11\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5647482012962287, "meta-math/MetaMath-Mistral-7B": 0.5827367073400778, "itpossible/Chinese-Mistral-7B-v0.1": 0.49914298100196, "HuggingFaceH4/zephyr-7b-beta": 0.9967570677550778, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6951852669448927, "meta-llama/Meta-Llama-3-8B": 0.8453459605597048, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.37727078502384037}}, {"question": "\u7532\u8bf4\u4e59\u80d6\uff0c\u4e59\u8bf4\u4e19\u80d6\uff0c\u4e19\u548c\u4e01\u90fd\u8bf4\u81ea\u5df1\u4e0d\u80d6\u3002 \u5982\u679c\u56db\u4eba\u9648\u8ff0\u53ea\u6709\u4e00\u4eba\u9519\uff0c\u90a3\u4e48\u8c01\u4e00\u5b9a\u80d6?\nA. \u4ec5\u4e19\nB. \u4ec5\u7532\nC. \u4ec5\u4e59\u548c\u4e19\nD. \u4ec5\u4e59\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5062980411941875, "itpossible/Chinese-Mistral-7B-v0.1": 0.3870998271060946, "HuggingFaceH4/zephyr-7b-beta": 0.9313570568053547, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bd7\u8bcd\u6b4c\u8d4b\u65e2\u662f\u5386\u4ee3\u6587\u4eba\u58a8\u5ba2\u548f\u6000\u3001\u8bb0\u6e38\u3001\u8a00\u5fd7\u7684\u6587\u5b66\u8868\u73b0\u5f62\u5f0f\uff0c\u4e5f\u5f80\u5f80\u8574\u542b\u7740\u4e30\u5bcc\u7684\u793e\u4f1a\u5386\u53f2\u5185\u5bb9\u3002\u4e0b\u5217\u6587\u53e5\uff0c\u4e0e\u5546\u4e1a\u7ecf\u6d4e\u65e0\u76f4\u63a5\u5173\u8054\u7684\u662f\nA. \u201c\u5ca2\u5ce8\u5927\u8236\u768e\u4e91\u65e5\uff0c\u8d3e\u5ba2\u5343\u5bb6\u4e07\u5bb6\u5ba4\u201d\uff08\u300a\u5e7f\u5dde\u6b4c\u300b\uff09\nB. \u201c\u8d1d\u9526\u6590\u6210\uff0c\u6fef\u8272\u6c5f\u6ce2\u201d\uff08\u300a\u8700\u90fd\u8d4b\u300b\uff09\nC. \u201c\u7ecf\u6e38\uff08\u8425\uff09\u5929\u4e0b\u904d\uff0c\u5374\u5230\u957f\u5b89\u57ce\u201d\uff08\u300a\u4f30\u5ba2\u4e50\u300b\uff09\nD. \u201c\u4e5d\u5e02\u5f00\u573a\uff0c\u8d27\u522b\u96a7\u5206\u201d\uff08\u300a\u897f\u90fd\u8d4b\u300b\uff09\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3976839419458531, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38812073416459414, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.47038960370145844}}, {"question": "\u5728\u6211\u56fd\uff0c\u4e13\u95e8\u7684\u6cd5\u5f8b\u76d1\u7763\u673a\u5173\u4e3a\nA. \u76d1\u5bdf\u673a\u5173\nB. \u5168\u56fd\u4eba\u5927\u6cd5\u5f8b\u59d4\u5458\u4f1a\nC. \u4eba\u6c11\u6cd5\u9662\nD. \u4eba\u6c11\u68c0\u5bdf\u9662\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.47906600495874585, "meta-math/MetaMath-Mistral-7B": 0.7335048666930114, "itpossible/Chinese-Mistral-7B-v0.1": 0.6238889014215926, "HuggingFaceH4/zephyr-7b-beta": 0.5060658099536283, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u76ee\u524d\u53d1\u9175\u5de5\u4e1a\u5927\u591a\u6570\u91c7\u7528\uff08\uff09\u6765\u5236\u5907\u5927\u91cf\u7684\u65e0\u83cc\u7a7a\u6c14\nA. \u9759\u7535\u9664\u83cc\nB. \u5c04\u7ebf\u706d\u83cc\nC. \u52a0\u70ed\u706d\u83cc\nD. \u4ecb\u8d28\u8fc7\u6ee4\u9664\u83cc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9294744794196629, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u906d\u9047\u5730\u9707\u65f6\uff0c\u5982\u679c\u5ba4\u5185\u6ca1\u6709\u53ef\u85cf\u8eab\u7684\u5bb6\u5177\u600e\u4e48\u529e\nA. \u5730\u9707\u53d1\u751f\u65f6\uff0c\u82e5\u5ba4\u5185\u65e0\u53ef\u85cf\u8eab\u7684\u5bb6\u5177\uff0c\u5e94\u7acb\u5373\u8eb2\u5230\u5899\u6839\u3001\u5899\u89d2\u6216\u8fdc\u79bb\u7a97\u5b50\u7684\u5ba4\u5185\u95e8\u9053\u91cc\nB. \u8eab\u4f53\u5e94\u7d27\u8d34\u5899\u6839\u3001\u5899\u89d2\uff0c\u5934\u90e8\u5c3d\u91cf\u9760\u8fd1\u5899\u9762\nC. \u5176\u4ed6\u9009\u9879\u5747\u53ef\nD. \u53ef\u968f\u624b\u53d6\u4e9b\u88ab\u8925\u3001\u6795\u5934\uff0c\u63a9\u4f4f\u81ea\u5df1\u7684\u5934\u90e8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ef4\u751f\u7d20A\u7f3a\u4e4f\u65f6\uff0c\u53ef\u80fd\u51fa\u73b0\u7684\u75c7\u72b6\u5305\u62ec\nA. \u94f6\u5c51\u75c5\nB. \u6bdb\u56ca\u4e0a\u76ae\u8fc7\u5ea6\u89d2\u5316\nC. \u76ae\u4e0b\u51fa\u8840\u70b9\nD. \u8102\u6ea2\u6027\u76ae\u708e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6391815534376057, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8246212567511064}}, {"question": "\u5728\u516c\u5173\u4eba\u5458\u5fc3\u7406\u7d20\u8d28\u4e2d\uff0c\u5176\u6700\u57fa\u672c\u7684\u8981\u6c42\u662f\nA. \u70ed\u60c5\u5fc3\u7406\nB. \u81ea\u4fe1\u5fc3\u7406\nC. \u521b\u65b0\u5fc3\u7406\nD. \u5f00\u653e\u5fc3\u7406\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.42886486310629984, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u9009\u9879\u4e2d\u4e0d\u5c5e\u4e8e\u6c11\u65cf\u57fa\u672c\u7279\u5f81\u7684\u662f\nA. \u5171\u540c\u98ce\u4fd7\u4e60\u60ef\nB. \u5171\u540c\u5730\u57df\nC. \u5171\u540c\u8840\u7edf\nD. \u5171\u540c\u8bed\u8a00\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4596151265178266, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.42684410673804984, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u4e00\u60a3\u8005\u80a1\u9aa8\u9aa8\u6298\uff0c\u884c\u52a8\u56f0\u96be\uff0c\u9760\u62c4\u62d0\u6756\u884c\u8d70\uff0c\u4e09\u4e2a\u6708\u540e\u4e0b\u80a2\u5b8c\u5168\u6062\u590d\u529f\u80fd\uff0c\u4f46\u51fa\u73b0\u60a3\u4fa7\u80a9\u5173\u8282\u5916\u5c55\u529f\u80fd\u969c\u788d\uff0c\u4f53\u6014\u8868\u73b0\u4e3a\u201c\u65b9\u80a9\u201d\u7578\u5f62\uff0c\u53ef\u80fd\u662f\u4e0b\u5217\u54ea\u4e2a\u808c\u635f\u4f24\uff1a\nA. \u4e09\u89d2\u808c\nB. \u5188\u4e0a\u808c\nC. \u659c\u89d2\u808c\nD. \u659c\u65b9\u808c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5e8a\u524d\u660e\u6708\u5149\u201d\u662f\u674e\u767d\u7684\u5343\u53e4\u540d\u53e5\uff0c\u5176\u4e2d\u201c\u5e8a\u201d\u6307\u7684\u662f\u4ec0\u4e48\nA. \u4e95\u4e0a\u7684\u56f4\u680f\nB. \u5367\u5177\nC. \u95e8\nD. \u7a97\u6237\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.36385828438381157, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7535\u8bdd\u62a5\u8b66\u65f6\u5e94\u8bb2\u6e05\u695a\uff1a\u706b\u707e\u53d1\u751f\u5730\u70b9\u7684\u5355\u4f4d\u540d\u79f0()\u548c\u70e7\u4f55\u7269\u54c1;\u5728\u54ea\u4e00\u5c42\u7740\u706b;\u62a5\u8b66\u4eba\u7684\u59d3\u540d\u548c\u7535\u8bdd\u53f7\u7801\nA. \u5730\u5740\nB. \u6027\u8d28\nC. \u6cd5\u4eba\u540d\u79f0\nD. AC\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.41129263448237996, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6828361477362936, "meta-llama/Meta-Llama-3-8B": 0.4349598965451875, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u57281\uff0c2\uff0c3\uff0c\u2026\uff0c10\u8fd910\u4e2a\u6570\u5b57\u4e2d\uff0c\u4efb\u53d63\u4e2a\u6570\u5b57\uff0c\u90a3\u4e48\u201c\u8fd9\u4e09\u4e2a\u6570\u5b57\u7684\u548c\u2f24\u4e8e6\u201d\u8fd9\u2f00\u4e8b\u4ef6\u662f\nA. \u4ee5\u4e0a\u9009\u9879\u5747\u4e0d\u6b63\u786e\nB. \u5fc5\u7136\u4e8b\u4ef6 \nC. \u4e0d\u53ef\u80fd\u4e8b\u4ef6\nD. \u968f\u673a\u4e8b\u4ef6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43637106587891944, "meta-math/MetaMath-Mistral-7B": 0.5540052805157856, "itpossible/Chinese-Mistral-7B-v0.1": 0.5621826482101557, "HuggingFaceH4/zephyr-7b-beta": 0.9999790045921272, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.37897239237143665, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u8fdb\u884c\u7ade\u4e89\u8005\u5206\u6790\u65f6\uff0c\u4f01\u4e1a\u9996\u5148\u8981\u505a\u7684\u662f\nA. \u786e\u5b9a\u7ade\u4e89\u8005\u7684\u76ee\u6807\u4e0e\u6218\u7565\nB. \u5efa\u7acb\u4f01\u4e1a\u7ade\u4e89\u60c5\u62a5\u7cfb\u7edf\nC. \u5224\u65ad\u7ade\u4e89\u8005\u7684\u5e02\u573a\u53cd\u5e94\nD. \u8bc6\u522b\u4f01\u4e1a\u7ade\u4e89\u8005\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6748164781787358, "meta-math/MetaMath-Mistral-7B": 0.9271804469697436, "itpossible/Chinese-Mistral-7B-v0.1": 0.6028536529189226, "HuggingFaceH4/zephyr-7b-beta": 0.9995632911889606, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6996400840151182, "meta-llama/Meta-Llama-3-8B": 0.9191740381149425, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.890646915838499}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u4e2d\u5b66\u201c\u8150\u4e73\u7684\u5236\u4f5c\u201d\u5b9e\u9a8c\uff0c\u53d9\u8ff0\u6b63\u786e\u7684\u662f\nA. \u52a0\u6599\u9152\u4e3b\u8981\u662f\u4e3a\u4e86\u706d\u83cc\uff0c\u907f\u514d\u8150\u4e73\u53d8\u8d28\nB. \u5b9e\u9a8c\u5ba4\u5236\u4f5c\u7684\u8150\u4e73\u4e0d\u5b9c\u76f4\u63a5\u98df\u7528\nC. \u53d1\u9175\u8fc7\u7a0b\u4e2d\u8d77\u4e3b\u8981\u4f5c\u7528\u7684\u662f\u4e73\u9178\u6746\u83cc\nD. \u52a0\u76d0\u4e3b\u8981\u662f\u4e3a\u4e86\u8c03\u8282\u6c34\u5206\uff0c\u5229\u4e8e\u6bdb\u9709\u751f\u957f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f15\u8d77\u547c\u5438\u5546\u589e\u5927\u7684\u662f\nA. \u4ee3\u8c22\u6027\u78b1\u4e2d\u6bd2\nB. \u80ba\u901a\u6c14\u8fc7\u5ea6\nC. \u957f\u671f\u9965\u997f\nD. \u7cd6\u5c3f\u75c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3160424181481997, "meta-math/MetaMath-Mistral-7B": 0.37897238681311185, "itpossible/Chinese-Mistral-7B-v0.1": 0.6035369486284418, "HuggingFaceH4/zephyr-7b-beta": 0.688724664139903, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5229506674151697, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u809d\u529f\u80fd\u4e0d\u5168\u75c5\u4eba\u80a0\u5916\u8425\u517b\u65f6\u5e94\u9009\u7528\u7684\u836f\u7269\u662f\nA. \u8c37\u6c28\u9170\u80fa\nB. \u7cbe\u6c28\u9178\nC. \u6838\u82f7\u9178\nD. \u652f\u94fe\u6c28\u57fa\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2821833983601388, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u52a8\u7269\u6fc0\u7d20\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u8840\u6db2\u4e2d\u80f0\u5c9b\u7d20\u589e\u52a0\u53ef\u4fc3\u8fdb\u80f0\u5c9bB\u7ec6\u80de\u5206\u6ccc\u80f0\u9ad8\u8840\u7cd6\u7d20\nB. \u901a\u8fc7\u5bf9\u8f6c\u5f55\u7684\u8c03\u8282\u53ef\u5f71\u54cd\u86cb\u767d\u8d28\u7c7b\u6fc0\u7d20\u7684\u5408\u6210\u91cf\nC. \u5207\u9664\u52a8\u7269\u5782\u4f53\u540e\uff0c\u8840\u6db2\u4e2d\u751f\u957f\u6fc0\u7d20\u7684\u6d53\u5ea6\u4e0b\u964d\nD. \u673a\u4f53\u5185\u3001\u5916\u73af\u5883\u7684\u53d8\u5316\u53ef\u5f71\u54cd\u6fc0\u7d20\u7684\u5206\u6ccc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4618304854370594, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.583521287440094, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5665306976013421}}, {"question": "\u9ad8\u65af\u6df7\u5408\u6a21\u578b(GMM)\u662f\u4e00\u79cd\u4ec0\u4e48\u6a21\u578b\nA. \u65e0\u76d1\u7763\u5b66\u4e60\u6a21\u578b\nB. \u5176\u4ed6\u9009\u9879\u90fd\u4e0d\u662f\nC. \u534a\u76d1\u7763\u5b66\u4e60\u6a21\u578b\nD. \u6709\u76d1\u7763\u5b66\u4e60\u6a21\u578b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9152818393121233, "meta-math/MetaMath-Mistral-7B": 0.9979408071303478, "itpossible/Chinese-Mistral-7B-v0.1": 0.3785916633064982, "HuggingFaceH4/zephyr-7b-beta": 0.9999879623300284, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.947250294992465, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9130208654439513}}, {"question": "\u5728\u5f53\u4eca\u56fd\u9645\u793e\u4f1a\uff0c\u8c03\u6574\u5916\u56fd\u4eba\u6c11\u4e8b\u8bc9\u8bbc\u5730\u4f4d\u7684\u4e00\u822c\u539f\u5219\u662f\nA. \u56fd\u6c11\u5f85\u9047\u539f\u5219\nB. \u666e\u904d\u4f18\u60e0\u5f85\u9047\u539f\u5219\nC. \u6700\u60e0\u56fd\u5f85\u9047\u539f\u5219\nD. \u4e0d\u6b67\u89c6\u5f85\u9047\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.31712010892822357, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3742744353722653}}, {"question": "\u6d66\u80af\u91ce\u7ea4\u7ef4\u7ec6\u80de\u548c\u5fc3\u5ba4\u808c\u7ec6\u80de\u52a8\u4f5c\u7535\u4f4d\u7684\u533a\u522b\u4e3b\u8981\u662f\nA. 4\u671f\u81ea\u52a8\u9664\u6781\u6709\u65e0\nB. 1\u671f\u5f62\u6210\u673a\u5236\u4e0d\u540c\nC. 3\u671f\u590d\u6781\u901f\u5ea6\u4e0d\u540c\nD. \u5e73\u53f0\u671f\u6301\u7eed\u65f6\u95f4\u76f8\u5dee\u8f83\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.46382610972719474, "meta-llama/Meta-Llama-3-8B": 0.2731272040287072, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f5b\u6559\u7684\u76ee\u4e2d\u6709\u201c\u4eba\u201d\u610f\u5473\u7740\u5b83\u7684\u6240\u6709\u6559\u6cd5\u5fc5\u987b\u662f\u4ece\u4e09\u4e2a\u89d2\u5ea6\u6765\u5b9e\u73b0\u7684\u3002\u8fd9\u6837\u624d\u80fd\u771f\u6b63\u79f0\u4e3a\u4e00\u79cd\u6559\u6cd5\uff0c\u8fd9\u4e09\u4e2a\u89d2\u5ea6\u4e0d\u5305\u62ec\nA. \u5951\u673a\nB. \u6210\u5c31\nC. \u5951\u7406\nD. \u89c9\u609f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ec4\u7ec7\u5185\u90e8\u7684\u6c9f\u901a\u8fc7\u7a0b\u5c31\u662f\nA. \u5206\u4eab\u4fe1\u606f\nB. \u534f\u4f5c\nC. \u89c2\u5ff5\u8d8b\u4e8e\u4e00\u81f4\nD. \u4e92\u52a9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6398183781927611, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5587467640052256, "meta-llama/Meta-Llama-3-8B": 0.27416108226793107, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8f66\u8f86\u9a76\u8fd1\u4eba\u884c\u6a2a\u9053\u65f6\uff0c\u5e94\u600e\u6837\u505a\nA. \u9e23\u5587\u53ed\u793a\u610f\u884c\u4eba\u8ba9\u9053\nB. \u7acb\u5373\u505c\u8f66\nC. \u5148\u51cf\u901f\u6ce8\u610f\u89c2\u5bdf\u884c\u4eba\u3001\u975e\u673a\u52a8\u8f66\u52a8\u6001\uff0c\u786e\u8ba4\u5b89\u5168\u540e\u518d\u901a\u8fc7\nD. \u52a0\u901f\u901a\u8fc7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7548500046036867, "meta-math/MetaMath-Mistral-7B": 0.8994758498407192, "itpossible/Chinese-Mistral-7B-v0.1": 0.895686853984005, "HuggingFaceH4/zephyr-7b-beta": 0.9881839272306485, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9101628456641805, "meta-llama/Meta-Llama-3-8B": 0.9710122861059528, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9814370210397291}}, {"question": "\u88ab\u9a6c\u514b\u601d\u6069\u683c\u65af\u79f0\u4e3a\u201c\u6709\u53f2\u4ee5\u6765\u6700\u4f1f\u5927\u7684\u8bbd\u523a\u5bb6\u201d\u7684\u662f\nA. \u5085\u7acb\u53f6\nB. \u8d39\u5c14\u5df4\u54c8\nC. \u5723\u897f\u95e8\nD. \u6b27\u6587\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.423442608492013, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5f53\u6211\u8d70\u5230\u6276\u68af\u6700\u540e\u4e00\u8282\uff0c\u8fc8\u6b65\u5411\u4ed6\uff08\u5468\u6069\u6765\uff09\u8d70\u53bb\u65f6\uff0c\u7279\u610f\u4f38\u51fa\u624b\u53bb\u2026\u2026\u201d\uff08\u300a\u5c3c\u514b\u677e\u56de\u5fc6\u5f55\u300b\uff09\u3002\u7f8e\u56fd\u603b\u7edf\u5c3c\u514b\u677e\u201c\u4f38\u624b\u201d\u7684\u539f\u56e0\u662f\nA. \u4e3b\u5f20\u5efa\u7acb\u548c\u5e73\u76f8\u5904\u7684\u56fd\u9645\u5173\u7cfb\nB. \u6b4c\u9882\u4e2d\u534e\u6c11\u65cf\u7684\u4f1f\u5927\nC. \u8ba4\u8bc6\u5230\u7f8e\u56fd\u4e0e\u4e2d\u56fd\u53d1\u5c55\u5173\u7cfb\u7684\u5fc5\u8981\u6027\nD. \u9610\u660e\u4e2d\u56fd\u5bf9\u7a33\u5b9a\u56fd\u9645\u5173\u7cfb\u7684\u91cd\u8981\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6190372724566078, "meta-math/MetaMath-Mistral-7B": 0.8767757754094737, "itpossible/Chinese-Mistral-7B-v0.1": 0.41588984558503955, "HuggingFaceH4/zephyr-7b-beta": 0.9959517582159929, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8479565706635767, "meta-llama/Meta-Llama-3-8B": 0.8380311260046079, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8781488916632354}}, {"question": "\u201c\u5916\u65e0\u52b3\u5f62\u4e4b\u4e8b\uff0c\u5185\u65e0\u601d\u60f3\u4e4b\u60a3\u201d\u51fa\u81ea\uff1a\nA. \u5468\u6613\nB. \u9053\u5fb7\u7ecf\nC. \u9ec4\u5e1d\u5185\u7ecf\nD. \u8001\u5b50\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b89\u5fbd\u8d3e\u6e56\u9057\u5740\u4e2d\u917f\u9020\u9152\u7c7b\u7684\u53d1\u73b0\u8bc1\u660e\u4e86\nA. \u81ea\u7136\u8d44\u6e90\u51cf\u5c11\u63a8\u52a8\u4e86\u5f53\u5730\u519c\u4e1a\nB. \u5f53\u5730\u6587\u5316\u5f53\u65f6\u5df2\u8fdb\u5165\u519c\u4e1a\u793e\u4f1a\nC. \u4eba\u53e3\u538b\u529b\u589e\u5927\u63a8\u52a8\u4e86\u5f53\u5730\u519c\u4e1a\nD. \u7ade\u4e89\u5bb4\u98e8\u73b0\u8c61\u63a8\u52a8\u4e86\u5f53\u5730\u519c\u4e1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u793e\u4f1a\u6210\u5458\u7ecf\u7531\u6559\u80b2\u7684\u57f9\u517b\uff0c\u5237\u9009\u548c\u63d0\u9ad8\uff0c\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u793e\u4f1a\u533a\u57df\u3001\u793e\u4f1a\u5c42\u6b21\u3001\u804c\u4e1a\u5c97\u4f4d\u4ee5\u53ca\u79d1\u5c42\u7ec4\u7ec7\u4e4b\u95f4\u8f6c\u5316\u548c\u8c03\u52a8\u3002\u8fd9\u79cd\u6559\u80b2\u529f\u80fd\u662f\nA. \u6587\u5316\u4f20\u9012\u529f\u80fd\nB. \u793e\u4f1a\u6d41\u52a8\u6027\u529f\u80fd\nC. \u793e\u4f1a\u6539\u9020\u529f\u80fd\nD. \u4eba\u53e3\u63a7\u5236\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9256975911725831, "meta-math/MetaMath-Mistral-7B": 0.9906315529127662, "itpossible/Chinese-Mistral-7B-v0.1": 0.5949640769364805, "HuggingFaceH4/zephyr-7b-beta": 0.999868515925087, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9747236940805969, "meta-llama/Meta-Llama-3-8B": 0.9428402083622937, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9841902435426946}}, {"question": "\u65e0\u4ea7\u9636\u7ea7\u9769\u547d\u53d6\u5f97\u80dc\u5229\u7684\u6839\u672c\u4fdd\u8bc1\u662f\nA. \u56fd\u5bb6\u653f\u6743\u95ee\u9898\nB. \u65e0\u4ea7\u9636\u7ea7\u653f\u515a\u7684\u6b63\u786e\u9886\u5bfc\nC. \u5efa\u7acb\u9769\u547d\u7684\u7edf\u4e00\u6218\u7ebf \nD. \u4eba\u6c11\u7fa4\u4f17\u7684\u9769\u547d\u79ef\u6781\u6027\u7684\u6781\u5927\u63d0\u9ad8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4567454578920176, "meta-math/MetaMath-Mistral-7B": 0.9345561791222982, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9030772250726383, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6957742425497034, "meta-llama/Meta-Llama-3-8B": 0.5133415387873755, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b9\u5dee\u5206\u6790\u4e2d\u5bf9\u6570\u636e\u7684\u8981\u6c42\u662f________\u3002\nA. \u4efb\u4f55\u4e24\u4e2a\u89c2\u5bdf\u503c\u4e4b\u95f4\u5747\u4e0d\u76f8\u5173\nB. \u6bcf\u4e00\u6c34\u5e73\u4e0b\u7684\u89c2\u5bdf\u503c\u5206\u522b\u670d\u4ece\u603b\u4f53\u5747\u6570\u4e3a$\\mu_i$\u7684\u6b63\u6001\u5206\u5e03\nC. A\uff0cB\u548cC\u5747\u5bf9\nD. \u5404\u603b\u4f53\u7684\u65b9\u5dee\u9f50\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u75c5\u6bd2\u611f\u67d3\u7ec6\u80de\u7684\u5bbf\u4e3b\u8303\u56f4\u4e3b\u8981\u53d6\u51b3\u4e8e\nA. \u75c5\u6bd2\u8868\u9762\u7684\u63a5\u89e6\u86cb\u767d\nB. \u7ec6\u80de\u8868\u9762\u7684\u53d7\u4f53\nC. \u75c5\u6bd2\u8868\u9762\u7684\u8840\u51dd\u7d20\nD. \u75c5\u6bd2\u7684\u8863\u58f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5be1\u5934\u5784\u65ad\u7684\u4e00\u4e2a\u663e\u8457\u7279\u5f81\u662f\nA. \u6709\u4e00\u6761\u5f39\u6027\u7684\u4f9b\u7ed9\u66f2\u7ebf\nB. \u4f01\u4e1a\u4e4b\u95f4\u4e92\u76f8\u4f9d\u5b58\nC. \u6709\u4e00\u6761\u975e\u5f39\u6027\u7684\u9700\u6c42\u66f2\u7ebf\nD. \u4e0d\u5b58\u5728\u5e02\u573a\u8fdb\u5165\u969c\u788d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.37354501646787847, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5e72\u6027\u652f\u6c14\u7ba1\u6269\u5f20\u75c7\u7684\u4e3b\u8981\u4e34\u5e8a\u8868\u73b0\u662f\nA. \u53cd\u590d\u54af\u8840\nB. \u5e72\u54b3\nC. \u6775\u72b6\u6307\nD. \u5927\u91cf\u54b3\u75f0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e8b\u7269\u7684\u6982\u5ff5\uff0c\u5728\u67cf\u62c9\u56fe\u8fd9\u91cc\u5177\u6709\u4ec0\u4e48\u6837\u7684\u610f\u4e49\u3002\nA. \u5b9e\u8df5\u8bba\nB. \u552f\u5fc3\u8bba\nC. \u8ba4\u8bc6\u8bba\nD. \u672c\u4f53\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6268009800252023, "HuggingFaceH4/zephyr-7b-beta": 0.7442719044108487, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5482603600826604, "meta-llama/Meta-Llama-3-8B": 0.7507613924277533, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9539332912685132}}, {"question": "\u300a\u65e5\u51fa\u300b\u7684\u7ed3\u6784\u91c7\u7528\u7684\u662f\nA. \u6563\u70b9\u900f\u89c6\u6cd5\nB. \u56de\u6eaf\u6cd5\nC. \u62f4\u6869\u6cd5\nD. \u5fc3\u7406\u5206\u6790\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3246546169818852, "meta-math/MetaMath-Mistral-7B": 0.49088001564004957, "itpossible/Chinese-Mistral-7B-v0.1": 0.3403147063768956, "HuggingFaceH4/zephyr-7b-beta": 0.79916565808759, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.39730623368822765, "meta-llama/Meta-Llama-3-8B": 0.3256774323330271, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u751f\u4ea7\u4e0d\u7b26\u5408\u98df\u54c1\u5b89\u5168\u6807\u51c6\u7684\u98df\u54c1\u6216\u8005\u9500\u552e\u660e\u77e5\u662f\u4e0d\u7b26\u5408\u98df\u54c1\u5b89\u5168\u6807\u51c6\u7684\u98df\u54c1\uff0c\u6d88\u8d39\u8005\u9664\u8981\u6c42\u8d54\u507f\u635f\u5931\u5916\uff0c\u8fd8\u53ef\u4ee5\u5411\u751f\u4ea7\u8005\u6216\u8005\u9500\u552e\u8005\u8981\u6c42\u652f\u4ed8\u4ef7\u6b3e\u591a\u5c11\u500d\u7684\u8d54\u507f\u91d1\nA. 5\nB. 2\nC. 4\nD. 10\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.33065623127838456, "HuggingFaceH4/zephyr-7b-beta": 0.9767883536805241, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u822c\u60c5\u51b5\u4e0b\u5f71\u54cd\u8212\u5f20\u538b\u6700\u4e3b\u8981\u7684\u56e0\u7d20\u662f\nA. \u5916\u5468\u963b\u529b\nB. \u6bcf\u640f\u8f93\u51fa\u91cf\nC. \u5fc3\u7387\nD. \u5927\u52a8\u8109\u7ba1\u58c1\u5f39\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.48617934653148487, "itpossible/Chinese-Mistral-7B-v0.1": 0.3148300531811561, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5327251063694514, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.42089214458112645}}, {"question": "\u5ba1\u5224\u7684\u65f6\u5019\u6000\u5b55\u7684\u5987\u5973\u4f9d\u6cd5\u4e0d\u9002\u7528\u6b7b\u5211\u3002\u5bf9\u8fd9\u4e00\u89c4\u5b9a\u7684\u7406\u89e3\uff0c\u4e0b\u5217\u9009\u9879\u9519\u8bef\u7684\u662f\nA. \u5173\u62bc\u671f\u95f4\u4eba\u5de5\u6d41\u4ea7\u7684\uff0c\u5c5e\u4e8e\u5ba1\u5224\u7684\u65f6\u5019\u6000\u5b55\u7684\u5987\u5973\nB. \u4e0d\u9002\u7528\u6b7b\u5211\uff0c\u662f\u6307\u4e0d\u9002\u7528\u6b7b\u5211\u7acb\u5373\u6267\u884c\u4f46\u53ef\u9002\u7528\u6b7b\u7f13\nC. \u4e0d\u9002\u7528\u6b7b\u5211\uff0c\u65e2\u5305\u62ec\u4e0d\u9002\u7528\u6b7b\u5211\u7acb\u5373\u6267\u884c\uff0c\u4e5f\u5305\u62ec\u4e0d\u9002\u7528\u6b7b\u7f13\nD. \u5173\u62bc\u671f\u95f4\u81ea\u7136\u6d41\u4ea7\u7684\uff0c\u5c5e\u4e8e\u5ba1\u5224\u7684\u65f6\u5019\u6000\u5b55\u7684\u5987\u5973\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5314002005047017, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.34031470637689565, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u60c5\u5f62\u4e2d\uff0c\u4e0d\u5c5e\u4e8e\u300a\u5211\u6cd5\u300b\u7b2c263\u6761\u89c4\u5b9a\u7684\u5bf9\u62a2\u52ab\u7f6a\u52a0\u91cd\u6cd5\u5b9a\u5211\u7684\u60c5\u5f62\u662f\nA. \u5165\u6237\u62a2\u52ab\u7684\nB. \u62a2\u52ab\u81f4\u4eba\u91cd\u4f24\u3001\u6b7b\u4ea1\u7684\nC. \u5728\u516c\u5171\u4ea4\u901a\u5de5\u5177\u4e0a\u62a2\u52ab\u7684\nD. \u5728\u529e\u516c\u5927\u697c\u62a2\u52ab\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6061547449349209, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5472805498585421, "meta-llama/Meta-Llama-3-8B": 0.45092007275187534, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u708e\u5e1d\u662f\u6211\u56fd\u4f20\u8bf4\u65f6\u4ee3\u7684\u82f1\u96c4\uff0c\u53c8\u79f0\u795e\u519c\u6c0f\u3002\u636e\u4f20\u4ed6\u4f7f\u7528\u201c\u711a\u6797\u800c\u7530\u201d\u7684\u8015\u4f5c\u65b9\u6cd5\uff0c\u8fd8\u53d1\u660e\u4e86\u8012\u3001\u801c\u7b49\u751f\u4ea7\u5de5\u5177\u3002\u8fd9\u4e9b\u4f20\u8bf4\nA. \u53ef\u4ee5\u4f5c\u4e3a\u4e2d\u56fd\u539f\u59cb\u519c\u8015\u7684\u76f4\u63a5\u8bc1\u636e\nB. \u8bc1\u660e\u4e86\u708e\u5e1d\u7684\u771f\u5b9e\u5b58\u5728\nC. \u662f\u4e00\u79cd\u6ca1\u6709\u53f2\u6599\u4ef7\u503c\u7684\u60f3\u8c61\nD. \u6298\u5c04\u51fa\u4e2d\u56fd\u519c\u8015\u6587\u660e\u6e90\u8fdc\u6d41\u957f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9052245624119994, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.47975862509603195, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8102\u80aa\u7684\u529f\u80fd\u6027\u548c\u6027\u8d28\u8868\u73b0\u5728\u5f88\u591a\u7528\u9014\u4e2d\uff0c\u9664\u4e86\u4ee5\u4e0b\u54ea\u4e2a\u7528\u9014\u5916\nA. \u8f6c\u8fd0\nB. \u4e73\u72b6\u6db2\nC. \u901a\u98ce\u548c\u53d1\u9175\nD. \u8d77\u9165\u548c\u5ae9\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.27727478132119343, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5404\u7ec4\u5b57\uff0c\u5168\u90e8\u5c5e\u4e8e\u4e49\u7b26\u76f8\u540c\u7684\u5f62\u58f0\u5b57\u7684\u4e00\u7ec4\u662f\nA. \u54c0\u8877\u521d\nB. \u982d\u9838\u9898\nC. \u554f\u805e\u95dc\nD. \u5e55\u8305\u8349\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3104582110496687, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.3075818303735863, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6bdb\u6cfd\u4e1c\u5728\u8c08\u53ca\u5b66\u751f\u65f6\u4ee3\u559c\u7231\u9605\u8bfb\u7684\u4e00\u4efd\u51fa\u7248\u7269\u65f6\u8bf4\uff1a\u201c\u6211\u5f88\u6b23\u8d4f\u80e1\u9002\u548c\u9648\u72ec\u79c0\u5199\u7684\u6587\u7ae0\uff0c\u4ed6\u4eec\u4e00\u5ea6\u6210\u4e3a\u6211\u6548\u4eff\u7684\u699c\u6837\uff0c\u53d6\u4ee3\u4e86\u6211\u5df2\u4e0d\u518d\u5d07\u62dc\u7684\u6881\u542f\u8d85\u548c\u5eb7\u6709\u4e3a\u3002\u201d\u8fd9\u4efd\u51fa\u7248\u7269\u5e94\u8be5\u662f\nA. \u300a\u65b0\u9752\u5e74\u300b\nB. \u300a\u6c11\u62a5\u300b\nC. \u300a\u65f6\u52a1\u62a5\u300b\nD. \u300a\u7533\u62a5\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2801288226217134, "meta-math/MetaMath-Mistral-7B": 0.6539544092657174, "itpossible/Chinese-Mistral-7B-v0.1": 0.6799657255577782, "HuggingFaceH4/zephyr-7b-beta": 0.7128373497320332, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.44350432477873025, "meta-llama/Meta-Llama-3-8B": 0.9467787640143456, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9033243714491751}}, {"question": "\u6ee1\u65cf\u5efa\u7acb\u4e86\u4e00\u7edf\u5168\u56fd\u5c06\u8fd1\u4e09\u767e\u5e74\u7684\u6e05\u738b\u671d\uff0c\u5bf9\u7956\u56fd\u7586\u57df\u7684\u5f00\u62d3\u548c\u5de9\u56fa\u505a\u4e86\u7684\u5386\u53f2\u8d21\u732e\u9664\u4e86\u4e0b\u5217\u54ea\u9879\nA. \u6539\u9769\u5f00\u653e\nB. \u7edf\u4e00\u4e86\u897f\u5357\u5730\u533a\u7684\u884c\u653f\u7ba1\u7406\u4f53\u7cfb\nC. \u5bf9\u897f\u85cf\u7684\u793e\u4f1a\u8fdb\u884c\u4e86\u6539\u9769\nD. \u7edf\u4e00\u53f0\u6e7e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5927\u53f6\u6027\u80ba\u708e\u7684\u5e76\u53d1\u75c7\uff0c\u9519\u8bef\u7684\u662f\nA. \u80ba\u8113\u80bf\nB. \u80ba\u8089\u8d28\u53d8\nC. \u80ba\u8910\u8272\u786c\u53d8\nD. \u8113\u80f8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3512536987011174, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4329196405880833, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.36078609119776345}}, {"question": "\u4fe1\u606f\u7f51\u7edc\u5b89\u5168\u7684\u7b2c\u4e09\u4e2a\u65f6\u4ee3\u662f\nA. \u4e3b\u673a\u65f6\u4ee3\uff0cPC\u673a\u65f6\u4ee3\uff0c\u7f51\u7edc\u65f6\u4ee3\nB. 2001\u5e74\uff0c2002\u5e74\uff0c2003\u5e74\nC. \u4e3b\u673a\u65f6\u4ee3\uff0c\u4e13\u7f51\u65f6\u4ee3\uff0c\u591a\u7f51\u5408\u4e00\u65f6\u4ee3\nD. PC\u673a\u65f6\u4ee3\uff0c\u7f51\u7edc\u65f6\u4ee3\uff0c\u4fe1\u606f\u65f6\u4ee3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4895760742220338, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6cd5\u5f8b\u7ec8\u6b62\u751f\u6548\u662f\u6cd5\u5f8b\u65f6\u95f4\u6548\u529b\u7684\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002\u5728\u4ee5\u9ed8\u793a\u5e9f\u6b62\u65b9\u5f0f\u7ec8\u6b62\u6cd5\u5f8b\u751f\u6548\u65f6\uff0c\u4e00\u822c\u5e94\u5f53\u9009\u62e9\u4e0b\u5217\u54ea\u4e00\u539f\u5219\uff1f\nA. \u540e\u6cd5\u4f18\u4e8e\u524d\u6cd5\nB. \u6cd5\u5f8b\u4f18\u4e8e\u884c\u653f\u6cd5\u89c4\nC. \u7279\u522b\u6cd5\u4f18\u4e8e\u4e00\u822c\u6cd5\nD. \u56fd\u9645\u6cd5\u4f18\u4e8e\u56fd\u5185\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36788233129305675, "meta-math/MetaMath-Mistral-7B": 0.6526398958820568, "itpossible/Chinese-Mistral-7B-v0.1": 0.4184552437211469, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7728897823289541, "meta-llama/Meta-Llama-3-8B": 0.5746624561183111, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7490323854793027}}, {"question": "\u7532\u4f01\u4e1a\u4e3a\u4e00\u5bb6\u5927\u578b\u5546\u4e1a\u6d41\u901a\u4f01\u4e1a\uff0c\u4e3b\u8981\u7ecf\u9500\u5bb6\u7535\u4ea7\u54c1\uff0c\u5176\u8d26\u52a1\u5904\u7406\u4f7f\u7528\u6c47\u603b\u8bb0\u8d26\u51ed\u8bc1\u8d26\u52a1\u5904\u7406\u7a0b\u5e8f\u3002\u4e0b\u5217\u5404\u9879\u4e2d\uff0c\u5c5e\u4e8e\u7532\u4f01\u4e1a\u6708\u672b\u767b\u8bb0\u603b\u8d26\u4f9d\u636e\u7684\u662f\nA. \u539f\u59cb\u51ed\u8bc1\u6c47\u603b\u8868\nB. \u8bb0\u8d26\u51ed\u8bc1\nC. \u6c47\u603b\u8bb0\u8d26\u51ed\u8bc1\nD. \u539f\u59cb\u51ed\u8bc1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5393885576575951, "meta-math/MetaMath-Mistral-7B": 0.9191212317095809, "itpossible/Chinese-Mistral-7B-v0.1": 0.8049257238947897, "HuggingFaceH4/zephyr-7b-beta": 0.9884739003460057, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7266470638389265, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7737997671348029}}, {"question": "\u5c71\u533a\u6cb3\u6d41\u7684\u6c34\u9762\u6bd4\u964d\u4e00\u822c\u6bd4\u5e73\u539f\u6cb3\u6d41\u7684\u6c34\u9762\u6bd4\u964d[]\u3002\nA. \u76f8\u5f53\nB. \u5e73\u7f13\nC. \u5c0f\nD. \u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7533732985864446, "HuggingFaceH4/zephyr-7b-beta": 0.6268322959007524, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3460058095032025, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6625\u79cb\u6218\u56fd\u65f6\u671f\u7684\u6559\u80b2\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f()\nA. \u7a37\u4e0b\u5b66\u5bab\u63d0\u5021\u5b66\u672f\u4e0a\u7684\u81ea\u7531\nB. \u5112\u5bb6\u548c\u9053\u5bb6\u662f\u79c1\u5b66\u4e2d\u7684\u4e24\u5927\u663e\u5b66\nC. \u79c1\u5b66\u5728\u6625\u79cb\u65f6\u671f\u4ea7\u751f\uff0c\u5728\u6218\u56fd\u65f6\u671f\u8fbe\u5230\u4e86\u5174\u76db\nD. \u7a37\u4e0b\u5b66\u5bab\u662f\u5b98\u65b9\u4e3e\u529e\u3001\u79c1\u4eba\u4e3b\u6301\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65f6\u95f4\u8ba1\u91cf\u5305\u62ec\u65f6\u95f4\u95f4\u9694\u548c\u65f6\u523b\u4e24\u65b9\u9762\uff0c\u524d\u8005\u6307\u7269\u8d28\u8fd0\u52a8\u7ecf\u5386\u7684\u65f6\u6bb5\uff0c\u540e\u8005\u6307\u7269\u8d28\u8fd0\u52a8\u7684\u67d0 \u4e00\u77ac\u95f4\u3002\u4ee5\u4e0b\u9009\u9879\u4e2d\uff0c\u65f6\u523b\u7684\u8868\u8ff0\u662f\nA. \u901a\u5e38\u4e00\u8282\u8bfe\u7684\u65f6\u95f4\u662f45\u5206\u949f\nB. \u4ece\u7532\u5730\u6b65\u884c\u5230\u4e59\u5730\u9700\u89812\u5c0f\u65f6\nC. \u65b0\u95fb\u8054\u64ad\u8282\u76ee\u6bcf\u592919\u65f6\u5f00\u59cb\nD. \u767e\u7c73\u8d5b\u8dd1\u4e16\u754c\u8bb0\u5f55\u572810\u79d2\u5185\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44069149174209415, "meta-math/MetaMath-Mistral-7B": 0.7873818034912603, "itpossible/Chinese-Mistral-7B-v0.1": 0.5014154751172246, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6988583168940703, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7881654311852659}}, {"question": "1979\u5e74\u4e3a\u7eaa\u5ff5\u67d0\u4f4d\u5fb7\u56fd\u5316\u5b66\u5bb6\u8bde\u8fb0150\u5468\u5e74\u53d1\u884c\u4e86\u90ae\u7968\uff0c\u8be5\u5316\u5b66\u5bb6\u63d0\u51fa\u4e86\u82ef\u7684\u5206\u5b50\u7ed3\u6784\u7406\u8bba\u3002\u8fd9\u4f4d\u5316\u5b66\u5bb6\u662f\nA. \u8bfa\u8d1d\u5c14\nB. \u51ef\u5e93\u52d2\nC. \u6cd5\u62c9\u7b2c\nD. \u62c9\u74e6\u9521\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3348238749819403, "meta-math/MetaMath-Mistral-7B": 0.6493938190153008, "itpossible/Chinese-Mistral-7B-v0.1": 0.7591540587804638, "HuggingFaceH4/zephyr-7b-beta": 0.9961315671145305, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7180255874134706, "meta-llama/Meta-Llama-3-8B": 0.93280905520983, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8108068106606279}}, {"question": "\u5173\u4e8e\u5982\u4f55\u63d0\u9ad8\u6211\u56fd\u53f8\u6cd5\u516c\u4fe1\u529b\uff0c\u4fdd\u8bc1\u516c\u6b63\u53f8\u6cd5\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u63d0\u5347\u53f8\u6cd5\u516c\u4fe1\u529b\u8981\u6c42\u6cd5\u9662\u5728\u88c1\u5224\u524d\u5fc5\u987b\u5e7f\u6cdb\u5f81\u6c42\u793e\u4f1a\u610f\u89c1\nB. \u63d0\u5347\u53f8\u6cd5\u516c\u4fe1\u529b\u9700\u8981\u68c0\u5bdf\u6743\u3001\u5ba1\u5224\u6743\u4e0e\u6267\u884c\u6743\u9ad8\u5ea6\u7edf\u4e00\nC. \u63d0\u5347\u53f8\u6cd5\u516c\u4fe1\u529b\u5fc5\u987b\u63a8\u8fdb\u4ee5\u4fa6\u67e5\u4e3a\u4e2d\u5fc3\u7684\u8bc9\u8bbc\u5236\u5ea6\u6539\u9769\nD. \u52a0\u5f3a\u4eba\u6743\u7684\u53f8\u6cd5\u4fdd\u969c\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u53f8\u6cd5\u516c\u4fe1\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8526657281500415, "meta-math/MetaMath-Mistral-7B": 0.940908747799414, "itpossible/Chinese-Mistral-7B-v0.1": 0.5658899917656353, "HuggingFaceH4/zephyr-7b-beta": 0.9989045948617908, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.895865211759782, "meta-llama/Meta-Llama-3-8B": 0.9419143376788417, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u300a\u4f01\u4e1a\u804c\u5de5\u4f24\u4ea1\u4e8b\u6545\u5206\u7c7b\u6807\u51c6\u300b\uff08GB6441-1986\uff09\uff0c\u4e0b\u5217\u4f24\u5bb3\u4e0d\u5c5e\u4e8e\u8d77\u91cd\u4f24\u5bb3\u7684\u5c31\u662f\nA. \u8d77\u91cd\u673a\u7684\u7535\u7ebf\u8001\u5316\uff0c\u4f5c\u4e1a\u65f6\u9020\u6210\u89e6\u7535\u4f24\u5bb3\nB. \u8d77\u91cd\u673a\u7531\u4e8e\u5730\u57fa\u4e0d\u7a33\u7a81\u7136\u5012\u584c\u9020\u6210\u7684\u4f24\u5bb3\nC. \u8d77\u91cd\u673a\u540a\u7269\u5760\u843d\u9020\u6210\u7684\u4f24\u5bb3\nD. \u5458\u5de5\u5728\u8d77\u91cd\u4f5c\u4e1a\u65f6\u4e0d\u614e\u5760\u843d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e09\u7ed5\u7ec4\u53d8\u538b\u5668\u7ed5\u7ec4\u7531\u91cc\u5411\u5916\u6392\u5217\u987a\u5e8f\nA. \u9ad8\u538b\uff0c\u4e2d\u538b\uff0c\u4f4e\u538b\nB. \u4f4e\u538b\uff0c\u9ad8\u538b\uff0c\u4e2d\u538b\nC. \u4e2d\u538b\uff0c\u4f4e\u538b\uff0c\u9ad8\u538b\nD. \u4f4e\u538b\uff0c\u4e2d\u538b\uff0c\u9ad8\u538b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6929404724687696, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9093\u5c0f\u5e73\u5728\u8c08\u53ca\u5904\u7406\u56fd\u4e0e\u56fd\u4e4b\u95f4\u5173\u7cfb\u65f6\u63d0\u51fa\uff1a\u201c\u4e3b\u8981\u5e94\u8be5\u4ece\u56fd\u5bb6\u81ea\u8eab\u7684\u6218\u7565\u5229\u76ca\u51fa\u53d1\u201d\uff0c\u201c\u4e0d\u53bb\u8ba1\u8f83\u5386\u53f2\u7684\u6069\u6028\uff0c\u4e0d\u53bb\u8ba1\u8f83\u793e\u4f1a\u5236\u5ea6\u548c\u610f\u8bc6\u5f62\u6001\u7684\u5dee\u522b\uff0c\u5e76\u4e14\u56fd\u5bb6\u4e0d\u5206\u5927\u5c0f\u5f3a\u5f31\uff0c\u90fd\u4e92\u76f8\u5c0a\u91cd\u3001\u5e73\u7b49\u76f8\u5f85\u3002\u201d\u5728\u8fd9\u4e00\u7cbe\u795e\u6307\u5bfc\u4e0b\u7684\u5916\u4ea4\u6d3b\u52a8\u662f\nA. \u4e2d\u82cf\u7b7e\u8ba2\u53cb\u597d\u540c\u76df\u4e92\u52a9\u6761\u7ea6\nB. \u4e2d\u56fd\u6062\u590d\u5728\u8054\u5408\u56fd\u7684\u5408\u6cd5\u5e2d\u4f4d\nC. \u4e2d\u7f8e\u6b63\u5f0f\u5efa\u7acb\u5916\u4ea4\u5173\u7cfb\nD. \u4e2d\u82f1\u7ecf\u8fc7\u8c08\u5224\u89e3\u51b3\u9999\u6e2f\u95ee\u9898\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.37605597990309264, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6073151462003539}}, {"question": "\u5173\u4e8e\u5168\u7403\u53d8\u6696\u7684\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u4fdd\u62a4\u2fac\u6797\u662f\u51cf\u5c11CO2\u542b\u91cf\u7684\u6709\u6548\u63aa\u65bd\nB. \u5185\u9646\u56fd\u5bb6\u8ddd\u6d77\u9065\u8fdc\uff0c\u4e0d\u5fc5\u8003\u8651\u6d77\u5e73\u2faf\u4e0a\u5347\u548c\u2f53\u5019\u53d8\u6696\u7684\u5f71\u54cd\nC. \u5168\u7403\u53d8\u6696\u4f1a\u4f7f\u2f24\u2f53\u964d\u2f54\u589e\u591a\uff0c\u519c\u4f5c\u7269\u589e\u4ea7\nD. \u2fbc\u7eac\u5ea6\u56fd\u5bb6\u4e0d\u9700\u8981\u9632\u6cbb\u5168\u7403\u53d8\u6696\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7248981436909547, "meta-math/MetaMath-Mistral-7B": 0.9913685918435846, "itpossible/Chinese-Mistral-7B-v0.1": 0.7510056740306674, "HuggingFaceH4/zephyr-7b-beta": 0.9990684846216731, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9769873878569372, "meta-llama/Meta-Llama-3-8B": 0.7244946291637983, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9914604408826765}}, {"question": "\u672a\u53d6\u5f97\u9a7e\u9a76\u8bc1\u7684\u5b66\u5458\u5728\u9053\u8def\u4e0a\u5b66\u4e60\u9a7e\u9a76\u6280\u80fd\uff0c\u4e0b\u5217\u54ea\u79cd\u505a\u6cd5\u662f\u6b63\u786e\u7684\nA. \u4f7f\u7528\u6240\u5b66\u8f66\u578b\u7684\u6559\u7ec3\u8f66\u7531\u6559\u7ec3\u5458\u968f\u8f66\u6307\u5bfc\nB. \u4f7f\u7528\u79c1\u5bb6\u8f66\u7531\u6559\u7ec3\u5458\u968f\u8f66\u6307\u5bfc\nC. \u4f7f\u7528\u6240\u5b66\u8f66\u578b\u7684\u6559\u7ec3\u8f66\u5355\u72ec\u9a7e\u9a76\u5b66\u4e60\nD. \u4f7f\u7528\u6240\u5b66\u8f66\u578b\u7684\u6559\u7ec3\u8f66\u7531\u975e\u6559\u7ec3\u5458\u7684\u9a7e\u9a76\u4eba\u968f\u8f66\u6307\u5bfc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8089438077785311, "meta-math/MetaMath-Mistral-7B": 0.9716090041339455, "itpossible/Chinese-Mistral-7B-v0.1": 0.959339859934339, "HuggingFaceH4/zephyr-7b-beta": 0.9994943783784928, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.982933340259903, "meta-llama/Meta-Llama-3-8B": 0.7850301448588595, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9881173983534738}}, {"question": "\u4e3a\u4e86\u589e\u5f3a\u516c\u6709\u5236\u7684\u4e3b\u4f53\u5730\u4f4d\uff0c\u5fc5\u987b\u6df1\u5316\u56fd\u6709\u4f01\u4e1a\u516c\u53f8\u5236\u80a1\u4efd\u5236\u6539\u9769\u3002\u8fd9\u662f\u56e0\u4e3a\u5b9e\u884c\u80a1\u4efd\u5236a\u53ef\u4ee5\u589e\u5f3a\u516c\u6709\u5236\u7ecf\u6d4e\u7684\u6d3b\u529b b\u53ef\u4ee5\u589e\u5f3a\u56fd\u6709\u7ecf\u6d4e\u7684\u63a7\u5236\u529b\u3001\u5f71\u54cd\u529b c\u6709\u5229\u4e8e\u5065\u5168\u73b0\u4ee3\u4f01\u4e1a\u5236\u5ea6 d\u53ef\u4ee5\u589e\u5f3a\u516c\u6709\u5236\u7684\u4e3b\u5bfc\u5730\u4f4d\nA. acd\nB. abcd\nC. abc\nD. bcd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.8879572564389064, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6750\u6599\u7684\u7535\u963b\u7387\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u6750\u6599\u7684\u7535\u963b\u7387\u968f\u6e29\u5ea6\u7684\u5347\u9ad8\u800c\u589e\u5927\nB. \u7535\u963b\u7387\u662f\u53cd\u6620\u6750\u6599\u5bfc\u7535\u6027\u80fd\u597d\u574f\u7684\u7269\u7406\u91cf\uff0c\u7535\u963b\u7387\u8d8a\u5927\u7684\u5bfc\u4f53\u5bf9\u7535\u6d41\u7684\u963b\u788d\u4f5c\u7528\u8d8a\u5927\nC. \u628a\u4e00\u6839\u957f\u5bfc\u7ebf\u622a\u6210\u7b49\u957f\u7684\u4e09\u6bb5\uff0c\u5219\u6bcf\u6bb5\u7684\u7535\u963b\u7387\u90fd\u662f\u539f\u6765\u76841/3\nD. \u7eaf\u91d1\u5c5e\u7684\u7535\u963b\u7387\u8f83\u5408\u91d1\u7684\u7535\u963b\u7387\u5c0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ece\u53c2\u4e0e\u8005\u7684\u89d2\u8272\u6765\u8bf4\uff0c\u7f51\u7edc\u4f20\u64ad\u4e0d\u540c\u4e8e\u5927\u4f17\u4f20\u64ad\u7684\u662f\nA. \u7f51\u7edc\u7528\u6237\u90fd\u4ee5\u5145\u5f53\u4fe1\u606f\u4f20\u64ad\u8005\u4e3a\u76ee\u6807\nB. \u53c2\u4e0e\u8005\u5373\u4f20\u64ad\u8005\nC. \u4fe1\u606f\u4f20\u64ad\u8005\u548c\u4fe1\u606f\u63a5\u53d7\u8005\u7684\u89d2\u8272\u53ef\u4e0d\u65ad\u8fdb\u884c\u4e92\u6362\nD. \u7f51\u7edc\u7528\u6237\u90fd\u662f\u4fe1\u606f\u89c2\u5bdf\u8005\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.38079952563188874, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7223832474452159, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5199028554703753, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6b66\u672f\u5728\u5386\u53f2\u4e0a\u66fe\u6709\u8fc7\u4e0d\u540c\u7684\u79f0\u8c13\uff0c\u6c11\u56fd\u65f6\u671f\u79f0\nA. \u6b66\u827a\nB. \u6280\u51fb\nC. \u6b66\u672f\nD. \u56fd\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3638582843838116, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4065058884099746}}, {"question": "\u75c5\u6bd2\u611f\u67d3\u9020\u6210\u7ec6\u80de\u635f\u4f24\u7684\u56e0\u7d20\u4e2d\u4e0d\u5305\u62ec\nA. \u75c5\u6bd2\u5f15\u8d77\u7ec6\u80de\u878d\u5408\nB. \u75c5\u6bd2\u5927\u91cf\u589e\u6b96\u635f\u4f24\u9776\u7ec6\u80de\nC. Tc\u7ec6\u80de\u76f4\u63a5\u6740\u6b7b\u9776\u7ec6\u80de\nD. \u4e2d\u548c\u6297\u4f53\u76f4\u63a5\u6eb6\u89e3\u9776\u7ec6\u80de\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6663212542723972, "meta-math/MetaMath-Mistral-7B": 0.7693838619910206, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8802910789014962, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5218267166010934, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "PowerPoint\u4e2d\u653e\u6620\u5e7b\u706f\u7247\u7684\u5feb\u6377\u952e\u4e3a\nA. F7\nB. F1\nC. F8\nD. F5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.3849961019862173, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8879234223341356, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.992392252672024}}, {"question": "\u8bbe\u8ba1\u5e08\u8bbe\u8ba1\u3001\u63cf\u7ed8\u57ce\u5e02\u89c4\u5212\u7684\u84dd\u56fe\u7684\u8fc7\u7a0b\u5c5e\u4e8e\nA. \u7a7a\u60f3\nB. \u521b\u9020\u60f3\u8c61\nC. \u518d\u9020\u60f3\u8c61\nD. \u65e0\u610f\u60f3\u8c61\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39868937529592685, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5167798444726742, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.49607638233654094, "meta-llama/Meta-Llama-3-8B": 0.44876258038870526, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9291289222727548}}, {"question": "\u7b2c\u4e00\u6b21\u7eff\u8272\u9769\u547d\uff0c\u89e3\u51b3\u4e86\u4eba\u7c7b\u793e\u4f1a\u56e0\u4eba\u53e3\u589e\u52a0\u9020\u6210\u7684\u98df\u7269\u77ed\u7f3a\uff0c\u54ea\u79cd\u5b66\u79d1\u7684\u4ea7\u751f\u548c\u53d1\u5c55\u4e3a\u6b64\u505a\u51fa\u4e86\u5de8\u5927\u8d21\u732e\nA. \u7eaf\u79cd\u57f9\u517b\u6280\u672f\nB. \u57fa\u56e0\u5b66\u8bf4\nC. \u4e73\u7cd6\u64cd\u7eb5\u5b50\u5b66\u8bf4\nD. \u9057\u4f20\u80b2\u79cd\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6494642559195057, "HuggingFaceH4/zephyr-7b-beta": 0.9929370406723375, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7942494242066825, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6885\u5965\u7b49\u4eba\u901a\u8fc7\u970d\u6851\u5b9e\u9a8c\u521b\u7acb\u4e86\nA. \u81ea\u6211\u5b9e\u73b0\u4eba\u5047\u8bf4\nB. \u793e\u4f1a\u4eba\u5047\u8bf4\nC. \u7ecf\u6d4e\u4eba\u5047\u8bf4\nD. \u590d\u6742\u4eba\u5047\u8bf4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7187311913227167, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.41181126583177446, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.946843890575605}}, {"question": "\u4e73\u5934\u9c9c\u7ea2\u8272\u8840\u6027\u6ea2\u6db2\u591a\u89c1\u4e8e\nA. \u4e73\u7ba1\u963b\u585e\u7684\u5bfc\u7ba1\u5185\u4e73\u5934\u72b6\u7624\nB. \u7ec8\u6b62\u54fa\u4e73\u540e\nC. \u5bfc\u7ba1\u5185\u4e73\u5934\u72b6\u7624\nD. \u6b63\u5e38\u6708\u7ecf\u671f\uff0c\u65e9\u671f\u598a\u5a20\u6216\u56ca\u6027\u589e\u751f\u75c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3782456243077549, "meta-math/MetaMath-Mistral-7B": 0.504540483486473, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5868821063341932, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4516954386215965, "meta-llama/Meta-Llama-3-8B": 0.38347926015350264, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.41065764583856135}}, {"question": "\u4e0b\u5217\u60c5\u666f\u4e0d\u53ef\u80fd\u53d1\u751f\u572819\u4e16\u7eaa\u7684\u662f\nA. \u6770\u514b\u6253\u7535\u8bdd\u7ea6\u739b\u4e3d\u4e00\u8d77\u53bb\u770b\u7535\u5f71\nB. \u7ea6\u7ff0\u4e58\u7535\u68af\u767b\u5927\u697c\u697c\u9876\u62cd\u7167\u7559\u5ff5\nC. \u53f2\u8482\u82ac\u900a\u4e58\u706b\u8f66\u5230\u65af\u6258\u514b\u987f\u65c5\u884c\nD. \u6c64\u59c6\u901a\u8fc7\u5e7f\u64ad\u6536\u542c\u845b\u5e95\u65af\u5821\u6f14\u8bf4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33787085580855214, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4641783112314807, "HuggingFaceH4/zephyr-7b-beta": 0.7128559657258022, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3060136256597631, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbeA\u4e3an\u9636\u65b9\u9635\uff0c$r(A)=r<n$\uff0c \u5219\u5728A\u7684n\u4e2a\u5217\u5411\u91cf\u4e2d\uff08\uff09.\nA. \u4efb\u610fr\u4e2a\u5217\u5411\u91cf\u90fd\u6784\u6210\u6781\u5927\u65e0\u5173\u7ec4\nB. \u4efb\u610f\u4e00\u4e2a\u5217\u5411\u91cf\u90fd\u80fd\u7531\u5176\u4ed6r\u4e2a\u5217\u5411\u91cf\u7ebf\u6027\u8868\u793a\nC. \u4efb\u610fr\u4e2a\u5217\u5411\u91cf\u7ebf\u6027\u65e0\u5173\nD. \u5fc5\u6709r\u4e2a\u5217\u5411\u91cf\u7ebf\u6027\u65e0\u5173\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4233867781668616, "meta-math/MetaMath-Mistral-7B": 0.6463382322278913, "itpossible/Chinese-Mistral-7B-v0.1": 0.5006221037540965, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4491084366430353, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u60a3\u8005\u7cbe\u795e\u6291\u90c1\uff0c\u8868\u60c5\u6de1\u6f20\uff0c\u6c89\u9ed8\u75f4\u5446\uff0c\u8bed\u65e0\u4f26\u6b21\uff0c\u9759\u800c\u591a\u559c\u5c11\u52a8\uff0c\u5f53\u8bca\u4e3a\nA. \u72c2\u8bc1\nB. \u90c1\u8bc1\nC. \u75f4\u5446\nD. \u766b\u8bc1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u66b4\u96e8\u5929\u6c14\u9a7e\u8f66\uff0c\u522e\u6c34\u5668\u65e0\u6cd5\u522e\u51c0\u96e8\u6c34\u65f6\uff0c\u5e94\u5f53\nA. \u7acb\u5373\u51cf\u901f\u9760\u8fb9\u505c\u9a76\nB. \u96c6\u4e2d\u6ce8\u610f\u529b\u8c28\u614e\u9a7e\u9a76\nC. \u4ee5\u6b63\u5e38\u901f\u5ea6\u884c\u9a76\nD. \u51cf\u901f\u884c\u9a76\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8058420050736987, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5610932094237381}}, {"question": "\u6c49\u8bed\u897f\u6587\u5b57\u6bcd\u8bcd\u88ab\u6c49\u8bed\u8bcd\u5178\u6536\u5f55\u7684\u5386\u53f2\u5df2\u6709100\u591a\u5e74\uff0c\u6536\u5f55\u5b57\u6bcd\u8bcd\u65e9\u5df2\u662f\u6c49\u8bed\u8bcd\u5178\u548c\u5de5\u5177\u4e66\u7f16\u7e82\u7684\u60ef\u4f8b\u548c\u901a\u884c\u7684\u505a\u6cd5\u30021903\u5e74\u51fa\u7248\u7684\u300a\u65b0\u5c14\u96c5\u300b\u5728\u6b63\u6587\u4e2d\u6536\u5f55\u4e86\u201cX\u5149\u7ebf\u201d\u4e00\u8bcd\uff0c\u8fd9\u662f\u6c49\u8bed\u8bcd\u5178\u9996\u6b21\u6536\u5165\u5b57\u6bcd\u8bcd\u3002\u5728\u8f9e\u4e66\u53f2\u4e0a\uff0c\u8fd9\u90e8\u8bcd\u5178\u662f\u8fd1\u4ee3\u4e2d\u56fd\u6700\u65e9\u7684\u4e00\u90e8\u65b0\u8bed\u8bcd\u8bcd\u5178\uff0c\u5b83\u867d\u7136\u8fd8\u4e0d\u662f\u7eaf\u7cb9\u73b0\u4ee3\u610f\u4e49\u4e0a\u7684\u6c49\u8bed\u8bcd\u5178\uff0c\u4f46\u5177\u6709\u627f\u524d\u542f\u540e\u7684\u91cd\u8981\u610f\u4e49\u3002\u8fd9\u6bb5\u6587\u5b57\u4ecb\u7ecd\u300a\u65b0\u5c14\u96c5\u300b\u7684\u76ee\u7684\u662f\nA. \u603b\u7ed3\u897f\u6587\u5b57\u6bcd\u8bcd\u5728\u6c49\u8bed\u8bcd\u5178\u4e2d\u7684\u6536\u5f55\u89c4\u5f8b\nB. \u8bf4\u660e\u6c49\u8bed\u8bcd\u5178\u6536\u5f55\u897f\u6587\u5b57\u6bcd\u8bcd\u65e9\u6709\u5148\u4f8b\nC. \u8bba\u8ff0\u5176\u5bf9\u4e8e\u8fd1\u4ee3\u4ee5\u6765\u8f9e\u4e66\u7f16\u7e82\u7684\u5f71\u54cd\nD. \u63a2\u8ba8\u6c49\u8bed\u8bcd\u5178\u6536\u8bcd\u89c4\u5f8b\u7684\u5386\u53f2\u6f14\u53d8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6745823266145932, "meta-math/MetaMath-Mistral-7B": 0.9212827029769256, "itpossible/Chinese-Mistral-7B-v0.1": 0.6619038847926709, "HuggingFaceH4/zephyr-7b-beta": 0.9976305296274484, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8566034490705983, "meta-llama/Meta-Llama-3-8B": 0.9011328067379735, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8971689122439498}}, {"question": "\u4e3a\u7d22\u53d6\u8d4c\u503a\u3001\u9ad8\u5229\u8d37\u7b49\u975e\u6cd5\u503a\u52a1\u800c\u975e\u6cd5\u5265\u593a\u4ed6\u4eba\u4eba\u8eab\u81ea\u7531\u7684\uff0c\u5e94\nA. \u6309\u7167\u6572\u8bc8\u52d2\u7d22\u7f6a\u548c\u975e\u6cd5\u62d8\u7981\u7f6a\u5b9e\u884c\u6570\u7f6a\u5e76\u7f5a\nB. \u6309\u7167\u7ed1\u67b6\u7f6a\u5b9a\u7f6a\u5904\u7f5a\nC. \u6309\u7167\u6572\u8bc8\u52d2\u7d22\u7f6a\u548c\u7ed1\u67b6\u7f6a\u5b9e\u884c\u6570\u7f6a\u5e76\u7f5a\nD. \u6309\u7167\u975e\u6cd5\u62d8\u7981\u7f6a\u5b9a\u7f6a\u5904\u7f5a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ec0\u4e48\u662f\u5bf9\u4e16\u754c\u7ecf\u6d4e\u653f\u6cbb\u6700\u9ad8\u5c42\u6b21\u7684\u5224\u65ad\nA. \u683c\u5c40\u7279\u5f81\nB. \u4e16\u754c\u7684\u57fa\u672c\u77db\u76fe\nC. \u4e16\u754c\u5f62\u52bf\nD. \u65f6\u4ee3\u4e3b\u9898\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.27697441697632474, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8089\u7c7b\u6700\u4e3b\u8981\u7684\u8272\u7d20\u662f\u4e00\u79cd\u79f0\u4e3a\uff08\uff09\u7684\u86cb\u767d\u8d28\uff0c\u5176\u529f\u80fd\u662f\u5728\u808c\u8089\u7ec4\u7ec7\u4e2d\u50a8\u5b58\u6c27\u6c14\nA. \u7403\u86cb\u767d\nB. \u4e73\u72b6\u86cb\u767d\nC. \u8840\u7ea2\u86cb\u767d\nD. \u808c\u7ea2\u86cb\u767d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9132502944881815, "meta-math/MetaMath-Mistral-7B": 0.9377863248243632, "itpossible/Chinese-Mistral-7B-v0.1": 0.9300503808149774, "HuggingFaceH4/zephyr-7b-beta": 0.9994874003044215, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9513751836393279, "meta-llama/Meta-Llama-3-8B": 0.9702890228585163, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.930870671929738}}, {"question": "\u67096\u4e2a\u68f1\u2ed3\u5206\u522b\u662f3cm\uff0c4cm\uff0c5cm\uff0c\u7684\u76f8\u540c\u7684\u2ed3\u2f45\u4f53\uff0c\u628a\u5b83\u4eec\u7684\u67d0\u4e9b\u2faf\u67d3\u4e0a\u7ea2\u2f8a\uff0c\u4f7f\u5f97\u6709\u7684\u2ed3\u2f45\u4f53\u53ea\u6709\u2f00\u4e2a\u2faf\u662f\u7ea2\u2f8a\u7684\uff0c\u6709\u7684\u2ed3\u2f45\u4f53\u6070\u6709\u4e24\u4e2a\u2faf\u662f\u7ea2\u2f8a\u7684\uff0c\u6709\u7684\u2ed3\u2f45\u4f53\u6070\u6709\u4e09\u4e2a\u2faf\u662f\u7ea2\u2f8a\u7684\uff0c\u6709\u7684\u2ed3\u2f45\u4f53\u6070\u6709\u56db\u4e2a\u2faf\u662f\u7ea2\u2f8a\u7684\uff0c\u6709\u7684\u2ed3\u2f45\u4f53\u6070\u6709\u4e94\u4e2a\u2faf\u662f\u7ea2\u2f8a\u7684\uff0c\u8fd8\u6709\u2f00\u4e2a\u2ed3\u2f45\u4f53\u516d\u4e2a\u2faf\u90fd\u662f\u7ea2\u2f8a\u7684\uff0c\u67d3\u2f8a\u540e\u628a\u6240\u6709 \u7684\u2ed3\u2f45\u4f53\u5206\u5272\u6210\u68f1\u2ed3\u4e3a1cm\u7684\u2f29\u6b63\u2f45\u4f53\uff0c\u5206\u5272\u5b8c\u6bd5\u540e\uff0c\u6070\u6709\u2f00\u2faf\u662f\u7ea2\u2f8a\u7684\u2f29\u6b63\u2f45\u4f53\u6700\u591a\u6709\u2f0f\u4e2a\nA. 199\nB. 177\nC. 166\nD. 188\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3035988665653362, "itpossible/Chinese-Mistral-7B-v0.1": 0.28108825044289903, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6559\u80b2\u7684\u4e09\u8981\u7d20\u7684\u5173\u7cfb\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f()\u3002\nA. \u53d7\u6559\u80b2\u8005\u662f\u6559\u80b2\u8005\u9009\u62e9\u548c\u65bd\u52a0\u6559\u80b2\u7684\u5bf9\u8c61\nB. \u6559\u80b2\u5f71\u54cd\u662f\u6559\u80b2\u8005\u5bf9\u53d7\u6559\u80b2\u8005\u8d77\u4f5c\u7528\u7684\u6865\u6881\nC. \u6559\u80b2\u8005\u662f\u6559\u80b2\u5f71\u54cd\u548c\u53d7\u6559\u80b2\u8005\u4e4b\u95f4\u7684\u7ebd\u5e26\nD. \u8fd9\u4e09\u4e2a\u57fa\u672c\u8981\u7d20\u76f8\u4e92\u72ec\u7acb\uff0c\u6ca1\u6709\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8785728295380167, "meta-math/MetaMath-Mistral-7B": 0.9626218975343606, "itpossible/Chinese-Mistral-7B-v0.1": 0.7213857470745729, "HuggingFaceH4/zephyr-7b-beta": 0.99962963160152, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9846002066107898, "meta-llama/Meta-Llama-3-8B": 0.9102215495144218, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9477498146874005}}, {"question": "2012\u5e74\u201c\u5bfb\u627e\u6700\u7f8e\u4e61\u6751\u6559\u5e08\u201d\u5927\u578b\u516c\u76ca\u6d3b\u52a8\u5c55\u73b0\u4e86\u4e61\u6751\u6559\u5e08\u6de1\u6cca\u540d\u5229\u3001\u7518\u4e8e\u5949\u732e\u7684\u7cbe\u795e\u54c1\u8d28\u548c\u9ad8\u5c1a\u60c5\u64cd\u3002\u8fd9\u4e00\u6d3b\u52a8\u6709\u5229\u4e8e\nA. \u704c\u8f93\u5404\u79cd\u601d\u60f3\u548c\u4ef7\u503c\u89c2\nB. \u63d0\u9ad8\u79d1\u5b66\u6587\u5316\u4fee\u517b\nC. \u6811\u7acb\u793e\u4f1a\u4e3b\u4e49\u8363\u8fb1\u89c2\nD. \u5bf9\u6587\u5316\u5e02\u573a\u52a0\u5f3a\u7ba1\u7406\u3001\u6b63\u786e\u5f15\u5bfc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5029677816900523, "HuggingFaceH4/zephyr-7b-beta": 0.6492570839896828, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7295141875980724, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8865703231942687}}, {"question": "\u75c5\u6bd2\u7532\u5177\u6709RNA\u7532\u548c\u86cb\u767d\u8d28\u7532\uff0c\u75c5\u6bd2\u4e59\u5177\u6709RNA\u4e59\u548c\u86cb\u767d\u8d28\u4e59\u3002\u82e5\u5c06RNA\u7532\u548c\u86cb\u767d\u8d28\u4e59\u7ec4\u6210\u4e00\u79cd\u75c5\u6bd2\u4e19\uff0c\u518d\u4ee5\u75c5\u6bd2\u4e19\u53bb\u611f\u67d3\u5bbf\u4e3b\u7ec6\u80de\uff0c\u5219\u7ec6\u80de\u4e2d\u7684\u75c5\u6bd2\u5177\u6709\nA. RNA\u7532\u548c\u86cb\u767d\u8d28\u7532\nB. RNA\u4e59\u548c\u86cb\u767d\u8d28\u7532\nC. RNA\u4e59\u548c\u86cb\u767d\u8d28\u4e59\nD. RNA\u7532\u548c\u86cb\u767d\u8d28\u4e59\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f7f\u4eba\u8111\u4e0d\u7ecf\u8fc7\u903b\u8f91\u601d\u7ef4\u5c31\u76f4\u63a5\u4ea7\u751f\u884c\u4e3a\u548c\u51b3\u7b56\u7684\u56e0\u7d20\u662f\nA. \u975e\u7406\u6027\u56e0\u7d20\nB. \u8d85\u7406\u6027\u56e0\u7d20\nC. \u7406\u6027\u56e0\u7d20\nD. \u611f\u6027\u56e0\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u957f\u671f\u80a1\u6743\u6295\u8d44\u91c7\u7528\u6743\u76ca\u6cd5\u6838\u7b97\uff0c\u88ab\u6295\u8d44\u4f01\u4e1a\u5ba3\u544a\u53d1\u653e\u73b0\u91d1\u80a1\u5229\u65f6\uff0c\u6295\u8d44\u4f01\u4e1a\u5e94\u501f\u8bb0\u7684\u4f1a\u8ba1\u79d1\u76ee\u662f\nA. \u201c\u957f\u671f\u80a1\u6743\u6295\u8d44\u2014\u2014\u6295\u8d44\u6210\u672c\u201d\nB. \u201c\u6295\u8d44\u6536\u76ca\u201d\nC. \u201c\u5e94\u6536\u80a1\u5229\u201d\nD. \u201c\u957f\u671f\u80a1\u6743\u6295\u8d44\u2014\u2014\u635f\u76ca\u8c03\u6574\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3525276284529676, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5025449849459913, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8568518549933356}}, {"question": "\u7ec4\u7ec7\u5e02\u573a\u662f\u7531\u5404\u79cd\u7ec4\u7ec7\u673a\u6784\u5f62\u6210\u7684\u5bf9\u4f01\u4e1a\u4ea7\u54c1\u548c\u670d\u52a1\u9700\u6c42\u7684\u603b\u548c\u3002\u5b83\u5206\u4e3a\u4e09\u79cd\u7c7b\u578b\uff0c\u4e00\u822c\u5730\uff0c\u901a\u8fc7\u8d2d\u4e70\u5546\u54c1\u548c\u670d\u52a1\u5e76\u8f6c\u552e\u6216\u51fa\u79df\u7ed9\u4ed6\u4eba\u6765\u83b7\u53d6\u5229\u6da6\u7684\u4e2a\u4eba\u6216\u7ec4\u7ec7\uff0c\u5c5e\u4e8e\nA. \u4e2d\u95f4\u5546\u5e02\u573a\nB. \u653f\u5e9c\u5e02\u573a\nC. \u4ea7\u4e1a\u5e02\u573a\nD. \u79c1\u4eba\u5e02\u573a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.380738678570574, "meta-math/MetaMath-Mistral-7B": 0.5712690181890502, "itpossible/Chinese-Mistral-7B-v0.1": 0.5940327246278464, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7215684845848221, "meta-llama/Meta-Llama-3-8B": 0.8090182315979729, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6258810973626721}}, {"question": "\u8ba1\u7b97\u673a\u7684\u7f51\u7edc\u79cd\u7c7b\u5f88\u591a\u3001\u6027\u80fd\u5404\u5f02\u3002\u6839\u636e\u5176\u901a\u4fe1\u7684\u8303\u56f4\u8ddd\u79bb\uff0c\u4e0b\u5217\u63d0\u6cd5\u6b63\u786e\u7684\u662f\nA. \u53ef\u5206\u4e3a\u57ce\u57df\u7f51\u3001\u5e7f\u57df\u7f51\nB. \u53ef\u5206\u4e3a\u5c40\u57df\u7f51\u3001\u6821\u56ed\u7f51\u548c\u5e7f\u57df\u7f51\nC. \u53ef\u5206\u4e3a\u5c40\u57df\u7f51\u3001\u57ce\u57df\u7f51\nD. \u53ef\u5206\u4e3a\u5c40\u57df\u7f51\u3001\u5e7f\u57df\u7f51\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u9769\u547d\uff0c\u9769\u547d\uff0c\u526a\u6389\u8fab\u5b50\u53cd\u671d\u5ef7\uff1b\u72ec\u7acb\uff0c\u72ec\u7acb\uff0c\u4e2d\u56fd\u5c82\u662f\u9791\u5b50\u7684\uff01\u201d\u8fd9\u9996\u6b4c\u8c23\u53cd\u6620\u4e86\u5f53\u65f6\nA. \u9769\u547d\u7684\u4e3b\u8981\u76ee\u7684\u662f\u79fb\u98ce\u6613\u4fd7\nB. \u6c11\u65cf\u533a\u57df\u81ea\u6cbb\u601d\u60f3\u6df1\u5165\u4eba\u5fc3\nC. \u6c11\u65cf\u4e3b\u4e49\u601d\u60f3\u5df2\u5f71\u54cd\u5230\u6c11\u4f17\nD. \u53cd\u5bf9\u5e1d\u56fd\u4e3b\u4e49\u6210\u4e3a\u601d\u60f3\u4e3b\u6d41\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4871416423637229, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5423325725267589, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5505693191922962, "meta-llama/Meta-Llama-3-8B": 0.823572492664379, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9862271355094911}}, {"question": "\u201c\u5728\uff08\u897f\u6b27\uff09\u5404\u56fd\u7686\u5f31\u3001\u552f\u7f8e\u56fd\u72ec\u5f3a\u4e4b\u9645\uff0c\u7f8e\u56fd\u4eba\u5929\u751f\u7684\u9886\u8896\u610f\u8bc6\u2026\u2026\u4f7f\u5b83\u4eec\u4e0d\u80fd\u8896\u624b\u5b89\u5750\uff0c\u5b83\u4eec\u51b3\u8ba1\u4e58\u6b64\u5343\u8f7d\u96be\u9022\u7684\u5386\u53f2\u673a\u9047\uff0c\u6709\u6240\u4f5c\u4e3a\u3002\u201d\u201c\u4f5c\u4e3a\u201d\u7684\u8868\u73b0\u662f\nA. \u6210\u7acb\u5317\u7f8e\u81ea\u7531\u8d38\u6613\u533a\nB. \u52a0\u5f3a\u56fd\u5bb6\u5bf9\u7ecf\u6d4e\u7684\u5e72\u9884\nC. \u63d0\u51fa\u53d1\u5c55\u7ecf\u6d4e\u7684\u9a6c\u6b47\u5c14\u8ba1\u5212\nD. \u4e0e\u82cf\u8054\u5c55\u5f00\u519b\u5907\u7ade\u8d5b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6205765772461097, "meta-math/MetaMath-Mistral-7B": 0.6876992605481914, "itpossible/Chinese-Mistral-7B-v0.1": 0.8498937827061178, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.504282436363375}}, {"question": "\u4e0b\u5217\u4e0e\u5b9e\u9a8c\u6709\u5173\u7684\u53d9\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u9274\u5b9a\u8102\u80aa\u7684\u5b9e\u9a8c\u4e2d\u89c2\u5bdf\u82b1\u751f\u5b50\u53f6\u5207\u7247\uff0c\u7ec6\u80de\u95f4\u53ef\u80fd\u51fa\u73b0\u6a58\u9ec4\u8272\u9897\u7c92\nB. \u63a2\u7a76\u6e29\u5ea6\u5bf9\u9176\u6d3b\u6027\u7684\u5f71\u54cd\u65f6\uff0c\u5c06\u8fc7\u6c27\u5316\u6c22\u9176\u548c\u8fc7\u6c27\u5316\u6c22\u5148\u5404\u81ea\u5728\u67d0\u4e00\u6e29\u5ea6\u4e0b\u4fdd\u6e29\u4e00\u6bb5\u65f6\u95f4\u540e\u518d\u6df7\u5408\nC. \u5728\u6027\u72b6\u5206\u79bb\u6bd4\u7684\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0c\u76d2\u5b50\u4ee3\u8868\u751f\u6b96\u5668\u5b98\uff0c\u5f69\u7403\u4ee3\u8868\u914d\u5b50\uff0c\u4e24\u4e2a\u76d2\u5b50\u7684\u5f69\u7403\u6570\u91cf\u8981\u6c42\u76f8\u540c\nD. \u5728\u201c\u63a2\u7a76\u9175\u6bcd\u83cc\u7ec6\u80de\u547c\u5438\u65b9\u5f0f\u201d\u5b9e\u9a8c\u4e2d\uff0c\u901a\u8fc7\u89c2\u5bdf\u6f84\u6e05\u77f3\u7070\u6c34\u662f\u5426\u53d8\u6df7\u6d4a\u6765\u5224\u65ad\u5176\u547c\u5438\u65b9\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28838734872311134, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2801288226217134, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u65b0\u95fb\u8206\u8bba\u76d1\u7763\u4e2d\uff0c\u6700\u91cd\u8981\u7684\u76d1\u7763\u5bf9\u8c61\u662f\nA. \u5404\u79cd\u793e\u4f1a\u6210\u5458\nB. \u516c\u6c11\u56e2\u4f53\nC. \u57fa\u5c42\u5e72\u90e8\nD. \u6743\u529b\u7ec4\u7ec7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6393430713555841, "meta-math/MetaMath-Mistral-7B": 0.8372633314581678, "itpossible/Chinese-Mistral-7B-v0.1": 0.9014234430792795, "HuggingFaceH4/zephyr-7b-beta": 0.9999433419692586, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9784068844208053, "meta-llama/Meta-Llama-3-8B": 0.40650589978597995, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u706b\u7bad\u53d1\u5c04\u3001\u540c\u6b65\u536b\u661f\u53d1\u5c04\u90fd\u9009\u5728\u665a\u95f47-9\u70b9\uff0c\u8fd9\u6837\u505a\u7684\u539f\u56e0\u53ef\u80fd\u662f\nA. \u9009\u5728\u665a\u95f4\u53d1\u5c04\u9690\u853d\u6027\u5f3a\uff0c\u4e0d\u6613\u88ab\u5176\u5b83\u56fd\u5bb6\u536b\u661f\u6216\u98de\u673a\u4fa6\u5bdf\u5230\nB. \u665a\u95f4\u5929\u6c14\u6761\u4ef6\u597d\uff0c\u98ce\u901f\u4f4e\uff0c\u6709\u5229\u4e8e\u706b\u7bad\u53d1\u5c04\nC. \u665a\u95f4\u6c14\u6e29\u4f4e\uff0c\u6709\u5229\u4e8e\u536b\u661f\u4fdd\u517b\u548c\u706b\u7bad\u7684\u6563\u70ed\nD. \u5f53\u536b\u661f\u5230\u8fbe\u540c\u6b65\u5b9a\u4f4d\u70b9\u65f6\uff0c\u536b\u661f\u81ea\u8eab\u7684\u592a\u9633\u80fd\u7535\u6c60\u6b63\u597d\u5bf9\u51c6\u592a\u9633\uff0c\u80fd\u4fdd\u8bc1\u592a\u9633\u80fd\u5411\u536b\u661f\u6301\u7eed\u4f9b\u7535\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u5173\u4e8e\u5904\u7406\u5e26\u6709\u6b67\u89c6\u6216\u8005\u4fae\u8fb1\u5c11\u6570\u6c11\u65cf\u6027\u8d28\u7684\u79f0\u8c13\u3001\u5730\u540d\u3001\u7891\u78a3\u3001\u533e\u8054\u7684\u6307\u793a\u300b\u662f\u5728\uff08\uff09\u5e74\u9881\u5e03\u7684\u3002\nA. 1950\nB. 1979\nC. 1978\nD. 1951\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u5728\u4e0b\u73ed\u56de\u5bb6\u7684\u8def\u4e0a\u9047\u5230\u4e59\u8ffd\u6253\u4e19\uff0c\u7532\u4fbf\u4e0a\u524d\u5236\u6b62\u3002\u7531\u4e8e\u4e59\u6ca1\u6709\u6ce8\u610f\u5230\u7532\u7684\u5230\u6765\uff0c\u5728\u6325\u62f3\u6253\u4e19\u65f6\u4e0d\u614e\u5c06\u7532\u7684\u7259\u6253\u6389\u4e00\u9897\u3002\u5982\u679c\u4e59\u3001\u4e19\u90fd\u6709\u80fd\u529b\u627f\u62c5\u8d23\u4efb\uff0c\u5219\u6839\u636e\u300a\u4fb5\u6743\u8d23\u4efb\u6cd5\u300b\u7684\u89c4\u5b9a\uff0c\u5173\u4e8e\u7532\u7684\u635f\u5bb3\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u5e94\u5f53\u7531\u4e19\u627f\u62c5\nB. \u5e94\u5f53\u7531\u4e59\u627f\u62c5\nC. \u5e94\u5f53\u7531\u4e59\u3001\u4e19\u627f\u62c5\u8fde\u5e26\u4fb5\u6743\u8d23\u4efb\nD. \u7532\u6709\u6743\u8981\u6c42\u4e19\u5bf9\u5176\u635f\u5bb3\u7ed9\u4e88\u9002\u5f53\u8865\u507f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5174\u8d77\u4e8e18\u4e16\u7eaa\u521d\u7684\u7f8e\u56fd\u6587\u5316\uff0c\u5b9e\u4e43\u542f\u8499\u8fd0\u52a8\u548c\u5b97\u6559\u6539\u9769\u8fd0\u52a8\u649e\u51fb\u878d\u5408\u540e\u7684\u4ea7\u7269\u3002\u4e24\u9879\u8fd0\u52a8\u6709\u4e2a\u5171\u901a\u4e4b\u5904\u2014\u2014\u800c\u8fd9\u6b62\u662f\u7f8e\u5f0f\u601d\u7ef4\u6a21\u5f0f\u7684\u6839\u672c\u3002\u201d\u8fd9\u91cc\u201c\u7f8e\u5f0f\u601d\u7ef4\u6a21\u5f0f\u7684\u6839\u672c\u201d\u662f\u6307\nA. \u5d07\u5c1a\u7406\u6027\u79d1\u5b66\nB. \u5f3a\u8c03\u4e2a\u4eba\u81ea\u7531\nC. \u5ba3\u626c\u6c11\u4e3b\u653f\u6cbb\nD. \u91cd\u89c6\u6cd5\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4061526610046333, "meta-math/MetaMath-Mistral-7B": 0.607145162115711, "itpossible/Chinese-Mistral-7B-v0.1": 0.4957404895145016, "HuggingFaceH4/zephyr-7b-beta": 0.9030950591241731, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6168242951124769, "meta-llama/Meta-Llama-3-8B": 0.5838300005443617, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u601d\u7ef4\u7684\u57fa\u672c\u8fc7\u7a0b\u65f6\nA. \u5177\u4f53\u5316\u548c\u7cfb\u7edf\u5316\nB. \u62bd\u8c61\u548c\u6982\u62ec\nC. \u5206\u6790\u548c\u7efc\u5408\nD. \u5206\u7c7b\u548c\u6bd4\u8f83\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4043458972036957, "meta-math/MetaMath-Mistral-7B": 0.79692080366085, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8156003538352689, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6413163922375393, "meta-llama/Meta-Llama-3-8B": 0.9139900796137285, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9655536374131033}}, {"question": "\u4e8c\u6c27\u5316\u78b3\u7531\u6db2\u6001\u53d8\u4e3a\u6c14\u6001\u65f6\u9700\u5438\u6536\u70ed\u91cf\uff0c\u4e00\u822c\u53ef\u4f7f\u5468\u56f4\u6e29\u5ea6\u4e0b\u964d\u81f3\uff08\uff09\u2103\u5de6\u53f3\u3002\nA. -70\nB. -50\nC. -40\nD. -60\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f01\u4e1a\u589e\u8d44\u6269\u80a1\u65f6\uff0c\u6295\u8d44\u8005\u5b9e\u9645\u51fa\u8d44\u989d\u5927\u4e8e\u6309\u7ea6\u5b9a\u6bd4\u4f8b\u8ba1\u7b97\u7684\u5176\u5728\u6ce8\u518c\u8d44\u672c\u4e2d\u6240\u5360\u4efd\u989d\u7684\u90e8\u5206\uff0c\u5e94\u8ba1\u5165\nA. \u8425\u4e1a\u5916\u6536\u5165\nB. \u80a1\u672c\nC. \u8425\u4e1a\u6536\u5165\nD. \u8d44\u672c\u516c\u79ef\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5276429903167725, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6437934522665545, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7132038330204933}}, {"question": "\u300a\u4eba\u6c11\u65e5\u62a5\u300b\u62a5\u9053\uff0c\u8fdb\u51652013\u5e74\uff0c\u7f8e\u56fd\u5b9e\u65bd\u91cf\u5316\u5bbd\u677e\u8d27\u5e01\u653f\u7b56\u7684\u52b2\u5934\u4e1d\u6beb\u672a\u51cf\u3002\u4e13\u5bb6\u6307\u51fa\uff0c\u7f8e\u56fd\u91cf\u5316\u5bbd\u677e\u5fc5\u7136\u589e\u53d1\u8d27\u5e01\uff0c\u5176\u8fc7\u5ea6\u6d41\u52a8\u6027\u6709\u53ef\u80fd\u52a0\u5267\u5176\u4ed6\u56fd\u5bb6\u901a\u80c0\u7684\u98ce\u9669\u3002\u4e2d\u56fd\u5e94\u5c3d\u5feb\u8f6c\u53d8\u4f9d\u8d56\u51fa\u53e3\u7684\u7ecf\u6d4e\u589e\u957f\u65b9\u5f0f\uff0c\u901a\u8fc7\u6269\u5927\u5185\u9700\u6446\u8131\u8d1f\u9762\u5f71\u54cd\u3002\u6750\u6599\u8868\u660e\nA. \u7ecf\u6d4e\u5168\u7403\u5316\u53ea\u5bf9\u53d1\u8fbe\u56fd\u5bb6\u6709\u5229\nB. \u7ecf\u6d4e\u5168\u7403\u5316\u52a0\u5267\u4e86\u5168\u7403\u7ecf\u6d4e\u7684\u4e0d\u7a33\u5b9a\u6027\nC. \u7ecf\u6d4e\u5168\u7403\u5316\u5b9e\u8d28\u4e0a\u662f\u4ee5\u53d1\u8fbe\u8d44\u672c\u4e3b\u4e49\u56fd\u5bb6\u4e3a\u4e3b\u5bfc\u7684\nD. \u7ecf\u6d4e\u5168\u7403\u5316\u9020\u6210\u4e16\u754c\u8303\u56f4\u5185\u7684\u4e24\u6781\u5206\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.554816834179277, "meta-llama/Meta-Llama-3-8B": 0.7523299181189942, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9699997676750727}}, {"question": "\u5728\u897f\u65b9\u7f8e\u5b66\u53f2\u4e0a\uff0c\u81ea\u7136\u4e3b\u4e49\u7f8e\u5b66\u7684\u4ee3\u8868\u662f\nA. \u6851\u5854\u4e9a\u90a3\nB. \u675c\u5a01\nC. \u7ef4\u7279\u6839\u65af\u5766\nD. \u67cf\u683c\u68ee\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.402028670754779, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4799561314620878, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.37098650971663727}}, {"question": "\u6839\u636e\u6211\u56fd\u73b0\u884c\u6cd5\u5f8b\uff0c\u4e0d\u8bc9\u514d\u8d23\nA. \u4e0d\u9002\u7528\u4e8e\u72af\u7f6a\u884c\u4e3a\nB. \u4e00\u822c\u9002\u7528\u4e8e\u6c11\u4e8b\u8fdd\u6cd5\u884c\u4e3a\uff0c\u4e5f\u9002\u7528\u4e8e\u67d0\u4e9b\u72af\u7f6a\u884c\u4e3a\nC. \u4ec5\u9002\u7528\u4e8e\u6c11\u4e8b\u8fdd\u6cd5\u884c\u4e3a\nD. \u9002\u7528\u4e8e\u6c11\u4e8b\u8fdd\u6cd5\u884c\u4e3a\uff0c\u4e5f\u9002\u7528\u4e8e\u5927\u591a\u6570\u72af\u7f6a\u884c\u4e3a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3612437443020473, "meta-math/MetaMath-Mistral-7B": 0.6012392372063693, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5272420483122042, "meta-llama/Meta-Llama-3-8B": 0.38822591501510517, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5127221936602786}}, {"question": "\u4ece\u4e00\u5927\u6279\u7269\u54c1\u4e2d\u62bd\u53d6\u4e00\u90e8\u5206\uff0c\u4ee5\u4ee3\u8868\u5168\u90e8\u7684\u6837\u54c1\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u79f0\u4e3a\u8bd5\u6837\u7684\nA. \u4ee5\u4e0a\u5747\u5bf9\nB. \u9884\u5904\u7406\nC. \u5236\u5907\nD. \u91c7\u96c6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6800705465983837, "meta-math/MetaMath-Mistral-7B": 0.9717091850357114, "itpossible/Chinese-Mistral-7B-v0.1": 0.6058557311098902, "HuggingFaceH4/zephyr-7b-beta": 0.9999341249958473, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9385043946502235, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5699778355832543}}, {"question": "\u4ee5\u4e0b\u54ea\u4e2a\u4e0d\u5c5e\u4e8e\u9752\u5c11\u5e74\u671f\u7684\u4e3b\u8981\u7279\u5f81\u4e0e\u53d1\u5c55\u4efb\u52a1\nA. \u667a\u5229\u53d1\u5c55\u7684\u9ad8\u5cf0\u671f\nB. \u5f00\u59cb\u638c\u63e1\u4e66\u9762\u8bed\u8a00\nC. \u751f\u7406\u3001\u5fc3\u7406\u53d1\u5c55\u4e0d\u5e73\u8861\nD. \u81ea\u6211\u540c\u4e00\u6027\u5f62\u6210\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.30300686740596605, "HuggingFaceH4/zephyr-7b-beta": 0.795374349338411, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5011905183254942, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e16\u754c\u4e0a\u7b2c\u2f00\u53f0\u8ba1\u7b97\u673aENIAC\u95ee\u4e16\u7684\u540c\u65f6\uff0c\u51af\u2022\u8bfa\u4f0a\u66fc\u63d0\u51fa\u7684\u6982\u5ff5\u662f\nA. \u8fdb\u7a0b\u7ba1\u7406\nB. \u7a0b\u5e8f\u63a7\u5236\nC. \u8fc7\u7a0b\u63a7\u5236\nD. \u5b58\u50a8\u7a0b\u5e8f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5235269588172833, "meta-llama/Meta-Llama-3-8B": 0.9815191452045728, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9991111755533407}}, {"question": "\u7535\u78c1\u6ce2\u5305\u542b\u4e86\u5c04\u7ebf\u3001\u7ea2\u5916\u7ebf\u3001\u7d2b\u5916\u7ebf\u3001\u2f46\u7ebf\u7535\u6ce2\u7b49\uff0c\u6309\u6ce2\u2ed3\u7531\u2ed3\u5230\u77ed\u7684\u6392\u5217\u987a\u5e8f\u662f\nA. \u03b3\u5c04\u7ebf\u3001\u7ea2\u5916\u7ebf\u3001\u7d2b\u5916\u7ebf\u3001\u2f46\u7ebf\u7535\u6ce2\nB. \u2f46\u7ebf\u7535\u6ce2\u3001\u7ea2\u5916\u7ebf\u3001\u7d2b\u5916\u7ebf\u3001\u03b3\u5c04\u7ebf\nC. \u7ea2\u5916\u7ebf\u3001\u2f46\u7ebf\u7535\u6ce2\u3001\u03b3\u5c04\u7ebf\u3001\u7d2b\u5916\u7ebf\nD. \u7d2b\u5916\u7ebf\u3001\u2f46\u7ebf\u7535\u6ce2\u3001\u03b3\u5c04\u7ebf\u3001\u7ea2\u5916\u7ebf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3354456100429363, "meta-math/MetaMath-Mistral-7B": 0.4801919809846873, "itpossible/Chinese-Mistral-7B-v0.1": 0.3436615088034303, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8ba1\u7b97\u673a\u786c\u4ef6\u80fd\u76f4\u63a5\u8bc6\u522b\u548c\u6267\u884c\u7684\u53ea\u6709\nA. \u9ad8\u7ea7\u8bed\u8a00\nB. \u7b26\u53f7\u8bed\u8a00\nC. \u6c47\u7f16\u8bed\u8a00\nD. \u673a\u5668\u8bed\u8a00\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9570931660751978, "meta-math/MetaMath-Mistral-7B": 0.9880942110522121, "itpossible/Chinese-Mistral-7B-v0.1": 0.936499902780406, "HuggingFaceH4/zephyr-7b-beta": 0.9999967615415671, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9903305155949391, "meta-llama/Meta-Llama-3-8B": 0.9865565183389687, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9996283547948667}}, {"question": "\u516c\u5173\u793c\u4eea\u4f5c\u4e3a\u4e00\u79cd()\u7684\u6280\u5de7\uff0c\u662f\u516c\u5173\u4eba\u5458\u5728\u793e\u4f1a\u4ea4\u5f80\u4e2d\u5fc5\u987b\u9075\u5faa\u7684\u793c\u8282\u548c\u4eea\u5f0f\u3002\nA. \u4f20\u64ad\u548c\u6c9f\u901a\nB. \u5f62\u6001\u4e0e\u624b\u52bf\nC. \u8bed\u8a00\u827a\u672f\nD. \u4f53\u6001\u4e0e\u975e\u81ea\u7136\u8bed\u8a00\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9999109788781767, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5414513811361071, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe\u6570\u5217 ${x_n}$ \u6536\u655b\uff0c \u5219\nA. \u5f53 $\\lim _{n \\rightarrow \\infty} \\sin x_n=0$ \u65f6\uff0c $\\lim _{n \\rightarrow \\infty} x_n=0$.\nB. \u5f53 $\\lim _{n \\rightarrow \\infty}(x_n+\\sqrt{|x_n|})=0$ \u65f6\uff0c $\\lim _{n \\rightarrow \\infty} x_n=0$.\nC. \u5f53 $\\lim _{n \\rightarrow \\infty}\\left(x_n+x_n^2\\right)=0$ \u65f6\uff0c $\\lim _{n \\rightarrow \\infty} x_n=0$.\nD. \u5f53 $\\lim _{n \\rightarrow \\infty}(x_n+\\sin x_n\\right)=0$ \u65f6\uff0c $\\lim _{n \\rightarrow \\infty} x_n=0$.\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.302901633140621, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2906893535433972, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u54ea\u4e00\u9879\u76ee\u7684\u4e13\u9879\u80fd\u529b\u7279\u5f81\u4e0d\u662f\u201c\u4ee5\u6700\u5927\u5f3a\u5ea6\u91cd\u590d\u5b8c\u6210\u6bd4\u8d5b\u52a8\u4f5c\u201d\uff1f\nA. \u4e3e\u91cd\nB. \u6807\u67aa\nC. \u8df3\u9ad8\nD. \u7bee\u7403\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.42609615695652503, "meta-math/MetaMath-Mistral-7B": 0.5729374070139257, "itpossible/Chinese-Mistral-7B-v0.1": 0.4127032970431934, "HuggingFaceH4/zephyr-7b-beta": 0.6848944223151544, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6834552422254566, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5421903508873069}}, {"question": "\u65b0\u5f62\u52bf\u4e0b\u515a\u7684\u601d\u60f3\u5ba3\u4f20\u5de5\u4f5c\u7684\u4f7f\u547d\u4efb\u52a1\u662f\nA. \u4e3e\u65d7\u5e1c\u3001\u805a\u6c11\u5fc3\u3001\u80b2\u65b0\u4eba\u3001\u5174\u6587\u5316\u3001\u5c55\u5f62\u8c61\nB. \u6811\u65b0\u98ce\u3001\u80b2\u65b0\u5fb7\u3001\u5174\u4ea7\u4e1a\u3001\u5efa\u5f62\u8c61\u3001\u987a\u6c11\u5fc3\nC. \u805a\u6c11\u5fc3\u3001\u987a\u65f6\u4ee3\u3001\u6811\u65b0\u98ce\u3001\u8bb2\u7f8e\u5fb7\u3001\u80b2\u65b0\u4eba\nD. \u805a\u6c11\u58f0\u3001\u80b2\u601d\u60f3\u3001\u5174\u6587\u5316\u3001\u6811\u65b0\u98ce\u3001\u5bcc\u4ea7\u4e1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4684622846709611, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6811473946512717, "HuggingFaceH4/zephyr-7b-beta": 0.47077907923744455, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5334858072121007, "meta-llama/Meta-Llama-3-8B": 0.6054585623596243, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1947\u60a8\uff0c\u7f8e\u56fd\u56fd\u52a1\u537f\u9a6c\u6b47\u5c14\u63d0\u51fa\u63f4\u52a9\u6b27\u6d32\u590d\u5174\u8ba1\u5212\uff0c\u5e76\u7763\u4fc3\u6b27\u6d32\u56fd\u5bb6\u65b9\u9762\u73b0\u5148\u62df\u5b9a\u4e00\u9879\u8054\u5408\u6027\u8d28\u7684\u8ba1\u5212\uff0c\u8981\u6c42\u8be5\u8ba1\u5212\u6025\u4e8b\u4e0d\u80fd\u5f97\u5230\u6240\u6709\u6b27\u6d32\u56fd\u5bb6\u7684\u540c\u610f\uff0c\u4e5f\u5e94\u5f81\u5f97\u4e00\u90e8\u5206\u56fd\u5bb6\u7684\u540c\u610f\uff0c\u9a6c\u6b47\u5c14\u8ba1\u5212\u4f53\u73b0\u51fa\u6765\u7684\u7f8e\u56fd\u5bf9\u6b27\u6d32\u653f\u7b56\nA. \u5bfc\u81f4\u6b27\u6d32\u51fa\u73b0\u5bf9\u5cd9\nB. \u6210\u4e3a\u5fb7\u56fd\u5206\u88c2\u7684\u6839\u6e90\nC. \u4fc3\u6210\u4e86\u6b27\u6d32\u5e73\u7b49\u4f19\u4f34\u5173\u7cfb\nD. \u6709\u5229\u4e8e\u7164\u94a2\u8054\u8425\u7684\u5efa\u7acb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5962772949872157}}, {"question": "\u4e0b\u5217\u4e0d\u5c5e\u4e8e\u64cd\u4f5c\u7cfb\u7edf\u7684\u8f6f\u4ef6\u662f\nA. Linux\nB. Windows\nC. Office\nD. DOS\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.623610100046658, "meta-math/MetaMath-Mistral-7B": 0.8747211896871726, "itpossible/Chinese-Mistral-7B-v0.1": 0.49427642521989895, "HuggingFaceH4/zephyr-7b-beta": 0.9649707143422439, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4437880721010773, "meta-llama/Meta-Llama-3-8B": 0.7021354591102259, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9342635897068957}}, {"question": "\u57281543\u5e74\u51fa\u7248\u7684\u300a\u5929\u4f53\u8fd0\u884c\u8bba\u300b\u4e2d\uff0c\u54e5\u767d\u5c3c\u63d0\u51fa\u592a\u9633\u662f\u5b87\u5b99\u7684\u4e2d\u5fc3\uff1b\u57281859\u5e74\u51fa\u7248\u7684\u300a\u7269\u79cd\u8d77\u6e90\u300b\u4e2d\uff0c\u8fbe\u5c14\u6587\u63d0\u51fa\u4e86\u751f\u7269\u8fdb\u5316\u7406\u8bba\u3002\u4e24\u79cd\u5b66\u8bf4\u7684\u5171\u540c\u610f\u4e49\u662f\nA. \u5960\u5b9a\u4e86\u8d44\u4ea7\u9636\u7ea7\u9769\u547d\u7684\u601d\u60f3\u57fa\u7840\nB. \u80af\u5b9a\u4e86\u4eba\u7684\u4ef7\u503c\u548c\u5c0a\u4e25\nC. \u63d0\u4f9b\u4e86\u89c2\u5bdf\u4e16\u754c\u7684\u65b0\u89c6\u89d2\nD. \u63a8\u52a8\u4e86\u8fd1\u4ee3\u79d1\u5b66\u4f53\u7cfb\u7684\u5f62\u6210\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7784586876177954, "meta-math/MetaMath-Mistral-7B": 0.7728334814891259, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.678965643483175, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4676172418294136, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9649857640218948}}, {"question": "\u9996\u5c4a\u4e9a\u6b27\u4f1a\u8bae\u4e3e\u529e\u5730\u662f\nA. \u4f26\u6566\nB. \u4e0a\u6d77\nC. \u5e03\u9c81\u585e\u5c14\nD. \u66fc\u8c37\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.307023896814742, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5113108330895029}}, {"question": "\u201c\u6839\u636e\u8fd9\u4e00\u7406\u8bba\uff0c\u5c06\u65f6\u95f4\u548c\u7a7a\u95f4\u8bf4\u6210\u662f\u7edd\u5bf9\u7684\u662f\u6ca1\u6709\u610f\u4e49\u7684\uff0c\u56e0\u4e3a\u5bf9\u8fd9\u4e24\u79cd\u8303\u7574\u7684\u6d4b\u91cf\u901a\u5e38\u968f\u7740\u89c2\u5bdf\u8005\u7684\u8fd0\u52a8\u800c\u6539\u53d8\u3002\u2026\u2026\u5bf9\u4e8e\u95e8\u5916\u6c49\u6765\u8bf4\uff0c\u8fd9\u4e9b\u89c2\u70b9\u2014\u2014\u2014\u901a\u5e38\u7528\u4e0d\u80fd\u7406\u89e3\u7684\u6570\u5b66\u516c\u5f0f\u89e3\u91ca\u2014\u2014\u2014\u8868\u660e\u79d1\u5b66\u5df2\u7ecf\u8fbe\u5230\u4e86\u80fd\u88ab\u4eba\u7c7b\u6240\u77e5\u7684\u754c\u9650\u3002\u4e0a\u8ff0\u201c\u7406\u8bba\u201d\u7684\u4e3b\u8981\u610f\u4e49\u662f\nA. \u5f00\u521b\u4e86\u4ee5\u5b9e\u9a8c\u4e3a\u57fa\u7840\u7684\u8fd1\u4ee3\u79d1\u5b66\nB. \u5f25\u8865\u7ecf\u5178\u529b\u5b66\u5bf9\u65f6\u7a7a\u8ba4\u8bc6\u7684\u4e0d\u8db3\nC. \u53d1\u73b0\u80fd\u91cf\u8f90\u5c04\u7684\u8fd0\u52a8\u539f\u5219\nD. \u9884\u89c1\u6d77\u738b\u661f\u5b58\u5728\u7684\u51c6\u786e\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5141454396375037, "meta-math/MetaMath-Mistral-7B": 0.6315496874941476, "itpossible/Chinese-Mistral-7B-v0.1": 0.38392624496445477, "HuggingFaceH4/zephyr-7b-beta": 0.9868472802225681, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.510735718907192, "meta-llama/Meta-Llama-3-8B": 0.7932821704881665, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9441134558548384}}, {"question": "\u300a\u8336\u82b1\u8d4b\u300b\u4e2d\u7684\u201c\u8336\u82b1\u201d\u8c61\u5f81\u7740\nA. \u7956\u56fd\u7684\u9762\u8c8c\nB. \u6218\u58eb\u7684\u9c9c\u8840\nC. \u7f8e\u597d\u7684\u7ae5\u5e74\nD. \u70ed\u70c8\u7684\u7231\u60c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3263118171970479, "meta-math/MetaMath-Mistral-7B": 0.6882562915194921, "itpossible/Chinese-Mistral-7B-v0.1": 0.5174255073248946, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7784091514458468, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4095427563634718}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6570\u5b57\u8bc1\u4e66\u7684\u8bf4\u6cd5\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u6570\u5b57\u8bc1\u4e66\u662f\u5728\u7f51\u4e0a\u8fdb\u884c\u4fe1\u606f\u4ea4\u6362\u548c\u5546\u52a1\u6d3b\u52a8\u7684\u8eab\u4efd\u8bc1\u660e\nB. \u6570\u5b57\u8bc1\u660e\u7528\u4e8e\u8eab\u4efd\u8bc1\u660e\uff0c\u4e0d\u53ef\u516c\u5f00\nC. \u5728\u7528\u6237\u7aef\uff0c\u53ea\u9700\u7ef4\u62a4\u5f53\u524d\u6709\u6548\u7684\u8bc1\u4e66\u5217\u8868\nD. \u6570\u5b57\u8bc1\u4e66\u4f7f\u7528\u516c\u94a5\u4f53\u5236\uff0c\u7528\u6237\u4f7f\u7528\u516c\u94a5\u8fdb\u884c\u52a0\u5bc6\u548c\u7b7e\u540d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6065385164464134, "meta-math/MetaMath-Mistral-7B": 0.8935000376164269, "itpossible/Chinese-Mistral-7B-v0.1": 0.6673016727365864, "HuggingFaceH4/zephyr-7b-beta": 0.9458117826620946, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7129184155859635, "meta-llama/Meta-Llama-3-8B": 0.46147264940900035, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6572330947974045}}, {"question": "\u6309\u53f0\u98ce\u53ef\u80fd\u9020\u6210\u7684\u5f71\u54cd\u7a0b\u5ea6\uff0c\u4ece\u8f7b\u5230\u91cd\u5411\u793e\u4f1a\u53d1\u5e03()\u53f0\u98ce\u9884\u8b66\u4fe1\u53f7\nA. \u7ea2\u6a59\u9ec4\u84dd\nB. \u7ea2\u84dd\u9ec4\u6a59\nC. \u84dd\u7ea2\u9ec4\u6a59\nD. \u84dd\u9ec4\u6a59\u7ea2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5263885979544514, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u6cd5\u7ed3\u6784\u4e2d\uff0c\u4e3b\u8bed\u662f\u9886\u4e8b\u6027\u8d28\u7684\u662f\nA. \u5c0f\u6885\u957f\u7740\u4e00\u53cc\u5927\u773c\u775b\nB. \u5988\u5988\u9886\u5c0f\u7ea2\u53bb\u516c\u56ed\nC. \u5c0f\u6885\u9886\u4e86\u4e00\u5957\u6559\u6750\nD. \u5171\u4ea7\u515a\u9886\u5bfc\u6211\u4eec\u5411\u524d\u8fdb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5bf9\u9884\u6d4b\u4e0e\u51b3\u7b56\u5173\u7cfb\u7684\u8868\u8ff0\uff0c\u4e0d\u6b63\u786e\u7684\u4e00\u53e5\u662f\nA. \u51b3\u7b56\u662f\u9884\u6d4b\u7684\u524d\u63d0\u6761\u4ef6\nB. \u9884\u6d4b\u8d2f\u7a7f\u4e8e\u51b3\u7b56\u7684\u5168\u8fc7\u7a0b\nC. \u51b3\u7b56\u548c\u9884\u6d4b\u5173\u7cfb\u5bc6\u5207\nD. \u9884\u6d4b\u662f\u4e3a\u51b3\u7b56\u670d\u52a1\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5469986248786401, "itpossible/Chinese-Mistral-7B-v0.1": 0.30359886656533613, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3818183045785905, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4421053352951277}}, {"question": "\u9a6c\u65af\u6d1b\u7684\u9700\u6c42\u5c42\u6b21\u7406\u8bba\u4e2d\u6700\u8457\u540d\u7684\u6982\u5ff5\u662f\nA. \u751f\u7406\u9700\u8981\nB. \u81ea\u6211\nC. \u81ea\u6211\u5b9e\u73b0\nD. \u8d85\u6211\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5342463515214007, "meta-math/MetaMath-Mistral-7B": 0.6475650035733101, "itpossible/Chinese-Mistral-7B-v0.1": 0.545648993475378, "HuggingFaceH4/zephyr-7b-beta": 0.9244545803870328, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5525861202336725, "meta-llama/Meta-Llama-3-8B": 0.8938882267512643, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.952182655345489}}, {"question": "\u7532\u5728\u4e00\u80e1\u540c\u53e3\u62a2\u52ab\u4e00\u5973\u9752\u5e74\u94b1\u5305\u3002\u62a2\u5230\u94b1\u540e\uff0c\u7a81\u7136\u53d1\u73b0\u8be5\u5973\u9752\u5e74\u662f\u81ea\u5df1\u7684\u90bb\u5c45\uff0c\u4e8e\u662f\u5c06\u94b1\u5305\u5f53\u9762\u8fd8\u7ed9\u5973\u9752\u5e74\uff0c\u58f0\u79f0\u521a\u624d\u7684\u884c\u4e3a\u662f\u5f00\u73a9\u7b11\u3002\u7532\u7684\u884c\u4e3a\u662f\nA. \u72af\u7f6a\u4e2d\u6b62\nB. \u72af\u7f6a\u65e2\u9042\nC. \u72af\u7f6a\u9884\u5907\nD. \u72af\u7f6a\u672a\u9042\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6c14\u4f53\u5728\u8840\u6db2\u4e2d\u8fd0\u8f93\u7684\u53d9\u8ff0\uff0c \u9519\u8bef\u7684\u662f\nA. $\\mathrm{CO}_2$ \u4e3b\u8981\u4ee5 $\\mathrm{HCO}_3^{-}$\u5f62\u5f0f\u8fd0\u8f93\nB. $\\mathrm{O}_2$ \u548c $\\mathrm{Hb}$ \u7ed3\u5408\u53cd\u5e94\u5feb\u5e76\u9700\u9176\u50ac\u5316\nC. $\\mathrm{CO}_2$ \u548c $\\mathrm{O}_2$ \u90fd\u6709\u7269\u7406\u6eb6\u89e3\u5f62\u5f0f\nD. $\\mathrm{CO}_2$ \u548c $\\mathrm{Hb}$ \u7ed3\u5408\u4e0d\u9700\u8981\u9176\u7684\u50ac\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35850821328802296, "meta-math/MetaMath-Mistral-7B": 0.8192785015653582, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9856601299945176, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8739043307070243, "meta-llama/Meta-Llama-3-8B": 0.45272358181533334, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5746624269805185}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6d88\u8d39\u6c34\u5e73\u5bf9\u6d88\u8d39\u7ed3\u6784\u7684\u5f71\u54cd\u7684\u8bf4\u6cd5\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u8010\u7528\u6d88\u8d39\u54c1\u652f\u51fa\u6bd4\u91cd\u968f\u6d88\u8d39\u6c34\u5e73\u63d0\u9ad8\u6301\u7eed\u63d0\u9ad8\nB. \u6d88\u8d39\u6c34\u5e73\u8f83\u4f4e\u65f6\uff0c\u98df\u54c1\u652f\u51fa\u6bd4\u91cd\u8f83\u9ad8\uff0c\u968f\u6d88\u8d39\u6c34\u5e73\u63d0\u9ad8\uff0c\u98df\u54c1\u652f\u51fa\u6bd4\u91cd\u6709\u4e0b\u964d\u8d8b\u52bf\nC. \u4f4f\u623f\u533b\u7597\u7b49\u652f\u51fa\u9879\u76ee\u968f\u6d88\u8d39\u6c34\u5e73\u63d0\u9ad8\u5448\u4e0b\u964d\u8d8b\u52bf\nD. \u4f4e\u6536\u5165\u56fd\u5bb6\u52b3\u52a1\u652f\u51fa\u6bd4\u91cd\u9ad8\uff0c\u9ad8\u6536\u5165\u56fd\u5bb6\u52b3\u52a1\u652f\u51fa\u6bd4\u91cd\u4f4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4681036181935766, "meta-math/MetaMath-Mistral-7B": 0.8837350349668124, "itpossible/Chinese-Mistral-7B-v0.1": 0.519076852076151, "HuggingFaceH4/zephyr-7b-beta": 0.9468472508877063, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6371872352844409, "meta-llama/Meta-Llama-3-8B": 0.4970893220318086, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.959334738837896}}, {"question": "\u4ee5\u8840\u7edf\u3001\u8840\u7f18\u548c\u804c\u4e1a\u4e3a\u6807\u51c6\uff0c\u5c06\u793e\u4f1a\u6210\u5458\u5212\u5206\u4e3a\u4e0d\u540c\u7fa4\u4f53\u6216\u793e\u4f1a\u96c6\u56e2\uff0c\u8fd9\u79cd\u793e\u4f1a\u7b49\u7ea7\u5236\u5ea6\u662f\nA. \u79cd\u59d3\nB. \u7b49\u7ea7\nC. \u9636\u5c42\nD. \u79cd\u65cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.47604483583345847, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u77ed\u65f6\u8bb0\u5fc6\u7684\u5bb9\u91cf\u6709\u9650\uff0c\u5927\u4f53\u4e0a\u662f\uff08\uff09\u4e2a\u7ec4\u5757\nA. 7\u00b12\nB. 3\u00b12\nC. 20\u00b12\nD. 10\u00b12\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4497818193146129, "meta-math/MetaMath-Mistral-7B": 0.561198054018164, "itpossible/Chinese-Mistral-7B-v0.1": 0.6449752082183617, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8599727189301879, "meta-llama/Meta-Llama-3-8B": 0.9580646500903849, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9976314192884546}}, {"question": "\u7814\u7a76\u4eba\u7c7b\u6027\u72b6\u9057\u4f20\u8f83\u597d\u7684\u65b9\u6cd5\u662f\nA. \u8fdb\u884cx2\u5361\u5e73\u65b9\u68c0\u9a8c\nB. \u4ee5\u4e0a\u6240\u6709\u65b9\u6cd5\nC. \u6784\u5efa\u548c\u8bc4\u4f30\u5bb6\u65cf\u7cfb\u8c31\u56fe\nD. \u8fdb\u884c\u53ef\u63a7\u7684\u4ea4\u914d\u5b9e\u9a8c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.475267084275683, "meta-math/MetaMath-Mistral-7B": 0.7676230419001427, "itpossible/Chinese-Mistral-7B-v0.1": 0.48651170809593314, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5756811746638139, "meta-llama/Meta-Llama-3-8B": 0.6304846321892986, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7227369233152727}}, {"question": "\u4e0b\u5217\u4e0e\u5b9e\u9a8c\u76f8\u5173\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u6807\u5fd7\u91cd\u6355\u6cd5\u8c03\u67e5\u7070\u559c\u9e4a\u79cd\u7fa4\u5bc6\u5ea6\u65f6\u90e8\u5206\u6807\u5fd7\u7269\u8131\u843d\uff0c\u6240\u5f97\u6570\u503c\u53ef\u80fd\u504f\u5927\nB. \u572895%\u4e59\u9187\u4e2d\u52a0\u5165\u65e0\u6c34Na2CO3\u540e\u53ef\u63d0\u9ad8\u8272\u7d20\u7684\u6eb6\u89e3\u5ea6\nC. \u53f6\u7eff\u4f53\u8272\u7d20\u7684\u7eb8\u5c42\u6790\u7ed3\u679c\u8868\u660e\uff0c\u53f6\u7eff\u7d20b\u5728\u5c42\u6790\u6db2\u4e2d\u7684\u6eb6\u89e3\u5ea6\u6700\u5927\nD. \u5149\u5b66\u663e\u5fae\u955c\u53ef\u7528\u4e8e\u89c2\u5bdf\u690d\u7269\u7ec6\u80de\u7684\u8d28\u58c1\u5206\u79bb\u73b0\u8c61\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.303006867405966, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u8db3\u667a\u591a\u8c0b\uff0c\u968f\u673a\u5e94\u53d8\u201d\u4f53\u73b0\u4e86\u601d\u7ef4\u7684\nA. \u72ec\u7acb\u6027\nB. \u903b\u8f91\u6027\nC. \u7075\u6d3b\u6027\nD. \u5e7f\u9614\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8630720413613774, "meta-math/MetaMath-Mistral-7B": 0.9867412644714441, "itpossible/Chinese-Mistral-7B-v0.1": 0.897992060377659, "HuggingFaceH4/zephyr-7b-beta": 0.998512225545839, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9617531700302143, "meta-llama/Meta-Llama-3-8B": 0.7752779189547618, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9982222423934716}}, {"question": "\u4eba\u7c7b\u6700\u8fd1\u4e00\u6b21\u89c2\u6d4b\u5230\u94f6\u6cb3\u7cfb\u5185\u7684\u8d85\u65b0\u661f\u7206\u53d1\u662f\u5728\u54ea\u4e00\u5e74\nA. 1604\nB. 2018\nC. 1987\nD. 1054\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e24\u5c0f\u6837\u672c\u6570\u503c\u53d8\u91cf\u8d44\u6599\u6bd4\u8f83\u7684\u5047\u8bbe\u68c0\u9a8c\uff0c\u9996\u5148\u5e94\u8003\u8651\nA. \u7528\u79e9\u548c\u68c0\u9a8c\nB. \u7528u\u68c0\u9a8c\nC. \u8d44\u6599\u7b26\u5408\u79e9\u548c\u68c0\u9a8c\u8fd8\u662ft\u68c0\u9a8c\u7684\u6761\u4ef6\nD. \u7528t\u68c0\u9a8c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8444007130409579, "meta-math/MetaMath-Mistral-7B": 0.9097613319711575, "itpossible/Chinese-Mistral-7B-v0.1": 0.5377748110289073, "HuggingFaceH4/zephyr-7b-beta": 0.9991684319397179, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5829451115631442, "meta-llama/Meta-Llama-3-8B": 0.735203593219254, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5778375239071355}}, {"question": "\u5b88\u6cd5\u7684\u6700\u4f4e\u72b6\u6001\u662f\nA. \u4f9d\u6cd5\u529e\u4e8b\uff0c\u5f62\u6210\u7edf\u4e00\u7684\u6cd5\u5f8b\u79e9\u5e8f\nB. \u5b88\u6cd5\u4e3b\u4f53\u4ee5\u6cd5\u7684\u4e3b\u4eba\u7684\u59ff\u6001\u81ea\u89c9\u3001\u79ef\u6781\u5730\u884c\u4f7f\u6743\u5229\u3001\u5c65\u884c\u4e49\u52a1\nC. \u5728\u5916\u5728\u884c\u4e3a\u548c\u5185\u5728\u52a8\u673a\u65b9\u9762\u90fd\u7b26\u5408\u6cd5\u7684\u7cbe\u795e\u8981\u6c42\nD. \u4e0d\u8fdd\u6cd5\u72af\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4047758958868366, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5579784254483179, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5881915604793102, "meta-llama/Meta-Llama-3-8B": 0.5340705984553206, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4534295244654593}}, {"question": "\u82e5sin(a)cos(a)=1/8\uff0c\u4e14pi/4<a<pi/2\uff0c\u5219sin(a)-cos(a)=\nA. -\\sqrt{3}/2\nB. 3/4\nC. -3/4\nD. \\sqrt{3}/2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.321937374146456, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3121480846627187, "HuggingFaceH4/zephyr-7b-beta": 0.91295754267049, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.2821833983601388, "meta-llama/Meta-Llama-3-8B": 0.2741610822679311, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u60a3\u8005\u54b3\u9006\u9635\u4f5c\uff0c\u75f0\u4e2d\u5e26\u8840\uff0c\u65f6\u65f6\u6c57\u51fa\uff0c\u80f8\u80c1\u80c0\u75db\uff0c\u53e3\u82e6\u54bd\u5e72\uff0c\u5c3f\u9ec4\u4fbf\u79d8\uff0c\u820c\u7ea2\u82d4\u8584\u9ec4\uff0c\u8109\u5f26\u6570\u3002\u5176\u8bca\u65ad\u662f\nA. \u80ba\u75e8\uff0c\u9634\u865a\u706b\u65fa\nB. \u80ba\u75c8\uff0c\u6210\u75c8\u671f\nC. \u54b3\u55fd\uff0c\u809d\u706b\u72af\u80ba\nD. \u54af\u8840\uff0c\u809d\u706b\u72af\u80ba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4320665390141267, "meta-math/MetaMath-Mistral-7B": 0.6846432832460222, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9575650797022133, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5039862323666473, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53e4\u7434\u6700\u521d\u53ea\u6709\u4e94\u6839\u5f26\uff0c\u4ee3\u8868\u7740\u91d1\u3001\u6728\u3001\u6c34\u3001\u706b\u3001\u571f\u3002\u540e\u6765\u53c8\u589e\u52a0\u4e86\u4e24\u6839\u5f26\uff0c\u8fd9\u4e24\u6839\u5f26\u4ee3\u8868\nA. \u9634\u3001\u9633\nB. \u6587\u3001\u6b66\nC. \u5357\u3001\u5317\nD. \u5929\u3001\u5730\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4377363824954003, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7535\u5b50\u90ae\u4ef6\u7cfb\u7edf\u7684\u90ae\u4ef6\u534f\u8bae\u6709\u53d1\u9001\u534f\u8baeSMTP\u548c\u63a5\u6536\u534f\u8baeP0P3/IMAP4\u3002SMTP\u53d1\u9001\u534f\u8bae\u4e2d\uff0c\u53d1\u9001\u8eab\u4efd\u6807\u8bc6\u7684\u6307\u4ee4\u662f\nA. HELO\nB. SAML\nC. HELP\nD. SEND\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9603429062614169, "meta-math/MetaMath-Mistral-7B": 0.999192159703341, "itpossible/Chinese-Mistral-7B-v0.1": 0.8574881851911439, "HuggingFaceH4/zephyr-7b-beta": 0.9999804639660447, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9957775482390443, "meta-llama/Meta-Llama-3-8B": 0.8627277452142111, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9282137291878676}}, {"question": "\u6b27\u76df\u6210\u7acb\u4ee5\u6765\u7684\u7b2c\u4e00\u4efd\u5171\u540c\u5b89\u5168\u6218\u7565\u6587\u4ef6\u4f1a\u8bae\u662f\u4ec0\u4e48\nA. \u56fd\u9632\u90e8\u957f\u4f1a\u8bae\nB. \u9632\u52a1\u5408\u4f5c\u4f1a\u8bae\nC. \u5e03\u9c81\u585e\u5c14\u9996\u8111\u4f1a\u8bae\nD. \u96c5\u5c14\u5854\u4f1a\u8bae\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3885978497153787, "meta-math/MetaMath-Mistral-7B": 0.547690360045609, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6554679759426459, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5899675946343428, "meta-llama/Meta-Llama-3-8B": 0.3324879294350306, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6722497502797956}}, {"question": "\u7532\u57282014\u5e746\u6708\u7acb\u4e0b\u9057\u5631\uff0c\u5c06\u5176\u9057\u4ea7\u7684\u4e00\u534a\u7531\u5176\u5b50\u4e59\u7ee7\u627f\u3002\u75322017\u5e74\u4e0d\u5e78\u6b7b\u4ea1\uff0c\u73b0\u7532\u7684\u7ee7\u627f\u4eba\u4e0e\u4e59\u5728\u6743\u5229\u4e0a\u53d1\u751f\u7ea0\u7eb7\uff0c\u5219\nA. \u4e59\u53ea\u53ef\u4ee5\u4f9d\u636e\u9057\u5631\u7ee7\u627f\u7532\u9057\u4ea7\u7684\u4e00\u534a\nB. \u7531\u4e8e\u9057\u5631\u65f6\u95f4\u5df2\u7ecf\u8d85\u8fc73\u5e74\uff0c\u4e59\u56e0\u9057\u5631\u800c\u53d6\u5f97\u7684\u6743\u5229\u5df2\u7ecf\u6d88\u706d\nC. \u4e59\u53ea\u80fd\u4f9d\u636e\u6cd5\u5b9a\u7ee7\u627f\u7ee7\u627f\u7532\u9057\u4ea7\u4e00\u534a\u4e2d\u7684\u90e8\u5206\nD. \u4e59\u4e0d\u4ec5\u53ef\u4ee5\u4f9d\u636e\u9057\u5631\u7ee7\u627f\u7532\u9057\u4ea7\u7684\u4e00\u534a\uff0c\u4e5f\u53ef\u4ee5\u4f9d\u636e\u6cd5\u5b9a\u7ee7\u627f\u7ee7\u627f\u7532\u9057\u4ea7\u53e6\u4e00\u534a\u4e2d\u7684\u90e8\u5206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5269050749187462, "meta-math/MetaMath-Mistral-7B": 0.7093295546022303, "itpossible/Chinese-Mistral-7B-v0.1": 0.7095423625301378, "HuggingFaceH4/zephyr-7b-beta": 0.8876136428372493, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5838300115426212, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53\u4eca\u4e16\u754c\u5145\u5f53\u56fd\u9645\u653f\u6cbb\u884c\u4e3a\u4f53\u7684\u56fd\u5bb6\u5171\u6709\nA. 170\u591a\u4e2a\nB. 200\u591a\u4e2a\nC. 100\u4e2a\nD. 190\u591a\u4e2a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u690d\u7269\u6027\u98df\u7269\u7684\u57fa\u7840\u4e0a\u518d\u6dfb\u52a0\u5c11\u91cf\u52a8\u7269\u6027\u98df\u7269\uff0c\u86cb\u767d\u8d28\u7684\u751f\u7269\u4ef7\nA. \u4f1a\u964d\u4f4e\nB. \u65e0\u6cd5\u9884\u8ba1\nC. \u4f1a\u63d0\u9ad8\nD. \u4e0d\u53d8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8039277121052101, "meta-math/MetaMath-Mistral-7B": 0.9666806378662479, "itpossible/Chinese-Mistral-7B-v0.1": 0.7101654304700179, "HuggingFaceH4/zephyr-7b-beta": 0.5260222384952601, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9312766525853376, "meta-llama/Meta-Llama-3-8B": 0.6536630792000145, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9892138883955381}}, {"question": "\u4e00\u4e2a\u5c0f\u9ea6\u6742\u4ea4\u7ec4\u5408\u7684F2\uff0cB1\uff0cB2\u4e09\u4e2a\u4e16\u4ee3\u7684\u7c92\u91cd\u65b9\u5dee\u5206\u522b\u4e3a500\uff0c400\u548c450\uff0c\u5219\u8be5\u6742\u4ea4\u7ec4\u5408\u7c92\u91cd\u7684\u72ed\u4e49\u9057\u4f20\u7387\u4e3a\nA. 60%\nB. 85%\nC. 15%\nD. 30%\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u84dd\u8272\u591a\u7459\u6cb3\u5706\u821e\u66f2\u300b\u662f\u5965\u5730\u5229\u4f5c\u66f2\u5bb6\u8c01\u7684\u4ee3\u8868\u4f5c\nA. \u8d1d\u591a\u82ac\nB. \u674e\u65af\u7279\nC. \u7ea6\u7ff0\u2022\u65bd\u7279\u52b3\u65af\nD. \u9a6c\u8d6b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6445511251041215, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8936828276832736, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9969907370511413}}, {"question": "\u4e0b\u5217\u7ecf\u6d4e\u4e1a\u52a1\u4e2d\uff0c\u5e94\u7f16\u5236\u8f6c\u8d26\u51ed\u8bc1\u7684\u662f\nA. \u6536\u56de\u5e94\u6536\u8d26\u6b3e\nB. \u5e94\u4ed8\u6295\u8d44\u8005\u5229\u6da6\nC. \u9884\u4ed8\u4fdd\u9669\u8d39\nD. \u652f\u4ed8\u501f\u6b3e\u5229\u606f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "zh\u3001ch\u3001sh\u3001r\u56db\u4e2a\u8f85\u97f3\u7684\u53d1\u97f3\u90e8\u4f4d\u662f\nA. \u820c\u5c16\u540e \nB. \u820c\u5c16\u524d \nC. \u820c\u9762\nD. \u820c\u5c16\u4e2d \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3543879727206675, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4551317783770933}}, {"question": "2005\u5e748\u6708\u5168\u56fd\u4eba\u5927\u5e38\u59d4\u4f1a\u5bf9\u300a\u5987\u5973\u6743\u76ca\u4fdd\u969c\u6cd5\u300b\u8fdb\u884c\u4e86\u4fee\u6b63\uff0c\u589e\u52a0\u4e86\u201c\u7981\u6b62\u5bf9\u5987\u5973\u5b9e\u65bd\u6027\u9a9a\u6270\u201d\u7684\u89c4\u5b9a\uff0c\u4f46\u6ca1\u6709\u5bf9\u201c\u6027\u9a9a\u6270\u201d\u4e88\u4ee5\u5177\u4f53\u754c\u5b9a\u30022007\u5e744\u6708\uff0c\u67d0\u7701\u4eba\u5927\u5e38\u59d4\u4f1a\u901a\u8fc7\u300a\u5b9e\u65bd\u3008\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u5987\u5973\u6743\u76ca\u4fdd\u969c\u6cd5\u3009\u529e\u6cd5\u300b\uff0c\u89c4\u5b9a\u201c\u7981\u6b62\u4ee5\u8bed\u8a00\u3001\u6587\u5b57\u3001\u7535\u5b50\u4fe1\u606f\u3001\u80a2\u4f53\u7b49\u5f62\u5f0f\u5bf9\u5987\u5973\u5b9e\u884c\u9a9a\u6270\u201d\u3002\u5173\u4e8e\u8be5\u300a\u529e\u6cd5\u300b\uff0c\u4e0b\u5217\u54ea\u4e00\u9009\u9879\u80fd\u6210\u7acb\uff1f\nA. \u300a\u529e\u6cd5\u300b\u5c5e\u4e8e\u5bf9\u300a\u5987\u5973\u6743\u76ca\u4fdd\u969c\u6cd5\u300b\u7684\u53d8\u901a\u6216\u8865\u5145\u89c4\u5b9a\nB. \u300a\u529e\u6cd5\u300b\u5bf9\u6784\u6210\u201c\u6027\u9a9a\u6270\u201d\u5177\u4f53\u884c\u4e3a\u6240\u4f5c\u7684\u754c\u5b9a\uff0c\u5c5e\u4e8e\u5bf9\u300a\u5987\u5973\u6743\u76ca\u4fdd\u969c\u6cd5\u300b\u7684\u7acb\u6cd5\u89e3\u91ca\nC. \u300a\u529e\u6cd5\u300b\u5bf9\u201c\u6027\u9a9a\u6270\u201d\u8fdb\u884c\u4e86\u4f53\u7cfb\u89e3\u91ca\nD. \u300a\u529e\u6cd5\u300b\u5c5e\u4e8e\u300a\u5987\u5973\u6743\u76ca\u4fdd\u969c\u6cd5\u300b\u7684\u4e0b\u4f4d\u6cd5\uff0c\u6309\u7167\u6cd5\u5f8b\u9ad8\u4e8e\u6cd5\u89c4\u7684\u539f\u5219\uff0c\u5176\u6548\u529b\u8f83\u4f4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u60a3\u8005\uff0c\u5973\uff0c25\u5c81\uff0c\u598a\u5a208\u5468\u3002\u4e0b\u5217\u5404\u9879\uff0c\u53ef\u4ee5\u670d\u7528\u7684\u836f\u7ec4\u662f\nA. \u4e09\u68f1\u3001\u83aa\u672f\u3001\u6c34\u86ed\nB. \u5f53\u5f52\u3001\u963f\u80f6\u3001\u4e39\u53c2\nC. \u5df4\u8c46\u3001\u7275\u725b\u5b50\u3001\u5546\u9646\nD. \u6591\u8765\u3001\u9e9d\u9999\u3001\u867b\u866b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3121829301806, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.46683853625913346, "HuggingFaceH4/zephyr-7b-beta": 0.9771373317963873, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5808696715925966, "meta-llama/Meta-Llama-3-8B": 0.5635695824823196, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8538203297308533}}, {"question": "1980\u5e7412\u670811\u65e5\uff0c\u6e29\u5dde\u4eba\u7ae0\u534e\u59b9\u9886\u5230\u4e86\u81ea\u5df1\u671f\u5f85\u5f88\u4e45\u7684\u8425\u4e1a\u6267\u7167\uff0c\u6210\u4e3a\u5168\u56fd\u7b2c\u4e00\u4e2a\u4e2a\u4f53\u5de5\u5546\u6237\u3002\u4f46\u957f\u671f\u4ee5\u6765\uff0c\u6211\u4eec\u4e00\u76f4\u628a\u975e\u516c\u6709\u5236\u7ecf\u6d4e\u770b\u505a\u662f\u793e\u4f1a\u4e3b\u4e49\u7ecf\u6d4e\u7684\u5bf9\u7acb\u9762\uff0c\u8c08\u201c\u79c1\u201d\u8272\u53d8\uff0c\u65e9\u572830\u5e74\u524d\uff0c\u6e29\u5dde\u548c\u5168\u56fd\u591a\u6570\u57ce\u5e02\u4e00\u6837\uff0c\u505a\u4e2a\u201c\u4e2a\u4f53\u6237\u201d\u5e76\u4e0d\u662f\u4ef6\u5149\u9c9c\u7684\u4e8b\u3002\u201c\u5973\u5b69\u5b50\u5f53\u4e2a\u4f53\u6237\uff0c\u8fde\u5bf9\u8c61\u90fd\u627e\u4e0d\u7740\u2026\u2026\u201d\u3002\u4f7f\u4eba\u4eec\u6446\u8131\u8fd9\u79cd\u6050\u201c\u79c1\u201d\u75c7\u56f0\u6270\u7684\u5236\u5ea6\u6027\u51b3\u7b56\u662f\nA. 1999\u4e5d\u5c4a\u4eba\u5927\u4e8c\u6b21\u4f1a\u8bae\u6b63\u5f0f\u786e\u7acb\u7684\u4f9d\u6cd5\u6cbb\u56fd\u6218\u7565\nB. 1997\u5e74\u4e2d\u5171\u5341\u4e94\u5927\u7684\u51b3\u7b56\nC. 1984\u5e74\u4e2d\u5171\u5341\u4e8c\u5c4a\u4e09\u4e2d\u5168\u4f1a\u901a\u8fc7\u300a\u5173\u4e8e\u7ecf\u6d4e\u4f53\u5236\u6539\u9769\u7684\u51b3\u5b9a\u300b\nD. 1992\u5e74\u9093\u5c0f\u5e73\u540c\u5fd7\u7684\u5357\u65b9\u8c08\u8bdd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7535\u6d41\u4e92\u611f\u5668\u94c1\u82af\u5185\u7684\u4ea4\u53d8\u4e3b\u78c1\u901a\u662f\u7531\uff08\uff09\u4ea7\u751f\u7684\u3002\nA. \u4e00\u6b21\u548c\u4e8c\u6b21\u7535\u6d41\u5171\u540c\nB. \u4e00\u6b21\u7ed5\u7ec4\u5185\u901a\u8fc7\u7684\u7535\u6d41\nC. \u4e8c\u6b21\u7ed5\u7ec4\u5185\u901a\u8fc7\u7684\u7535\u6d41\nD. \u4e00\u6b21\u7ed5\u7ec4\u4e24\u7aef\u7535\u538b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6561702114960617, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6122928314962282, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6569477623723531, "meta-llama/Meta-Llama-3-8B": 0.5217975898032587, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u300a\u7269\u6743\u6cd5\u300b\u7684\u89c4\u5b9a\uff0c\u4e0b\u5217\u6709\u5173\u5404\u7c7b\u7528\u76ca\u7269\u6743\u7684\u8868\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u571f\u5730\u627f\u5305\u7ecf\u8425\u6743\u5728\u8bbe\u5b9a\u4e0a\u987b\u5b58\u5728\u4e09\u65b9\u5f53\u4e8b\u4eba\nB. \u5b85\u57fa\u5730\u4f7f\u7528\u6743\u4e0d\u5f97\u62b5\u62bc\uff0c\u4f46\u53ef\u4ee5\u7ee7\u627f\nC. \u5efa\u8bbe\u7528\u5730\u4f7f\u7528\u6743\u5e94\u5f53\u4ee5\u5212\u62e8\u65b9\u5f0f\u8bbe\u7acb\nD. \u5730\u5f79\u6743\u53ef\u4ee5\u6709\u507f\uff0c\u4e5f\u53ef\u4ee5\u65e0\u507f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4702008436521869, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u810a\u9ad3\u578b\u9888\u690e\u75c5\u65e9\u671f\u51fa\u73b0\u7684\u75c7\u72b6\u662f\nA. \u56db\u80a2\u4e4f\u529b\uff0c\u6301\u7269\u4e0d\u7a33\nB. \u5934\u75db\u3001\u5fc3\u60b8\nC. \u9888\u80a9\u75db\u5411\u4e0a\u80a2\u653e\u5c04\nD. \u7729\u6655\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u70ed\u2f12\u73af\u6d41\u7684\u5f62\u6210\u8fc7\u7a0b\u4e3a a\u8fd1\u5730\u2faf\u7a7a\u2f53\u53d7\u70ed\u6216\u51b7\u5374\uff1bb\u540c\u2f00\u2f54\u5e73\u2faf\u4e0a\u4ea7\u2f63\u2f53\u538b\u5dee\u5f02\uff1bc\u5f62\u6210\u2f24\u2f53\u7684\u2f54\u5e73\u8fd0\u52a8\uff1bd\u2f53\u6d41\u7684\u4e0a\u5347\u6216\u4e0b\u964d\u8fd0\u52a8\nA. dbac\nB. abcd\nC. adbc\nD. acbd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3689108554330874}}, {"question": "\u6cd5\u5f8b\u89c4\u5219\u662f\u6cd5\u5f8b\u7684\u57fa\u672c\u6784\u6210\u56e0\u7d20\u3002\u4e0b\u5217\u5173\u4e8e\u6cd5\u5f8b\u89c4\u5219\u5206\u7c7b\u7684\u8868\u8ff0\uff0c\u54ea\u4e00\u9879\u53ef\u4ee5\u6210\u7acb\uff1f\nA. \u300a\u4e2d\u5c0f\u4f01\u4e1a\u4fc3\u8fdb\u6cd5\u300b\u7b2c31\u6761\u89c4\u5b9a\uff1a\u56fd\u5bb6\u9f13\u52b1\u4e2d\u5c0f\u4f01\u4e1a\u4e0e\u7814\u7a76\u673a\u6784\u3001\u5927\u4e13\u9662\u6821\u5f00\u5c55\u6280\u672f\u5408\u4f5c\u3001\u5f00\u53d1\u4e0e\u4ea4\u6d41\uff0c\u4fc3\u8fdb\u79d1\u6280\u6210\u679c\u4ea7\u4e1a\u5316\uff0c\u79ef\u6781\u53d1\u5c55\u79d1\u6280\u578b\u4e2d\u5c0f\u4f01\u4e1a\u3002\u6b64\u89c4\u5b9a\u4e3a\u5f3a\u884c\u6027\u89c4\u5219\nB. \u300a\u5f8b\u5e08\u6cd5\u300b\u7b2c13\u6761\u89c4\u5b9a\uff1a\u6ca1\u6709\u53d6\u5f97\u5f8b\u5e08\u6267\u4e1a\u8bc1\u4e66\u7684\u4eba\u5458\uff0c\u4e0d\u5f97\u4ee5\u5f8b\u5e08\u540d\u4e49\u4ece\u4e8b\u6cd5\u5f8b\u670d\u52a1\u4e1a\u52a1\uff1b\u9664\u6cd5\u5f8b\u53e6\u6709\u89c4\u5b9a\u5916\uff0c\u4e0d\u5f97\u4ece\u4e8b\u8bc9\u8bbc\u4ee3\u7406\u6216\u8005\u8fa9\u62a4\u4e1a\u52a1\u3002\u6b64\u89c4\u5b9a\u4e3a\u4e49\u52a1\u6027\u89c4\u5219\nC. \u300a\u533b\u7597\u4e8b\u6545\u5904\u7406\u6761\u4f8b\u300b\u7b2c62\u6761\u89c4\u5b9a\uff1a\u519b\u961f\u533b\u7597\u673a\u6784\u7684\u533b\u7597\u4e8b\u6545\u5904\u7406\u529e\u6cd5\uff0c\u7531\u4e2d\u56fd\u4eba\u6c11\u89e3\u653e\u519b\u536b\u751f\u4e3b\u7ba1\u90e8\u95e8\u4f1a\u540c\u56fd\u52a1\u9662\u536b\u751f\u884c\u653f\u90e8\u95e8\u4f9d\u636e\u672c\u6761\u4f8b\u5236\u5b9a\u3002\u6b64\u89c4\u5b9a\u4e3a\u51c6\u7528\u6027\u89c4\u5219\nD. \u300a\u5baa\u6cd5\u300b\u7b2c40\u6761\u89c4\u5b9a\uff1a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u516c\u6c11\u7684\u901a\u4fe1\u81ea\u7531\u548c\u901a\u4fe1\u79d8\u5bc6\u53d7\u6cd5\u5f8b\u7684\u4fdd\u62a4\u3002\u9664\u56e0\u56fd\u5bb6\u5b89\u5168\u6216\u8005\u8ffd\u67e5\u5211\u4e8b\u72af\u7f6a\u7684\u9700\u8981\uff0c\u7531\u516c\u5b89\u673a\u5173\u6216\u8005\u68c0\u5bdf\u673a\u5173\u4f9d\u7167\u6cd5\u5f8b\u89c4\u5b9a\u7684\u7a0b\u5e8f\u5bf9\u901a\u4fe1\u8fdb\u884c\u68c0\u67e5\u5916\uff0c\u4efb\u4f55\u7ec4\u7ec7\u6216\u8005\u4e2a\u4eba\u4e0d\u5f97\u4ee5\u4efb\u4f55\u7406\u7531\u4fb5\u72af\u516c\u6c11\u7684\u901a\u4fe1\u81ea\u7531\u548c\u901a\u4fe1\u79d8\u5bc6\u3002\u6b64\u89c4\u5b9a\u4e3a\u547d\u4ee4\u6027\u89c4\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.26994874905427885, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u4e2a\u9009\u9879\u4e0d\u5c5e\u4e8e\u6211\u56fd\u56fd\u5bb6\u76d1\u7763\u4f53\u7cfb\uff1f\nA. \u5168\u56fd\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u5bf9\u4e0d\u7b26\u5408\u5baa\u6cd5\u3001\u6cd5\u5f8b\u7684\u884c\u653f\u6cd5\u89c4\u548c\u5730\u65b9\u6027\u6cd5\u89c4\u7684\u64a4\u9500\nB. \u5404\u7ea7\u4eba\u6c11\u6cd5\u9662\u5bf9\u884c\u653f\u673a\u5173\u7684\u76d1\u7763\nC. \u4e2d\u56fd\u4eba\u6c11\u653f\u6cbb\u534f\u5546\u4f1a\u8bae\u5bf9\u6cd5\u5f8b\u5b9e\u65bd\u7684\u5408\u6cd5\u6027\u7684\u76d1\u7763\nD. \u56fd\u5bb6\u5ba1\u8ba1\u673a\u5173\u5bf9\u56fd\u5bb6\u7684\u8d22\u653f\u91d1\u878d\u673a\u6784\u548c\u4f01\u4e1a\u4e8b\u4e1a\u7ec4\u7ec7\u8d22\u52a1\u6536\u652f\u7684\u76d1\u7763\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6285534530769757, "meta-math/MetaMath-Mistral-7B": 0.9376876835130811, "itpossible/Chinese-Mistral-7B-v0.1": 0.3632122790984034, "HuggingFaceH4/zephyr-7b-beta": 0.9931639576392718, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.935413036396228, "meta-llama/Meta-Llama-3-8B": 0.8184173996102773, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9129924977681388}}, {"question": "\u03b1-\u916e\u620a\u4e8c\u9178\u5f7b\u5e95\u6c27\u5316\u6210CO2\u548cH2O\uff0c\u53ef\u751f\u6210\u591a\u5c11\u5206\u5b50ATP\nA. 8\nB. 12\nC. 20\nD. 9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u598a\u5a20 36 \u5468\u7684\u9611\u5c3e\u708e\u5982\u4f55\u6cbb\u7597\nA. \u4e2d\u836f\u6cbb\u7597\nB. \u5c3d\u65e9\u624b\u672f\nC. \u6297\u751f\u7d20\u6cbb\u7597\nD. \u4fdd\u5b88\u6cbb\u7597\u65e0\u6548\u518d\u624b\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7815499509639482, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5325505691875027, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5153000910109642}}, {"question": "\u4f3d\u5229\u7565\u7684\u7406\u60f3\u5b9e\u9a8c\u6307\u51fa\u4e86\nA. \u2f12\u662f\u4ea7\u2f63\u8fd0\u52a8\u548c\u7ef4\u6301\u7269\u4f53\u8fd0\u52a8\u7684\u539f\u56e0\nB. \u2f00\u5207\u7269\u4f53\u90fd\u5177\u6709\u60ef\u6027\nC. \u7ef4\u6301\u7269\u4f53\u8fd0\u52a8\u4e0d\u9700\u8981\u2f12\u7684\u4f5c\u2f64\nD. \u2f12\u662f\u4f7f\u7269\u4f53\u4ea7\u2f63\u52a0\u901f\u5ea6\u7684\u539f\u56e0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6480936046335484, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6210\u7ec4\u8bbe\u8ba1\u7684\u65b9\u5dee\u5206\u6790\u4e2d\uff0c\u5fc5\u7136\u6709\nA. $S S_{\\text {\u7ec4\u5185 }}<S S_{\\text {\u7ec4\u95f4 }}$\nB. $M S_{\\text {\u7ec4\u5185 }}<M S_{\\text {\u7ec4\u95f4 }}$\nC. $M S_{\\text {\u603b }}=M S_{\\text {\u7ec4\u5185 }}+M S_{\\text {\u7ec4\u95f4 }}$\nD. $S S_{\\text {\u603b }}=S S_{\\text {\u7ec4\u5185 }}+S S_{\\text {\u7ec4\u95f4 }}$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9762544739777825, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5173630868820975}}, {"question": "\u67d0\u79d1\u5b66\u5bb6\u901d\u4e16\u540e\uff0c\u5a92\u4f53\u8fd9\u6837\u8bc4\u4ef7\uff1a\u201c\u5728\u6211\u4eec\u8fd9\u4e00\u65f6\u4ee3\u7684\u7269\u7406\u5b66\u53f2\u4e2d\uff0c\u4ed6\u73b0\u5728\u662f\uff0c\u5c06\u6765\u4e5f\u8fd8\u662f\u4eba\u7c7b\u5b87\u5b99\u4e2d\u6709\u5934\u7b49\u5149\u8f89\u7684\u4e00\u9897\u5de8\u661f\u2026\u2026\u4ed6\u4e5f\u8bb8\u6bd4\u725b\u987f\u66f4\u4f1f\u5927\uff0c\u56e0\u4e3a\u4ed6\u5bf9\u4e8e\u79d1\u5b66\u7684\u8d21\u732e\uff0c\u66f4\u52a0\u6df1\u523b\u5730\u8fdb\u5165\u4e86\u4eba\u7c7b\u601d\u60f3\u57fa\u672c\u6982\u5ff5\u7684\u7ed3\u6784\u4e2d\u3002\u201d\u8fd9\u4f4d\u79d1\u5b66\u5bb6\u662f\nA. \u8fbe\u5c14\u6587\nB. \u7231\u8fea\u751f\nC. \u666e\u6717\u514b\nD. \u7231\u56e0\u65af\u5766\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.736839359324855, "meta-math/MetaMath-Mistral-7B": 0.950643911637286, "itpossible/Chinese-Mistral-7B-v0.1": 0.8049629106057471, "HuggingFaceH4/zephyr-7b-beta": 0.950244943933449, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8354348748181614, "meta-llama/Meta-Llama-3-8B": 0.9160693106732789, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9828439617134118}}, {"question": "\u4e0a\u4e0b\u5b66\u8def\u4e0a\u906d\u96f7\u96e8\u65f6\nA. \u8eb2\u5230\u6811\u4e0b\nB. \u4ee5\u4e0a\u90fd\u662f\nC. \u8eb2\u5230\u9ad8\u697c\u5899\u8fb9\nD. \u5c3d\u91cf\u907f\u5f00\u5927\u6811\u4e0e\u9ad8\u697c\u5899\u8fb9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6806653834569287, "meta-math/MetaMath-Mistral-7B": 0.5510225598431364, "itpossible/Chinese-Mistral-7B-v0.1": 0.9295656434943039, "HuggingFaceH4/zephyr-7b-beta": 0.9998649682979383, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7296553930728815, "meta-llama/Meta-Llama-3-8B": 0.5085732694571238, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8771545334734941}}, {"question": "\u8ba1\u7b97\u673a\u7cfb\u7edf\u7684\u5b89\u5168\u7ea7\u522b\u5206\u4e3a\u56db\u7ea7\uff1a D\u3001C\uff08C1\u3001C2\uff09\u3001B\uff08B1\u3001B2\u3001B3\uff09\u548c A\u3002\u5176\u4e2d\u88ab\u79f0\u4e3a\u9009\u62e9\u6027\u4fdd\u62a4\u7ea7\u7684\u662f\nA. B2\nB. C2\nC. B1\nD. C1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.46293987445390106, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u6cd5\u5f8b\u5236\u5b9a\u7a0b\u5e8f\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u6b65\u9aa4\u662f\nA. \u6cd5\u5f8b\u8349\u6848\u7684\u5ba1\u8bae\nB. \u6cd5\u5f8b\u8349\u6848\u7684\u516c\u5e03\nC. \u6cd5\u5f8b\u7684\u516c\u5e03\nD. \u6cd5\u5f8b\u8349\u6848\u7684\u8868\u51b3\u548c\u901a\u8fc7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4503886988997803, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38676893933915935, "meta-llama/Meta-Llama-3-8B": 0.494022911575675, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe A\uff0cB \u4e3an\u9636\u77e9\u9635\uff0c \u5219 $|-2(\\begin{array}{cc}A^T & O \\\\ O & B^{-1}\\end{array})|$ \u7b49\u4e8e()\nA. $-2|A^T||B|$\nB. $(-2)^{2n}|A||B|^{-1}$\nC. $(-2)^n|A||B|^{-1}$\nD. $-2|A||B|^{-1}$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f01\u4e1a\u5b9a\u671f\u7f16\u5236\u8d22\u52a1\u62a5\u544a\u6240\u4f9d\u636e\u7684\u4f1a\u8ba1\u5047\u8bbe\u662f\nA. \u4f1a\u8ba1\u5206\u671f\nB. \u4f1a\u8ba1\u4e3b\u4f53\nC. \u8d27\u5e01\u8ba1\u91cf\nD. \u6301\u7eed\u7ecf\u8425\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u534e\u6c11\u65cf\u5728\u4e94\u5343\u591a\u5e74\u7684\u53d1\u5c55\u4e2d\u5f62\u6210\u7684\u4f1f\u5927\u6c11\u65cf\u7cbe\u795e\u7684\u6838\u5fc3\u662f\nA. \u56fd\u9645\u4e3b\u4e49\nB. \u7231\u56fd\u4e3b\u4e49\nC. \u56e2\u7ed3\u7edf\u4e00\nD. \u7231\u597d\u548c\u5e73\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5530824698441223, "meta-math/MetaMath-Mistral-7B": 0.6307443951081417, "itpossible/Chinese-Mistral-7B-v0.1": 0.7946164679850168, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.849495951719741, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5265264311743131}}, {"question": "\u572819\u4e16\u7eaa\u540e\u534a\u53f6\uff0c\u68c9\u7eba\u7ec7\u54c1\u59cb\u7ec8\u5217\u4e3a\u4e2d\u56fd\u8fdb\u53e3\u5546\u54c1\u4e2d\u7684\u91cd\u8981\u9879\u76ee\u30021874\u5e74\u81f31894\u5e7420\u5e74\u95f4\uff0c\u6d0b\u5e03\u8fdb\u53e3\u503c\u589e\u957f88.4%\u3002\u6d0b\u7eb1\u8fdb\u53e3\u503c\u589e\u957f\u5c06\u8fd1986.7%\u3002\u4ee5\u4e0a\u73b0\u8c61\u8868\u660e\nA. \u81ea\u7136\u7ecf\u6d4e\u88ab\u5f7b\u5e95\u7834\u574f\nB. \u4e2d\u56fd\u68c9\u7eba\u7ec7\u4e1a\u53d6\u5f97\u8f83\u5feb\u53d1\u5c55\nC. \u5916\u56fd\u5546\u54c1\u5b8c\u5168\u5360\u636e\u4e2d\u56fd\u5e02\u573a\nD. \u4f20\u7edf\u624b\u5de5\u4e1a\u4ecd\u6709\u62b5\u5236\u80fd\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u884c\u8f66\u4e2d\u9047\u6709\u975e\u673a\u52a8\u8f66\u51c6\u5907\u7ed5\u8fc7\u505c\u653e\u7684\u8f66\u8f86\u65f6\uff0c\u5e94\u600e\u6837\u505a\nA. \u8ba9\u5176\u5148\u884c\nB. \u52a0\u901f\u7ed5\u884c\nC. \u7d27\u968f\u5176\u540e\u9e23\u5587\u53ed\nD. \u9e23\u5587\u53ed\u793a\u610f\u5176\u8ba9\u9053\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28012882262171346, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7934332261141531, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5095922698651916}}, {"question": "\u5fc3\u7406\u5b66\u5bb6\u5361\u7279\u5c14\u628a\u4e3b\u8981\u53d7\u4eba\u7684\u751f\u7269\u5b66\u56e0\u7d20\u5f71\u54cd\uff0c\u5e76\u968f\u751f\u7406\u6210\u957f\u66f2\u7ebf\u53d8\u5316\u800c\u53d8\u5316\u7684\u667a\u529b\u79f0\u4e3a\nA. \u4e00\u822c\u56e0\u7d20\nB. \u6676\u4f53\u667a\u529b\nC. \u6d41\u4f53\u667a\u529b\nD. \u7279\u6b8a\u56e0\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3762807130738324, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4777635491413946, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7318041249979115}}, {"question": "\u5728\u8ba1\u7b97\u673a\u4e2d\uff0cROM\u5c5e\u4e8e\nA. \u8f93\u5165\u8bbe\u5907\nB. \u8f93\u51fa\u8bbe\u5907\nC. \u5916\u5b58\u50a8\u5668\nD. \u5185\u5b58\u50a8\u5668\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9032829289114734, "meta-math/MetaMath-Mistral-7B": 0.9915733497398898, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9998501275477474, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8562039807452845, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7951471343661158}}, {"question": "\u80a0\u7ed3\u6838\u7684\u6e83\u75a1\u7279\u70b9\u662f\nA. \u70e7\u74f6\u72b6\nB. \u706b\u5c71\u53e3\u72b6\nC. \u88c2\u9699\u72b6\nD. \u73af\u5f62\u8170\u5e26\u72b6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3837616204620929, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8840\u53cb\u75c5\u7684\u9057\u4f20\u5c5e\u4e8e\u4f34\u6027\u9057\u4f20\uff0e\u67d0\u7537\u5b69\u4e3a\u8840\u53cb\u75c5\u60a3\u8005\uff0c\u4f46\u4ed6\u7684\u7236\u6bcd\u4eb2\u3001\u7956\u7236\u6bcd\u3001\u5916\u7956\u7236\u6bcd\u90fd\u4e0d\u662f\u60a3\u8005\u3002\u8840\u53cb\u75c5\u57fa\u56e0\u5728\u8be5\u5bb6\u65cf\u4e2d\u4f20\u9012\u7684\u987a\u5e8f\u662f\nA. \u7956\u6bcd\u2192\u7236\u4eb2\u2192\u7537\u5b69\nB. \u5916\u7956\u7236\u2192\u6bcd\u4eb2\u2192\u7537\u5b69\nC. \u5916\u7956\u6bcd\u2192\u6bcd\u4eb2\u2192\u7537\u5b69\nD. \u7956\u6bcd\u2192\u7236\u4eb2\u2192\u7537\u5b69\u7956\u7236\u2192\u7236\u4eb2\u2192\u7537\u5b69\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.515982926893108, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.29972404264597124, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u4e59\u4e24\u6d41\u57df\u9664\u6cb3\u7f51\u5bc6\u5ea6\u7532\u5927\u4e8e\u4e59\u7684\u5916\uff0c\u5176\u5b83\u6d41\u57df\u4e0b\u57ab\u9762\u56e0\u7d20\u548c\u6c14\u8c61\u56e0\u7d20\u5747\u76f8\u540c\uff0c\u5bf9\u76f8\u540c\u964d\u96e8\u6240\u5f62\u6210\u7684\u6d41\u91cf\u8fc7\u7a0b\uff0c\u7532\u6d41\u57df\u7684\u6d2a\u5cf0\u6d41\u91cf\u6bd4\u4e59\u6d41\u57df\u7684[ ]\u3002\nA. \u5cf0\u73b0\u65f6\u95f4\u65e9\u3001\u6d2a\u5cf0\u6d41\u91cf\u5927\nB. \u5cf0\u73b0\u65f6\u95f4\u665a\u3001\u6d2a\u5cf0\u6d41\u91cf\u5927\nC. \u5cf0\u73b0\u65f6\u95f4\u65e9\u3001\u6d2a\u5cf0\u6d41\u91cf\u5c0f\nD. \u5cf0\u73b0\u65f6\u95f4\u665a\u3001\u6d2a\u5cf0\u6d41\u91cf\u5c0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.660313384729793, "meta-math/MetaMath-Mistral-7B": 0.9629996732710497, "itpossible/Chinese-Mistral-7B-v0.1": 0.38574528084703813, "HuggingFaceH4/zephyr-7b-beta": 0.9998988975241242, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8162846671874306, "meta-llama/Meta-Llama-3-8B": 0.43291964698921265, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6965348658109486}}, {"question": "\u5728\u521b\u9020\u4e13\u5229\u7533\u8bf7\u6587\u4ef6\u4e2d\uff0c\u786e\u5b9a\u4e13\u5229\u6743\u4fdd\u62a4\u8303\u56f4\u7684\u4e3b\u8981\u4f9d\u636e\u662f\nA. \u8bf4\u660e\u4e66\u6458\u8981\nB. \u8bf7\u6c42\u4e66\nC. \u6743\u5229\u8981\u6c42\u4e66\nD. \u8bf4\u660e\u4e66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.927112259457007, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9350735225775619, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.793179672517112}}, {"question": "\u7ba1\u7406\u5173\u7cfb\u4e3b\u8981\u662f\u6307\u4eba\u4eec\u5728\u7ba1\u7406\u6d3b\u52a8\u8fc7\u7a0b\u4e2d\u5f62\u6210\u7684\u4e00\u79cd\u4e0d\u65ad\u53d8\u5316\u7740\u7684\nA. \u4eba\u4e0e\u4eba\u7684\u5173\u7cfb\nB. \u4eba\u4e0e\u7269\u7684\u5173\u7cfb\nC. \u4eba\u4e0e\u4fe1\u606f\u7684\u5173\u7cfb\nD. \u4eba\u4e0e\u8d22\u7684\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9639060087997076, "meta-math/MetaMath-Mistral-7B": 0.9982517107407637, "itpossible/Chinese-Mistral-7B-v0.1": 0.9380452047518479, "HuggingFaceH4/zephyr-7b-beta": 0.9999666650979279, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8092655359662474, "meta-llama/Meta-Llama-3-8B": 0.9342965209340126, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9610100763333155}}, {"question": "\u8ba1\u7b97\u673a\u4fe1\u606f\u8ba1\u91cf\u5355\u4f4d\u4e2d\u7684K\u4ee3\u8868\nA. 102\nB. 103\nC. 28\nD. 210\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u666e\u901a\u5c0f\u9ea6\u4e3a\u5f02\u6e90\u516d\u500d\u4f53\u4f5c\u7269\uff082n=6x=42\uff09\uff0c\u5b83\u53ef\u4ee5\u5f62\u6210\u5355\u4f53\nA. 21\u79cd\nB. 42\u79cd\nC. 28\u79cd\nD. 7\u79cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3361950722028681, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u84b8\u53d1\u6d53\u7f29\u201d\u9700\u9009\u7528\u7684\u4eea\u5668\u9664\u4e86\u5706\u5e95\u70e7\u74f6\u3001\u84b8\u998f\u5934\u3001\u6e29\u5ea6\u8ba1\u3001\u63a5\u6536\u7ba1\u4e4b\u5916\uff0c\u8fd8\u6709\nA. \u76f4\u5f62\u51b7\u51dd\u7ba1\nB. \u70e7\u676f\nC. \u9525\u5f62\u74f6\nD. \u7403\u5f62\u51b7\u51dd\u7ba1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.32545507259594497, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3984585186882419, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8156809330929173}}, {"question": "\u4eba\u7c7b\u5236\u4f5c\u719f\u8089\u98df\u54c1\u6700\u65e9\u91c7\u7528\u7684\u65b9\u6cd5\u662f\nA. \u70e4\nB. \u84b8\nC. \u7092\nD. \u716e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.40067969011095805, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6493938225869548, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u9690\u9a6c\u5c14\u79d1\u592b\u6a21\u578b\u4e2d,\u5982\u679c\u5df2\u77e5\u89c2\u5bdf\u5e8f\u5217\u548c\u4ea7\u751f\u89c2\u5bdf\u5e8f\u5217\u7684\u72b6\u6001\u5e8f\u5217,\u90a3\u4e48\u53ef\u7528\u4ee5\u4e0b\u54ea\u79cd\u65b9\u6cd5\u76f4\u63a5\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\nA. \u524d\u5411\u540e\u5411\u7b97\u6cd5\nB. \u6781\u5927\u4f3c\u7136\u4f30\u8ba1 \nC. \u7ef4\u7279\u6bd4\u7b97\u6cd5\nD. EM\u7b97\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.48081329226641856, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5044262585573023}}, {"question": "\u4e0b\u5217\u5404\u7ec4\u5f62\u58f0\u5b57\uff0c\u58f0\u7b26\u76f8\u540c\u7684\u4e00\u7ec4\u662f\nA. \u798f\u7948\nB. \u901a\u8fc7\nC. \u95ee\u95f7\nD. \u6c5f\u6cb3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.32205625344145955, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6b63\u786e\u8ba4\u8bc6\u4ef7\u503c\u521b\u9020\u548c\u8d22\u5bcc\u751f\u4ea7\u7684\u5173\u7cfb\uff0c\u5173\u952e\u662f\u8fd0\u7528\nA. \u8d44\u672c\u6709\u673a\u6784\u6210\u5b66\nB. \u5e73\u5747\u5229\u6da6\u5b66\u8bf4\nC. \u52b3\u52a8\u4e8c\u91cd\u6027\u5b66\u8bf4\nD. \u5269\u4f59\u4ef7\u503c\u5b66\u8bf4 \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1960\u5e74\u5c3c\u514b\u677e\u66fe\u8bf4\uff1a\u201c\u4e00\u4e2a\u56fd\u5bb6\u5fc5\u987b\u7231\u597d\u548c\u5e73\u624d\u5177\u5907\u8fdb\u5165\u8054\u5408\u56fd\u7684\u8d44\u683c\u3002\u5341\u5206\u660e\u663e\uff0c\u4e2d\u56fd\u4e0d\u662f\u3002\u201d1971\u5e74\u5c3c\u514b\u677e\u53c8\u8bf4\uff1a\u201c\u6ca1\u6709\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u7684\u53c2\u52a0\uff0c\u662f\u4e0d\u80fd\u6709\u7a33\u5b9a\u4e0e\u6301\u4e45\u7684\u548c\u5e73\u7684\u3002\u6b63\u56e0\u4e3a\u5982\u6b64\uff0c\u6211\u5728\u597d\u51e0\u4e2a\u65b9\u9762\u91c7\u53d6\u4e3b\u52a8\u884c\u52a8\uff0c\u4e3a\u4e24\u56fd\u4e4b\u95f4\u7684\u6bd4\u8f83\u6b63\u5e38\u7684\u5173\u7cfb\u655e\u4e0c\u95e8\u6237\u3002\u201d\u5c3c\u514b\u677e\u524d\u540e\u6001\u5ea6\u7684\u53d8\u5316\u8bf4\u660e\uff1aa-\u6218\u540e\u521d\u671f\u7f8e\u56fd\u5bf9\u4e2d\u56fd\u91c7\u53d6\u904f\u5236\u548c\u5b64\u7acb\u653f\u7b56\u662f\u6b63\u786e\u7684\uff1bb\u7f8e\u56fd\u8981\u4e3b\u52a8\u91c7\u53d6\u884c\u52a8\u4fc3\u8fdb\u4e24\u56fd\u5173\u7cfb\u6b63\u5e38\u5316\uff1bc\u7f8e\u56fd\u59cb\u7ec8\u4ee5\u81ea\u8eab\u5229\u76ca\u5f97\u5931\u4f5c\u4e3a\u5904\u7406\u56fd\u9645\u5173\u7cfb\u7684\u3001\u51fa\u53d1\u70b9\uff1bd\u4e2d\u56fd\u5728\u56fd\u9645\u4e8b\u52a1\u4e2d\u53d1\u6325\u7740\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u4f5c\u7528\nA. bcd\nB. abc\nC. abcd\nD. abd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe $f(x)=u(x)+v(x)\uff0c g(x)=u(x)-v(x)$\uff0c \u8bbe $\\lim _{x \\rightarrow x_0} u(x)$ \u4e0e $\\lim _{x \\rightarrow x_0} v(x)$ \u90fd\u4e0d\u5b58\u5728\uff0c \u4e0b\u5217\u5224\u65ad\u6b63\u786e\u7684\u662f ( ).\nA. \u82e5 $\\lim _{x \\rightarrow x_0} f(x)$ \u5b58\u5728\uff0c \u5219 $\\lim _{x \\rightarrow x_0} g(x)$ \u5fc5\u4e0d\u5b58\u5728\nB. \u82e5 $\\lim _{x \\rightarrow x_0} f(x)$ \u4e0d\u5b58\u5728\uff0c \u5219 $\\lim _{x \\rightarrow x_0} g(x)$ \u5fc5\u4e0d\u5b58\u5728\nC. \u82e5 $\\lim _{x \\rightarrow x_0} f(x)$ \u4e0d\u5b58\u5728\uff0c \u5219 $\\lim _{x \\rightarrow x_0} g(x)$ \u5fc5\u5b58\u5728\nD. \u82e5 $\\lim _{x \\rightarrow x_0} f(x)$ \u5b58\u5728\uff0c \u5219 $\\lim _{x \\rightarrow x_0} g(x)$ \u5fc5\u5b58\u5728\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.40590908947935506, "meta-math/MetaMath-Mistral-7B": 0.7412399937906989, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4494721009626485, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.36329356528119183}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\uff0c\u4e0e\u201c\u79e6\u6642\u660e\u6708\u6f22\u6642\u95dc\u201d\u6240\u7528\u8868\u8fbe\u65b9\u5f0f\u76f8\u540c\u7684\u4e00\u53e5\u662f\nA. \u8f14\u4f9d\u8eca\uff0c\u8eca\u4ea6\u4f9d\u8f14\u3002\nB. \u4e8b\u51fa\u65bc\u6c89\u601d\uff0c\u7fa9\u6b78\u4e4e\u7ff0\u85fb\u3002\nC. \u8cdc\u5973\u571f\u5730\uff0c\u8cea\u4e4b\u4ee5\u72a7\u7272\uff0c\u4e16\u4e16\u5b50\u5b6b\u7121\u76f8\u5bb3\u4e5f\u3002\nD. \u5c0f\u77e5\u4e0d\u53ca\u5927\u77e5\uff0c\u5c0f\u5e74\u4e0d\u53ca\u5927\u5e74\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.38347924606159883, "itpossible/Chinese-Mistral-7B-v0.1": 0.37887217364564624, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.38726019115386}}, {"question": "\u7537\u6027\uff0c80\u5c81\u3002\u56e0\u7a81\u53d1\u5168\u8179\u5267\u75db8 \u5c0f\u65f6\u6765\u9662\uff0c\u9014\u4e2d\u9891\u7e41\u5455\u5410\uff0c\u968f\u540e\u51fa\u73b0\u6c14\u6025\u3001\u5598\u618b\u3002\u65e2\u5f80\u6709\u623f\u98a4\u53f2\u3002\u5165\u9662\u67e5\u4f53;P 95 \u6b21/\u5206\uff0cR 36 \u6b21/\u5206\uff0cBP 84/60 mmHg\uff0c\u6025\u6027\u75c5\u5bb9\uff0c\u547c\u5438\u6025\u4fc3\uff0c\u4e24\u80ba\u5747\u53ef\u95fb\u53ca\u54ee\u9e23\u97f3\u53ca\u6e7f\u5570\u97f3\uff0c\u53f3\u4fa7\u4e3a\u8457\uff0c\u5fc3\u7387120 \u6b21/\u5206\uff0c\u8282\u5f8b\u4e0d\u6574\uff0c\u8179\u90e8\u81a8\u9686\uff0c\u5168\u8179\u5747\u6709\u538b\u75db\uff0c\u53f3\u4fa7\u66f4\u660e\u663e\uff0c\u8f7b\u5ea6\u53cd\u8df3\u75db\u3001\u808c\u7d27\u5f20\uff0c\u80a0\u9e23\u97f3\u5f31\u3002\u5316\u9a8c Hb 124 g/L\uff0cWBC 30X10/L\uff0c\u4e2d\u6027\u7c92\u7ec6\u80de90%\u3002\u60a3\u8005\u7a81\u53d1\u4e25\u91cd\u5598\u618b\u6700\u53ef\u80fd\u7684\u539f\u56e0\u662f\nA. \u6025\u6027\u5fc3\u8870\nB. \u54ee\u5598\u53d1\u4f5c\nC. \u5438\u5165\u6027\u80ba\u708e\nD. \u80ba\u6813\u585e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u63a8\u5f00\u6591\u9a73\u539a\u91cd\u7684\u5386\u53f2\u5927\u95e8\uff0c\u7ffb\u68c0\u6c57\u725b\u5145\u680b\u7684\u6587\u5316\u5178\u7c4d\uff0c\u6211\u4eec\u89e6\u6478\u7740\u4e94\u5343\u5e74\u4e2d\u534e\u6587\u660e\u7684\u8109\u640f\u3002\u5176\u4e2d\u6709\u8fd9\u4e48\u4e00\u90e8\u5178\u7c4d\uff0c\u6587\u5b57\u7cbe\u7ec3\uff0c\u4eba\u7269\u523b\u753b\u4e0e\u53d9\u4e8b\u751f\u52a8\uff0c\u4e0d\u865a\u7f8e\uff0c\u4e0d\u9690\u6076\uff0c\u662f\u4e00\u90e8\u517c\u5177\u53f2\u5b66\u548c\u6587\u5b66\u7279\u8272\u7684\u4e0d\u673d\u540d\u8457\uff0c\u88ab\u8a89\u4e3a\u201c\u53f2\u5bb6\u4e4b\u7edd\u5531\uff0c\u65e0\u97f5\u4e4b\u79bb\u9a9a\u201d\u3002\u8be5\u5178\u7c4d\u662f\nA. \u300a\u6c49\u4e66\u300b\nB. \u300a\u6625\u79cb\u300b\nC. \u300a\u8d44\u6cbb\u901a\u9274\u300b\nD. \u300a\u53f2\u8bb0\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7396625006184323, "meta-math/MetaMath-Mistral-7B": 0.9266270259046251, "itpossible/Chinese-Mistral-7B-v0.1": 0.9314256538586303, "HuggingFaceH4/zephyr-7b-beta": 0.8911926573447183, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5269642307264539, "meta-llama/Meta-Llama-3-8B": 0.943263617740711, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3963287975238087}}, {"question": "\u5c0f\u660e\u770b\u4e66\u65f6\u53ef\u4ee5\u201c\u4e00\u76ee\u5341\u884c\u201d\uff0c\u800c\u5c0f\u534e\u5219\u201c\u4e00\u76ee\u4e00\u884c\u201d\u3002\u8fd9\u53cd\u6620\u4e86\u4ed6\u4eec\u5728\u54ea\u79cd\u6ce8\u610f\u54c1\u8d28\u4e0a\u5b58\u5728\u5dee\u5f02\nA. \u6ce8\u610f\u8f6c\u79fb\nB. \u6ce8\u610f\u5e7f\u5ea6\nC. \u6ce8\u610f\u5206\u914d\nD. \u6ce8\u610f\u7a33\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4988344802984541, "meta-math/MetaMath-Mistral-7B": 0.8837912427261079, "itpossible/Chinese-Mistral-7B-v0.1": 0.5594256355999565, "HuggingFaceH4/zephyr-7b-beta": 0.9993831061228324, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.835978858874164, "meta-llama/Meta-Llama-3-8B": 0.4734044326799291, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u5c01\u5efa\u793e\u4f1a\u5b98\u5b66\u7684\u6700\u663e\u8457\u7279\u70b9\u662f\nA. \u9636\u7ea7\u6027\nB. \u7b49\u7ea7\u6027\nC. \u5c01\u95ed\u6027\nD. \u5784\u65ad\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u75c5\u56e0\u7814\u7a76\u7684\u8f6e\u72b6\u6a21\u578b\u4e2d\uff0c\u5f3a\u8c03\u5bbf\u4e3b\u4e0e\u4e0b\u5217\u54ea\u79cd\u56e0\u7d20\u7684\u5173\u7cfb\nA. \u7cbe\u795e\u56e0\u7d20\nB. \u73af\u5883\u56e0\u7d20\nC. \u751f\u7269\u56e0\u7d20\nD. \u7269\u7406\u56e0\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47492339297865355, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e00\u4e2a\u5b8c\u6574\u7684\u8bad\u7ec3\u5927\u5468\u671f\u91cc\uff0c\u8bad\u7ec3\u51c6\u5907\u671f\u4e0e\u7ade\u6280\u72b6\u6001\u53d8\u5316\u76f8\u5bf9\u5e94\u7684\u9636\u6bb5\u662f\nA. \u76f8\u5bf9\u7a33\u5b9a\u9636\u6bb5\nB. \u6062\u590d\u9636\u6bb5\nC. \u6682\u65f6\u6d88\u5931\u9636\u6bb5\nD. \u83b7\u5f97\u9636\u6bb5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34865081485179605, "meta-math/MetaMath-Mistral-7B": 0.42689714545069396, "itpossible/Chinese-Mistral-7B-v0.1": 0.36260628017702756, "HuggingFaceH4/zephyr-7b-beta": 0.6845415776634507, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.613713136220576, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3939288246716701}}, {"question": "\u6cd5\u5f8b\u5173\u7cfb\u7684\u5185\u5bb9\u662f\u6cd5\u5f8b\u5173\u7cfb\u4e3b\u4f53\u4e4b\u95f4\u7684\u6cd5\u5f8b\u6743\u5229\u548c\u6cd5\u5f8b\u4e49\u52a1\uff0c\u6cd5\u5f8b\u6743\u5229\u548c\u6cd5\u5f8b\u4e49\u52a1\u4e24\u8005\u4e4b\u95f4\u5177\u6709\u7d27\u5bc6\u7684\u8054\u7cfb\u3002\u4e0b\u5217\u6709\u5173\u6cd5\u5f8b\u6743\u5229\u548c\u6cd5\u5f8b\u4e49\u52a1\u76f8\u4e92\u5173\u7cfb\u7684\u8868\u8ff0\u4e2d\uff0c\u54ea\u4e00\u79cd\u6ca1\u6709\u6b63\u786e\u63ed\u793a\u8fd9\u4e00\u5173\u7cfb\uff1f\nA. \u4e49\u52a1\u7684\u8bbe\u5b9a\u76ee\u7684\u662f\u4fdd\u969c\u6743\u5229\u7684\u5b9e\u73b0\nB. \u6743\u5229\u548c\u4e49\u52a1\u7684\u5b58\u5728\u3001\u53d1\u5c55\u90fd\u5fc5\u987b\u4ee5\u53e6\u4e00\u65b9\u7684\u5b58\u5728\u548c\u53d1\u5c55\u4e3a\u6761\u4ef6\nC. \u6743\u5229\u548c\u4e49\u52a1\u5728\u6cd5\u5f8b\u5173\u7cfb\u4e2d\u7684\u5730\u4f4d\u6709\u4e3b\u6b21\u4e4b\u5206\nD. \u4eab\u6709\u6743\u5229\u662f\u4e3a\u4e86\u66f4\u597d\u5730\u5c65\u884c\u4e49\u52a1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5235087897043122, "meta-math/MetaMath-Mistral-7B": 0.49171754910554016, "itpossible/Chinese-Mistral-7B-v0.1": 0.35823796793174434, "HuggingFaceH4/zephyr-7b-beta": 0.9963645795389989, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5423436874778066, "meta-llama/Meta-Llama-3-8B": 0.6264764360508656, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u96c4\u6027\u4e0d\u80b2\u7cfb\uff08ms/ms\uff09\u00d7\u6742\u5408\u53ef\u80b2\u682a\uff08Ms/ms\uff09\u7684F1\nA. \u5168\u4e3a\u4e0d\u80b2\u682a\nB. \u5168\u4e3a\u53ef\u80b2\u682a\nC. \u53ef\u80b2\u682a\u548c\u4e0d\u80b2\u682a\u5404\u53601/2\nD. 3/4\u4e3a\u53ef\u80b2\u682a\uff0c1/4\u4e3a\u4e0d\u80b2\u682a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4768476010894913, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.47104487015986324, "HuggingFaceH4/zephyr-7b-beta": 0.9512643211798635, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7328525949372611, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1073\u5e74\uff0c\u6559\u4f1a\u7684\u5f3a\u786c\u6d3e\u4eba\u7269\u5e0c\u5c14\u5fb7\u5e03\u5170\u767b\u4e0a\u4e86\u6559\u7687\u5b9d\u5ea7\uff0c\u6b64\u4eba\u5373\u662f\nA. \u535c\u5c3c\u6cd5\u65af\nB. \u683c\u91cc\u9ad8\u5229\u4e03\u4e16\nC. \u514b\u96f7\u8292\u4e03\u4e16\nD. \u82f1\u8bfa\u68ee\u4e09\u4e16\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.394061933244316, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3624670691963443, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6309\u7528\u9014\u548c\u7ed3\u6784\u5206\u7c7b\uff0c\u201c\u672c\u5e74\u5229\u6da6\u201d\u8d26\u6237\u5c5e\u4e8e\nA. \u6240\u6709\u8005\u6743\u76ca\u7c7b\u8d26\u6237\nB. \u635f\u76ca\u7c7b\u8d26\u6237\nC. \u8d22\u52a1\u6210\u679c\u8d26\u6237\nD. \u6536\u5165\u8d26\u6237\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7607436483182203, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6cd5\u5f8b\u9002\u7528\u7684\u8bf4\u6cd5\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u6cd5\u5f8b\u9002\u7528\u7684\u8fc7\u7a0b\u6216\u7ed3\u679c\u4ee5\u793e\u4f1a\u8206\u8bba\u4e3a\u540e\u76fe\nB. \u6cd5\u5f8b\u9002\u7528\u662f\u4e25\u683c\u6309\u7167\u6cd5\u5b9a\u7a0b\u5e8f\u8fdb\u884c\u7684\nC. \u6cd5\u5f8b\u9002\u7528\u65e2\u662f\u884c\u653f\u673a\u5173\u7684\u4e00\u9879\u804c\u6743\uff0c\u4e5f\u662f\u884c\u653f\u673a\u5173\u7684\u4e00\u9879\u4e49\u52a1\nD. \u6cd5\u5f8b\u9002\u7528\u7684\u4e3b\u4f53\u662f\u56fd\u5bb6\u884c\u653f\u673a\u5173\u53ca\u5176\u516c\u804c\u4eba\u5458\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.46275238281573583, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7284590443541159}}, {"question": "\u5728\u524d\u4eba\u8fdb\u884c\u7684\u4e0b\u5217\u7814\u7a76\u4e2d\uff0c\u91c7\u7528\u7684\u6838\u5fc3\u6280\u672f\u76f8\u540c\uff08\u6216\u76f8\u4f3c\uff09\u7684\u4e00\u7ec4\u662f\uff081\uff09\u8bc1\u660e\u5149\u5408\u4f5c\u7528\u6240\u91ca\u653e\u7684\u6c27\u6c14\u6765\u81ea\u4e8e\u6c34\uff1b\uff082\uff09\u7528\u7d2b\u5916\u7ebf\u7b49\u5904\u7406\u9752\u9709\u83cc\u9009\u80b2\u9ad8\u4ea7\u9752\u9709\u7d20\u83cc\u682a\uff1b\uff083\uff09\u7528T2\u566c\u83cc\u4f53\u4fb5\u67d3\u5927\u80a0\u6746\u83cc\u8bc1\u660eDNA\u662f\u9057\u4f20\u7269\u8d28\uff1b\uff084\uff09\u7528\u7532\u57fa\u7eff\u548c\u5421\u7f57\u7ea2\u5bf9\u7ec6\u80de\u67d3\u8272\uff0c\u89c2\u5bdf\u6838\u9178\u7684\u5206\u5e03\nA. \uff081\uff09\uff082\uff09\nB. \uff082\uff09\uff084\uff09\nC. \uff081\uff09\uff083\uff09\nD. \uff083\uff09\uff084\uff09\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33482349867891054, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2810882504428991, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3155567922076969, "meta-llama/Meta-Llama-3-8B": 0.37354500251984707, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.280128869973513}}, {"question": "\u201c\u8bf4\u6587\u56db\u5927\u5bb6\u201d\u4e2d\u6ce8\u91cd\u5206\u6790\u5b57\u4e49\u6765\u6e90\u548c\u53d1\u5c55\u7684\u662f\nA. \u738b\u7b60\nB. \u6731\u9a8f\u58f0\nC. \u6842\u99a5\nD. \u6bb5\u7389\u88c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.42089211552482236, "itpossible/Chinese-Mistral-7B-v0.1": 0.32567743233302715, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.31292412927280244, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u4e24\u79cd\u7269\u8d28\u95f4\u53d1\u751f\u53cd\u5e94\uff0c\u6d88\u8017\u7b49\u7269\u8d28\u7684\u91cf\u7684\u9178\uff0c\u4ea7\u751f\u6c14\u4f53\u6700\u591a\u7684\u662f\nA. \u6728\u70ad\u4e0e\u6d53\u785d\u9178\nB. \u94dc\u4e0e\u7a00\u785d\u9178\nC. \u950c\u4e0e\u7a00\u786b\u9178\nD. \u6728\u70ad\u4e0e\u6d53\u786b\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5201448968316537, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7531\u56fd\u5bb6\u5236\u5b9a\u6216\u8ba4\u53ef\u7684\u5177\u4f53\u89c4\u5b9a\u6743\u5229\u3001\u4e49\u52a1\u53ca\u5176\u540e\u679c\u7684\u884c\u4e3a\u51c6\u5219\u662f\nA. \u9053\u5fb7\u89c4\u8303\nB. \u7eaa\u5f8b\u89c4\u8303\nC. \u653f\u7b56\u89c4\u8303\nD. \u6cd5\u5f8b\u89c4\u8303\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.961816676757221, "meta-math/MetaMath-Mistral-7B": 0.998404390755187, "itpossible/Chinese-Mistral-7B-v0.1": 0.9223766640205069, "HuggingFaceH4/zephyr-7b-beta": 0.9991231225941278, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9905263508312396, "meta-llama/Meta-Llama-3-8B": 0.9779866217223779, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9982575740200992}}, {"question": "\u67d0\u65e513\u65f610\u5206\uff0c\u7f57\u67d0\u9a7e\u9a76\u4e00\u8f86\u4e2d\u578b\u5ba2\u8f66\u4ece\u9ad8\u901f\u516c\u8def0\u516c\u91cc\u5904\u51fa\u53d1\uff0c\u4e0b\u534814\u65f610\u5206\u884c\u81f3\u8be5\u9ad8\u901f\u516c\u8def125\u516c\u91cc\u52a0200\u7c73\u5904\u65f6\uff0c\u53d1\u751f\u8ffd\u5c3e\u78b0\u649e\uff0c\u673a\u52a8\u8f66\u9a76\u51fa\u897f\u5357\u4fa7\u8def\u5916\u8fb9\u5761\uff0c\u9020\u621011\u4eba\u6b7b\u4ea1\u30012\u4eba\u53d7\u4f24\u3002\u7f57\u67d0\u7684\u4e3b\u8981\u8fdd\u6cd5\u884c\u4e3a\u662f\u4ec0\u4e48\nA. \u8d85\u901f\u884c\u9a76\nB. \u5ba2\u8f66\u8d85\u5458\nC. \u75b2\u52b3\u9a7e\u9a76\nD. \u4e0d\u6309\u4ea4\u901a\u6807\u7ebf\u884c\u9a76\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4839879942290105, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5208125011242347, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5386\u53f2\u4e0a\uff0c\u82cf\u5dde\u2f08\u5229\u2f64\u5f53\u5730\u781a\u74e6\u2f2d\u7684\u5ca9\u2f6f\u505a\u539f\u6599\u2f63\u4ea7\u4f18\u8d28\u781a\u53f0\u3002\u8fd9\u79cd\u5ca9\u2f6f\u5ca9\u6027\u81f4\u5bc6\uff0c\u5448\u8584\u677f\u72b6\u3002\u8be5\u5ca9\u2f6f\u53ef\u80fd\u662f\nA. \u53d8\u8d28\u5ca9\nB. \u55b7\u51fa\u5ca9\nC. \u6c89\u79ef\u5ca9\nD. \u4fb5\u2f0a\u5ca9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3197560245290534, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u534a\u5f71\u6708\u98df\u53d1\u751f\u671f\u95f4\uff0c\u5982\u679c\u7ad9\u5728\u6708\u9762\u4e0a\u7684\u534a\u5f71\u533a\u57df\uff0c\u4f60\u4f1a\u770b\u5230\u4ec0\u4e48\u73b0\u8c61\nA. \u65e5\u5168\u98df\nB. \u6ca1\u6709\u53d1\u751f\u65e5\u98df\nC. \u65e5\u504f\u98df\nD. \u770b\u4e0d\u5230\u592a\u9633\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3395958258011606, "meta-math/MetaMath-Mistral-7B": 0.5485293268972065, "itpossible/Chinese-Mistral-7B-v0.1": 0.3436615088034304, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6959257885203446}}, {"question": "\u7532\u56e0\u4e3a\u7537\u53cb\u4e59\u4e0d\u5fe0\u800c\u751f\u6068\u610f\uff0c\u51b3\u5b9a\u6740\u4e59\u3002\u67d0\u65e5\u628a\u4e59\u5f15\u5230\u5bb6\u4e2d\uff0c\u5c06\u4e00\u74f6\u5b89\u7720\u836f\uff0850\u7247\uff09\u63ba\u5165\u5496\u5561\u8ba9\u4e59\u559d\u4e0b\u3002\u4e59\u5728\u7532\u7684\u5e8a\u4e0a\u660f\u7761\uff0c\u7532\u79bb\u5bb6\u5230\u9644\u8fd1\u4e00\u5ea7\u5c71\u4e0a\u6253\u7b97\u81ea\u6740\u3002\u7532\u5728\u5c71\u4e0a\u72b9\u8c6b\u5f98\u5f8a\u4e00\u663c\u591c\uff0c\u5fc3\u751f\u6094\u610f\u6025\u56de\u5bb6\uff0c\u53d1\u73b0\u4e59\u5df2\u7ecf\u88ab\u4eba\u9001\u533b\u9662\u62a2\u6551\uff0c\u672a\u6b7b\u3002\u7532\u5927\u559c\u8fc7\u671b\u3002\u5bf9\u7532\nA. \u5e94\u5f53\u514d\u9664\u5904\u7f5a\nB. \u53ef\u4ee5\u6bd4\u7167\u65e2\u9042\u72af\u4ece\u8f7b\u3001\u51cf\u8f7b\u6216\u8005\u514d\u9664\u5904\u7f5a\nC. \u53ef\u4ee5\u6bd4\u7167\u65e2\u9042\u72af\u4ece\u8f7b\u6216\u8005\u51cf\u8f7b\u5904\u7f5a\nD. \u6309\u65e2\u9042\u72af\u5904\u7f5a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7238518635418477, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.36078609119776345, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6105727592420961}}, {"question": "\u4ee5\u4e0b\u54ea\u4e2a\u4e0d\u662f\u5f53\u4eca\u65f6\u4ee3\u7684\u7279\u5f81\nA. \u7ecf\u6d4e\u5168\u7403\u5316\nB. \u751f\u6001\u591a\u6837\u5316\nC. \u793e\u4f1a\u4fe1\u606f\u5316\nD. \u4e16\u754c\u591a\u6781\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35449048834219393, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.34412869855849515, "HuggingFaceH4/zephyr-7b-beta": 0.8934097136443085, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.976707988730526, "meta-llama/Meta-Llama-3-8B": 0.39076155811083163, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8479565766933407}}, {"question": "\u67d0\u516c\u53f8\u4e3a\u4e86\u8ba9\u7ec4\u7ec7\u6210\u5458\u4e86\u89e3\u7ec4\u7ec7\u603b\u4f53\u76ee\u6807\u548c\u5177\u4f53\u63aa\u65bd\uff0c\u5c06\u5404\u9879\u7ba1\u7406\u653f\u7b56\u3001\u7ec4\u7ec7\u76ee\u6807\u3001\u5de5\u4f5c\u7a0b\u5e8f\u548c\u89c4\u7ae0\u5236\u5ea6\u9010\u7ea7\u5411\u4e0b\u4f20\u9012\u3002\u8fd9\u79cd\u6c9f\u901a\u65b9\u5f0f\u53eb\nA. \u53cd\u9988\u6c9f\u901a\nB. \u5355\u5411\u6c9f\u901a\nC. \u76f8\u5411\u6c9f\u901a\nD. \u8f6e\u5f0f\u6c9f\u901a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5818229035504209, "itpossible/Chinese-Mistral-7B-v0.1": 0.4323369207209773, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7751474780873976, "meta-llama/Meta-Llama-3-8B": 0.8830036048354369, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8776365773847593}}, {"question": "\u67d0\u5bb6\u7535\u751f\u4ea7\u4f01\u4e1a\u4ece\u5176\u76ee\u6807\u5e02\u573a\u7684\u987e\u5ba2\u4e2d\uff0c\u6309\u7167\u968f\u673a\u539f\u5219\u62bd\u53d6\u4e8650\u540d\u987e\u5ba2\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u4ee5\u4e86\u89e3\u76ee\u6807\u987e\u5ba2\u5bf9\u5176\u4ea7\u54c1\u7684\u8ba4\u77e5\u3001\u504f\u597d\u7b49\u539f\u59cb\u4fe1\u606f\uff0c\u8be5\u4f01\u4e1a\u8fd9\u79cd\u6536\u96c6\u539f\u59cb\u6570\u636e\u7684\u65b9\u6cd5\u5c5e\u4e8e\nA. \u89c2\u5bdf\u6cd5\nB. \u8c03\u67e5\u6cd5\nC. \u5b9e\u9a8c\u6cd5\nD. \u4e13\u5bb6\u4f30\u8ba1\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9149828843144194, "meta-math/MetaMath-Mistral-7B": 0.9737240041566637, "itpossible/Chinese-Mistral-7B-v0.1": 0.918704719051329, "HuggingFaceH4/zephyr-7b-beta": 0.9999905708749321, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9900775197345398, "meta-llama/Meta-Llama-3-8B": 0.6987523515781654, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9998005759572655}}, {"question": "\u67d0\u2f2f\u2f1a\u7684\u2f00\u4e2a\u2f63\u4ea7\u2f29\u7ec4\uff0c\u2f63\u4ea7\u2f00\u6279\u96f6\u4ef6\uff0c\u5f53\u6bcf\u4e2a\u2f2f\u2f08\u5728\u2f83\u2f30\u539f\u5c97\u4f4d\u2f2f\u4f5c\u65f6\uff0c9\u2f29\u65f6\u53ef\u5b8c\u6210\u8fd9\u9879\u2f63\u4ea7\u4efb\u52a1\u3002\u5982\u679c\u4ea4\u6362\u2f2f\u2f08A\u548cB\u7684\u2f2f\u4f5c\u5c97\u4f4d\uff0c\u5176\u5b83\u2f2f\u2f08\u2f63\u4ea7\u6548\u7387\u4e0d\u53d8\u65f6\uff0c\u53ef\u63d0\u524d\u2f00\u2f29\u65f6\u5b8c\u6210\u8fd9\u9879\u2f63\u4ea7\u4efb\u52a1\uff1b\u5982\u679c\u4ea4\u6362\u2f2f\u2f08C\u548cD\u7684\u2f2f\u4f5c\u5c97\u4f4d\uff0c\u5176\u5b83\u2f2f\u2f08\u2f63\u4ea7\u6548\u7387\u4e0d\u53d8\u65f6\uff0c\u4e5f\u53ef\u4ee5\u63d0\u524d\u2f00\u2f29\u65f6\u5b8c\u6210\u8fd9\u9879\u2f63\u4ea7\u4efb\u52a1\u3002\u95ee\uff1a \u5982\u679c\u540c\u65f6\u4ea4\u6362A\u4e0eB\uff0cC\u4e0eD\u7684\u2f2f\u4f5c\u5c97\u4f4d\uff0c\u5176\u5b83\u2f2f\u2f08\u2f63\u4ea7\u6548\u7387\u4e0d\u53d8\uff0c\u53ef\u4ee5\u63d0\u524d\u2f0f\u5206\u5272\u5b8c\u6210\u8fd9\u9879\u2f63\u4ea7\u4efb\u52a1\nA. 108\nB. 111\nC. 110\nD. 105\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "21\u4e16\u7eaa\u521d\u671f\u6ce2\u5170\u7eba\u7ec7\u4e1a\u5411\u73b0\u4ee3\u5316\u8f6c\u578b\u4e3b\u8981\u4f53\u73b0\u5728\nA. \u6dd8\u6c70\u7eba\u7ec7\u4e1a\u57fa\u7840\u4ea7\u4e1a\u90e8\u2ed4\nB. \u4e13\u6ce8\u7eba\u7ec7\u4e1a\u94fe\u6761\u2fbc\u7aef\u73af\u8282\nC. \u56fd\u9645\u5e02\u573a\u62d3\u5c55\u548c\u54c1\u724c\u5efa\u8bbe\nD. \u4ea7\u4e1a\u7ed3\u6784\u8c03\u6574\u548c\u6280\u672f\u5347\u7ea7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6940943850311613, "meta-math/MetaMath-Mistral-7B": 0.925405309341757, "itpossible/Chinese-Mistral-7B-v0.1": 0.8526656819671834, "HuggingFaceH4/zephyr-7b-beta": 0.9998098425590366, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9101876038173576, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9424172163012198}}, {"question": "\u4e2d\u6307\u8113\u6027\u6307\u5934\u708e\u5982\u6cbb\u7597\u4e0d\u53ca\u65f6\uff0c\u6700\u6613\u53d1\u751f\u7684\u5e76\u53d1\u75c7\u662f\nA. \u8d25\u8840\u75c7\nB. \u638c\u4e2d\u95f4\u9699\u611f\u67d3\nC. \u672b\u8282\u6307\u9aa8\u7f3a\u8840\u574f\u6b7b\nD. \u5316\u8113\u6027\u8171\u9798\u708e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6274250846926516, "HuggingFaceH4/zephyr-7b-beta": 0.9223491155198084, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.580869657081244, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5850107995623036}}, {"question": "\u5728\u620f\u5267\u8fd9\u4e00\u7efc\u5408\u4f53\u4e2d\uff0c\u5c45\u4e8e\u4e2d\u5fc3\u5730\u4f4d\u548c\u4e3b\u5bfc\u5730\u4f4d\u7684\u662f\nA. \u6f14\u5458\u7684\u8868\u6f14\u827a\u672f\nB. \u821e\u53f0\u7684\u5e03\u666f\u827a\u672f\nC. \u97f3\u4e50\u4f34\u594f\u827a\u672f\nD. \u670d\u88c5\u9053\u5177\u827a\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7796877178118934, "meta-math/MetaMath-Mistral-7B": 0.9651100201418152, "itpossible/Chinese-Mistral-7B-v0.1": 0.6519197773707622, "HuggingFaceH4/zephyr-7b-beta": 0.9878379478933336, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7655057804273091, "meta-llama/Meta-Llama-3-8B": 0.6385487828405959, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8467493247611282}}, {"question": "\u8fd1\u767e\u5e74\u6765\u7329\u7ea2\u70ed\u53d1\u75c5\u7387\u6709\u660e\u663e\u4e0b\u964d\uff0c\u8f7b\u578b\u75c5\u4eba\u589e\u591a\uff0c\u75c5\u6b7b\u738750\u5e74\u4ee3\u4e3a20\u5e74\u4ee3\u76841/30\uff0c\u79f0\u75be\u75c5\u53d8\u5316\u4e3a\nA. \u957f\u671f\u53d8\u52a8\nB. \u5b63\u8282\u6027\nC. \u5468\u671f\u6027\nD. \u77ed\u671f\u6ce2\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33490177288810025, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9998522336137853, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8844569803065994, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u7f6a\u5211\u6cd5\u5b9a\u539f\u5219\uff0c\u6cd5\u5f8b\u660e\u6587\u89c4\u5b9a\u4e3a\u72af\u7f6a\u884c\u4e3a\u7684\uff0c\u4f9d\u7167\u6cd5\u5f8b\u5b9a\u7f6a\u5904\u5211\uff1b\u6cd5\u5f8b\u6ca1\u6709\u660e\u6587\u89c4\u5b9a\u4e3a\u72af\u7f6a\u884c\u4e3a\u7684\nA. \u53ef\u4ee5\u6bd4\u7167\u5211\u6cd5\u4e2d\u6700\u76f8\u7c7b\u4f3c\u7684\u6761\u6587\u63a8\u5b9a\u9002\u7528\u5211\u7f5a\nB. \u5e94\u5f53\u4ece\u8f7b\u6216\u51cf\u8f7b\u5211\u4e8b\u5904\u7f5a\nC. \u4e0d\u5f97\u5b9a\u7f6a\u5904\u5211\nD. \u5e94\u5f53\u514d\u4e88\u5211\u4e8b\u5904\u7f5a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.45209366997387573, "meta-math/MetaMath-Mistral-7B": 0.7009220845178434, "itpossible/Chinese-Mistral-7B-v0.1": 0.6243335104822408, "HuggingFaceH4/zephyr-7b-beta": 0.5410437580709963, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.94770779453653, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u5b50\u6ca1\u6709\u8bed\u75c5\u7684\u4e00\u9879\u662f\nA. \u5b66\u4e60\u662f\u5426\u52e4\u594b\uff0c\u662f\u53d6\u5f97\u597d\u6210\u7ee9\u7684\u91cd\u8981\u6761\u4ef6\nB. \u6807\u70b9\u7528\u5f97\u6070\u5f53\uff0c\u4e0d\u4ec5\u80fd\u51c6\u786e\u5730\u8868\u8fbe\u6587\u7ae0\u7684\u5185\u5bb9\uff0c\u5728\u4e00\u5b9a\u7684\u8bed\u5883\u91cc\uff0c\u8fd8\u80fd\u4ee3\u66ff\u6587\u5b57\u76f4\u63a5\u8868\u8fbe\u4eba\u4eec\u7684\u601d\u60f3\u611f\u60c5\nC. \u4ece\u8fd9\u5957\u539a\u539a\u7684\u77e5\u8bc6\u4e1b\u4e66\u4e2d\uff0c\u4f7f\u6211\u83b7\u5f97\u4e86\u4e0d\u5c11\u7684\u8bfe\u5916\u77e5\u8bc6\nD. \u4e3a\u4e86\u907f\u514d\u4eca\u540e\u4e0d\u518d\u72af\u540c\u6837\u7684\u9519\u8bef\uff0c\u6211\u4eec\u5e94\u5f53\u597d\u597d\u8ba2\u6b63\u8fd9\u9053\u9898\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5028667378660308, "meta-math/MetaMath-Mistral-7B": 0.5777517929053557, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8441590425430466, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38907220878367704, "meta-llama/Meta-Llama-3-8B": 0.3689108554330874, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4914398048647685}}, {"question": "\u5173\u4e8eATP\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. ATP\u5728\u53cd\u5e94\u4e2d\u4f9b\u51fa\u9ad8\u80fd\u78f7\u9178\u57fa\u540e\u5373\u53d8\u4e3aADP\nB. ATP\u90fd\u662f\u7531\u547c\u5438\u94fe\u8fc7\u7a0b\u4e2d\u7ecf\u6c27\u5316\u78f7\u9178\u5316\u4ea7\u751f\u7684\nC. ATP\u662f\u751f\u7269\u4f53\u7684\u76f4\u63a5\u4f9b\u80fd\u7269\u8d28\nD. ATP\u7684\u5316\u5b66\u80fd\u53ef\u8f6c\u53d8\u4e3a\u673a\u68b0\u80fd\uff0c\u6e17\u900f\u80fd\uff0c\u7535\u80fd\uff0c\u70ed\u80fd\u7b49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6283613743239068, "meta-math/MetaMath-Mistral-7B": 0.8762260630889714, "itpossible/Chinese-Mistral-7B-v0.1": 0.6888143977579613, "HuggingFaceH4/zephyr-7b-beta": 0.9971632586249576, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9485378574221357, "meta-llama/Meta-Llama-3-8B": 0.6036570663342438, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6308803593938338}}, {"question": "\u5357\u6d77\u8bf8\u5c9b\u2fac\u91cf\u4e30\u6c9b\uff0c\u70ed\u91cf\u5145\u2f9c\uff0c\u5c9b\u4e0a\u7684\u5ca9\u2f6f\u4e3b\u8981\u662f\u73ca\u745a\u3001\u2ec9\u58f3\u788e\u5c51\u7802\uff0c\u8d28\u5730\u758f\u677e\u3002\u8bf8\u5c9b\u2faf\u79ef\u2f29\uff0c\u6d77\u62d4\u4f4e\uff0c\u2ee6\u7c7b\u591a\uff0c\u690d\u88ab\u6709200\u4f59\u79cd\u3002\u9664\u6ee8\u6d77\u5730\u533a\u5916\uff0c\u2f1f\u58e4\u542b\u76d0\u91cf\u2f24\u90e8\u5206\u8f83\u4f4e\u3002\u5357\u6d77\u8bf8\u5c9b\u662f\u2f00\u4e2a\u72ec\u7279\u7684\u5bcc\u78f7\u2f63\u6001\u7cfb\u7edf\uff0c\u2f1f\u58e4\u662f\u8fd9\u4e2a\u2f63\u6001\u7cfb\u7edf\u7684\u7ebd\u5e26\u3002\u5357\u6d77\u8bf8\u5c9b\u9664\u6ee8\u6d77\u5730\u533a\u5916\uff0c\u5176\u4ed6\u5730\u533a\u2f1f\u58e4\u542b\u76d0\u91cf\u8f83\u4f4e\uff0c\u5176\u4e3b\u8981\u539f\u56e0\u662f\nA. \u8ddd\u6d77\u8fdc\uff0c\u6d77\u2f54\u5f71\u54cd\u2f29\nB. \u2f53\u6e29\u4f4e\uff0c\u84b8\u53d1\u5f31\nC. \u964d\u2f54\u591a\uff0c\u6dcb\u6eb6\u4f5c\u2f64\u5f3a\nD. \u690d\u7269\u591a\uff0c\u5438\u6536\u76d0\u5206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4125161317632695, "meta-math/MetaMath-Mistral-7B": 0.3908304748611699, "itpossible/Chinese-Mistral-7B-v0.1": 0.5356302911834471, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.34695068109082333, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u5173\u4e8e\u7f51\u7edc\u6d41\u91cf\u76d1\u63a7\u7684\u53d9\u8ff0\u4e2d\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u6d41\u91cf\u76d1\u6d4b\u4e2d\u6240\u76d1\u6d4b\u7684\u6d41\u91cf\u901a\u5e38\u91c7\u96c6\u81ea\u4e3b\u673a\u8282\u70b9\u3001\u670d\u52a1\u5668\u3001\u8def\u7531\u5668\u63a5\u53e3\u3001\u94fe\u8def\u548c\u8def\u5f84\u7b49\nB. \u6d41\u91cf\u76d1\u63a7\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u5bf9\u654f\u611f\u6570\u636e\u7684\u8fc7\u6ee4\nC. \u6570\u636e\u91c7\u96c6\u63a2\u9488\u662f\u4e13\u95e8\u7528\u4e8e\u83b7\u53d6\u7f51\u7edc\u94fe\u8def\u6d41\u91cf\u6570\u636e\u7684\u786c\u4ef6\u8bbe\u5907\nD. \u7f51\u7edc\u6d41\u91cf\u76d1\u63a7\u5206\u6790\u7684\u57fa\u7840\u662f\u534f\u8bae\u884c\u4e3a\u89e3\u6790\u6280\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5292563570923206, "HuggingFaceH4/zephyr-7b-beta": 0.8257737859968429, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7503974419723014, "meta-llama/Meta-Llama-3-8B": 0.6612446598624095, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5200277313137446}}, {"question": "\u6d41\u884c\u6027\u4e59\u578b\u8111\u708e\u4f20\u64ad\u5a92\u4ecb\u662f\uff1a\nA. \u9769\u87a8\nB. \u8671\nC. \u868a\nD. \u8747\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.592672961021979, "meta-math/MetaMath-Mistral-7B": 0.39677575123416625, "itpossible/Chinese-Mistral-7B-v0.1": 0.5901040645092825, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.43291436513060333, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u4e0d\u5c5e\u4e8e\u5f71\u54cd\u805a\u7c7b\u7b97\u6cd5\u7ed3\u679c\u7684\u4e3b\u8981\u56e0\u7d20\u6709\nA. \u7279\u5f81\u9009\u53d6\nB. \u5df2\u77e5\u7c7b\u522b\u7684\u6837\u672c\u8d28\u91cf\nC. \u5206\u7c7b\u51c6\u5219\nD. \u6a21\u5f0f\u76f8\u4f3c\u6027\u6d4b\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4745338900985261, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u8fde\u9501\u8d85\u5e02\u7528\u201c\u5929\u5929\u4f4e\u4ef7\u201d\u7684\u53e3\u53f7\u6765\u5438\u5f15\u4e00\u4e9b\u7cbe\u6253\u7ec6\u7b97\u7684\u987e\u5ba2\uff0c\u8be5\u8d85\u5e02\u7684\u5e02\u573a\u5b9a\u4f4d\u5c5e\u4e8e\nA. \u4f7f\u7528\u8005\u5b9a\u4f4d\nB. \u987e\u5ba2\u5229\u76ca\u5b9a\u4f4d\nC. \u7ade\u4e89\u5b9a\u4f4d\nD. \u4ea7\u54c1\u7279\u8272\u5b9a\u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6483554854546089, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6709\u5173\u52a8\u7269\u6fc0\u7d20\u7684\u63cf\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u673a\u4f53\u4e2d\u6fc0\u7d20\u7684\u5206\u6ccc\u91cf\u662f\u4e0d\u53d8\u7684\nB. \u4fc3\u7532\u72b6\u817a\u6fc0\u7d20\u7684\u53d7\u4f53\u5206\u5e03\u5728\u4f53\u5185\u5404\u79cd\u7ec6\u80de\u4e0a\nC. \u6027\u6fc0\u7d20\u7684\u5316\u5b66\u672c\u8d28\u662f\u7531\u6c28\u57fa\u9178\u7ec4\u6210\u7684\u86cb\u767d\u8d28\nD. \u6fc0\u7d20\u662f\u5177\u6709\u9ad8\u6548\u6027\u7684\u751f\u7269\u6d3b\u6027\u7269\u8d28\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7370660910984684, "meta-math/MetaMath-Mistral-7B": 0.9335118017036034, "itpossible/Chinese-Mistral-7B-v0.1": 0.42096270677603886, "HuggingFaceH4/zephyr-7b-beta": 0.9992577165277404, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8727156416140373, "meta-llama/Meta-Llama-3-8B": 0.949594268915762, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7252360098603546}}, {"question": "\u5728\u5404\u79cd\u8425\u517b\u7d20\u4e2d\uff0c\u598a\u5a20\u671f\u95f4\u589e\u52a0\u7684\u503c\u8f83\u9ad8\u7684\u662f\nA. \u53f6\u9178\u3001\u80fd\u91cf\u3001\u7ef4\u751f\u7d20C\nB. \u53f6\u9178\u3001\u94c1\u3001\u7ef4\u751f\u7d20B1\nC. \u53f6\u9178\u3001\u94c1\u3001\u7ef4\u751f\u7d20D\nD. \u80fd\u91cf\u3001\u53f6\u9178\u3001\u94c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u9884\u7b97\u5b9a\u989d\u7684\u8bf4\u6cd5\uff0c\u6b63\u786e\u7684\u662f\nA. \u9884\u7b97\u5b9a\u989d\u662f\u7f16\u5236\u6982\u7b97\u5b9a\u989d\u7684\u57fa\u7840\nB. \u9884\u7b97\u5b9a\u989d\u4ee5\u5de5\u5e8f\u4e3a\u5bf9\u8c61\u8fdb\u884c\u7f16\u5236\nC. \u9884\u7b97\u5b9a\u989d\u53ef\u4ee5\u76f4\u63a5\u7528\u4e8e\u65bd\u5de5\u4f01\u4e1a\u4f5c\u4e1a\u8ba1\u5212\u7684\u7f16\u5236\nD. \u9884\u7b97\u5b9a\u989d\u662f\u7f16\u5236\u65bd\u5de5\u5b9a\u989d\u7684\u4f9d\u636e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.42797332725514636, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8954067236097202}}, {"question": "\u201c\u8fd1\u6731\u8005\u8d64\uff0c\u8fd1\u58a8\u8005\u9ed1\u201d\u6240\u8574\u542b\u7684\u9053\u7406\u548c\u4e0b\u5217\u54ea\u53e5\u8bdd\u6700\u76f8\u4f3c\nA. \u84ec\u751f\u9ebb\u4e2d\uff0c\u4e0d\u6276\u800c\u76f4\nB. \u51fa\u6de4\u6ce5\u800c\u4e0d\u67d3\uff0c\u6fef\u6e05\u6d9f\u800c\u4e0d\u5996\nC. \u516c\u751f\u660e\uff0c\u504f\u751f\u6697\nD. \u9752\u51fa\u4e8e\u84dd\uff0c\u800c\u80dc\u4e8e\u84dd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6cd5\u5f8b\u90e8\u95e8\u7684\u8868\u8ff0\uff0c\u6b63\u786e\u7684\u6709\nA. \u540c\u4e00\u6cd5\u5f8b\u5236\u5ea6\u53ea\u80fd\u89c4\u5b9a\u5728\u540c\u4e00\u6cd5\u5f8b\u90e8\u95e8\nB. \u6cd5\u5f8b\u7684\u5236\u5b9a\u548c\u5b9e\u65bd\u662f\u4e00\u79cd\u5ba2\u89c2\u7684\u4e8b\u5b9e\uff0c\u56e0\u6b64\uff0c\u6cd5\u5f8b\u90e8\u95e8\u7684\u5212\u5206\u662f\u5ba2\u89c2\u7684\nC. \u51e1\u662f\u8c03\u6574\u540c\u4e00\u79cd\u7c7b\u793e\u4f1a\u5173\u7cfb\u7684\u6cd5\u5f8b\u89c4\u8303\u5c31\u5e94\u8be5\u5f52\u5165\u540c\u4e00\u6cd5\u5f8b\u90e8\u95e8\nD. \u6cd5\u5f8b\u90e8\u95e8\u662f\u7531\u89c4\u8303\u6027\u6cd5\u5f8b\u6587\u4ef6\u6784\u6210\u7684\uff0c\u56e0\u6b64\uff0c\u4e00\u4e2a\u89c4\u8303\u6027\u6cd5\u5f8b\u6587\u4ef6\u53ea\u80fd\u5f52\u5165\u540c\u4e00\u6cd5\u5f8b\u90e8\u95e8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.40805750249314754, "meta-math/MetaMath-Mistral-7B": 0.7351419182118695, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9426683861526256, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.40544191343510655, "meta-llama/Meta-Llama-3-8B": 0.4068068948079533, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9538927495433939}}, {"question": "\u62cd\u6444\u4eba\u50cf\u65f6\uff0c\u6700\u597d\u4f7f\u7528\nA. \u4fa7\u5149\nB. \u9876\u5149\nC. \u987a\u5149\nD. \u9006\u5149\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u592a\u9633\u7684\u7edd\u5bf9\u661f\u7b49\u662f\nA. 8.6\nB. -26.8\nC. 0\nD. 4.8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ec4\u6210\u89c6\u4ea4\u53c9\u7684\u795e\u7ecf\u7ea4\u7ef4\u662f\nA. \u5de6\u53f3\u773c\u9f3b\u4fa7\u534a\u89c6\u7f51\u819c\u53d1\u51fa\u7684\u7ea4\u7ef4\nB. \u53f3\u773c\u9f3b\u4fa7\u534a\u89c6\u7f51\u819c\u4e0e\u53f3\u773c\u989e\u4fa7\u534a\u89c6\u7f51\u819c\u53d1\u51fa\u7684\u7ea4\u7ef4\nC. \u5de6\u773c\u989e\u4fa7\u534a\u89c6\u7f51\u819c\u4e0e\u53f3\u773c\u9f3b\u4fa7\u534a\u89c6\u7f51\u819c\u53d1\u51fa\u7684\u7ea4\u7ef4\nD. \u5de6\u53f3\u773c\u989e\u4fa7\u534a\u89c6\u7f51\u819c\u53d1\u51fa\u7684\u7ea4\u7ef4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u88ab\u72d7\u54ac\u4f24\uff0c\u5e94\u5728()\u5c0f\u65f6\u5185\u6ce8\u5c04\u72c2\u72ac\u75c5\u75ab\u82d7\u548c\u7834\u4f24\u98ce\u6297\u6bd2\u7d20\nA. 1.5\nB. \u534a\nC. 24\nD. 2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4073394521145539, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7436426948292924, "HuggingFaceH4/zephyr-7b-beta": 0.9939589434843803, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7660126421063498, "meta-llama/Meta-Llama-3-8B": 0.29863342676099575, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5572604626284698}}, {"question": "\u56db\u5ddd\u7701\u6daa\u6c5f\u57fa\u5ca9\u6cb3\u5e8a\u4e0a\u6709\u2f00\u5904\u2faf\u79ef\u8d85\u8fc710\u4e07\u5e73\u2f45\u2f76\u7684\u201c\u602a\u2f6f\u6ee9\u201d\uff0c\u6bcf\u5f53\u67af\u2f54\u671f\u5c31\u4f1a\u6d74\u2f54\u2f7d\u51fa\u3002\u2f46\u6570\u7684\u201c\u58f6\u2f73\u201d\uff08\u58f6\u5f62\u51f9\u5751\uff09\uff0c\u50cf\u662f\u201c\u5d4c\u2f0a\u201d\u5728\u5468\u8fb9\u7684\u7802\u5375\u2f6f\u4e2d\uff0c\u5f53\u5730\u2f08\u79f0\u4e4b\u4e3a\u201c\u2f6f\u2ef0\u8fc7\u6c5f\u201d\u3002\u201c\u2f6f\u2ef0\u8fc7\u6c5f\u201d\u666f\u89c2\u4e2d\uff0c\u57fa\u5ca9\u6cb3\u5e8a\u4e0a\u201c\u58f6\u2f73\u201d\u7684\u6210\u56e0\u662f\nA. \u6cb3\u6d41\u643a\u5e26\u7684\u783e\u2f6f\u3001\u6ce5\u6c99\u6c89\u79ef\u2f7d\u6210\nB. \u6025\u6d41\u3001\u65cb\u6da1\u5939\u5e26\u783e\u2f6f\u78e8\u8680\u6cb3\u5e8a\nC. \u57fa\u5ca9\u6cb3\u5e8a\u53d7\u51b0\u5ddd\u4f5c\u2f64\u7684\u5386\u53f2\u9057\u8ff9\nD. \u5730\u58f3\u62ac\u5347\uff0c\u6cb3\u6d41\u6c89\u79ef\u7269\u88ab\u2edb\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3264339496862684, "meta-math/MetaMath-Mistral-7B": 0.36887349089950283, "itpossible/Chinese-Mistral-7B-v0.1": 0.31604271402418366, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5236882854811111, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.507843505562176}}, {"question": "\u8111\u6897\u6b7b\u7684\u7279\u70b9\u662f\nA. \u51fa\u8840\u6027\u6897\u6b7b\uff0c\u6db2\u5316\u6027\u574f\u6b7b\nB. \u51fa\u8840\u6027\u6897\u6b7b\uff0c\u51dd\u56fa\u6027\u574f\u6b7b\nC. \u8d2b\u8840\u6027\u6897\u6b7b\uff0c\u6db2\u5316\u6027\u574f\u6b7b\nD. \u8d2b\u8840\u6027\u6897\u6b7b\uff0c\u51dd\u56fa\u6027\u574f\u6b7b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f20\u7edf\u540d\u83dc\u201c\u4f5b\u8df3\u5899\u301e\u5728\u98ce\u5473\u4e0a\u5c5e\u4e8e\nA. \u5e7f\u4e1c\u98ce\u5473\nB. \u5b89\u5fbd\u98ce\u5473\nC. \u798f\u5efa\u98ce\u5473\nD. \u6d59\u6c5f\u98ce\u5473\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.45574461368189956, "meta-math/MetaMath-Mistral-7B": 0.6940607122769757, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6736954444099724, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u9175\u6bcd\u83cc\u548c\u5927\u80a0\u6746\u83cc\u7684\u53d9\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. ATP\u548c\u8461\u8404\u7cd6\u5747\u80fd\u4e3a\u9175\u6bcd\u83cc\u548c\u5927\u80a0\u6746\u83cc\u7684\u751f\u547d\u6d3b\u52a8\u76f4\u63a5\u63d0\u4f9b\u80fd\u6e90\nB. \u5728\u65e0\u6c27\u6761\u4ef6\u4e0b\uff0c\u5927\u80a0\u6746\u83cc\u80fd\u751f\u5b58\uff0c\u800c\u9175\u6bcd\u83cc\u4e0d\u80fd\u751f\u5b58\nC. \u9175\u6bcd\u83cc\u6709\u591a\u79cd\u7ec6\u80de\u5668\uff0c\u800c\u5927\u80a0\u6746\u83cc\u53ea\u6709\u4e00\u79cd\u7ec6\u80de\u5668\nD. \u9175\u6bcd\u83cc\u7684\u9057\u4f20\u7269\u8d28\u4e3aDNA\uff0c\u800c\u5927\u80a0\u6746\u83cc\u7684\u9057\u4f20\u7269\u8d28\u4e3aRNA\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f51\u9875\u6728\u9a6c\u662f\u4e00\u79cd\u901a\u8fc7\u653b\u51fb\u6d4f\u89c8\u5668\u6216\u6d4f\u89c8\u5668\u5916\u6302\u7a0b\u5e8f\u7684\u6f0f\u6d1e\uff0c\u5411\u76ee\u6807\u7528\u6237\u673a\u5668\u690d\u5165\u6728\u9a6c\u3001\u75c5\u6bd2\u3001\u5bc6\u7801\u76d7\u53d6\u7b49\u6076\u610f\u7a0b\u5e8f\u7684\u624b\u6bb5\uff0c\u4e3a\u4e86\u8981\u5b89\u5168\u6d4f\u89c8\u7f51\u9875\uff0c\u4e0d\u5e94\u8be5\nA. \u5728\u4ed6\u4eba\u8ba1\u7b97\u673a\u4e0a\u4f7f\u7528\u201c\u81ea\u52a8\u767b\u5f55\u201d\u548c\u201c\u8bb0\u4f4f\u5bc6\u7801\u201d\u529f\u80fd\nB. \u5b9a\u671f\u6e05\u7406\u6d4f\u89c8\u5668Cookies\nC. \u5b9a\u671f\u6e05\u7406\u6d4f\u89c8\u5668\u7f13\u5b58\u548c\u4e0a\u7f51\u5386\u53f2\u8bb0\u5f55\nD. \u7981\u6b62\u4f7f\u7528ActiveX\u63a7\u4ef6\u548c_Java\u811a\u672c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "20\u4e16\u7eaa60\u5e74\u4ee3\u4e2d\u671f\u7684\u6c11\u610f\u6d4b\u9a8c\u663e\u793a\uff0c\u5927\u591a\u6570\u7f8e\u56fd\u4eba\u5bf9\u4e2d\u56fd\u7684\u5370\u8c61\u662f\u201c\u65e0\u77e5\u201d\u3001\u201c\u597d\u6218\u201d\u3001\u201c\u72e1\u8bc8\u201d\u3001\u201c\u5371\u9669\u201d\uff1b\u5230\u4e8670\u5e74\u4ee3\u521d\u671f\uff0c\u8fd9\u79cd\u5370\u8c61\u53d8\u4e3a\u201c\u52e4\u52b3\u201d\u3001\u201c\u667a\u6167\u201d\u3001\u201c\u7075\u5de7\u201d\u3001\u201c\u5584\u4e8e\u8fdb\u53d6\u201d\u3001\u201c\u8bb2\u6c42\u5b9e\u9645\u201d\u3002\u5bfc\u81f4\u8fd9\u4e24\u79cd\u5370\u8c61\u7684\u539f\u56e0\u5206\u522b\u662f\nA. \u65b0\u4e2d\u56fd\u5b9e\u884c\u201c\u4e00\u8fb9\u5012\u201d\u7684\u5916\u4ea4\u653f\u7b56\uff1b1979\u5e74\u4e2d\u7f8e\u4e24\u56fd\u6b63\u5f0f\u5efa\u4ea4\nB. \u65b0\u4e2d\u56fd\u6210\u7acb\u521d\uff0c\u7f8e\u56fd\u5bf9\u534e\u91c7\u53d6\u654c\u89c6\u653f\u7b56\uff1b1972\u5e74\u7f8e\u56fd\u603b\u7edf\u5c3c\u514b\u677e\u8bbf\u534e\nC. \u4e2d\u82cf\u5efa\u4ea4\uff1b70\u5e74\u4ee3\uff0c\u4e2d\u56fd\u7efc\u5408\u56fd\u529b\u589e\u5f3a\nD. \u7f8e\u82cf\u51b7\u6218\u7684\u5f71\u54cd\uff1b1971\u5e74\u8054\u5408\u56fd\u6062\u590d\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u7684\u5408\u6cd5\u5e2d\u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4939745523543223, "meta-math/MetaMath-Mistral-7B": 0.9049120097577689, "itpossible/Chinese-Mistral-7B-v0.1": 0.4301816307692888, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8788524663658032, "meta-llama/Meta-Llama-3-8B": 0.48783852322368093, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4733160203525662}}, {"question": "\u6c34\u95f8\u6c89\u964d\u7f1d\u5bbd\u4e3a\u591a\u5c11\nA. 0.5-1.0cm\nB. 0.3-0.5cm\nC. 1.0-2.5cm\nD. 2.5-3.0cm\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5546\u54c1\u7684\u672c\u8d28\u56e0\u7d20\u662f\nA. \u4ea4\u6362\u4ef7\u503c\nB. \u4f7f\u7528\u4ef7\u503c\nC. \u4ef7\u503c\nD. \u4ef7\u683c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5546617990841485, "meta-math/MetaMath-Mistral-7B": 0.7715320147342747, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9707953017021229, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7201578783369469, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9a7e\u9a76\u673a\u52a8\u8f66\u9047\u5230\u6709\u6f2b\u6c34\u8def\u65f6\uff0c\u8981\u91c7\u53d6\u7684\u6b63\u786e\u505a\u6cd5\u662f\u4ec0\u4e48\uff1fa\u505c\u8f66\u5bdf\u660e\u6c34\u60c5\uff1bb\u786e\u8ba4\u5b89\u5168\u540e\uff0c\u4f4e\u901f\u901a\u8fc7\uff1bc\u673a\u52a8\u8f66\u6d89\u6c34\u540e\uff0c\u95f4\u65ad\u8f7b\u8e0f\u5236\u52a8\u8e0f\u677f\uff1bd\u673a\u52a8\u8f66\u6d89\u6c34\u540e\uff0c\u6301\u7eed\u8f7b\u8e0f\u5236\u52a8\u8e0f\u677f\nA. bc\nB. abc\nC. cd\nD. bcd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4414010411515525, "meta-math/MetaMath-Mistral-7B": 0.8653524411170207, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9452081486035614, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38336773256430384, "meta-llama/Meta-Llama-3-8B": 0.46331119565501916, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.44670214434341965}}, {"question": "\u73ed\u4e3b\u4efb\u674e\u8001\u5e08\u63a5\u53d7\u4e00\u4e2a\u65b0\u73ed\u540e\uff0c\u9488\u5bf9\u8be5\u73ed\u7eaa\u5f8b\u6563\u6f2b\u3001\u5b66\u98ce\u61c8\u6020\u7684\u60c5\u51b5\uff0c\u9996\u5148\u8fd0\u7528\u677f\u62a5\u3001\u5899\u58c1\u7b49\u5a92\u4ecb\u505a\u597d\u8206\u8bba\u5ba3\u4f20\uff0c\u5efa\u7acb\u826f\u597d\u7684\u73ed\u98ce\uff0c\u540c\u65f6\u4ee5\u771f\u8bda\u7684\u7231\u611f\u5316\u5b66\u751f\uff0c\u4fc3\u4f7f\u5b66\u751f\u79ef\u6781\u8fdb\u53d6\u3002\u4e00\u4e2a\u5b66\u671f\u4e0b\u6765\uff0c\u8be5\u73ed\u73ed\u98ce\u3001\u5b66\u98ce\u7115\u7136\u4e00\u65b0\u3002\u674e\u8001\u5e08\u8fd0\u7528\u7684\u4e3b\u8981\u5fb7\u80b2\u65b9\u6cd5\u662f\nA. \u699c\u6837\u793a\u8303\u6cd5\nB. \u5b9e\u9645\u953b\u70bc\u6cd5\nC. \u4e2a\u4eba\u4fee\u517b\u6cd5\nD. \u60c5\u611f\u9676\u51b6\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3502827040846273, "meta-math/MetaMath-Mistral-7B": 0.3947943359188727, "itpossible/Chinese-Mistral-7B-v0.1": 0.5530822543348924, "HuggingFaceH4/zephyr-7b-beta": 0.9355278055726558, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9371059360742101, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9948751553811015}}, {"question": "\u300a\u5211\u6cd5\u300b\u7b2c232\u6761\u89c4\u5b9a\uff1a\u6545\u610f\u6740\u4eba\u7684\uff0c\u5904\u6b7b\u5211\u3001\u65e0\u671f\u5f92\u5211\u6216\u800510\u5e74\u4ee5\u4e0a\u6709\u671f\u5f92\u5211\uff1b\u60c5\u8282\u8f83\u8f7b\u7684\uff0c\u59043\u5e74\u4ee5\u4e0a10\u5e74\u4ee5\u4e0b\u6709\u671f\u5f92\u5211\u3002\u8fd9\u4e00\u89c4\u5b9a\u5c5e\u4e8e\nA. \u51c6\u7528\u6027\u89c4\u8303\nB. \u786e\u5b9a\u6027\u89c4\u8303\nC. \u59d4\u6258\u6027\u89c4\u8303\nD. \u4e49\u52a1\u6027\u89c4\u8303\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43799402254810144, "meta-math/MetaMath-Mistral-7B": 0.7612046810062476, "itpossible/Chinese-Mistral-7B-v0.1": 0.6119753881209163, "HuggingFaceH4/zephyr-7b-beta": 0.9980365061904549, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.699914154948476, "meta-llama/Meta-Llama-3-8B": 0.3568632977686014, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5852418149235808}}, {"question": "1994\u5e74\u5230l997\u5e744\u5e74\u65f6\u95f4\uff0c\u8d22\u653f\u6536\u5165\u5360GDP\u7684\u6bd4\u91cd\u4ec5l0%\u591a\u4e00\u70b9\uff0c\u52302011\u5e74\u63a5\u8fd122%\uff0c\u5982\u679c\u8003\u8651\u5230\u571f\u5730\u51fa\u8ba9\u548c\u5176\u4ed6\u653f\u5e9c\u57fa\u91d1\u6027\u6536\u5165\uff0c\u6709\u4e13\u5bb6\u4f30\u8ba1\u653f\u5e9c\u6536\u5165\u5360GDP\u7684\u6bd4l\u91cd\u8d85\u8fc730%\u3002\u800c\u540c\u671f\uff0c\u4f01\u4e1a\u6216\u8005\u662f\u8d44\u672c\u6536\u5165\u5360\u6bd4\u4e5f\u5927\u5e45\u5ea6\u63d0\u9ad8\u3002\u4e00\u4e2a\u4f50\u8bc1\u662f\uff0c2002\u5e74\u5e95\uff0c\u4f01\u4e1a\u5b58\u6b3e\u4e3a64298\uff0e47\u4ebf\u5143\uff0c\u4f4e\u4e8e\u5f53\u65f6\u7684\u4e2a\u4eba\u5b58\u6b3e\uff0c\u800c\u52302011\u5e74\u5e95\uff0c\u4f01\u4e1a\u5b58\u6b3e(\u592e\u54e5\u65b0\u7684\u7edf\u8ba1\u79d1\u65e7\u4e3a\u201c\u5355\u4f4d\u5b58\u6b3e\u201d\uff0c\u53e3\u5f84\u76f8\u5f53)\u4e0a\u5347\u5230423086\uff0e61\u4ebf\u5143\uff0c\u589e\u957f\u4e865\uff0e58\u500d\uff0c\u8fdc\u8d85\u540c\u671f\u4e2a\u4eba\u5b58\u6b3e2\uff0e68\u500d\u7684\u589e\u5e45\u3002\u901a\u8fc7\u8fd9\u6bb5\u6587\u5b57\uff0c\u4f5c\u8005\u60f3\u8868\u8fbe\u7684\u89c2\u70b9\u662f\nA. \u6536\u5165\u8d8a\u6765\u8d8a\u5411\u653f\u5e9c\u548c\u8d44\u672c\u503e\u659c\nB. \u6211\u56fd\u7ecf\u6d4e\u589e\u957f\u7684\u901f\u5ea6\u8f83\u5feb\nC. \u653f\u5e9c\u6536\u5165\u8fc7\u9ad8\u4f1a\u6210\u4e3a\u4f01\u4e1a\u7684\u8d1f\u62c5\nD. \u653f\u5e9c\u5e94\u9002\u5f53\u964d\u4f4e\u7a0e\u6536\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7546709318357858, "meta-math/MetaMath-Mistral-7B": 0.9146336834747308, "itpossible/Chinese-Mistral-7B-v0.1": 0.5782268375412561, "HuggingFaceH4/zephyr-7b-beta": 0.9999942667901115, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9904586924557158, "meta-llama/Meta-Llama-3-8B": 0.9062358655033451, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9928591096682635}}, {"question": "\u4e24\u4e2a\u6216\u4e24\u4e2a\u4ee5\u4e0a\u7684\u4eba\u4e4b\u95f4\u501f\u52a9\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u7b26\u53f7\u4e92\u901a\u4fe1\u606f\u3001\u4ea4\u6d41\u601d\u60f3\u611f\u60c5\u7684\u6d3b\u52a8\u662f\nA. \u7ec4\u7ec7\u4f20\u64ad\nB. \u5927\u4f17\u4f20\u64ad\nC. \u4eba\u9645\u4f20\u64ad\nD. \u5185\u5411\u4f20\u64ad\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7442475037143271, "meta-math/MetaMath-Mistral-7B": 0.962002412629184, "itpossible/Chinese-Mistral-7B-v0.1": 0.926118743516677, "HuggingFaceH4/zephyr-7b-beta": 0.9998102871302598, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9235903193629532, "meta-llama/Meta-Llama-3-8B": 0.7466987555529064, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9158316444671627}}, {"question": "\u4e0b\u9762\u7684\u53e5\u5b50\u4e2d\uff0c\u8865\u8bed\u540c\u5bbe\u8bed\u53d1\u751f\u8bed\u4e49\u8054\u7cfb\u7684\u662f\nA. \u5c0f\u674e\u5403\u9971\u4e86\u996d\nB. \u8001\u5218\u8d70\u51fa\u4e86\u623f\u95f4\nC. \u5c0f\u5f20\u558a\u54d1\u4e86\u55d3\u5b50\nD. \u8001\u738b\u559d\u9189\u4e86\u9152\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ec4\u7ec7\u7684\u81ea\u6211\u5f62\u8c61\u662f\u5176\nA. \u5b9e\u9645\u7684\u793e\u4f1a\u5f62\u8c61\nB. \u671f\u671b\u5efa\u7acb\u7684\u793e\u4f1a\u5f62\u8c61\nC. \u8fc7\u53bb\u5df2\u5efa\u7acb\u7684\u793e\u4f1a\u5f62\u8c61\nD. \u516c\u4f17\u5f62\u8c61\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7203112470782289, "meta-math/MetaMath-Mistral-7B": 0.8535856552873976, "itpossible/Chinese-Mistral-7B-v0.1": 0.6322728749418358, "HuggingFaceH4/zephyr-7b-beta": 0.9992876145944405, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7863471950792442, "meta-llama/Meta-Llama-3-8B": 0.3926712614558533, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "X\u3001Y\u3001Z\u5747\u4e3a\u77ed\u5468\u671f\u4e3b\u65cf\u5143\u7d20\uff0c\u5b83\u4eec\u539f\u5b50\u7684\u6700\u5916\u5c42\u7535\u5b50\u6570\u4e4b\u548c\u4e3a10\u3002X\u4e0eZ\u540c\u65cf\uff0cY\u6700\u5916\u5c42\u7535\u5b50\u6570\u7b49\u4e8eX\u6b21\u5916\u5c42\u7535\u5b50\u6570\uff0c\u4e14Y\u539f\u5b50\u534a\u5f84\u5927\u4e8eZ\u3002\u4e0b\u5217\u53d9\u8ff0\u6b63\u786e\u7684\u662f\nA. \u7194\u70b9\uff1aX\u7684\u6c27\u5316\u7269\u6bd4Y\u7684\u6c27\u5316\u7269\u9ad8\nB. Y\u7684\u5355\u8d28\u4e0eZ\u7684\u5355\u8d28\u5747\u80fd\u6eb6\u4e8e\u6d53\u9178\nC. X\u4e0eZ\u53ef\u5f62\u6210\u79bb\u5b50\u5316\u5408\u7269ZX\nD. \u70ed\u7a33\u5b9a\u6027\uff1aX\u7684\u6c22\u5316\u7269\u5927\u4e8eZ\u7684\u6c22\u5316\u7269\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.308176776288574, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4fdd\u9669\u4eba\u627f\u4fdd\u4e86\u4e24\u7ec4\u98ce\u9669\uff0c$A$\u98ce\u9669\u7ec4\u5408\u5728\u6bcf\u5c0f\u65f6\u53d1\u751f\u7684\u7406\u8d54\u6b21\u6570\u670d\u4ece\u5747\u503c\u4e3a 3 \u7684\u6cca\u677e\u8fc7\u7a0b\uff0c$B$\u98ce\u9669\u7ec4\u5408\u5728\u6bcf\u5c0f\u65f6\u53d1\u751f\u7684\u7406\u478c\u6b21\u6570\u670d\u4ece\u5747\u503c\u4e3a 5 \u7684\u6cca\u677e\u8fc7\u7a0b\uff0c\u4e24\u4e2a\u8fc7\u7a0b\u662f\u72ec\u7acb\u7684\uff0c\u5219\u5728\u98ce\u9669\u7ec4\u5408$B$\u53d1\u751f 3 \u6b21\u7406\u8d54\u4e4b\u524d\uff0c\u98ce\u9669\u7ec4\u5408$A$\u53d1\u751f 3 \u6b21\u7406\u8d54\u7684\u6982\u7387\u662f( )\u3002\nA. 0.33\nB. 0.43\nC. 0.28\nD. 0.38\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3526379845723398, "meta-math/MetaMath-Mistral-7B": 0.498039794787083, "itpossible/Chinese-Mistral-7B-v0.1": 0.27416108226793107, "HuggingFaceH4/zephyr-7b-beta": 0.7936426949120594, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.35125369870111733, "meta-llama/Meta-Llama-3-8B": 0.3638582843838116, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728VIS\u4e2d\uff0c\u5e94\u7528\u6700\u5e7f\u6cdb\u3001\u51fa\u73b0\u9891\u7387\u6700\u9ad8\u7684\u8981\u7d20\u662f\nA. \u4f01\u4e1a\u9020\u578b\nB. \u6807\u51c6\u8272\nC. \u6807\u51c6\u5b57\nD. \u6807\u5fd7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.46488044532255274, "meta-math/MetaMath-Mistral-7B": 0.3825322550539375, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9917232777540855, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5476903600456089, "meta-llama/Meta-Llama-3-8B": 0.7149975962079553, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6587\u5b66\u7684\u4e3b\u8981\u8868\u73b0\u624b\u6bb5\u662f\nA. \u60c5\u8282\nB. \u7ed3\u6784\nC. \u6545\u4e8b\nD. \u8bed\u8a00\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.785294267644953, "meta-math/MetaMath-Mistral-7B": 0.9575950153459115, "itpossible/Chinese-Mistral-7B-v0.1": 0.4248118961227964, "HuggingFaceH4/zephyr-7b-beta": 0.9495594522924358, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8375487700682872, "meta-llama/Meta-Llama-3-8B": 0.9735706261038394, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9981508099421426}}, {"question": "\u53d1\u73b0\u98df\u7269\u4e2d\u6bd2\u540e\uff0c\u81ea\u5df1\u80fd\u91c7\u53d6\u7684\u6700\u6709\u6548\u7684\u4e00\u9879\u5e94\u6025\u63aa\u65bd\u662f\u4ec0\u4e48\nA. \u50ac\u5410\nB. \u670d\u836f\nC. \u547c\u6551\nD. \u4f11\u606f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41431190256242645, "meta-math/MetaMath-Mistral-7B": 0.5638563529164229, "itpossible/Chinese-Mistral-7B-v0.1": 0.9413434029536194, "HuggingFaceH4/zephyr-7b-beta": 0.9999845705071946, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9192556292803629}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u4eba\u4f53\u6dcb\u5df4\u7ec6\u80de\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. T\u7ec6\u80de\u548cB\u7ec6\u80de\u90fd\u662f\u7531\u9020\u8840\u5e72\u7ec6\u80de\u53d1\u80b2\u6210\u7684\nB. \u6548\u5e94T\u7ec6\u80de\u53ef\u653b\u51fb\u88ab\u75c5\u539f\u4f53\u611f\u67d3\u7684\u5bbf\u4e3b\u7ec6\u80de\nC. T\u7ec6\u80de\u91ca\u653e\u7684\u6dcb\u5df4\u56e0\u5b50\u4e0d\u80fd\u4f7f\u53d7\u5230\u6297\u539f\u523a\u6fc0\u7684B\u7ec6\u80de\u589e\u6b96\nD. \u5728\u80f8\u817a\u4e2d\u53d1\u80b2\u6210\u719f\u7684T\u7ec6\u80de\u53ef\u53c2\u4e0e\u7ec6\u80de\u514d\u75ab\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4739804749299326, "meta-math/MetaMath-Mistral-7B": 0.541670050915434, "itpossible/Chinese-Mistral-7B-v0.1": 0.6655445197784698, "HuggingFaceH4/zephyr-7b-beta": 0.9897886743411085, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8691156905584374, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6158147544171269}}, {"question": "\u4e0b\u5217\u60c5\u51b5\u4e2d\uff0c\u8179\u819c\u523a\u6fc0\u5f81\u6700\u8f7b\u7684\u662f\nA. \u6025\u6027\u91cd\u75c7\u80f0\u817a\u708e\nB. \u813e\u7834\u88c2\nC. \u6025\u6027\u80c6\u56ca\u708e\nD. \u6d88\u5316\u9053\u7a7f\u5b54\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1500\u5e74\uff0c\u6b27\u6d32\u822a\u6d77\u5bb6\u5361\u5e03\u62c9\u5c14\u7387\u8fdc\u5f81\u961f\u51c6\u5907\u4e1c\u53bb\u5370\u5ea6\uff0c\u4f46\u9014\u4e2d\u5728\u8d64\u9053\u6d77\u6d41\u51b2\u51fb\u4e0b\u79bb\u5f00\u4e86\u822a\u9053\uff0c\u7ed3\u679c\u98d8\u6d41\u5230\u5357\u7f8e\u6d32\u4e1c\u90e8\u3002\u4ed6\u767b\u9646\u540e\u5ba3\u5e03\u8be5\u5730\u4e3a\u672c\u56fd\u9886\u5730\u3002\u8fd9\u4f4d\u822a\u6d77\u5bb6\u5e94\u5c5e\u4e8e\nA. \u897f\u73ed\u7259\nB. \u82f1\u56fd\nC. \u8377\u5170\nD. \u8461\u8404\u7259\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5005725658266937, "meta-math/MetaMath-Mistral-7B": 0.7587556310205057, "itpossible/Chinese-Mistral-7B-v0.1": 0.673127803628461, "HuggingFaceH4/zephyr-7b-beta": 0.9982211078762515, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9908601570480587, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9919841911144728}}, {"question": "\u4ee5\u4e0b\u6709\u5173\u795e\u7ecf\u5174\u594b\u7684\u53d9\u8ff0\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u9759\u606f\u72b6\u6001\u65f6\u795e\u7ecf\u5143\u7ec6\u80de\u819c\u5185\u5916\u6ca1\u6709\u79bb\u5b50\u8fdb\u51fa\nB. \u795e\u7ecf\u9012\u8d28\u4e0e\u7a81\u89e6\u540e\u819c\u4e0a\u7684\u7279\u5f02\u6027\u53d7\u4f53\u7ed3\u5408\nC. \u795e\u7ecf\u9012\u8d28\u7ecf\u80de\u5410\u4f5c\u7528\u7531\u7a81\u89e6\u524d\u819c\u91ca\u653e\uff0c\u8fdb\u5165\u7a81\u89e6\u95f4\u9699\nD. \u795e\u7ecf\u7ea4\u7ef4\u7684\u5174\u594b\u90e8\u4f4d\uff0c\u819c\u4e24\u4fa7\u7535\u4f4d\u8868\u73b0\u4e3a\u5185\u6b63\u5916\u8d1f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.344785902992216, "itpossible/Chinese-Mistral-7B-v0.1": 0.4406914770505923, "HuggingFaceH4/zephyr-7b-beta": 0.546078921867663, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5109597150412231, "meta-llama/Meta-Llama-3-8B": 0.628155905431291, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7902064301000156}}, {"question": "\u4e0b\u5217\u5bf9WINDOWS\u7a97\u53e3\u7684\u63cf\u8ff0\u4e2d\uff0c\u9519\u8bef\u7684\u662f\nA. \u53ef\u4ee5\u901a\u8fc7\u9f20\u6807\u6216\u952e\u76d8\u8fdb\u884c\u7a97\u53e3\u7684\u5207\u6362\nB. \u53ef\u4ee5\u5bf9\u7a97\u53e3\u8fdb\u884c\u201c\u6700\u5c0f\u5316\u201d\u3001\u201c\u6700\u5927\u5316\u201d\u64cd\u4f5c\nC. \u53ef\u4ee5\u6539\u53d8\u7a97\u53e3\u5927\u5c0f\uff0c\u4f46\u4e0d\u80fd\u79fb\u52a8\nD. \u53ef\u4ee5\u540c\u65f6\u6253\u5f00\u591a\u4e2a\u7a97\u53e3\uff0c\u4f46\u53ea\u6709\u4e00\u4e2a\u6d3b\u52a8\u7a97\u53e3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8078626138594138, "meta-math/MetaMath-Mistral-7B": 0.9200166879884223, "itpossible/Chinese-Mistral-7B-v0.1": 0.5969477063076561, "HuggingFaceH4/zephyr-7b-beta": 0.9999226094921472, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9769075431832829, "meta-llama/Meta-Llama-3-8B": 0.9135649736896005, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9985210044077364}}, {"question": "\u4ee5\u4e0b\u4e0d\u5c5e\u4e8e\u80fd\u529b\u62d3\u5c55\u8bad\u7ec3\u65b9\u6cd5\u7684\u662f\nA. \u6297\u632b\u6298\u80fd\u529b\u62d3\u5c55\u8bad\u7ec3\nB. \u81ea\u6211\u6559\u80b2\u80fd\u529b\u62d3\u5c55\u8bad\u7ec3\nC. \u81ea\u5236\u529b\u62d3\u5c55\u8bad\u7ec3\nD. \u8eab\u4f53\u7d20\u8d28\u62d3\u5c55\u8bad\u7ec3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8676246122911386, "meta-math/MetaMath-Mistral-7B": 0.9458570641870943, "itpossible/Chinese-Mistral-7B-v0.1": 0.470200850949345, "HuggingFaceH4/zephyr-7b-beta": 0.9997453865820141, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9953905430119601, "meta-llama/Meta-Llama-3-8B": 0.38754811993034344, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5996737521525733}}, {"question": "\u6839\u636e1930\u5e74\u300a\u65e5\u5185\u74e6\u7edf\u4e00\u6c47\u7968\u3001\u672c\u7968\u516c\u7ea6\u300b\u7684\u89c4\u5b9a\uff0c\u5bf9\u4e8e\u89c1\u7968\u5373\u4ed8\u7684\u6c47\u7968\uff0c\u5728\u7279\u522b\u89c4\u5b9a\u7684\u573a\u5408\u4e0b\uff0c\u6301\u7968\u4eba\u5e94\u4e8e\u8be5\u6c47\u7968\u51fa\u7968\u65e5\u540e\u4e00\u5b9a\u671f\u9650\u5185\u63d0\u793a\u4ed8\u6b3e\u3002\u800c\u8be5\u671f\u9650\u662f\nA. 6\u4e2a\u6708\nB. 1\u4e2a\u6708\nC. 1\u5e74\nD. 2\u5e74\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.41601988974296555, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u7ec6\u80de\u6709\u4e1d\u5206\u88c2\u7684\u53d9\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u6709\u4e1d\u5206\u88c2\u95f4\u671f\uff0cDNA\u590d\u5236\u7684\u8fc7\u7a0b\u9700\u8981\u89e3\u65cb\u9176\u7684\u53c2\u4e0e\nB. \u6709\u4e1d\u5206\u88c2\u4e2d\u671f\uff0c\u53d1\u751f\u8054\u4f1a\u7684\u540c\u6e90\u67d3\u8272\u4f53\u6392\u5217\u5728\u8d64\u9053\u677f\u4e0a\nC. \u5728\u7ec6\u80de\u5468\u671f\u4e2d\uff0c\u5206\u88c2\u95f4\u671f\u7684\u6301\u7eed\u65f6\u95f4\u901a\u5e38\u6bd4\u5206\u88c2\u671f\u7684\u77ed\nD. \u8d64\u9053\u677f\u662f\u7ec6\u80de\u6709\u4e1d\u5206\u88c2\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u4e00\u79cd\u7ed3\u6784\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.445794905146293, "meta-math/MetaMath-Mistral-7B": 0.8515126428420579, "itpossible/Chinese-Mistral-7B-v0.1": 0.4324117302203187, "HuggingFaceH4/zephyr-7b-beta": 0.9646583241988895, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4225799211775481, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u4e2d\uff0c\u6700\u53e4\u8001\u3001\u6700\u76f4\u63a5\u7684\u63a7\u5236\u65b9\u6cd5\u662f\nA. \u4eba\u5458\u7ba1\u7406\u63a7\u5236\u6cd5\nB. \u73b0\u573a\u89c2\u5bdf\u6cd5\nC. \u4e13\u9898\u62a5\u544a\u5206\u6790\u6cd5\nD. \u7edf\u8ba1\u6570\u636e\u8d44\u6599\u5206\u6790\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8362602780778328, "meta-math/MetaMath-Mistral-7B": 0.9741554155732507, "itpossible/Chinese-Mistral-7B-v0.1": 0.6633024573830241, "HuggingFaceH4/zephyr-7b-beta": 0.9998483136473699, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7706960512420803, "meta-llama/Meta-Llama-3-8B": 0.9064826258258006, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9723359835292068}}, {"question": "\u4e4b\u6240\u4ee5\u8981\u52a0\u5feb\u56fd\u6709\u4f01\u4e1a\u80a1\u4efd\u5236\u6539\u9020\uff0c\u662f\u56e0\u4e3a\u80a1\u4efd\u5236\u4f5c\u4e3a\u516c\u6709\u5236\u7ecf\u6d4e\u7684\u5b9e\u73b0\u5f62\u5f0f\uff0c\u6709\u5229\u4e8ea\u589e\u5f3a\u516c\u6709\u5236\u7ecf\u6d4e\u7684\u6d3b\u529b b\u63d0\u9ad8\u4f01\u4e1a\u548c\u8d44\u672c\u7684\u8fd0\u4f5c\u6548\u7387 c\u6269\u5927\u516c\u6709\u8d44\u672c\u7684\u652f\u914d\u8303\u56f4 d\u589e\u5f3a\u516c\u6709\u5236\u7684\u4e3b\u4f53\u4f5c\u7528\nA. acd\nB. bcd\nC. abcd\nD. abc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37239554368780975, "meta-math/MetaMath-Mistral-7B": 0.6692518218602239, "itpossible/Chinese-Mistral-7B-v0.1": 0.39564263321815757, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.44223815353089685, "meta-llama/Meta-Llama-3-8B": 0.5971291336503706, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7069023330072476}}, {"question": "\u5c06\u94a2\u6dec\u706b\u540e\uff0c\u518d\u7ecf\u9ad8\u6e29\u56de\u706b\u7684\u70ed\u5904\u7406\u65b9\u5f0f\u53eb\nA. \u56de\u706b\nB. \u6dec\u706b\nC. \u8c03\u8d28\u5904\u7406\nD. \u6b63\u706b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7734649310372118, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c5e\u4e8e\u5b66\u6821\u6559\u80b2\u7684\u6f5c\u529f\u80fd\u7684\u662f\nA. \u6587\u51ed\u529f\u80fd\nB. \u793e\u4f1a\u9009\u62e9\u529f\u80fd\nC. \u793e\u4f1a\u63a7\u5236\u529f\u80fd\nD. \u4f20\u64ad\u77e5\u8bc6\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4250020490922126, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7973626190434091, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5404\u5c3d\u6240\u80fd\uff0c\u6309\u9700\u5206\u914d\u662f\nA. \u793e\u4f1a\u4e3b\u4e49\u793e\u4f1a\u7684\u5206\u914d\u65b9\u5f0f\nB. \u539f\u59cb\u793e\u4f1a\u7684\u5206\u914d\u65b9\u5f0f\nC. \u5171\u4ea7\u4e3b\u4e49\u793e\u4f1a\u7684\u5206\u914d\u65b9\u5f0f\nD. \u9636\u7ea7\u793e\u4f1a\u7684\u5206\u914d\u65b9\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u793e\u4f1a\u4e3b\u4e49\u6cd5\u5f8b\u7684\u5b9e\u65bd\uff0c\u4e00\u822c\u5206\u4e3a\u6267\u6cd5\u3001\u53f8\u6cd5\u3001\u5b88\u6cd5\u548c\u6cd5\u5f8b\u76d1\u7763\u3002\u5176\u4e2d\u53f8\u6cd5\u7684\u4e3b\u4f53\u662f\nA. \u56fd\u5bb6\u5ba1\u5224\u673a\u5173\u548c\u68c0\u5bdf\u673a\u5173\nB. \u56fd\u5bb6\u76d1\u5bdf\u673a\u5173\nC. \u56fd\u5bb6\u6743\u529b\u673a\u5173\nD. \u56fd\u5bb6\u884c\u653f\u673a\u5173\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9804030051496793, "meta-math/MetaMath-Mistral-7B": 0.9988457217175367, "itpossible/Chinese-Mistral-7B-v0.1": 0.9348607776554122, "HuggingFaceH4/zephyr-7b-beta": 0.9999707586812406, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9976521283134372, "meta-llama/Meta-Llama-3-8B": 0.9847375552049449, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.983472857835978}}, {"question": "\u82cf\u683c\u62c9\u5e95\u7684\u201c\u672a\u7ecf\u7701\u5bdf\u7684\u4eba\u751f\u4e0d\u503c\u5f97\u8fc7\u201d\u7684\u4e3b\u5f20\u7ecf\u5e38\u88ab\u4eba\u4eec\u5f15\u7528\u4e3a\u4eba\u4eec\u6d3b\u52a8\u7684\u4e2d\u5fc3\u4e3b\u9898\u3002 \u901a\u8fc7\u5b83\uff0c\u82cf\u683c\u62c9\u5e95\u901a\u5e38\u88ab\u7406\u89e3\u4e3a\uff1a\nA. \u6709\u65f6\u6839\u672c\u4e0d\u503c\u5f97\u4ed8\u51fa\u6240\u6709\u52aa\u529b\u6765\u8be6\u7ec6\u68c0\u67e5\u751f\u6d3b\u53ca\u5176\u95ee\u9898\uff1b \u6709\u65f6\u6700\u597d\u53ea\u662f\u201c\u987a\u5176\u81ea\u7136\u201d\u3002\nB. \u5750\u5728\u90a3\u91cc\u601d\u8003\u751f\u6d3b\u662f\u5426\u503c\u5f97\u8fc7\u662f\u6d6a\u8d39\u65f6\u95f4\uff1b\u6211\u4eec\u5e94\u8be5\u628a\u8fd9\u6837\u7684\u53cd\u601d\u7559\u7ed9\u8131\u53e3\u79c0\u4e3b\u6301\u4eba\u3001\u653f\u6cbb\u4eba\u7269\u548c\u5b97\u6559\u9886\u8896\u3002\nC. \u53ea\u662f\u505a\u522b\u4eba\u505a\u7684\u4e8b\u800c\u4e0d\u8003\u8651\u4e3a\u4ec0\u4e48\u6211\u4eec\u5e94\u8be5\u505a\u6211\u4eec\u505a\u7684\u4e8b\uff0c\u5f88\u96be\u88ab\u8ba4\u4e3a\u662f\u6709\u4ef7\u503c\u7684\u3001\u9ad8\u5c1a\u7684\u6216\u4ee4\u4eba\u94a6\u4f69\u7684\u3002\nD. \u867d\u7136\u5bf9\u751f\u6d3b\u91c7\u53d6\u53cd\u601d\u7684\u6001\u5ea6\u5f88\u6709\u8da3\uff0c\u6709\u65f6\u751a\u81f3\u5f88\u91cd\u8981\uff0c\u4f46\u5927\u591a\u6570\u8ba9\u751f\u6d3b\u503c\u5f97\u8fc7\u7684\u4e1c\u897f\u90fd\u4e0d\u503c\u5f97\u7814\u7a76\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9297808490840702, "meta-math/MetaMath-Mistral-7B": 0.9834936420564068, "itpossible/Chinese-Mistral-7B-v0.1": 0.6217694750995927, "HuggingFaceH4/zephyr-7b-beta": 0.9981846556851061, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9669074861824875, "meta-llama/Meta-Llama-3-8B": 0.47980191557449564, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9261842227436606}}, {"question": "\u4f7f\u2f64Cache\u53ef\u4ee5\u63d0\u2fbc\u8ba1\u7b97\u673a\u8fd0\u2f8f\u901f\u5ea6\uff0c\u8fd9\u662f\u56e0\u4e3a\nA. Cache\u53ef\u4ee5\u5b58\u653e\u7a0b\u5e8f\u548c\u6570\u636e\nB. Cache\u589e\u2f24\u4e86\u5185\u5b58\u7684\u5bb9\u91cf\nC. Cache\u7f29\u77ed\u4e86CPU\u7684\u7b49\u5f85\u65f6\u95f4\nD. Cache\u6269\u2f24\u4e86\u786c\u76d8\u7684\u5bb9\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9644937833998605, "meta-math/MetaMath-Mistral-7B": 0.9989863299838216, "itpossible/Chinese-Mistral-7B-v0.1": 0.979066684650386, "HuggingFaceH4/zephyr-7b-beta": 0.9999536587708899, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.99561311507816, "meta-llama/Meta-Llama-3-8B": 0.9571979557401412, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9913592502269322}}, {"question": "\u56e2\u961f\u5b66\u4e60\u884c\u4e3a\u662f\u6307\u56e2\u961f\u6210\u5458\u4e3a\u4e86\u6ee1\u8db3\u56e2\u961f\u548c\u4e2a\u4eba\u7684\u53d1\u5c55\u9700\u6c42\uff0c\u5728\u5206\u4eab\u5404\u81ea\u7ecf\u9a8c\u7684\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u77e5\u8bc6\u548c\u4fe1\u606f\u4ea4\u6d41\u7b49\u9014\u5f84\u63d0\u51fa\u95ee\u9898\u3001\u5bfb\u6c42\u53cd\u9988\u3001\u8fdb\u884c\u8bd5\u9a8c\u3001\u53cd\u601d\u7ed3\u679c\u3001\u5bf9\u5931\u8bef\u6216\u672a\u9884\u671f\u7ed3\u679c\u8fdb\u884c\u8ba8\u8bba\uff0c\u4fc3\u8fdb\u56e2\u961f\u4e0e\u4e2a\u4eba\u77e5\u8bc6\u6216\u6280\u80fd\u6c34\u5e73\u4e0d\u65ad\u63d0\u9ad8\uff0c\u8fdb\u800c\u5b9e\u73b0\u56e2\u961f\u5c42\u9762\u77e5\u8bc6\u4e0e\u6280\u80fd\u76f8\u5bf9\u6301\u4e45\u53d8\u5316\u3002\u6839\u636e\u4e0a\u8ff0\u5b9a\u4e49\uff0c\u4e0b\u5217\u5c5e\u4e8e\u56e2\u961f\u5b66\u4e60\u884c\u4e3a\u7684\u662f\nA. \u5c0f\u738b\u4e3a\u4e86\u4eca\u540e\u7684\u53d1\u5c55\uff0c\u5728\u4e0a\u73ed\u671f\u95f4\u62bd\u7a7a\u548c\u5176\u4ed6\u51e0\u4f4d\u540c\u4e8b\u4e00\u8d77\u5b66\u4e60\u82f1\u8bed\nB. \u4e3a\u4e86\u83b7\u5f97\u5956\u5b66\u91d1\uff0c\u67d0\u5bbf\u820d\u5168\u4f53\u5973\u751f\u7ea6\u5b9a\u5927\u5bb6\u76f8\u4e92\u76d1\u7763\uff0c\u665a\u4e0a\u4e00\u8d77\u81ea\u4e60\nC. \u67d0\u9879\u76ee\u7ec4\u4e3a\u4e86\u5728\u9879\u76ee\u653b\u5173\u9636\u6bb5\u53d1\u6325\u56e2\u961f\u7684\u51dd\u805a\u529b\u548c\u6218\u6597\u529b\uff0c\u6bcf\u5468\u672b\u90fd\u4e3e\u884c\u91ce\u5916\u62d3\u5c55\u8bad\u7ec3\nD. \u9500\u552e\u90e8\u4e3a\u63d0\u9ad8\u90e8\u95e8\u6574\u4f53\u4e1a\u7ee9\uff0c\u63a8\u9009\u4f18\u79c0\u4ee3\u8868\u5b9a\u671f\u4e0e\u5927\u5bb6\u5206\u4eab\u9500\u552e\u7ecf\u9a8c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9489151285264167, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6791336906888658, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u53f2\u5b9e\u201d\u201c\u53f2\u8bba\u201d\u201c\u53f2\u8bc6\u201d\u662f\u6784\u6210\u53f2\u5b66\u7684\u4e09\u5927\u8981\u7d20\u3002\u53f2\u5b9e\u5373\u5386\u53f2\u4e8b\u5b9e\uff1b\u53f2\u8bba\u5373\u5bf9\u5386\u53f2\u4e8b\u4ef6\u548c\u5386\u53f2\u4eba\u7269\u7684\u8bc4\u8bba\uff1b\u53f2\u8bc6\u5373\u662f\u4ee5\u79d1\u5b66\u7684\u53f2\u89c2\u4f5c\u6307\u5bfc\uff0c\u5206\u6790\u5927\u91cf\u7684\u53f2\u5b9e\uff0c\u7136\u540e\u5f97\u51fa\u79d1\u5b66\u7684\u7ed3\u8bba\u3002\u4e0b\u5217\u5bf9\u90e1\u53bf\u5236\u7684\u53d9\u8ff0\u5c5e\u4e8e\u201c\u53f2\u8bc6\u201d\u7684\u662f\nA. \u90e1\u53bf\u5236\u5f00\u4e2d\u56fd\u5355\u4e00\u5236\u56fd\u5bb6\u7ec4\u7ec7\u7ed3\u6784\u5148\u6cb3\uff0c\u5f71\u54cd\u6df1\u8fdc\nB. \u90e1\u5b88\u548c\u53bf\u4ee4\u3001\u53bf\u957f\u90fd\u7531\u7687\u5e1d\u76f4\u63a5\u4efb\u547d\nC. \u90e1\u53bf\u5236\u662f\u4e2d\u592e\u5bf9\u5730\u65b9\u653f\u6743\u8fdb\u884c\u6709\u6548\u63a7\u5236\u7684\u5236\u5ea6\nD. \u5b83\u5206\u90e1\u53bf\u4e24\u7ea7\uff0c\u4e00\u90e1\u4e4b\u5185\u53c8\u5206\u82e5\u5e72\u53bf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4775986244518356, "meta-math/MetaMath-Mistral-7B": 0.4771975313724849, "itpossible/Chinese-Mistral-7B-v0.1": 0.41098417559358036, "HuggingFaceH4/zephyr-7b-beta": 0.9811034784201865, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6408866336223903, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u73b0\u5b9e\u662f\u6b64\u5cb8\uff0c\u7406\u60f3\u662f\u5f7c\u5cb8\u3002\u4e2d\u95f4\u9694\u7740\u6e4d\u6025\u7684\u6cb3\u6d41\uff0c\u884c\u52a8\u5219\u662f\u67b6\u5728\u5ddd\u4e0a\u7684\u6865\u6881\u3002\u201d\u8fd9\u5f62\u8c61\u5730\u8bf4\u660e\nA. \u7406\u60f3\u662f\u53ef\u4ee5\u81ea\u53d1\u5b9e\u73b0\u7684\nB. \u73b0\u5b9e\u4e0d\u80fd\u6210\u4e3a\u7406\u60f3\nC. \u7406\u60f3\u5c31\u662f\u73b0\u5b9e\nD. \u7406\u60f3\u7684\u5b9e\u73b0\u5fc5\u987b\u843d\u5b9e\u5728\u884c\u52a8\u4e0a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9729773675111048, "meta-math/MetaMath-Mistral-7B": 0.9979007533977965, "itpossible/Chinese-Mistral-7B-v0.1": 0.985848430682116, "HuggingFaceH4/zephyr-7b-beta": 0.9993280530060189, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9943463336424336, "meta-llama/Meta-Llama-3-8B": 0.9839178333261522, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9968842421725448}}, {"question": "\u4e0b\u9762\u5173\u4e8e\u53e4\u4e66\u65e7\u6ce8\u7c7b\u578b\u8868\u8ff0\u6b63\u786e\u7684\u4e00\u9879\u662f\nA. \u4e49\u758f\u7c7b\u662f\u6307\u65e2\u89e3\u91ca\u7ecf\u6587\u53c8\u7ed9\u524d\u4eba\u7684\u6ce8\u4f5c\u89e3\u91ca\u7684\u4e00\u79cd\u53e4\u6ce8\u7c7b\u578b\u3002\nB. \u300a\u6bdb\u8bd7\u6545\u8bad\u4f20\u300b\u5c5e\u4e8e\u8865\u6ce8\u7c7b\u3002\nC. \u6700\u6709\u540d\u7684\u7ae0\u53e5\u7c7b\u6ce8\u91ca\u4e13\u4e66\u662f\u6881\u4ee3\u7687\u4f83\u7684\u300a\u5b5f\u5b50\u7ae0\u53e5\u300b\u3002\nD. \u7ae0\u53e5\u7c7b\u662f\u5bf9\u53e4\u4e66\u4e2d\u6587\u5b57\u8fdb\u884c\u8fa8\u97f3\u91ca\u4e49\u7684\u6ce8\u91ca\u7c7b\u578b\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3748084697576336, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5530824463643484}}, {"question": "\u5e02\u573a\u8425\u9500\u4e13\u4e1a\u5316\u7ec4\u7ec7\u4e00\u822c\u5305\u62ec\u56db\u79cd\u7c7b\u578b\uff0c\u4ee5\u5f3a\u8c03\u5e02\u573a\u8425\u9500\u4e2d\u7684\u5404\u79cd\u804c\u80fd\u5982\u9500\u552e\u3001\u5e7f\u544a\u548c\u8c03\u7814\u7684\u91cd\u8981\u6027\u4e3a\u7279\u5f81\u7684\u4e13\u4e1a\u5316\u7ec4\u7ec7\uff0c\u6307\u7684\u662f\nA. \u5e02\u573a\u578b\u7ec4\u7ec7\nB. \u804c\u80fd\u578b\u7ec4\u7ec7\nC. \u4ea7\u54c1\u578b\u7ec4\u7ec7\nD. \u5730\u7406\u578b\u7ec4\u7ec7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5112691986997943, "meta-math/MetaMath-Mistral-7B": 0.7278919969195184, "itpossible/Chinese-Mistral-7B-v0.1": 0.8175216669119536, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9348607835536591, "meta-llama/Meta-Llama-3-8B": 0.40553129844907915, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.995356024907282}}, {"question": "\u786e\u8ba4\u504f\u5dee\u662f\u6307\u4eba\u4e00\u65e6\u4ea7\u751f\u67d0\u4e2a\u4fe1\u5ff5\uff0c\u5c31\u4f1a\u52aa\u529b\u5bfb\u627e\u4e0e\u5b83\u76f8\u7b26\u7684\u4f8b\u5b50\uff0c\u5e76\u65e0\u89c6\u90a3\u4e9b\u4e0d\u7b26\u7684\u3002\u6839\u636e\u4e0a\u8ff0\u5b9a\u4e49\uff0c\u4e0b\u5217\u5c5e\u4e8e\u786e\u8ba4\u504f\u5dee\u7684\u662f\nA. \u5c0f\u4e1c\u542c\u5230\u67d0\u4e2a\u6240\u8c13\u7684\u201c\u9884\u8a00\u5bb6\u201d\u65ad\u5b9a\u81ea\u5df1\u4f1a\u906d\u9047\u8f66\u7978\u540e\u65f6\u5e38\u611f\u5230\u62c5\u5fe7\uff0c\u67d0\u5929\u4ed6\u7a81\u7136\u53d1\u751f\u8f66\u7978\uff0c\u4e8e\u662f\u4ed6\u66f4\u76f8\u4fe1\u90a3\u4f4d\u201c\u9884\u8a00\u5bb6\u201d\u4e86\nB. \u80a1\u7968\u7ecf\u7406\u4eba\u544a\u8bc9\u5ba2\u6237\u5c0f\u660e\u67d0\u80a1\u7968\u4f1a\u6da8\u7684\u540c\u65f6\u53c8\u80cc\u7740\u5c0f\u660e\u544a\u8bc9\u5176\u4ed6\u5ba2\u6237\u8be5\u80a1\u7968\u4f1a\u8dcc\uff0c\u7ed3\u679c\u8be5\u80a1\u7968\u5927\u6da8\uff0c\u4ece\u6b64\u5c0f\u660e\u5bf9\u8be5\u7ecf\u7406\u4eba\u5341\u5206\u4fe1\u4efb\nC. \u5c0f\u521a\u8ba4\u4e3a\u7ec8\u6709\u4e00\u5929\u4f1a\u5929\u964d\u6a2a\u8d22\uff0c\u4fbf\u75f4\u8ff7\u4e8e\u5f69\u7968\uff0c\u5c3d\u7ba1\u4ece\u672a\u4e2d\u5956\uff0c\u4ed6\u8fd8\u662f\u6574\u65e5\u6e38\u624b\u597d\u95f2\uff0c\u751a\u81f3\u8d37\u6b3e\u4e70\u5f69\u7968\nD. \u5c3d\u7ba1\u522b\u4eba\u544a\u8bc9\u5c0f\u9ec4\u6240\u6709\u6ce1\u83dc\u575b\u91cc\u7684\u6ce1\u83dc\u539f\u6599\u3001\u6ce1\u5236\u65f6\u95f4\u90fd\u4e00\u6837\uff0c\u4f46\u5c0f\u9ec4\u4ecd\u8ba4\u4e3a\u7528\u9ec4\u8272\u6ce1\u575b\u91cc\u7684\u6ce1\u83dc\u70f9\u996a\u9c7c\u9999\u8089\u4e1d\u4f1a\u66f4\u53ef\u53e3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.43967622406207074, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e8c\u5341\u9762\u4f53\u5bf9\u79f0\u7684\u75c5\u6bd2\u7c92\u5b50\u7684\u4e94\u90bb\u4f53\u4f4d\u4e8e\nA. 12 \u4e2a\u9876\u89d2\u4e0a\nB. 20 \u4e2a\u9762\u4e0a\nC. 30 \u6761\u68f1\u4e0a\nD. \u5206\u522b\u4f4d\u4e8e 12 \u4e2a\u9876\u89d2\u4e0a\u300120 \u4e2a\u9762\u4e0a\u300130 \u6761\u68f1\u4e0a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u813e\u5207\u9664\u3001\u8d32\u95e8\u5468\u56f4\u8840\u7ba1\u79bb\u65ad\u672f\u6cbb\u7597\u809d\u786c\u5316\u95e8\u9759\u8109\u9ad8\u538b\u75c7\u98df\u7ba1\u9759\u8109\u7834\u88c2\u51fa\u8840\u7684\u4f18\u70b9\u4e2d\uff0c\u4e0d\u88ab\u666e\u904d\u8ba4\u53ef\u7684\u662f\nA. \u672f\u540e\u809d\u529f\u80fd\u6539\u5584\nB. \u6b62\u8840\u6548\u679c\u8f83\u6ee1\u610f\nC. \u672f\u540e\u8111\u75c5\u53d1\u751f\u7387\u8f83\u4f4e\nD. \u624b\u672f\u6253\u51fb\u76f8\u5bf9\u8f83\u5c0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.333183235354062, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u6e05\u672b\u6cd5\u5b66\u5bb6\u6c88\u5bb6\u672c\u8bf4\uff1a\u201c\u6cd5\u7acb\u800c\u4e0d\u884c\uff0c\u4e0e\u65e0\u6cd5\u7b49\u3002\u201d\u8fd9\u53e5\u8bdd\u5f3a\u8c03\u4e86\nA. \u5b88\u6cd5\u7684\u91cd\u8981\u6027\nB. \u62a4\u6cd5\u7684\u91cd\u8981\u6027\nC. \u7acb\u6cd5\u7684\u91cd\u8981\u6027\nD. \u5b66\u6cd5\u7684\u91cd\u8981\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3060136256597631, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4948050318977868, "meta-llama/Meta-Llama-3-8B": 0.6137571638774524, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6732406422533147}}, {"question": "\u56db\u683c\u8868\u7684\u81ea\u7531\u5ea6\nA. \u7b49\u4e8e\u6837\u672c\u542b\u91cf\u51cf1\nB. \u4e00\u5b9a\u7b49\u4e8e1\nC. \u7b49\u4e8e\u884c\u6570$\\times$\u5217\u6570\nD. \u4e0d\u4e00\u5b9a\u7b49\u4e8e1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "MIGA\u516c\u7ea6\u751f\u6548\u65f6\uff0c\u4e0e\u5404\u56fd\u6295\u8d44\u4fdd\u9669\u5236\u5ea6\u76f8\u6bd4\uff0c\u589e\u8bbe\u7684\u9669\u79cd\u662f\nA. \u5f81\u6536\u548c\u7c7b\u4f3c\u63aa\u65bd\u9669\nB. \u8fdd\u7ea6\u9669\nC. \u8d27\u5e01\u6c47\u5151\u9669\nD. \u6218\u4e89\u548c\u5185\u4e71\u9669\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4345817613168092, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6b63\u786e\u7684\u5b57\u7b26\u5e38\u91cf\u662f\nA. GO\nB. \u2019/1010\u2019\nC. \u201968\u2019\nD. \u201cB\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.639093503266698, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5 $a\\gt 0,a\\neq 0$\uff0c\u5219$a^{0}+log_{a}a=$\nA. 1\nB. 2\nC. a\nD. 0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5693269996406791}}, {"question": "\u9a6c\u514b\u601d\u6069\u683c\u65af\u8fdb\u4e00\u6b65\u53d1\u5c55\u548c\u5b8c\u5584\u4e86\u82f1\u56fd\u53e4\u5178\u7ecf\u6d4e\u5b66\u7406\u8bba\u662f\nA. \u5386\u53f2\u89c2\nB. \u52b3\u52a8\u4ef7\u503c\u8bba\nC. \u8fa9\u8bc1\u6cd5\nD. \u5269\u4f59\u4ef7\u503c\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u5728\u4e2d\u56fd\u76f4\u63a5\u6295\u8d44\u8bbe\u7acb\u7684\u4e09\u79cd\u5916\u8d44\u4f01\u4e1a\u4ece\u4e8b\u7ecf\u8425\u6d3b\u52a8\uff0c\u6211\u56fd\u4e0d\u518d\u91c7\u53d6\u8ba4\u8bb8\u7a0b\u5e8f\uff0c\u662f\u56e0\u4e3a\nA. \u5df2\u7ecf\u8fc7\u6982\u62ec\u8ba4\u8bb8\u7a0b\u5e8f\nB. \u5df2\u7ecf\u8fc7\u5206\u522b\u8ba4\u8bb8\u7a0b\u5e8f\nC. \u5df2\u7ecf\u8fc7\u7279\u522b\u8ba4\u8bb8\u7a0b\u5e8f\nD. \u5b83\u4eec\u5747\u5c5e\u4e2d\u56fd\u6cd5\u4eba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3220565245827037, "meta-math/MetaMath-Mistral-7B": 0.7303838957075778, "itpossible/Chinese-Mistral-7B-v0.1": 0.5859787306514935, "HuggingFaceH4/zephyr-7b-beta": 0.9987013876516442, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.64112749289674, "meta-llama/Meta-Llama-3-8B": 0.308176776288574, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3354456100429363}}, {"question": "\u53ef\u4ee5\u8ba4\u4e3a\u662f\u4eba\u4e0e\u4eba\u4e4b\u95f4\u7684\u4e00\u79cd\u201c\u7b49\u5229\u201d\u6216\u201c\u7b49\u5bb3\u201d\u4ea4\u6362\u7684\u662f\nA. \u81ea\u7531\nB. \u516c\u6b63\nC. \u5747\u8861\nD. \u5e73\u7b49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5a92\u4ecb\u5173\u7cfb\u53c8\u79f0\uff08\uff09\u5173\u7cfb\uff0c\u662f\u793e\u4f1a\u7ec4\u7ec7\u4e0e\u62a5\u7eb8\u3001\u7535\u53f0\u3001\u7535\u89c6\u53f0\u7b49\u5927\u4f17\u4f20\u64ad\u5a92\u4ecb\u673a\u6784\u4ee5\u53ca\u4e0e\u7f16\u8f91\u3001\u8bb0\u8005\u3001\u8282\u76ee\u4e3b\u6301\u4eba\u7b49\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\nA. \u5e7f\u64ad\u5173\u7cfb\nB. \u793e\u4f1a\u5173\u7cfb\nC. \u65b0\u95fb\u754c\u5173\u7cfb\nD. \u516c\u5171\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6796775193504948, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6485130921287493, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7048295611125879}}, {"question": "\u76ee\u524d\uff0c\u6211\u56fd\u5df2\u5efa\u7acb\u7684\u6c11\u65cf\u81ea\u6cbb\u5730\u65b9\u6709\u591a\u5c11\u4e2a\nA. 145\nB. 135\nC. 155\nD. 165\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.29863342676099575, "HuggingFaceH4/zephyr-7b-beta": 0.6079137175595952, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8981\u5728\u519c\u5386\u5341\u516d\u89c2\u6d4b\u5230\u6708\u63a9\u6634\u661f\u56e2\uff0c\u4ee5\u4e0b\u56db\u4e2a\u6708\u4efd\u4e2d\u53ea\u53ef\u80fd\u53d1\u751f\u5728\nA. 11\u6708\nB. 5\u6708\nC. 2\u6708\nD. 8\u6708\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5065965450666703, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.31180336173373097}}, {"question": "\u4ee5\u4e0b\u5c5e\u4e8e\u65e0\u5f62\u7684\u804c\u4e1a\u9662\u6821\u5236\u5ea6\u6587\u5316\u7684\u4e00\u9879\u662f\nA. \u7fa4\u4f53\u884c\u4e3a\u89c4\u8303\nB. \u540e\u52e4\u7ba1\u7406\u5236\u5ea6\nC. \u5b66\u751f\u5de5\u4f5c\u5236\u5ea6\nD. \u4eba\u4e8b\u5236\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4134033029848103, "meta-math/MetaMath-Mistral-7B": 0.49402290886950606, "itpossible/Chinese-Mistral-7B-v0.1": 0.668485788968039, "HuggingFaceH4/zephyr-7b-beta": 0.9989186488539897, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7589988793563766, "meta-llama/Meta-Llama-3-8B": 0.7500304042566392, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9911813094327578}}, {"question": "\u4e0b\u5217\u4e0e\u65bd\u5de5\u8fdb\u5ea6\u6709\u5173\u7684\u8ba1\u5212\u4e2d\uff0c\u5c5e\u4e8e\u65bd\u5de5\u4f01\u4e1a\u751f\u4ea7\u8ba1\u5212\u7684\u662f\nA. \u9879\u76ee\u65bd\u5de5\u8fdb\u5ea6\u8ba1\u5212\nB. \u4f9b\u8d27\u5de5\u4f5c\u8fdb\u5ea6\u8ba1\u5212\nC. \u751f\u4ea7\u8d44\u6e90\u8c03\u914d\u8ba1\u5212\nD. \u65bd\u5de5\u603b\u8fdb\u5ea6\u8ba1\u5212\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4177918154516695, "meta-math/MetaMath-Mistral-7B": 0.5123862626455431, "itpossible/Chinese-Mistral-7B-v0.1": 0.4655094774097466, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7022345994526359, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u4eec\u8bf4\u5e02\u573a\u7ecf\u6d4e\u5c31\u662f\uff08\uff09\uff0c\u5b83\u662f\u73b0\u4ee3\u5e02\u573a\u7ecf\u6d4e\u7684\u57fa\u77f3\nA. \u5229\u76ca\u7ecf\u6d4e\nB. \u4fe1\u7528\u7ecf\u6d4e\nC. \u5546\u4e1a\u7ecf\u6d4e\nD. \u9053\u5fb7\u7ecf\u6d4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3976839562230386, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u539f\u7801\u4e58\u6cd5\u662f\nA. \u2f64\u539f\u7801\u8868\u793a\u64cd\u4f5c\u6570\uff0c\u7136\u540e\u76f4\u63a5\u76f8\u4e58\nB. \u88ab\u4e58\u6570\u2f64\u539f\u7801\u8868\u793a\uff0c\u4e58\u6570\u53d6\u7edd\u5bf9\u503c\uff0c\u7136\u540e\u76f8\u4e58\nC. \u4e58\u6570\u2f64\u539f\u7801\u8868\u793a\uff0c\u88ab\u4e58\u6570\u53d6\u7edd\u5bf9\u503c\uff0c\u7136\u540e\u76f8\u4e58\nD. \u5148\u53d6\u64cd\u4f5c\u6570\u7edd\u5bf9\u503c\u76f8\u4e58\uff0c\u7b26\u53f7\u4f4d\u5355\u72ec\u5904\u7406\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.44540477104687987, "HuggingFaceH4/zephyr-7b-beta": 0.8649547433209036, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5572543787176446, "meta-llama/Meta-Llama-3-8B": 0.33482349867891054, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u4e2a\u4eba\u7684\u957f\u671f\u884c\u4e3a\u6240\u8868\u73b0\u548c\u5f62\u6210\u7684\u7a33\u5b9a\u7684\u3001\u6052\u4e45\u7684\u3001\u6574\u4f53\u7684\u5fc3\u7406\u72b6\u6001\u662f\nA. \u54c1\u5fb7\nB. \u4e3b\u4f53\u9053\u5fb7\u672c\u8d28\nC. \u4eba\u683c\nD. \u9053\u5fb7\u610f\u5fd7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5058610098687853, "meta-math/MetaMath-Mistral-7B": 0.792350928199657, "itpossible/Chinese-Mistral-7B-v0.1": 0.6662656662211847, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6414450162694255, "meta-llama/Meta-Llama-3-8B": 0.5970437440024797, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6468980866449842}}, {"question": "\u4f01\u4e1a\u5458\u5de5\u5177\u5907\u4f18\u79c0\u4f01\u4e1a\u8bda\u4fe1\u54c1\u8d28\u7684\u9996\u8981\u8981\u6c42\u662f\nA. \u4fdd\u6301\u6b63\u76f4\u5ba2\u89c2\u7acb\u573a\nB. \u4f01\u4e1a\u7ecf\u8425\u5224\u65ad\u4e0e\u8d23\u4efb\u80fd\u529b\nC. \u7b03\u4fe1\u8654\u656c\u6001\u5ea6\nD. \u516c\u6b63\u5e73\u7b49\u610f\u8bc6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.45162389206804976, "itpossible/Chinese-Mistral-7B-v0.1": 0.3837616204620929, "HuggingFaceH4/zephyr-7b-beta": 0.8767711362540976, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4980398162857746, "meta-llama/Meta-Llama-3-8B": 0.38797403658066, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5482602347788242}}, {"question": "\u8d6b\u9c81\u6653\u592b\u5f53\u653f\u65f6\u671f\uff0c\u57fa\u4e8e\u82cf\u8054\u7ecf\u6d4e\u519b\u4e8b\u529b\u91cf\u4e0e\u7f8e\u56fd\u5b58\u5728\u7684\u5dee\u8ddd\u548c\u5de9\u56fa\u81ea\u8eab\u6743\u529b\u8003\u8651\uff0c\u63a8\u884c\u4e86\nA. \u65e0\u9650\u6269\u5f20\u6218\u7565\nB. \u5168\u7403\u4e89\u9738\u6218\u7565\nC. \u8d85\u8d8a\u904f\u5236\u6218\u7565\nD. \u6709\u9650\u6269\u5f20\u6218\u7565\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3460058095032024, "HuggingFaceH4/zephyr-7b-beta": 0.6039079277691145, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.809775996114367, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5730576433760538}}, {"question": "\u9636\u7ea7\u5b9e\u8d28\u4e0a\u662f\u4e00\u4e2a\nA. \u6587\u5316\u8303\u7574\nB. \u653f\u6cbb\u8303\u7574\nC. \u7ecf\u6d4e\u8303\u7574\nD. \u601d\u60f3\u8303\u7574\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9418199079368549, "meta-math/MetaMath-Mistral-7B": 0.9982632181196656, "itpossible/Chinese-Mistral-7B-v0.1": 0.8236800131258694, "HuggingFaceH4/zephyr-7b-beta": 0.9974371542953772, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9260188158763432, "meta-llama/Meta-Llama-3-8B": 0.943403723790764, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9662667726762748}}, {"question": "\u4e2d\u6bd2\u540e\u4e3b\u8981\u5f15\u8d77\u6eb6\u8840\u6027\u8d2b\u8840\u548c\u9ec4\u75b8\u7684\u662f\nA. \u7532\u9187\nB. \u6c28\u57fa\u7532\u9178\u916f\nC. \u6eb4\u654c\u9686\nD. \u7837\u5316\u6c22\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b0\u9c9c\u8089\u4e2d\u808c\u7ea2\u86cb\u767d\u7684\u5b58\u5728\u5f62\u5f0f\u662f\nA. \u6c27\u5408\u578b\u808c\u7ea2\u86cb\u767d\nB. \u4e00\u6c27\u5316\u6c2e\u808c\u7ea2\u86cb\u767d\nC. \u8fd8\u539f\u578b\u808c\u7ea2\u86cb\u767d\nD. \u6c27\u5316\u578b\u808c\u7ea2\u86cb\u767d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u5408\u540c\u6cd5\u53ca\u6709\u5173\u53f8\u6cd5\u89e3\u91ca\u7684\u89c4\u5b9a\uff0c\u4e0b\u5217\u6709\u5173\u5408\u540c\u89e3\u9664\u7684\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u66fe\u67d0\u548c\u674e\u67d0\u7ea6\u5b9a\uff0c\u674e\u67d0\u52a0\u5de5\u670d\u88c51\u4e07\u5957\uff0c\u66fe\u67d0\u5728\u670d\u88c5\u52a0\u5de5\u5b8c\u6bd5\u540e\u5373\u884c\u4ed8\u6b3e\uff0c\u4f46\u670d\u88c5\u52a0\u5de5\u5b8c\u6bd5\u540e\u66fe\u67d0\u672a\u6309\u7ea6\u4ed8\u6b3e\uff0c\u5219\u674e\u67d0\u6709\u6743\u5373\u884c\u89e3\u9664\u5408\u540c\u5e76\u8981\u6c42\u66fe\u67d0\u627f\u62c5\u8fdd\u7ea6\u8d23\u4efb\nB. \u859b\u67d0\u4e0e\u67d0\u623f\u5730\u4ea7\u516c\u53f8\u7ea6\u5b9a\uff0c\u859b\u67d0\u4ee5\u6309\u63ed\u8d37\u6b3e\u7684\u5f62\u5f0f\u8d2d\u7f6e\u4e00\u5957\u5546\u54c1\u623f\uff0c\u540e\u56e0\u94f6\u884c\u5229\u7387\u4e0a\u8c03\u548c\u6309\u63ed\u9996\u4ed8\u6b3e\u589e\u52a0\u800c\u5bfc\u81f4\u859b\u67d0\u65e0\u529b\u8d2d\u623f\uff0c\u5219\u859b\u67d0\u6709\u6743\u89e3\u9664\u5408\u540c\nC. \u4e19\u3001\u4e01\u7b7e\u8ba2\u519c\u7528\u5316\u80a5\u4e70\u5356\u5408\u540c\u540e\uff0c\u56e0\u519c\u7528\u5316\u80a5\u4ef7\u683c\u5927\u6da8\uff0c\u4e19\u5411\u4e01\u8868\u793a\uff0c\u519c\u7528\u5316\u80a5\u4ef7\u683c\u5927\u6da8\u5c5e\u4e8e\u5546\u4e1a\u98ce\u9669\uff0c\u56e0\u800c\u4e0d\u518d\u5c65\u884c\u4ea4\u4ed8\u5316\u80a5\u7684\u4e49\u52a1\u5e76\u6709\u6743\u89e3\u9664\u5408\u540c\nD. \u7532\u3001\u4e59\u7b7e\u8ba2\u5408\u540c\u540e\uff0c\u7532\u5728\u5408\u540c\u5c65\u884c\u671f\u9650\u5230\u6765\u4e4b\u524d\u5411\u4e59\u8868\u793a\u4e0d\u518d\u5c65\u884c\u5408\u540c\u4e49\u52a1\uff0c\u5219\u4e59\u6709\u6743\u8981\u6c42\u7532\u627f\u62c5\u8fdd\u7ea6\u8d23\u4efb\uff0c\u4f46\u65e0\u6743\u89e3\u9664\u5408\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.44667014546583506, "itpossible/Chinese-Mistral-7B-v0.1": 0.3220562534414596, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7332921396824424}}, {"question": "\u201c\u5f31\u51a0\u201d\u6307\u7684\u662f\u7537\u5b50\u591a\u5c11\u5c81\nA. \u5341\u516b\u5c81\nB. \u5341\u516d\u5341\nC. \u5341\u4e94\u5c81\nD. \u4e8c\u5341\u5c81\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u8425\u517b\u5b66\u4f1a\u5efa\u8bae\u4e73\u6bcd\u9499\u7684AI\u4e3a\nA. 1500mg\nB. 1200mg\nC. 800mg\nD. 1000mg\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.304726395275542, "meta-math/MetaMath-Mistral-7B": 0.5423436874778066, "itpossible/Chinese-Mistral-7B-v0.1": 0.35347162922091135, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u8fd1\u73b0\u4ee3\u201c\u6c11\u65cf\u201d\u4e00\u8bcd\u7684\u6765\u6e90\nA. \u4e2d\u56fd\u81ea\u53e4\u56fa\u6709\u7684\nB. \u4e2d\u56fd\u8fd1\u4ee3\u81ea\u9020\u7684\nC. \u4ece\u56fd\u5916\u5f15\u8fdb\u7684\nD. \u4ece\u5916\u6587\u4e2d\u8bd1\u9020\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5206325814419607, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2019\u5e741\u670812\u65e5\uff0c\u4e2d\u56fd\u5929\u6587\u5b66\u4f1a\u548c\u5317\u4eac\u5929\u6587\u9986\u5728\u4eac\u8054\u5408\u5ba3\u5e03\u542f\u52a8\u56fd\u9645\u5929\u6587\u5b66\u8054\u5408\u4f1a\uff08IAU\uff09\u53d1\u8d77\u7684\u201c\u540c\u4e00\u5929\u7a7a\u4e0b\u201d\u5168\u7403\u5929\u6587\u884c\u52a8\uff08\u4e2d\u56fd\u5927\u9646\u5730\u533a\uff09\uff0c\u4ee5\u7eaa\u5ff5IAU\u6210\u7acb\u591a\u5c11\u5468\u5e74\nA. 60\nB. 200\nC. 80\nD. 100\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7604790133874983, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u690d\u7269\u751f\u957f\u7d20\u548c\u751f\u957f\u7d20\u7c7b\u4f3c\u7269\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u7528\u9002\u5b9c\u6d53\u5ea6\u7684\u751f\u957f\u7d20\u7c7b\u4f3c\u7269\u5904\u7406\u63d2\u6761\u53ef\u4fc3\u8fdb\u5176\u751f\u6839\nB. \u9002\u5b9c\u6d53\u5ea6\u7684\u751f\u957f\u7d20\u7c7b\u4f3c\u7269\u53ef\u4fc3\u8fdb\u65e0\u5b50\u679c\u5b9e\u7684\u53d1\u80b2\nC. \u540c\u4e00\u690d\u682a\u6839\u548c\u82bd\u751f\u957f\u6240\u9700\u7684\u6700\u9002\u751f\u957f\u7d20\u6d53\u5ea6\u76f8\u540c\nD. \u5355\u4fa7\u5149\u7167\u5c04\u71d5\u9ea6\u80da\u82bd\u9798\u53ef\u4f7f\u5176\u751f\u957f\u7d20\u5206\u5e03\u53d1\u751f\u53d8\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44105661310623645, "meta-math/MetaMath-Mistral-7B": 0.5561950009033003, "itpossible/Chinese-Mistral-7B-v0.1": 0.6085790183486356, "HuggingFaceH4/zephyr-7b-beta": 0.9886672729557313, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6631241234051666, "meta-llama/Meta-Llama-3-8B": 0.462265476250188, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.40826366839133016}}, {"question": "\u5c4b\u5916\u9ad8\u538b\u7535\u5668\u5728\u8fdb\u884c\u8bd5\u9a8c\u6216\u8ba1\u7b97\u65f6\uff0c\u65e5\u7167\u5f3a\u5ea6\u53d60.1W/cm2\uff0c\u98ce\u901f\u53d6\nA. 1m/s\nB. 0.5m/s\nC. 2m/s\nD. 1.5m/s\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36464267153587787, "meta-math/MetaMath-Mistral-7B": 0.7583493339292504, "itpossible/Chinese-Mistral-7B-v0.1": 0.47995613146208777, "HuggingFaceH4/zephyr-7b-beta": 0.6574449533599748, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3867689393391593, "meta-llama/Meta-Llama-3-8B": 0.40979327494059936, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6264763896063941}}, {"question": "\u4ece\u9a6c\u514b\u601d\u63d0\u51fa\u7684\u201c\u4e3a\u7edd\u5927\u591a\u6570\u4eba\u8c0b\u5229\u76ca\u201d\uff0c\u5230\u5217\u5b81\u63d0\u51fa\u7684\u201c\u4e3a\u5343\u5343\u4e07\u4e07\u52b3\u52a8\u4eba\u6c11\u670d\u52a1\u201d\uff0c\u518d\u5230\u6bdb\u6cfd\u4e1c\u7cbe\u8f9f\u6982\u62ec\u7684\u201c\u4e3a\u4eba\u6c11\u670d\u52a1\u201d\uff0c\u90fd\u662f\u5efa\u7acb\u5728\nA. \u4e2a\u4eba\u53f2\u89c2\u57fa\u7840\u4e4b\u4e0a\u7684\nB. \u552f\u7269\u53f2\u89c2\u57fa\u7840\u4e4b\u4e0a\u7684\nC. \u552f\u5fc3\u53f2\u89c2\u57fa\u7840\u4e4b\u4e0a\u7684\nD. \u82f1\u96c4\u53f2\u89c2\u57fa\u7840\u4e4b\u4e0a\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6908029389090697, "meta-math/MetaMath-Mistral-7B": 0.8436881741844525, "itpossible/Chinese-Mistral-7B-v0.1": 0.8949905913129726, "HuggingFaceH4/zephyr-7b-beta": 0.9924038098275829, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9036827919094905, "meta-llama/Meta-Llama-3-8B": 0.9391222247913831, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9907688657479738}}, {"question": "\u4ece\u8bfe\u7a0b\u7684\u8868\u73b0\u5f62\u5f0f\u6765\u770b\uff0c\u6821\u56ed\u6587\u5316\u5c5e\u4e8e\nA. \u6821\u672c\u8bfe\u7a0b\nB. \u9690\u6027\u8bfe\u7a0b\nC. \u663e\u6027\u8bfe\u7a0b\nD. \u7efc\u5408\u8bfe\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8241587386801285, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.507856700502442, "meta-llama/Meta-Llama-3-8B": 0.5987341678410105, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bdd\u8bf4\u5f53\u65f6\u4f4f\u6301\u771f\u4eba\u5bf9\u6d2a\u5927\u5c09\u8bf4\u9053\uff1a\u201c\u592a\u5c09\u4e0d\u77e5\uff0c\u6b64\u6bbf\u4e2d\u5f53\u521d\u662f\u7956\u8001\u5929\u5e08\u6d1e\u7384\u771f\u4eba\u4f20\u4e0b\u6cd5\u7b26\uff0c\u5631\u4ed8\u9053\uff1a\u2018\u6b64\u6bbf\u5185\u9547\u9501\u7740\u4e09\u5341\u516d\u5458\u5929\u7f61\u661f\uff0c\u4e03\u5341\u4e8c\u5ea7\u5730\u715e\u661f\uff0c\u5171\u662f\u4e00\u767e\u5355\u516b\u4e2a\u9b54\u541b\u5728\u91cc\u9762\uff0c\u4e0a\u7acb\u77f3\u7891\uff0c\u51ff\u7740\u9f99\u7ae0\u51e4\u7bc6\u5929\u7b26\uff0c\u9547\u4f4f\u5728\u6b64\uff0c\u82e5\u8fd8\u653e\u4ed6\u51fa\u4e16\uff0c\u5fc5\u607c\u4e0b\u65b9\u751f\u7075\u3002\u2019\u5982\u4eca\u5927\u5c09\u653e\u4ed6\u8d70\u4e86\uff0c\u600e\u751f\u662f\u597d\uff1f\u201d\u5173\u4e8e\u8fd9\u6bb5\u6587\u5b57\u6240\u5c5e\u8457\u4f5c\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u4ee5\u5317\u5b8b\u672b\u5e74\u4e3a\u6545\u4e8b\u80cc\u666f\nB. \u5176\u4f5c\u8005\u4e0e\u66f9\u96ea\u82b9\u751f\u6d3b\u5728\u540c\u4e00\u65f6\u671f\nC. \u201c\u767d\u5e1d\u57ce\u6258\u5b64\u201d\u662f\u4e66\u4e2d\u7684\u4e00\u4e2a\u60c5\u8282\nD. \u662f\u6211\u56fd\u53e4\u4ee3\u8457\u540d\u7684\u957f\u7bc7\u795e\u9b54\u5c0f\u8bf4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4479780596806147}}, {"question": "\u4e2d\u5b66\u751f\u90b9\u67d0\u4e0a\u8bfe\u65f6\u73a9\u6e38\u620f\uff0c\u73ed\u4e3b\u4efb\u738b\u8001\u5e08\u53d1\u73b0\u540e\uff0c\u5f53\u573a\u5220\u9664\u4e86\u90b9\u67d0\u7684\u6e38\u620f\u8d26\u53f7\u548c\u4ed6\u8d2d\u4e70\u7684\u6e38\u620f\u88c5\u5907\uff0c\u5e76\u544a\u8bda\u90b9\u67d0\u4e0d\u8981\u5728\u4e0a\u8bfe\u65f6\u73a9\u6e38\u620f\u3002\u8bfe\u540e\uff0c\u738b\u8001\u5e08\u5c06\u624b\u673a\u8fd4\u8fd8\u7ed9\u4e86\u90b9\u67d0\u3002\u738b\u8001\u5e08\u7684\u505a\u6cd5\nA. \u4e0d\u5408\u6cd5\uff0c\u4fb5\u72af\u4e86\u90b9\u67d0\u7684\u8d22\u4ea7\u6743\nB. \u5408\u6cd5\uff0c\u6559\u5e08\u65e0\u6743\u6ca1\u6536\u5b66\u751f\u7684\u624b\u673a\nC. \u4e0d\u5408\u6cd5\uff0c\u4fb5\u72af\u4e86\u90b9\u67d0\u7684\u9690\u79c1\u6743\nD. \u5408\u6cd5\uff0c\u6559\u5e08\u6709\u6743\u6279\u8bc4\u548c\u7ba1\u6559\u5b66\u751f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.796741137031983}}, {"question": "\u82f1\u56fd\u7684\u4e00\u9879\u5b9e\u9a8c\u53d1\u73b0\uff0c\u628a\u6bcd\u72d7\u548c\u4ed6\u4eec\u7684\u5e7c\u5c0f\u5b50\u5973\u5206\u5f00\u540e\uff0c\u5c06\u8fd9\u4e9b\u5b50\u5973\u6df7\u5165\u4e00\u7fa4\u540c\u7c7b\u7684\u6210\u5e74\u72d7\u548c\u5e7c\u72d7\u4e2d\u53bb\uff0c\u7136\u540e\u518d\u628a\u6bcd\u72d7\u653e\u5165\u72d7\u7fa4\u3002\u6bcd\u72d7\u5f88\u5feb\u5c31\u548c\u81ea\u5df1\u7684\u5b50\u5973\u4f1a\u5408\u5230\u4e00\u8d77\u3002\u7814\u7a76\u8868\u660e\uff0c\u72d7\u8eab\u4e0a\u7684\u4f53\u5473\u662f\u5b83\u4eec\u4e92\u76f8\u8fa8\u8ba4\u7684\u4f9d\u636e\u3002\u800c\u5e7c\u72d7\u65e0\u6cd5\u533a\u5206\u81ea\u5df1\u6bcd\u4eb2\u548c\u5176\u5b83\u6bcd\u72d7\u8eab\u4e0a\u7684\u5473\u9053\u3002\u56e0\u6b64\u6bcf\u4e2a\u6bcd\u72d7\u90fd\u80fd\u5206\u8fa8\u51fa\u81ea\u5df1\u5b50\u5973\u7684\u4f53\u5473 \u4e0a\u8ff0\u8bba\u8bc1\u91c7\u7528\u4e86\u4e0b\u5217\u90a3\u79cd\u8bba\u8ff0\u65b9\u6cd5?\nA. \u8bf4\u660e\u67d0\u4e00\u7279\u6b8a\u60c5\u51b5\uff0c\u4ee5\u8bba\u8bc1\u4e00\u4e2a\u89c4\u5f8b\nB. \u5728\u5bf9\u67d0\u79cd\u73b0\u8c61\u7684\u4e24\u79cd\u53ef\u4f9b\u9009\u62e9\u7684\u89e3\u91ca\u4e2d\uff0c\u901a\u8fc7\u6392\u9664\u5176\u4e2d\u7684\u4e00\u79cd\uff0c\u6765\u786e\u5b9a\u53e6\u4e00\u79cd\nC. \u901a\u8fc7\u5bf9\u53d1\u751f\u73b0\u8c61\u7684\u5ba2\u89c2\u63cf\u8ff0\uff0c\u652f\u6301\u5173\u4e8e\u67d0\u4e2a\u53ef\u80fd\u53d1\u751f\u73b0\u8c61\u7684\u5047\u8bf4\nD. \u8fd0\u7528\u7c7b\u6bd4\u7684\u65b9\u6cd5\uff0c\u6839\u636e\u4e24\u7ec4\u5bf9\u8c61\u6709\u67d0\u4e9b\u7c7b\u4f3c\u7684\u7279\u5f81\uff0c\u5f97\u51fa\u5b83\u4eec\u5177\u6709\u53e6\u4e00\u4e2a\u76f8\u540c\u7279\u5f81\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.29863342676099575, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.39008524332734895}}, {"question": "\u67d0\u6d41\u57df\u6709\u4e24\u6b21\u66b4\u96e8\uff0c\u9664\u66b4\u96e8\u4e2d\u5fc3\u524d\u8005\u5728\u4e0a\u6e38\uff0c\u540e\u8005\u5728\u4e0b\u6e38\u5916\uff0c\u5176\u5b83\u60c5\u51b5\u90fd\u4e00\u6837\uff0c\u5219\u524d\u8005\u5728\u6d41\u57df\u51fa\u53e3\u65ad\u9762\u5f62\u6210\u7684\u6d2a\u5cf0\u6d41\u91cf\u6bd4\u540e\u8005\u7684[].\nA. \u6d2a\u5cf0\u6d41\u91cf\u5c0f\u3001\u5cf0\u73b0\u65f6\u95f4\u665a\nB. \u6d2a\u5cf0\u6d41\u91cf\u5927\u3001\u5cf0\u73b0\u65f6\u95f4\u665a\nC. \u6d2a\u5cf0\u6d41\u91cf\u5c0f\u3001\u5cf0\u73b0\u65f6\u95f4\u65e9\nD. \u6d2a\u5cf0\u6d41\u91cf\u5927\u3001\u5cf0\u73b0\u65f6\u95f4\u65e9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30308918294772175, "meta-math/MetaMath-Mistral-7B": 0.39405548463546086, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.48062157955395934, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4158422361704812, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ecf\u8fc7\u719f\u5316\uff0c\u2f54\u7a3b\u2f1f\u7684\u7ed3\u6784\u7279\u70b9\u53ca\u529f\u80fd\u53d1\u2f63\u7684\u53d8\u5316\u8868\u73b0\u4e3a\nA. \u8015\u4f5c\u5c42\u6d45\u8584\u758f\u677e\uff0c\u53d7\u2f08\u7c7b\u6d3b\u52a8\u5f71\u54cd\u2f24\nB. \u8868\u2f1f\u5c42\u8f6f\u7cca\uff0c\u63d0\u4f9b\u4e86\u519c\u4f5c\u7269\u624e\u6839\u2f74\u2f9c\u7684\u6761\u4ef6\nC. \u2e9f\u5ca9\u5c42\u8f83\u8584\uff0c\u963b\u65ad\u4e86\u6709\u673a\u754c\u4e0e\u2f46\u673a\u754c\u7684\u8054\u7cfb\nD. \u7281\u5e95\u5c42\u7d27\u5b9e\uff0c\u63d0\u2fbc\u4e86\u4fdd\u2f54\u3001\u4fdd\u80a5\u80fd\u2f12\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36455022711440066, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "()\u662f\u667a\u80fd\u5316\u6b66\u5668\u88c5\u5907\u7684\u96c6\u4e2d\u4ee3\u8868\u3002\nA. \u667a\u80fd\u5730\u96f7\nB. \u667a\u80fd\u5bfc\u5f39\nC. \u667a\u80fd\u673a\u5668\u4eba\nD. \u667a\u80fd\u5766\u514b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4569342691111923, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u76f4\u6d41\u7535\u52a8\u673a\u7535\u67a2\u56de\u8def\u4e32\u8054\u7535\u963b\u7684\u8c03\u901f\u65b9\u6cd5\u7684\u6027\u80fd\u63cf\u8ff0\u9519\u8bef\u7684\u662f\nA. \u4e00\u822c\u5728\u57fa\u901f\u4ee5\u4e0a\u9700\u8981\u63d0\u9ad8\u8f6c\u901f\u65f6\u4f7f\u7528\nB. \u56e0\u5176\u673a\u68b0\u7279\u6027\u53d8\u8f6f\uff0c\u7cfb\u7edf\u8f6c\u901f\u53d7\u8d1f\u8f7d\u5f71\u54cd\u5927\uff0c\u91cd\u8f7d\u65f6\u8fd8\u4f1a\u4ea7\u751f\u5835\u8f6c\u73b0\u8c61\nC. \u7535\u67a2\u7535\u8def\u4e32\u8054\u7535\u963b\u7684\u8c03\u901f\u65b9\u6cd5\u5c5e\u4e8e\u6052\u8f6c\u77e9\u8c03\u901f\nD. \u5728\u4e32\u8054\u7535\u963b\u4e2d\u6d41\u8fc7\u7684\u662f\u7535\u67a2\u7535\u6d41\uff0c\u957f\u671f\u8fd0\u884c\u65f6\u635f\u8017\u8f83\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u2f64\u6027\u8d28\u63cf\u8ff0\u6cd5\u8868\u793a\u7b2c\u2f00\u8c61\u9650\u7684\u6240\u6709\u70b9\u7684\u96c6\u5408\nA. {(x,y)|x<0,y>0}\nB. {(x,y)|x<0.y<0}\nC. {(x,y)|x>0,y>0}\nD. {(x,y)|x>0,Y<0} \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3702580020579642, "meta-math/MetaMath-Mistral-7B": 0.6709265344560877, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5444768174914687, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5147150684573033, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5303554148858188}}, {"question": "\u4f01\u4e1a\u4f26\u7406\u5b88\u5219\u7684\u57fa\u672c\u4f9d\u636e\u662f\nA. \u793e\u4f1a\u6cd5\u5f8b\u89c4\nB. \u793e\u4f1a\u6587\u660e\u89c4\nC. \u793e\u4f1a\u884c\u4e3a\u89c4\nD. \u793e\u4f1a\u4f26\u7406\u89c4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7344924100803919, "meta-math/MetaMath-Mistral-7B": 0.8813614836296213, "itpossible/Chinese-Mistral-7B-v0.1": 0.8034120418445886, "HuggingFaceH4/zephyr-7b-beta": 0.996093339717707, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4773394020291152, "meta-llama/Meta-Llama-3-8B": 0.6206149584906332, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u9a6c\u514b\u601d\u4e3b\u4e49\u6cd5\u5b66\u7684\u89c2\u70b9\uff0c\u4e0b\u5217\u54ea\u4e00\u79cd\u8bf4\u6cd5\u662f\u9519\u8bef\u7684\uff1f\nA. \u6cd5\u4f53\u73b0\u4e86\u7edf\u6cbb\u9636\u7ea7\u610f\u5fd7\nB. \u6cd5\u4f53\u73b0\u4e86\u7edf\u6cbb\u9636\u7ea7\u6574\u4f53\u610f\u5fd7\nC. \u6cd5\u4f53\u73b0\u4e86\u4e00\u79cd\u610f\u5fd7\nD. \u6240\u6709\u7684\u6cd5\u5f8b\u90fd\u4e0d\u53ef\u80fd\u53cd\u6620\u88ab\u7edf\u6cbb\u9636\u7ea7\u7684\u67d0\u4e9b\u5229\u76ca\u548c\u613f\u671b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5566128982980807, "meta-math/MetaMath-Mistral-7B": 0.5954144829015845, "itpossible/Chinese-Mistral-7B-v0.1": 0.5243223215919554, "HuggingFaceH4/zephyr-7b-beta": 0.9981563519721649, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7319938475340947, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u53f2>\u5728\u89e3\u91ca\u201c\u6253\u626b\u5e72\u51c0\u5c4b\u5b50\u518d\u8bf7\u5ba2\u201d\u65f6\u5f15\u8ff0\u4e86\u6bdb\u6cfd\u4e1c\u7684\u4e00\u6bb5\u8bdd\uff1a\u201c\u5173\u4e8e\u5e1d\u56fd\u4e3b\u4e49\u5bf9\u6211\u56fd\u7684\u627f\u8ba4\u95ee\u9898\uff0c\u4e0d\u4f46\u73b0\u5728\u4e0d\u5e94\u6025\u4e8e\u53bb\u89e3\u51b3\uff0c\u5c31\u662f\u5728\u5168\u56fd\u80dc\u5229\u4ee5\u540e\u7684\u4e00\u4e2a\u76f8\u5f53\u957f\u65f6\u671f\u5185\uff0c\u4e5f\u4e0d\u5e94\u6025\u4e8e\u53bb\u89e3\u51b3\u3002\u56e0\u4e3a\u867d\u7136\u6211\u4eec\u613f\u610f\u6309\u7167\u5e73\u7b49\u539f\u5219\u540c\u4e00\u5207\u56fd\u5bb6\u5efa\u7acb\u5916\u4ea4\u5173\u7cfb\uff0c\u4f46\u5e1d\u56fd\u4e3b\u4e49\u662f\u7edd\u4e0d\u80fd\u5f88\u5feb\u5730\u5c31\u4ee5\u5e73\u7b49\u6001\u5ea6\u5bf9\u5f85\u6211\u4eec\u7684\u3002\u201d\u4e0a\u8ff0\u6750\u6599\u4f53\u73b0\u4e86\u65b0\u4e2d\u56fd\nA. \u4e0d\u627f\u8ba4\u56fd\u6c11\u653f\u5e9c\u5efa\u7acb\u7684\u65e7\u5916\u4ea4\nB. \u575a\u5b9a\u5730\u7ad9\u5728\u793e\u4f1a\u4e3b\u4e49\u9635\u8425\u4e00\u8fb9\nC. \u53d6\u7f14\u5217\u5f3a\u5728\u534e\u7279\u6743\u7684\u5f3a\u70c8\u613f\u671b\nD. \u575a\u51b3\u4fdd\u969c\u793e\u4f1a\u4e3b\u4e49\u9769\u547d\u7684\u80dc\u5229\u6210\u679c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.333183235354062, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c40\u5c81\uff0c\u53d1\u73b0\u7532\u72b6\u817a\u7ed3\u828210\u5e74\uff0c\u8fd1\u5e74\u6765\u6613\u51fa\u6c57\uff0c\u5fc3\u60b8\uff0c\u6e10\u611f\u547c\u5438\u56f0\u96be\uff0c\u67e5\u4f53\uff1a\u8109\u640f104\u6b21/\u5206\uff0c\u8840\u538b130/70mmHg\uff0c\u65e0\u7a81\u773c\uff0c\u7532\u72b6\u817a\u2162\u5ea6\u80bf\u5927\uff0c\u7ed3\u8282\u72b6\uff0c\u5fc3\u7535\u56fe\u793a\uff1a\u7aa6\u6027\u5fc3\u5f8b\u4e0d\u9f50\u3002\u9996\u9009\u7684\u6839\u6cbb\u6027\u6cbb\u7597\u65b9\u6cd5\nA. \u7532\u72b6\u817a\u5168\u5207\u672f\nB. \u7532\u72b6\u817a\u5927\u90e8\u5206\u5207\u9664\u672f\nC. \u6297\u7532\u72b6\u817a\u836f\u7269\u6cbb\u7597\nD. \u540c\u4f4d\u7d20\u6cbb\u7597\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5706x^2+y^2-2x-2y+1=0\u4e0a\u7684\u70b9\u5230\u76f4\u7ebfx-y=2\u7684\u8ddd\u79bb\u6700\u5927\u7684\u662f\nA. 1+2\\sqrt{2}\nB. 2\nC. 1+\\sqrt{2}/2\nD. 1+\\sqrt{2}\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.545801240528333, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.42096270677603886, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u793e\u4f1a\u53d1\u5c55\u603b\u7684\u65b9\u5411\u3001\u5927\u76ee\u6807\u3001\u4e3b\u8981\u6b65\u9aa4\u4e0e\u91cd\u5927\u63aa\u65bd\u7684\u8bbe\u60f3\uff0c\u88ab\u79f0\u505a\nA. \u5168\u7403\u5316\nB. \u793e\u4f1a\u53d1\u5c55\u8ba1\u5212\nC. \u793e\u4f1a\u73b0\u4ee3\u5316\nD. \u6709\u8ba1\u5212\u7684\u793e\u4f1a\u53d8\u8fc1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6094600446923811, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5025184549034408, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u96c6\u5408\u67095\u4e2a\u5143\u7d20\uff0c\u5b83\u7684\u2f26\u96c6\u6570\u4e3a\nA. 24\nB. 27\nC. 25\nD. 26\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2972773227082014, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7542801814041278, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3254550725959451, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b66\u4e60\u540e\u7acb\u5373\u7761\u89c9\uff0c\u4fdd\u6301\u7684\u6548\u679c\u5f80\u5f80\u6bd4\u5b66\u4e60\u540e\u7ee7\u7eed\u6d3b\u52a8\u4fdd\u6301\u7684\u6548\u679c\u66f4\u597d\uff0c\u8fd9\u662f\u7531\u4e8e\nA. \u8fc7\u5ea6\u5b66\u4e60\nB. \u65e0\u524d\u6444\u6291\u5236\u7684\u5f71\u54cd\nC. \u8bb0\u5fc6\u7684\u6062\u590d\u73b0\u8c61\nD. \u65e0\u5012\u6444\u6291\u5236\u7684\u5f71\u54cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8865\u4f53 C3b \u7684\u708e\u75c7\u4ecb\u8d28\u6548\u5e94\u4e3b\u8981\u662f\nA. \u589e\u5f3a\u7ec6\u80de\u541e\u566c\u529f\u80fd\nB. \u4fc3\u8fdb\u767d\u7ec6\u80de\u9ecf\u9644\u4e8e\u5185\u76ae\u7ec6\u80de\nC. \u5f15\u8d77\u8840\u7ba1\u901a\u900f\u6027\u589e\u9ad8\nD. \u8d8b\u5316\u4f5c\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.35160429054563636, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c45\u5c81\uff0c\u56e0\u7532\u72b6\u817a\u764c\u884c\u5de6\u53f6\u7532\u72b6\u817a\u5168\u5207\u672f\uff0c\u672f\u540e\u51fa\u73b0\u996e\u6c34\u65f6\u545b\u54b3\uff0c\u53d1\u97f3\u65e0\u6539\u53d8\u3002\u4e3a\u907f\u514d\u518d\u6b21\u51fa\u73b0\u6b64\u624b\u672f\u5e76\u53d1\u75c7\uff0c\u5e94\u6ce8\u610f\nA. \u8981\u6ce8\u610f\u4fdd\u62a4\u7532\u72b6\u817a\u817a\u4f53\u7684\u80cc\u9762\nB. \u672f\u4e2d\u7ed3\u624e\u7532\u72b6\u817a\u4e0b\u52a8\u8109\u8981\u8fdc\u79bb\u4e0b\u6781\nC. \u672f\u524d\u9700\u8981\u4f7f\u7528\u7898\u5242\uff0c\u5e76\u884c\u9888\u90e8\u6444\u7247\nD. \u672f\u4e2d\u7ed3\u624e\u7532\u72b6\u817a\u4e0a\u52a8\u8109\u8981\u7d27\u8d34\u4e0a\u6781\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3825322550539375, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5317\u7f8e\u6b96\u6c11\u5730\u65f6\u671f\uff0c\u56e0\u53cd\u5bf9\u6b96\u6c11\u5f53\u5c40\u800c\u917f\u6210\u201c\u66fe\u683c\u6848\u4ef6\u201d\u7684\u62a5\u7eb8\u662f\nA. \u300a\u7ebd\u7ea6\u5468\u62a5\u300b\nB. \u300a\u6ce2\u58eb\u987f\u65b0\u95fb\u4fe1\u300b\nC. \u300a\u65b0\u82f1\u683c\u5170\u62a5\u300b\nD. \u300a\u56fd\u5185\u5916\u516c\u5171\u4e8b\u4ef6\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u4e2a\u8f93\u51fa\u7535\u538b\u51e0\u4e4e\u4e0d\u53d8\u7684\u8bbe\u5907\u6709\u8f7d\u8fd0\u884c\uff0c\u5f53\u8d1f\u8f7d\u589e\u5927\u65f6\uff0c\u662f\u6307\nA. \u8d1f\u8f7d\u7535\u963b\u51cf\u5c0f\nB. \u7535\u6e90\u8f93\u51fa\u7684\u7535\u6d41\u51cf\u5c0f\nC. \u8d1f\u8f7d\u7535\u963b\u589e\u5927\nD. \u7535\u6e90\u8f93\u51fa\u7684\u7535\u6d41\u589e\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7090852005299281, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b0\u4e16\u7eaa\u65b0\u9636\u6bb5\u6c11\u65cf\u5de5\u4f5c\u7684\u4e3b\u9898\u662f\nA. \u575a\u6301\u548c\u5b8c\u5584\u6c11\u65cf\u533a\u57df\u81ea\u6cbb\u5236\u5ea6\nB. \u5404\u6c11\u65cf\u5171\u540c\u56e2\u7ed3\u594b\u6597\u3001\u5171\u540c\u7e41\u8363\u53d1\u5c55\nC. \u5e73\u7b49\u3001\u56e2\u7ed3\u3001\u4e92\u52a9\u3001\u548c\u8c10\nD. \u5404\u6c11\u65cf\u4e00\u5f8b\u5e73\u7b49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3780790271786411, "meta-math/MetaMath-Mistral-7B": 0.7582597922993698, "itpossible/Chinese-Mistral-7B-v0.1": 0.7469733273787972, "HuggingFaceH4/zephyr-7b-beta": 0.9952055093969652, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6647982896285615, "meta-llama/Meta-Llama-3-8B": 0.5907723771346058, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9522186419985638}}, {"question": "\u4eba\u751f\u4ef7\u503c\u89c2\u5ff5\u662f\nA. \u5bf9\u4eba\u4e3a\u4ec0\u4e48\u6d3b\u7740\u7684\u57fa\u672c\u89c2\u70b9\nB. \u5bf9\u4eba\u751f\u76ee\u7684\u548c\u5b9e\u8df5\u6d3b\u52a8\u8fdb\u884c\u8ba4\u8bc6\u53ca\u8bc4\u4ef7\u65f6\u6240\u6301\u7684\u57fa\u672c\u89c2\u70b9\nC. \u5bf9\u81ea\u6211\u4ef7\u503c\u4e0e\u793e\u4f1a\u4ef7\u503c\u7684\u8ba4\u8bc6\nD. \u5bf9\u4eba\u7684\u4e3b\u4f53\u9700\u8981\u548c\u5ba2\u4f53\u6ee1\u8db3\u4e3b\u4f53\u9700\u8981\u7684\u5173\u7cfb\u7684\u8ba4\u8bc6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5326190993940749, "meta-math/MetaMath-Mistral-7B": 0.7905366736077232, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9782419847581894, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8296711094445203, "meta-llama/Meta-Llama-3-8B": 0.8546051213910189, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9068017528924144}}, {"question": "\u5982\u679c\u98df\u7269\u94fe\u4e0a\u5404\u8425\u517b\u7ea7\u5747\u4ee5\u751f\u7269\u4e2a\u4f53\u7684\u6570\u91cf\u6765\u8868\u793a\uff0c\u5e76\u4ee5\u98df\u7269\u94fe\u8d77\u70b9\u7684\u751f\u7269\u4e2a\u4f53\u6570\u4f5c\u5e95\u5c42\u6765\u7ed8\u5236\u6570\u91cf\u91d1\u5b57\u5854\uff0c\u5219\u53ea\u6709\u4e24\u4e2a\u8425\u517b\u7ea7\u7684\u590f\u5b63\u8349\u539f\u751f\u6001\u7cfb\u7edf\uff08\u5047\u8bbe\u7b2c\u4e00\u8425\u517b\u7ea7\u662f\u7267\u8349\uff0c\u7b2c\u4e8c\u8425\u517b\u7ea7\u662f\u7f8a\uff09\u548c\u68ee\u6797\u751f\u6001\u7cfb\u7edf\uff08\u5047\u8bbe\u7b2c\u4e00\u8425\u517b\u7ea7\u662f\u4e54\u6728\uff0c\u7b2c\u4e8c\u8425\u517b\u7ea7\u662f\u6606\u866b\uff09\u6570\u91cf\u91d1\u5b57\u5854\u7684\u5f62\u72b6\u6700\u53ef\u80fd\u662f\nA. \u524d\u8005\u4e3a\u91d1\u5b57\u5854\u5f62\uff0c\u540e\u8005\u4e3a\u5012\u91d1\u5b57\u5854\u5f62\nB. \u524d\u8005\u4e3a\u5012\u91d1\u5b57\u5854\u5f62\uff0c\u540e\u8005\u4e3a\u5012\u91d1\u5b57\u5854\u5f62\nC. \u524d\u8005\u4e3a\u5012\u91d1\u5b57\u6e2f\u5f62\uff0c\u540e\u8005\u4e3a\u91d1\u5b57\u5854\u5f62\nD. \u524d\u8005\u4e3a\u91d1\u5b57\u5854\u5f62\uff0c\u540e\u8005\u4e3a\u91d1\u5b57\u5854\u5f62\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3544827847193782, "meta-math/MetaMath-Mistral-7B": 0.46275238281573583, "itpossible/Chinese-Mistral-7B-v0.1": 0.3499320087587727, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.49501296234811365, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7223367197879432}}, {"question": "1988\u5e74\uff0c\u4e0a\u6d77\u5e02\u53d1\u751f\u7532\u578b\u809d\u708e\u5927\u6d41\u884c\uff0c\u75c5\u6bd2\u4e3b\u8981\u901a\u8fc7\u751f\u98df\u6bdb\u86b6\u800c\u4fb5\u5165\u4eba\u4f53\uff0c\u6bdb\u86b6\u4e3a\u75c5\u56e0\u7684\u5de5\u4f5c\u5047\u8bbe\u662f\u901a\u8fc7\u4e0b\u5217\u54ea\u79cd\u65b9\u6cd5\u627e\u51fa\u7684\nA. \u6c42\u540c\u6cd5\nB. \u6392\u9664\u6cd5\nC. \u6c42\u5f02\u6cd5\nD. \u5b9e\u9a8c\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "IP\u5730\u5740\u662f202.114.18.10\uff0c\u63a9\u7801\u662f255.255.255.252\uff0c\u5176\u2f34\u64ad\u5730\u5740\u662f\u591a\u5c11\nA. 202.114.18.11\nB. 202.114.18.12\nC. 202.114.18.8\nD. 202.114.18.255\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8d44\u672c\u5e02\u573a\u662f\nA. \u957f\u671f\u8d44\u91d1\u5e02\u573a\nB. \u8d27\u5e01\u5e02\u573a\nC. \u77ed\u671f\u8d44\u91d1\u5e02\u573a\nD. \u671f\u8d27\u5e02\u573a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5623651221705197, "meta-math/MetaMath-Mistral-7B": 0.860135309574841, "itpossible/Chinese-Mistral-7B-v0.1": 0.555363569482486, "HuggingFaceH4/zephyr-7b-beta": 0.6817504073141761, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7344132065568366, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5538162149909288}}, {"question": "\u522b\u560c\u5464\u9187\u6cbb\u7597\u75db\u98ce\u7684\u53ef\u80fd\u673a\u5236\u662f\nA. \u6291\u5236\u8131\u6c27\u6838\u7cd6\u6838\u9178\u7684\u751f\u6210\nB. \u4fc3\u8fdbdUMP\u7684\u7532\u57fa\u5316\nC. \u6291\u5236\u9ec4\u560c\u541f\u6c27\u5316\u9176\nD. \u4fc3\u8fdb\u5c3f\u9178\u751f\u6210\u7684\u9006\u53cd\u5e94\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5113634443120345, "meta-math/MetaMath-Mistral-7B": 0.7645962971813481, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9986884425968021, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9139312289339561, "meta-llama/Meta-Llama-3-8B": 0.34239623393788804, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5317259085541577}}, {"question": "\u2f00\u4e2a\u5706\u67f1\u7684\u4fa7\u2faf\u79ef\u662f12.56dm2\uff0c\u2fbc\u662f0.5dm\uff0c\u5b83\u7684\u5e95\u2faf\u76f4\u5f84\u662f\uff08 \uff09dm\nA. 8\nB. 4\nC. 16\nD. 12\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6267\u4e1a\u533b\u5e08\u6cd5\u89c4\u5b9a\uff0c\u533b\u5e08\u7684\u533b\u5b66\u4e13\u4e1a\u6280\u672f\u804c\u79f0\u548c\u533b\u5b66\u4e13\u4e1a\u6280\u672f\u804c\u52a1\u7684\u8bc4\u5b9a\u3001\u8058\u4efb\uff0c\u6309\u7167\u56fd\u5bb6\u7684\u6709\u5173\u89c4\u5b9a\u5904\u7406\u3002\u8fd9\u4e00\u89c4\u5b9a\u5c5e\u4e8e\u4e0b\u5217\u54ea\u4e00\u4e2a\u9009\u9879\uff1f\nA. \u786e\u5b9a\u6027\u89c4\u5219\nB. \u59d4\u4efb\u6027\u89c4\u5219\nC. \u51c6\u7528\u6027\u89c4\u5219\nD. \u6743\u4e49\u590d\u5408\u6027\u89c4\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u95ee:\u94c1\u4e3a\u4ec0\u4e48\u80fd\u538b\u5ef6?\u7b54:\u56e0\u4e3a\u94c1\u6709\u538b\u5ef6\u7684\u7279\u6027\u3002\u201d\u5c5e\u4e8e\nA. \u76f4\u63a5\u5faa\u73af\u8bba\u8bc1\nB. \u76f8\u5e72\u8c2c\u8bef\nC. \u652f\u6301\u8c2c\u8bef\nD. \u95f4\u63a5\u5faa\u73af\u8bba\u8bc1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3112296656009273, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "RIP\u7684\u6700\u2f24\u8df3\u6570\u662f\nA. 24\nB. 15\nC. 18\nD. 12\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.371703799222106, "meta-math/MetaMath-Mistral-7B": 0.7526151329978437, "itpossible/Chinese-Mistral-7B-v0.1": 0.3423962339378881, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6365283684211914, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7555609570144316}}, {"question": "\u5b54\u5b50\u63d0\u5021\u4e2d\u5eb8\u4e4b\u9053\u7684\u7406\u8bba\u57fa\u7840\u662f\nA. \u9053\u6cd5\u81ea\u7136\nB. \u5929\u4eba\u5408\u4e00\nC. \u9634\u9633\u4e94\u884c\nD. \u65e0\u4e3a\u800c\u6cbb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.40124046446473, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.713791602350665, "meta-llama/Meta-Llama-3-8B": 0.4658923272442273, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u516c\u6c11\u51fa\u56fd\u65c5\u6e38\u65f6\uff0c\u4ed6\uff08\u5979\uff09\u5e94\u5f53\u9075\u5b88\u7684\u6cd5\u5f8b\u662f\nA. \u56fd\u9645\u6cd5\nB. \u4e2d\u56fd\u7684\u6cd5\u5f8b\nC. \u6240\u5728\u56fd\u7684\u6cd5\u5f8b\nD. \u4e2d\u56fd\u6cd5\u5f8b\u4ee5\u53ca\u6240\u5728\u56fd\u7684\u6cd5\u5f8b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5257541078379924, "meta-math/MetaMath-Mistral-7B": 0.5827339792437671, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9758501527992729, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7269532914545106, "meta-llama/Meta-Llama-3-8B": 0.8655712102539158, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8247383312482853}}, {"question": "\u7537\u6027\uff0c23\u5c81\u3002\u53f3\u8179\u80a1\u6c9f\u53ef\u590d\u6027\u5305\u57572\u5e74\u3002\u67e5\u4f53\uff1a\u80bf\u5757\u8fd8\u7eb3\u540e\uff0c\u538b\u8feb\u5185\u73af\u53e3\u4e0d\u590d\u51fa\uff0c\u65e0\u538b\u75db\u3002\u624b\u672f\u4e2d\u6700\u6709\u53ef\u80fd\u7684\u53d1\u73b0\u662f\nA. \u76f4\u759d\u4e09\u89d2\u90e8\u4f4d\u8179\u58c1\u8584\nB. \u759d\u5185\u5bb9\u7269\u5e38\u4e3a\u5927\u7f51\u819c\nC. \u76f2\u80a0\u7ec4\u6210\u759d\u56ca\u58c1\u7684\u4e00\u90e8\u5206\nD. \u759d\u56ca\u9888\u4f4d\u4e8e\u8179\u58c1\u4e0b\u52a8\u8109\u5916\u4fa7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3436615088034303, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.33424000363035195, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679c\u4e24\u4e2a\u6709\u7406\u6570\u7684\u5546\u7b49\u4e8e0\uff0c\u5219\nA. \u88ab\u9664\u6570\u4e0d\u4e3a0\u9664\u6570\u4e3a0\nB. \u4e24\u4e2a\u6570\u4e2d\u6709\u4e00\u4e2a\u4e3a0\nC. \u88ab\u9664\u6570\u4e3a0\u9664\u6570\u4e0d\u4e3a0\nD. \u4e24\u4e2a\u6570\u90fd\u4e3a0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.333500896811094, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3534716292209113, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5200544918012361}}, {"question": "\u6309\u793e\u4f1a\u5371\u5bb3\u7a0b\u5ea6\u3001\u5f71\u54cd\u8303\u56f4\u7b49\u56e0\u7d20\uff0c\u81ea\u7136\u707e\u5bb3\u3001\u4e8b\u6545\u707e\u96be\u3001\u516c\u5171\u536b\u751f\u4e8b\u4ef6\u5206\u4e3a()\u548c\u4e00\u822c\u56db\u7ea7\nA. \u91cd\u5927\u3001\u8f83\u5927\nB. \u4e00\u7ea7\u3001\u4e8c\u7ea7\u3001\u4e09\u7ea7\nC. \u4e09\u7ea7\u3001\u4e8c\u7ea7\u3001\u4e00\u7ea7\nD. \u7279\u522b\u91cd\u5927\u3001\u91cd\u5927\u3001\u8f83\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5971450110092341, "meta-math/MetaMath-Mistral-7B": 0.6880303558165142, "itpossible/Chinese-Mistral-7B-v0.1": 0.4542144129838145, "HuggingFaceH4/zephyr-7b-beta": 0.9992409436821919, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.633441452537769, "meta-llama/Meta-Llama-3-8B": 0.48280111371476736, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5510\u6587\u5b97\u592a\u548c\u516d\u5e74\uff08832\u5e74\uff09\uff0c\u5bb0\u76f8\u738b\u6daf\u4e0a\u594f\uff1a\u201c\u5546\u4eba\u4e58\u9a6c\uff0c\u524d\u4ee3\u6240\u7981\u3002\u8fd1\u65e5\u5f97\u4ee5\u6063\u5176\u4e58\u9a91\uff0c\u96d5\u978d\u94f6\u956b\uff0c\u88c5\u9970\u7115\u70c2\uff0c\u4ece\u4ee5\u7ae5\u9a91\uff0c\u9a8b\u4ee5\u5eb7\u5e84\uff0c\u6b64\u6700\u4e3a\u50ed\u8d8a\u3002\u4f0f\u8bf7\u5207\u4ee4\u7981\u65ad\u3002\u201d\u4ee5\u4e0b\u5bf9\u5f53\u65f6\u5546\u4eba\u5730\u4f4d\u7684\u5224\u65ad\uff0c\u9519\u8bef\u7684\u662f\nA. \u7ecf\u6d4e\u5730\u4f4d\u4f18\u8d8a\nB. \u5b9e\u9645\u4e0a\u6ca1\u6709\u81ea\u7531\u6c11\u7684\u5e73\u7b49\u5730\u4f4d\nC. \u653f\u6cbb\u5730\u4f4d\u4f4e\u4e0b\nD. \u906d\u53d7\u5168\u793e\u4f1a\u7684\u9119\u89c6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8587473973861427, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u62bd\u6837\u8bef\u5dee\u6307\u7684\u662f:\nA. \u4e0d\u540c\u7684\u603b\u4f53\u53c2\u6570\u4e4b\u5dee\nB. \u4e2a\u4f53\u503c\u548c\u6837\u672c\u7edf\u8ba1\u91cf\u503c\u4e4b\u5dee\nC. \u6837\u672c\u7edf\u8ba1\u91cf\u503c\u548c\u603b\u4f53\u53c2\u6570\u503c\u4e4b\u5dee\nD. \u4e2a\u4f53\u503c\u548c\u603b\u4f53\u53c2\u6570\u503c\u4e4b\u5dee\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8464268722509936, "meta-math/MetaMath-Mistral-7B": 0.9660185358376663, "itpossible/Chinese-Mistral-7B-v0.1": 0.5052963379994225, "HuggingFaceH4/zephyr-7b-beta": 0.9990671370088773, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9192617399624504, "meta-llama/Meta-Llama-3-8B": 0.6555783257832388, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6838289845559263}}, {"question": "\u6211\u56fd\u6cbf\u6d77\u2f9a\u6f6e\u2f00\u822c\u53d1\u2f63\u5728\nA. 6\u2f49\u52308\u2f49\nB. 12\u2f49\u52302\u2f49\nC. 9\u2f49\u523011\u2f49\nD. 3\u2f49\u52305\u2f49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4318598357611035, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4435041431524655}}, {"question": "\u65e5\u672c\u8fdb\u884c\u660e\u6cbb\u7ef4\u65b0\u65f6\uff0c\u4e2d\u56fd\u6b63\u5728\u8fdb\u884c\nA. \u4e49\u548c\u56e2\u8fd0\u52a8\nB. \u620a\u620c\u53d8\u6cd5\nC. \u592a\u5e73\u5929\u56fd\u8fd0\u52a8\nD. \u6d0b\u52a1\u8fd0\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u692d\u5706\u661f\u7cfbM87\u4f4d\u4e8e\uff08\uff09\u5929\u533a\u5185\nA. \u540e\u53d1\u5ea7\nB. \u5de8\u87f9\u5ea7\nC. \u72d0\u72f8\u5ea7\nD. \u5ba4\u5973\u5ea7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u54ea\u79cd\u4e0d\u662f\u827e\u6ecb\u75c5\u4e3b\u8981\u4f20\u64ad\u65b9\u5f0f\nA. \u8840\u6db2\u4f20\u64ad\nB. \u5782\u76f4\u4f20\u64ad\nC. \u6027\u4f20\u64ad\nD. \u553e\u6db2\u4f20\u64ad\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6051264048874757, "meta-math/MetaMath-Mistral-7B": 0.6211206278614885, "itpossible/Chinese-Mistral-7B-v0.1": 0.6804111680743842, "HuggingFaceH4/zephyr-7b-beta": 0.9813843346950039, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7541651562897935, "meta-llama/Meta-Llama-3-8B": 0.6805037568535796, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9758854090236654}}, {"question": "\u6027\u522b\u89d2\u8272\u53d1\u5c55\u6700\u5bb9\u6613\u53d7\u5bb6\u5ead\u56e0\u7d20\u5f71\u54cd\u7684\u65f6\u671f\u662f\nA. \u5a74\u513f\u671f\nB. \u5e7c\u513f\u671f\nC. \u8001\u5e74\u671f\nD. \u9752\u6625\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5676412990951967, "meta-math/MetaMath-Mistral-7B": 0.7920585380642193, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8497908520730593, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8959018632460553}}, {"question": "\u4ee5\u4e0b\u5173\u4e8e\u76f4\u6d41\u7535\u52a8\u673a\u8c03\u901f\u65b9\u6cd5\u548c\u8f6c\u901fn\u7684\u53d8\u5316\u7ed3\u679c\u7684\u53d9\u8ff0\u6b63\u786e\u7684\u662f\nA. \u6539\u53d8\u52a0\u5728\u56de\u8def\u7684\u5916\u52a0\u7535\u538bU\uff0c\u968f\u7740U\u7684\u4e0a\u5347n\u4e0a\u5347\nB. \u6539\u53d8\u7535\u52a8\u673a\u7535\u67a2\u5916\u52a0\u7535\u538b\u7684\u9891\u7387f\uff0c\u968f\u7740f\u7684\u4e0a\u5347n\u4e0b\u964d\nC. \u6539\u53d8\u52a0\u5728\u7535\u52a8\u673a\u52b1\u78c1\u56de\u8def\u7684\u78c1\u901aF\uff0c\u968f\u7740F\u7684\u4e0a\u5347n\u4e0a\u5347\nD. \u6539\u53d8\u52a0\u5728\u56de\u8def\u7684\u5916\u52a0\u7535\u538bU\uff0c\u968f\u7740U\u7684\u4e0a\u5347n\u4e0b\u964d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38744616324658326, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u675c\u751f\u8d1d\u7684\u6d88\u8d39\u51fd\u6570\u7406\u8bba\u88ab\u79f0\u4e3a\nA. \u76f8\u5bf9\u6536\u5165\u5047\u5b9a\nB. \u8fb9\u9645\u6548\u7528\u9012\u51cf\nC. \u7edd\u5bf9\u6536\u5165\u5047\u5b9a\nD. \u751f\u547d\u5468\u671f\u5047\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.44238633498214136, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3833677286549997, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6700\u65e9\u63d0\u51fa\u201c\u793e\u4f1a\u63a7\u5236\u201d\u4e00\u8bcd\u5e76\u52a0\u4ee5\u9610\u8ff0\u7684\u793e\u4f1a\u5b66\u5bb6\u662f\nA. \u5e15\u514b\nB. \u5e93\u5229\nC. \u7f57\u65af\nD. \u7c73\u5fb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3306562312783846, "HuggingFaceH4/zephyr-7b-beta": 0.6350675926788137, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u98df\u54c1\u5c5e\u4e8e\u9632\u8150\u5242\u7684\u662f\nA. \u4e01\u7f9f\u57fa\u7532\u82ef\nB. \u4e9a\u785d\u9178\u76d0\u548c\u4e9a\u785d\u9178\u94a0\u3001\u4e9a\u785d\u9178\u94be\nC. \u7845\u9178\u9499\nD. \u78b3\u9178\u9541\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44887465570531665, "meta-math/MetaMath-Mistral-7B": 0.45092006580737265, "itpossible/Chinese-Mistral-7B-v0.1": 0.7386320734028746, "HuggingFaceH4/zephyr-7b-beta": 0.9880291017648484, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7485781143892738, "meta-llama/Meta-Llama-3-8B": 0.7721663945642822, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8661722228230364}}, {"question": "\u4e0b\u5217\u5c3f\u4e2d\u6392\u51fa\u7684\u542b\u6c2e\u7269\u4e0d\u53d7\u81b3\u98df\u6444\u5165\u6c2e\u5f71\u54cd\u7684\u662f\nA. \u5c3f\u7d20\nB. \u808c\u9150\nC. \u6c28\nD. \u5c3f\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.370258293151442, "meta-math/MetaMath-Mistral-7B": 0.5085730126773698, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7318152683241352, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.2963332999770349, "meta-llama/Meta-Llama-3-8B": 0.4862640676704594, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1806\u5e74\uff0c\u5fb7\u56fd\u8d6b\u5c14\u5df4\u7279\uff08J\uff0eF\uff0eHerbart\uff09\u51fa\u7248\u54ea\u4e00\u672c\u4e66\u88ab\u8a89\u4e3a\u6559\u80b2\u5b66\u72ec\u7acb\u7684\u6807\u5fd7\nA. \u300a\u666e\u901a\u6559\u80b2\u5b66\u300b\nB. \u300a\u8bba\u6f14\u8bf4\u5bb6\u7684\u6559\u80b2\u300b\nC. \u300a\u6559\u80b2\u5b66\u300b\nD. \u300a\u65b0\u6559\u80b2\u5927\u7eb2\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u897f\u65b9\u6559\u80b2\u53f2\u4e0a\uff0c\u63d0\u51fa\u201c\u6cdb\u667a\u6559\u80b2\u201d\u548c\u666e\u53ca\u521d\u7b49\u6559\u80b2\u7684\u4e3b\u5f20\uff0c\u5e76\u5bf9\u73ed\u7ea7\u6388\u8bfe\u5236\u505a\u51fa\u7cfb\u7edf\u9610\u8ff0\u7684\u8457\u4f5c\u662f\nA. \u8d6b\u5c14\u5df4\u7279\u7684\u300a\u6559\u80b2\u5b66\u300b\nB. \u5938\u7f8e\u7ebd\u65af\u7684\u5927\u6559\u5b66\u8bba\nC. \u6606\u4f53\u826f\u7684\u300a\u8bba\u6f14\u8bf4\u5bb6\u7684\u6559\u80b2\u300b\nD. \u67cf\u62c9\u56fe\u7684\u300a\u7406\u60f3\u56fd\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.47224042662033305, "meta-math/MetaMath-Mistral-7B": 0.8448452180839757, "itpossible/Chinese-Mistral-7B-v0.1": 0.6034309722499165, "HuggingFaceH4/zephyr-7b-beta": 0.9572204240590256, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6805037568535796, "meta-llama/Meta-Llama-3-8B": 0.7851148654969905, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8557120259657931}}, {"question": "\u5c06\u5564\u9152\u82b1\u52a0\u5165\u9ea6\u82bd\u6c41\uff0c\u8981\u716e\u6cb8\uff0c\u716e\u6cb8\u7684\u76ee\u7684\u9664\u4e86\u4e0b\u5217\nA. \u6740\u6b7b\u5fae\u751f\u7269\nB. \u6d53\u7f29\u6db2\u4f53\nC. \u706d\u6d3b\u9176\nD. \u6eb6\u89e3\u5564\u9152\u82b1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c34\u6eb6\u6027\u7ef4\u751f\u7d20\u6444\u5165\u8fc7\u591a\u65f6\nA. \u53ef\u5728\u4f53\u5185\u5927\u91cf\u8d2e\u5b58\nB. \u6781\u6613\u5f15\u8d77\u4e2d\u6bd2\nC. \u901a\u8fc7\u80c6\u6c41\u7f13\u6162\u6392\u51fa\u4f53\u5916\nD. \u53ef\u7ecf\u5c3f\u6db2\u6392\u51fa\u4f53\u5916\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5738900654495687, "meta-math/MetaMath-Mistral-7B": 0.7927968186760472, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9846753376236255, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7136628327781211, "meta-llama/Meta-Llama-3-8B": 0.508906861659202, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8ba1\u7b97\u673a\u53d6\u8bc1\u662f\u5c06\u8ba1\u7b97\u673a\u8c03\u67e5\u548c\u5206\u6790\u6280\u672f\u5e94\u7528\u4e8e\u5bf9\u6f5c\u5728\u7684\u3001\u6709\u6cd5\u5f8b\u6548\u529b\u7684\u8bc1\u636e\u7684\u786e\u5b9a\u4e0e\u63d0\u53d6\u3002\u4ee5\u4e0b\u5173\u4e8e\u8ba1\u7b97\u673a\u53d6\u8bc1\u7684\u63cf\u8ff0\u4e2d\uff0c\u9519\u8bef\u7684\u662f\nA. \u8ba1\u7b97\u673a\u53d6\u8bc1\u56f4\u7ed5\u7535\u5b50\u8bc1\u636e\u8fdb\u884c\uff0c\u7535\u5b50\u8bc1\u636e\u5177\u6709\u9ad8\u79d1\u6280\u6027\u3001\u65e0\u5f62\u6027\u548c\u6613\u7834\u574f\u6027\u7b49\u7279\u70b9\nB. \u8ba1\u7b97\u673a\u53d6\u8bc1\u5305\u62ec\u5bf9\u4ee5\u78c1\u4ecb\u8d28\u7f16\u7801\u4fe1\u606f\u65b9\u5f0f\u5b58\u50a8\u7684\u8ba1\u7b97\u673a\u8bc1\u636e\u7684\u4fdd\u62a4\u3001\u786e\u8ba4\u3001 \u63d0\u53d6\u548c\u5f52\u6863\nC. \u8ba1\u7b97\u673a\u53d6\u8bc1\u662f\u4e00\u95e8\u5728\u72af\u7f6a\u8fdb\u884c\u8fc7\u7a0b\u4e2d\u6216\u4e4b\u540e\u6536\u96c6\u8bc1\u636e\u7684\u6280\u672f\nD. \u8ba1\u7b97\u673a\u53d6\u8bc1\u5305\u62ec\u4fdd\u62a4\u76ee\u6807\u8ba1\u7b97\u673a\u7cfb\u7edf\u3001\u786e\u5b9a\u6536\u96c6\u548c\u4fdd\u5b58\u7535\u5b50\u8bc1\u636e\uff0c\u5fc5\u987b\u5728\u5f00\u673a\u7684\u72b6\u6001\u4e0b\u8fdb\u884c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8084221596985754, "meta-math/MetaMath-Mistral-7B": 0.8695249292344894, "itpossible/Chinese-Mistral-7B-v0.1": 0.851634510649246, "HuggingFaceH4/zephyr-7b-beta": 0.9972218841242543, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.738877654793084, "meta-llama/Meta-Llama-3-8B": 0.9363476808018203, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8182836699799763}}, {"question": "\u6839\u636e\u300a\u56fd\u5bb6\u5b89\u5168\u6cd5\u300b\u7684\u89c4\u5b9a\uff0c\u5173\u4e8e\u5168\u56fd\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u5e38\u52a1\u59d4\u5458\u7684\u804c\u6743\u6b63\u786e\u7684\u662f()\u3002\nA. \u51b3\u5b9a\u4e2a\u522b\u7701\u3001\u81ea\u6cbb\u533a\u3001\u76f4\u8f96\u5e02\u8fdb\u5165\u7d27\u6025\u72b6\u6001\nB. \u4e0d\u80fd\u51b3\u5b9a\u5168\u56fd\u8fdb\u5165\u7d27\u6025\u72b6\u6001\nC. \u4e0d\u80fd\u51b3\u5b9a\u5168\u56fd\u603b\u52a8\u5458\nD. \u4e0d\u80fd\u51b3\u5b9a\u6218\u4e89\u72b6\u6001\u7684\u5b9c\u5e03\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0e\u6c11\u65cf\u6700\u8fd1\u4f3c\u7684\u4eba\u4eec\u5171\u540c\u4f53\u7684\u662f\nA. \u80de\u65cf\nB. \u90e8\u843d\u8054\u76df\nC. \u6c0f\u65cf\nD. \u90e8\u843d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2656046866868781, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.34412869855849515, "meta-llama/Meta-Llama-3-8B": 0.32304351211676674, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u897f\u65b9\uff0c\u9996\u6b21\u8f83\u6709\u7cfb\u7edf\u5730\u9610\u8ff0\u827a\u672f\u8d77\u6e90\u4e8e\u6a21\u4eff\u7684\u7406\u8bba\u5bb6\u662f\nA. \u82cf\u683c\u62c9\u5e95\nB. \u5eb7\u5fb7\nC. \u8d3a\u62c9\u65af\nD. \u4e9a\u91cc\u58eb\u591a\u5fb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33539461944129995, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6061547623194425, "HuggingFaceH4/zephyr-7b-beta": 0.6100782072264002, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.49101867590112, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7414013581001859}}, {"question": "\u4e00\u56fd\u6709\u6743\u51b3\u5b9a\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\u5141\u8bb8\u5916\u56fd\u4eba\u4eba\u5883\nA. \u4e0e\u5916\u56fd\u4eba\u672c\u56fd\u8ba2\u6709\u534f\u8bae\nB. \u8fd9\u5c5e\u4e8e\u4e00\u56fd\u5185\u653f\u95ee\u9898\nC. \u56fd\u9645\u516c\u7ea6\u4e2d\u6709\u6b64\u89c4\u5b9a\nD. \u8fd9\u5c5e\u4e8e\u533a\u57df\u56fd\u9645\u6cd5\u7684\u89c4\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3300364758848943, "HuggingFaceH4/zephyr-7b-beta": 0.9749919832200548, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4087009877924863, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u4f1a\u8ba1\u79d1\u76ee\u7684\u8868\u8ff0\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u4f1a\u8ba1\u79d1\u76ee\u8bbe\u7f6e\u7684\u4f9d\u636e\u662f\u4f1a\u8ba1\u8d26\u6237\nB. \u4f1a\u8ba1\u79d1\u76ee\u6709\u4e00\u5b9a\u7684\u7ed3\u6784\nC. \u4f1a\u8ba1\u79d1\u76ee\u662f\u5bf9\u4f1a\u8ba1\u5bf9\u8c61\u8fdb\u884c\u5206\u7c7b\u6838\u7b97\u7684\u9879\u76ee\nD. \u6240\u6709\u4f1a\u8ba1\u79d1\u76ee\u5fc5\u987b\u8bbe\u7f6e\u660e\u7ec6\u79d1\u76ee\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5093952797069128, "meta-math/MetaMath-Mistral-7B": 0.8760584931417272, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9981852277711523, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5696695963845606, "meta-llama/Meta-Llama-3-8B": 0.9163316161110153, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9536060770507516}}, {"question": "\u67d0\u516c\u53f8\u4eca\u5e744\u2f49\u7684\u8425\u4e1a\u989d\u4e3a2500\u4e07\u5143\uff0c\u6309\u8ba1\u5212\u7b2c\u2f06\u5b63\u5ea6\u7684\u603b\u8425\u4e1a\u989d\u8981\u8fbe\u52309100\u4e07\u5143\uff0c\u8bbe\u8be5\u516c\u53f85\u30016 \u4e24\u2f49\u7684\u8425\u4e1a\u989d\u7684\u2f49\u5e73\u5747\u589e\u2ed3\u7387\u4e3a x\uff0c\u6839\u636e\u9898\u610f\u5217\u2f45\u7a0b\uff0c\u5219\u4e0b\u5217\u2f45\u7a0b\u6b63\u786e\u7684\u662f\nA. $2500(1+x)^{2}=9100$\nB. $2500(1+x)+2500(1+x)^{2}=9100$\nC. $2500+2500(1+x)+2500(1+x)^{2}=9100$\nD. $2500(1+x\\%)^{2}=9100$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.43571005274105923}}, {"question": "\u8bbe$\\bigtriangleup ABC$\u7684\u5185\u89d2A\uff0cB\uff0cC\u6240\u5bf9\u8fb9\u7684\u957f\u5206\u522b\u662fa\uff0cb\uff0cc\uff0c\u82e5$a^{2}=b^{2}+c^{2}-bc$\uff0c\u5219A=\nA. $2\\frac{\\pi}{3}$\nB. $\\frac{\\pi}{4}$\nC. $\\frac{\\pi}{6}$\nD. $\\frac{\\pi}{3}$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4792568906830746, "itpossible/Chinese-Mistral-7B-v0.1": 0.28650612283664895, "HuggingFaceH4/zephyr-7b-beta": 0.776611921221372, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3860362730051736, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3718549851402879}}, {"question": "\u4e0b\u5217\u6709\u5173\u7ec6\u80de\u751f\u547d\u5386\u7a0b\u7684\u8bf4\u6cd5\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u7ec6\u80de\u589e\u6b96\uff0c\u53ef\u80fd\u4e0d\u51fa\u73b0\u6838\u819c\u3001\u6838\u4ec1\u7684\u5468\u671f\u6027\u53d8\u5316\nB. \u7ec6\u80de\u5206\u5316\uff0c\u6838\u9057\u4f20\u7269\u8d28\u6ca1\u6709\u53d1\u751f\u6539\u53d8\nC. \u7ec6\u80de\u51cb\u4ea1\uff0c\u6709\u5229\u4e8e\u591a\u7ec6\u80de\u751f\u7269\u4f53\u5b8c\u6210\u6b63\u5e38\u751f\u957f\u53d1\nD. \u7ec6\u80de\u751f\u957f\uff0c\u7269\u8d28\u8fd0\u8f93\u7684\u6548\u7387\u4f1a\u589e\u5f3a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u5b50\u4e2d\uff0c\u610f\u601d\u4e0d\u540c\u7684\u4e00\u53e5\u662f\nA. \u4ed6\u4eca\u5929\u4e0d\u4e00\u5b9a\u4f1a\u6765\u3002\nB. \u4ed6\u4eca\u5929\u4e00\u5b9a\u4e0d\u4f1a\u6765\u3002\nC. \u4eca\u5929\u4ed6\u4e00\u5b9a\u4e0d\u4f1a\u6765\u3002\nD. \u4eca\u5929\u4e00\u5b9a\u4ed6\u4e0d\u4f1a\u6765\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e3a\u4e86\u9632\u6b62\u76ae\u80a4\u53d8\u9ed1\u5e94\u8be5\u5c11\u5403\nA. \u9a6c\u94c3\u85af\nB. \u9ec4\u74dc\nC. \u9752\u83dc\nD. \u897f\u7ea2\u67ff\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3157387487413705, "meta-math/MetaMath-Mistral-7B": 0.8312937652260673, "itpossible/Chinese-Mistral-7B-v0.1": 0.5673848116830029, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5212060993156957, "meta-llama/Meta-Llama-3-8B": 0.3404063339940579, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.39825185170545957}}, {"question": "1952\u5e74\u52301964\u5e74\uff0c\u82cf\u8054\u653f\u5e9c\u6536\u8d2d\u8c37\u7269\u7684\u5e73\u5747\u4ef7\u683c\u6307\u6570\u63d0\u9ad8\u4e867.48\u500d\uff0c\u6536\u8d2d\u755c\u4ea7\u54c1\u7684\u4ef7\u683c\u6307\u6570\u63d0\u9ad8\u4e8615.69\u500d\u3002\u8fd9\u662f\nA. \u9002\u5e94\u201c\u52a0\u901f\u53d1\u5c55\u6218\u7565\u201d\u7684\u9700\u8981\nB. \u63a8\u884c\u519c\u4e1a\u96c6\u4f53\u5316\u5bfc\u81f4\u7684\u53d8\u5316\nC. \u8c03\u6574\u519c\u4e1a\u653f\u7b56\u7684\u7ed3\u679c\nD. \u5e94\u5bf9\u4e16\u754c\u519c\u4ea7\u54c1\u4ef7\u683c\u6ce2\u52a8\u7684\u63aa\u65bd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4028736962599066, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5220127520613518, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5414468106387769}}, {"question": "2018\u5e74\u67d0\u5927\u578b\u73bb\u7483\u5382\u8d85\u6807\u6392\u653e\u5927\u6c14\u6c61\u67d3\u7269\uff0c\u4e25\u91cd\u6c61\u67d3\u4e86\u5468\u8fb9\u7684\u5927\u6c14\u73af\u5883\u3002\u6839\u636e\u6211\u56fd\u76f8\u5173\u6cd5\u5f8b\uff0c\u4e0b\u5217\u54ea\u4e00\u65e0\u8fdd\u6cd5\u8bb0\u5f55\u4e14\u7b26\u5408\u5e74\u9650\u8981\u6c42\u7684\u4e3b\u4f53\u6709\u6743\u63d0\u8d77\u516c\u76ca\u8bc9\u8bbc\nA. \u4f9d\u6cd5\u5728\u73bb\u7483\u5382\u6240\u5728\u5730\u7684\u53bf\u6c11\u653f\u5c40\u767b\u8bb0\u7684\u67d0\u73af\u4fdd\u4e2d\u5fc3\nB. \u5728\u56fd\u5916\u5408\u6cd5\u8bbe\u7acb\u4f46\u5c1a\u672a\u5728\u6211\u56fd\u6c11\u653f\u90e8\u95e8\u767b\u8bb0\u7684\u67d0\u73af\u4fdd\u57fa\u91d1\u4f1a\nC. \u7531\u5468\u8fb9\u4f4f\u6237\u5408\u6cd5\u6709\u6548\u63a8\u9009\u51fa\u7684\u67d0\u73af\u4fdd\u5fd7\u613f\u8005\nD. \u4f9d\u6cd5\u5728\u90bb\u7701\u6c11\u653f\u5385\u767b\u8bb0\u7684\u67d0\u73af\u4fdd\u534f\u4f1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c56\u5c81\u3002\u52b3\u7d2f\u540e\u53d1\u70ed1\u5468\uff0c\u4f53\u6e29\u6700\u9ad839.5\u2103\uff0c\u4f34\u591a\u6c57\u3001\u7eb3\u5dee\uff0c\u4f53\u91cd\u4e0b\u964d2\u516c\u65a4\uff0c\u540c\u65f6\u51fa\u73b0\u53f3\u4fa7\u5b63\u808b\u90e8\u75bc\u75db\uff0c\u75bc\u75db\u8f83\u5267\u70c8\uff0c\u591c\u95f4\u65e0\u6cd5\u5b89\u7761\uff0c\u4e8e\u5916\u9662\u8f93\u6ce8\u54cc\u62c9\u897f\u6797\uff0f\u4ed6\u5511\u5df4\u57663\u5929\u65e0\u6548\u6536\u5165\u9662\u3002\u65e2\u5f80\u53f2\uff1a\u7c7b\u98ce\u6e7f\u5173\u8282\u708e20\u5e74\uff0c\u5e73\u7d20\u6bcf\u65e5\u670d\u7528\u6765\u6c1f\u7c73\u7279\u53ca\u5f3a\u7684\u677e\u6cbb\u7597\uff0c\u75c5\u60c5\u7a33\u5b9a\u3002\u5165\u9662\u65f6\u80f8\u7247\u663e\u793a\u53f3\u4fa7\u80f8\u8154\u79ef\u6db2\u3002\u80f8\u8154\u7a7f\u523a\u5316\u9a8c\u7ed3\u679c\uff1a\u80f8\u6c34\u6bd4\u91cd1.038\uff0c\u767d\u7ec6\u80de16800\u00d7106\uff0fL\uff0c\u591a\u6838\u7ec6\u80de80\uff05\uff0c\u5355\u6838\u7ec6\u80de20\uff05\uff0cLDH3367U\uff0fL\uff0cADA116U\uff0fL\u3002\u8be5\u60a3\u8005\u6700\u53ef\u80fd\u7684\u8bca\u65ad\u662f\nA. \u7c7b\u98ce\u6e7f\u76f8\u5173\u80f8\u8154\u79ef\u6db2\nB. \u6076\u6027\u80f8\u8154\u79ef\u6db2\nC. \u8113\u80f8\nD. \u7ed3\u6838\u6027\u80f8\u819c\u708e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5462673667202453, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.32486943884392533, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u73b0\u4ee3\u6b66\u5668\uff0c\u4e0b\u5217\u8bf4\u6cd5\u9519\u8bef\u7684\u662f\nA. \u9a71\u9010\u8230\u5177\u6709\u9632\u7a7a\u3001\u53cd\u6f5c\u548c\u5bf9\u5730\u653b\u51fb\u7684\u7efc\u5408\u4f5c\u6218\u80fd\u529b\nB. \u8feb\u51fb\u70ae\u901a\u5e38\u914d\u5c5e\u88c5\u7532\u5175\u4f7f\u7528\nC. \u6d32\u9645\u5f39\u9053\u5bfc\u5f39\u662f\u76ee\u524d\u5c04\u7a0b\u6700\u8fdc\u7684\u5bfc\u5f39\nD. \u9646\u519b\u822a\u7a7a\u5175\u4ee5\u76f4\u5347\u673a\u4e3a\u4e3b\u8981\u88c5\u5907\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6013269478954925, "meta-math/MetaMath-Mistral-7B": 0.9211661700549544, "itpossible/Chinese-Mistral-7B-v0.1": 0.6363925848925637, "HuggingFaceH4/zephyr-7b-beta": 0.9995719398683449, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9321416454146588, "meta-llama/Meta-Llama-3-8B": 0.4406479204361804, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8814275716178062}}, {"question": "\u67d03\u4e2a\u8fde\u9501\u57fa\u56e0\u6742\u79cd\u7684\u4eb2\u672c\u578b\u914d\u5b50\u662fBDE\u548cbde\uff0c\u53cc\u4ea4\u6362\u578b\u914d\u5b50\u662fbdE\u5404BDe\u3002\u7531\u6b64\u53ef\u77e5\uff0c3\u4e2a\u57fa\u56e0\u5728\u67d3\u8272\u4f53\u4e0a\u7684\u6392\u5217\u987a\u5e8f\u662f\nA. EDB\nB. DBE\nC. BDE\nD. BED\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7528\u51b2\u7a81\u89c4\u8303\u8c03\u6574\u6d89\u5916\u6c11\u4e8b\u5173\u7cfb\uff0c\u5176\u65b9\u6cd5\u5c5e\u4e8e\nA. \u76f4\u63a5\u8c03\u6574\nB. \u95f4\u63a5\u8c03\u6574\nC. \u81ea\u7136\u8c03\u6574\nD. \u6df7\u5408\u8c03\u6574\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7069597101788235, "meta-math/MetaMath-Mistral-7B": 0.8568893902473124, "itpossible/Chinese-Mistral-7B-v0.1": 0.40428437682654317, "HuggingFaceH4/zephyr-7b-beta": 0.998437558262416, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8303271094145721, "meta-llama/Meta-Llama-3-8B": 0.6439140608714028, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7088855560949314}}, {"question": "\u8fa9\u8bc1\u601d\u7ef4\u65b9\u6cd5\u4ece\u62bd\u8c61\u4e0a\u5347\u5230\u5177\u4f53\u7684\u8fc7\u7a0b\u662f\nA. \u4ece\u8ba4\u8bc6\u5230\u5b9e\u8df5\u7684\u8fc7\u7a0b\nB. \u601d\u7ef4\u751f\u6210\u73b0\u5b9e\u5177\u4f53\u7684\u8fc7\u7a0b\nC. \u5728\u601d\u7ef4\u4e2d\u5f62\u6210\u201c\u591a\u79cd\u89c4\u5b9a\u7684\u7edf\u4e00\u201d\u7684\u8fc7\u7a0b\nD. \u4ece\u5b9e\u8df5\u5230\u8ba4\u8bc6\u7684\u8fc7\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5688830063660747, "itpossible/Chinese-Mistral-7B-v0.1": 0.5451708460875704, "HuggingFaceH4/zephyr-7b-beta": 0.7537390703197956, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5730151725595691, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7009220691236777}}, {"question": "\u674e\u4e3d\u540c\u5b66\u7684\u7238\u7238\u8ddf\u5b66\u6821\u6821\u957f\u662f\u597d\u670b\u53cb\uff0c\u73ed\u4e3b\u4efb\u77e5\u9053\u540e\uff0c\u4e3b\u52a8\u7ed9\u674e\u4e3d\u8c03\u6574\u4e86\u5ea7\u4f4d\uff0c\u5728\u8bfe\u5802\u6559\u5b66\u4e2d\u7ed9\u674e\u4e3d\u4e89\u53d6\u66f4\u591a\u7684\u65f6\u673a\u7b54\u590d\u4ee5\u4e0b\u95ee\u9898\uff0c\u5e76\u8ba9\u5176\u62c5\u4efb\u73ed\u957f\u3002\u73ed\u4e3b\u4efb\u7684\u884c\u4e3a\nA. \u6709\u5229\u4e8e\u4fc3\u8fdb\u5bb6\u6821\u5408\u4f5c\nB. \u6709\u5229\u4e8e\u5c65\u884c\u73ed\u4e3b\u4efb\u7684\u804c\u8d23\nC. \u5f71\u54cd\u4e86\u6821\u957f\u7684\u5ec9\u6d01\u4ece\u6559\nD. \u5f71\u54cd\u4e86\u5176\u4ed6\u5b66\u751f\u7684\u6210\u957f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u4ee5\u8ff7\u4fe1\u65b9\u5f0f\u6050\u5413\u5218\u67d0\u6709\u707e\u7978\uff0c\u5218\u67d0\u4e00\u65f6\u614c\u4e71\uff0c\u8bf7\u7532\u5e2e\u52a9\u81ea\u5df1\u3002\u7532\u8ba9\u5218\u67d0\u5e26 10 \u4e07\u5143\u73b0\u91d1\u4f5c\u201c\u9547\u90aa\u7269\u201d\uff0c\u627e\u6cd5\u5e08\u201c\u6d88\u706d\u201d\u3002\u9014\u4e2d\uff0c\u7532\u8d81\u5e2e\u5218\u67d0\u62ff\u5305\u4e4b\u673a\uff0c\u7528\u4e66\u672c\u8c03\u6362\u4e86 10\u4e07\u5143\u73b0\u91d1\u3002\u7532\u7684\u884c\u4e3a\u6784\u6210\nA. \u4fb5\u5360\u7f6a\nB. \u76d7\u7a83\u7f6a\nC. \u6572\u8bc8\u52d2\u7d22\u7f6a\nD. \u8bc8\u9a97\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.36253011129797796, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.31911523504877376, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u56db\u5927\u77f3\u7a9f\u4e2d\u7684\u9ea6\u79ef\u5c71\u77f3\u7a9f\u4f4d\u4e8e\nA. \u5c71\u897f\nB. \u5929\u6c34\nC. \u6d1b\u9633\nD. \u7518\u8083\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3456577062434727, "itpossible/Chinese-Mistral-7B-v0.1": 0.6411971628687528, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53d9\u8ff0\u4e86\u7559\u5b66\u5f52\u6765\u7684\u201c\u6211\u201d\u4e0e\u5973\u5de5\u9648\u4e8c\u59b9\u7531\u76f8\u9047\u3001\u731c\u5fcc\u5230\u4e92\u76f8\u4e86\u89e3\u3001\u540c\u60c5\u7684\u5c0f\u8bf4\u662f\nA. \u300a\u6625\u98ce\u6c89\u9189\u7684\u665a\u4e0a\u300b\nB. \u300a\u9001\u62a5\u592b\u300b\nC. \u300a\u91d1\u9501\u8bb0\u300b\nD. \u300a\u8377\u82b1\u6dc0\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30239379048437676, "meta-math/MetaMath-Mistral-7B": 0.5277408414344442, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.2963332999770349, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.46667350468561497}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\uff0c\u5bbe\u8bed\u501f\u52a9\u4ee3\u8bcd\u590d\u6307\u800c\u524d\u7f6e\u7684\u4e00\u53e5\u662f\nA. \u4f55\u7232\u68c4\u58b3\u4e95\uff0c\u5728\u5c71\u8c37\u7232\u5bc7\u4e5f?\nB. \u5927\u9053\u4e4b\u884c\u4e5f\uff0c\u8f3f\u4e09\u4ee3\u4e4b\u82f1\uff0c\u4e18\u672a\u4e4b\u902e\u4e5f\u3002\nC. \u6211\u7121\u723e\u8a50\uff0c\u723e\u7121\u6211\u865e\u3002\nD. \u5176\u4e00\u4eba\u5c08\u5fc3\u81f4\u5fd7\uff0c\u60df\u5955\u79cb\u4e4b\u7232\u807d\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4558147033879993, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5927\u9646\u6f02\u79fb\u8bf4\u662f\u4e00\u79cd\u89e3\u91ca\u5730\u58f3\u8fd0\u52a8\u548c\u6d77\u8def\u5206\u5e03\u3001\u6f14\u53d8\u7684\u79d1\u5b66\u5047\u8bf4\uff0c\u4e3a\u677f\u5757\u6784\u9020\u5b66\u8bf4\u7684\u5efa\u7acb\u548c\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5bf9\u5730\u7403\u79d1\u5b66\u7684\u53d1\u5c55\u8d77\u5230\u4e86\u5f88\u5927\u7684\u63a8\u52a8\u4f5c\u7528\u3002\u4ee5\u4e0b\u4eba\u7269\u4e2d\uff0c\u6b63\u5f0f\u63d0\u51fa\u8be5\u5b66 \u8bf4\u7684\u662f\nA. \u6d2a\u5821\nB. \u9b4f\u683c\u7eb3\nC. \u8fbe\u5c14\u6587\nD. \u57f9\u6839\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.805207293983337, "HuggingFaceH4/zephyr-7b-beta": 0.67654425555413, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4180371668839167, "meta-llama/Meta-Llama-3-8B": 0.9499621968457973, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9720771881279938}}, {"question": "\u67d0\u9ad8\u8840\u538b\u60a3\u8005\uff0c\u660f\u8ff7\u6570\u5c0f\u65f6\u540e\u6e05\u9192\uff0c\u68c0\u67e5\u53d1\u73b0\uff1a\u53f3\u4fa7\u4e0a\u3001\u4e0b\u80a2\u75c9\u5b6a\u6027\u762b\u75ea\uff0c\u8171\u53cd\u5c04\u4ea2\u8fdb\uff0c Babinski\u5f81\u9633\u6027\uff0c\u4f38\u820c\u820c\u5c16\u504f\u5411\u5de6\u4fa7\uff0c\u5de6\u4fa7\u820c\u808c\u840e\u7f29\uff0c\u53f3\u4fa7\u8eaf\u5e72\u3001\u4e0a\u3001\u4e0b\u80a2\u632f\u52a8\u89c9\u3001\u7cbe\u7ec6\u89e6\u89c9\u4e27\u5931\uff0c\u4f46\u5168\u8eab\u75db\u3001\u6e29\u89c9\u6b63\u5e38\u3002\u8bd5\u5206\u6790\u53ef\u80fd\u635f\u4f24\u7684\u7ed3\u6784\u662f\nA. \u5de6\u4fa7\u9525\u4f53\u675f\u3001\u5de6\u4fa7\u5185\u4fa7\u4e18\u7cfb\uff0c\u5de6\u4fa7\u820c\u4e0b\u795e\u7ecf\nB. \u5de6\u4fa7\u9525\u4f53\u675f\uff0c\u53f3\u4fa7\u5185\u4fa7\u4e18\u7cfb\uff0c\u5de6\u4fa7\u820c\u4e0b\u795e\u7ecf\nC. \u53f3\u4fa7\u9525\u4f53\u675f\uff0c\u5de6\u4fa7\u5185\u4fa7\u4e18\u7cfb\uff0c\u53f3\u4fa7\u820c\u4e0b\u795e\u7ecf\nD. \u53f3\u4fa7\u76ae\u8d28\u810a\u9ad3\u675f\uff0c\u5de6\u4fa7\u5185\u4fa7\u4e18\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5\u968f\u673a\u53d8\u91cf$X$\u7684\u5371\u9669\u7387\u51fd\u6570\u4e3a$h(x)=3 x^4\uff0cx \\geqslant 0$\uff0c\u4f5c\u53d8\u6362$Y=\\ln X$\uff0c\u5219$Y$\u7684\u5371\u9669\u7387\u51fd\u6570\u4e3a( )\u3002\nA. $5 e^{-3 y}$\nB. $5 e^{3 y}$\nC. $\\frac{3}{5} e^{5 y}$\nD. $3 e^{5 y}$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8660937795513076, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f01\u4e1a\u6574\u4f53\u7684\u9053\u5fb7\u8d23\u4efb\uff08\uff09a\u4f01\u4e1a\u5185\u90e8\u6210\u5458\u884c\u4e3a\u5f53\u6709\u5229\u4e8e\u4f01\u4e1a\u6574\u4f53\u9053\u5fb7\u8d23\u4efb\u7684\u5c65\u884c\uff1bb\u9ad8\u6548\u7387\u5730\u4e3a\u793e\u4f1a\u63d0\u4f9b\u6240\u9700\u8981\u7684\u4ea7\u54c1\u548c\u670d\u52a1\uff1bc\u4f01\u4e1a\u5f53\u5c0a\u91cd\u6240\u6709\u53c2\u4e0e\u8005\u548c\u76f8\u5173\u8005\u7684\u6743\u5229\u6309\u8d21\u732e\u5206\u914d\u7684\u539f\u5219\uff1bd\u4f01\u4e1a\u5185\u90e8\u6210\u5458\u90fd\u8981\u505a\u597d\u672c\u804c\u5de5\u4f5c\nA. abc\nB. acd\nC. bc\nD. abcd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u56e0\u7ecf\u8425\u4e0d\u5584\u6b20\u4e0b\u5de8\u989d\u503a\u52a1\uff0c\u4e3a\u8f6c\u79fb\u8d22\u4ea7\uff0c\u4e0e\u670b\u53cb\u4e59\u4f2a\u9020\u7532\u5411\u4e59\u501f\u6b3e200\u4e07\u5143\u7684\u501f\u6b3e\u5408\u540c\uff0c\u8ba9\u4e59\u5411\u6cd5\u9662\u63d0\u8d77\u8bc9\u8bbc\uff0c\u7b2c\u4e09\u4eba\u4e19\u5f97\u77e5\u540e\u7533\u8bf7\u53c2\u52a0\u8bc9\u8bbc\uff0c\u6cd5\u9662\u7ecf\u5f00\u5ead\u5ba1\u7406\u67e5\u660e\u8be5\u501f\u6b3e\u5408\u540c\u5c5e\u4e8e\u7532\u3001\u4e59\u6076\u610f\u4f2a\u9020\uff0c\u7532\u7684\u884c\u4e3a\u5e94\u8ba4\u5b9a\u4e3a\nA. \u4f2a\u8bc1\u7f6a\nB. \u865a\u5047\u8bc9\u8bbc\u7f6a\nC. \u6270\u4e71\u6cd5\u5ead\u79e9\u5e8f\u7f6a\nD. \u59a8\u5bb3\u4f5c\u8bc1\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.47935415171001294, "meta-math/MetaMath-Mistral-7B": 0.6549936009914744, "itpossible/Chinese-Mistral-7B-v0.1": 0.8269126345760738, "HuggingFaceH4/zephyr-7b-beta": 0.6482458233553733, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7237484731088268, "meta-llama/Meta-Llama-3-8B": 0.4699766936560869, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7891713841452354}}, {"question": "\u4f01\u4e1a\u6309\u7167\u5355\u4f4d\u6210\u672c\u52a0\u4e0a\u4e00\u5b9a\u767e\u5206\u6bd4\u7684\u52a0\u6210\u6765\u786e\u5b9a\u4ea7\u54c1\u9500\u552e\u4ef7\u683c\uff0c\u8fd9\u79cd\u5b9a\u4ef7\u65b9\u6cd5\u6307\u7684\u662f\nA. \u76ee\u6807\u5b9a\u4ef7\u6cd5\nB. \u8ba4\u77e5\u4ef7\u503c\u5b9a\u4ef7\u6cd5\nC. \u6210\u672c\u52a0\u6210\u5b9a\u4ef7\u6cd5\nD. \u968f\u884c\u5c31\u5e02\u5b9a\u4ef7\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9613794165221811, "meta-math/MetaMath-Mistral-7B": 0.9949649019966792, "itpossible/Chinese-Mistral-7B-v0.1": 0.9851677220179318, "HuggingFaceH4/zephyr-7b-beta": 0.9998967372526344, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9836552105775853, "meta-llama/Meta-Llama-3-8B": 0.9272221393535797, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9888747996346855}}, {"question": "\u300a\u9a6c\u7ea6\u300b\u786e\u5b9a\u4e86\u653f\u6cbb\u5408\u4f5c\u673a\u5236\u7684\u5730\u4f4d\u662f\u4ec0\u4e48\nA. \u6b27\u6d32\u5171\u540c\u9632\u52a1\nB. \u53f8\u6cd5\u548c\u8b66\u52a1\u5408\u4f5c\nC. \u6b27\u6d32\u4e00\u4f53\u5316\nD. \u5171\u540c\u5916\u4ea4\u4e0e\u5b89\u5168\u653f\u7b56\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5210221055167514}}, {"question": "\u6db2\u6c2e\u55b7\u6dcb\u51bb\u7ed3\u5728\u51bb\u7ed3\u901f\u5ea6\u4e0a\u5c5e\u4e8e\nA. \u4e2d\u901f\u51bb\u7ed3\nB. \u5feb\u901f\u51bb\u7ed3\nC. \u4ee5\u4e0a\u5168\u90e8\nD. \u7f13\u6162\u51bb\u7ed3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5838304124661431, "meta-math/MetaMath-Mistral-7B": 0.7227809400809885, "itpossible/Chinese-Mistral-7B-v0.1": 0.6483555043028547, "HuggingFaceH4/zephyr-7b-beta": 0.7323188867187151, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7271183493597321, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7535\u5b50\u90ae\u4ef6\u5730\u5740\u7684\u4e00\u822c\u683c\u5f0f\u4e3a\nA. \u57df\u540d@IP\u5730\u5740\nB. IP\u5730\u5740@\u57df\u540d\nC. \u7528\u6237\u540d@\u57df\u540d\nD. \u57df\u540d@\u7528\u6237\u540d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.943862518119226, "meta-math/MetaMath-Mistral-7B": 0.9954187538557214, "itpossible/Chinese-Mistral-7B-v0.1": 0.8369445678187064, "HuggingFaceH4/zephyr-7b-beta": 0.9992790430176886, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.98655502271827, "meta-llama/Meta-Llama-3-8B": 0.9258568812017297, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f8e\u56fd\u5fc3\u7406\u5b66\u5bb6\u65af\u6ed5\u4f2f\u683c\uff08Robert J. Sternberg\uff09\u7684\u7231\u60c5\u4e09\u89d2\u7406\u8bba\u8ba4\u4e3a\u6240\u6709\u7684\u7231\u60c5\u4e0d\u5305\u62ec\nA. \u6fc0\u60c5\nB. \u4eb2\u5bc6\nC. \u627f\u8bfa\nD. \u7406\u60f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.46331119565501916, "HuggingFaceH4/zephyr-7b-beta": 0.9014617223514387, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5364815904182947, "meta-llama/Meta-Llama-3-8B": 0.7322538215062776, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6597163941916949}}, {"question": "\u4e0b\u9762\u54ea\u6761\u662f\u78b3\u6c34\u5316\u5408\u7269\u72ec\u7279\u7684\u751f\u7406\u529f\u80fd\nA. \u6784\u6210\u673a\u4f53\u7ec4\u7ec7\u6210\u5206\nB. \u662f\u7ef4\u6301\u795e\u7ecf\u7cfb\u7edf\u6b63\u5e38\u6d3b\u52a8\u4e0d\u53ef\u7f3a\u5c11\u7684\u7269\u8d28\nC. \u6297\u751f\u916e\u4f5c\u7528\nD. \u4f9b\u7ed9\u70ed\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.40287369625990666, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ebf\u8def\u505c\u7535\u64cd\u4f5c\u7684\u987a\u5e8f\uff1a\u4e00\u822c\u5728\u62c9\u5f00\u5f00\u5173\u540e\uff0c\u518d\nA. \u62c9\u5f00\u6bcd\u7ebf\u4fa7\u5200\u95f8\uff0c\u6700\u540e\u62c9\u5f00\u7ebf\u8def\u4fa7\u5200\u95f8\nB. \u62c9\u5f00\u6bcd\u7ebf\u8fdb\u7ebf\u7535\u6e90\u5f00\u5173\nC. \u968f\u4fbf\u62c9\u5f00\u4efb\u4f55\u4e00\u4fa7\u5200\u95f8\nD. \u62c9\u5f00\u7ebf\u8def\u4fa7\u5200\u95f8\uff0c\u6700\u540e\u62c9\u5f00\u6bcd\u7ebf\u4fa7\u5200\u95f8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4241091157762924, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.997507557167389, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5872666228717053, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\uff0d3.782\nA. \u4e0d\u662f\u5206\u6570\uff0c\u662f\u6709\u7406\u6570\nB. \u662f\u5206\u6570\uff0c\u4e0d\u662f\u6709\u7406\u6570 \nC. \u662f\u8d1f\u6570\uff0c\u4e5f\u662f\u5206\u6570 \nD. \u662f\u8d1f\u6570\uff0c\u4e0d\u662f\u5206\u6570\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3270261070804116, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.49670465162226823, "meta-llama/Meta-Llama-3-8B": 0.39325342265749486, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5513389996462448}}, {"question": "18\u4e16\u7eaa\u7684\u82f1\u56fd\u4fdd\u6301\u7740\u4e00\u79cd\u72ec\u7279\u571f\u5730\u5236\u5ea6\uff0c\u4e00\u7aef\u662f\u4e00\u4e9b\u5927\u5730\u4e3b\uff0c\u53e6\u4e00\u7aef\u5219\u662f\u5927\u91cf\u6ca1\u6709\u571f\u5730\u4ee5\u5de5\u8d44\u4e3a\u751f\u7684\u52b3\u52a8\u529b\uff0c\u81ea\u8015\u519c\u7684\u6570\u91cf\u5728\u6301\u7eed\u51cf\u5c11\u3002\u82f1\u56fd\u519c\u6751\u53d1\u751f\u7684\u53d8\u5316\u8868\u660e\nA. \u82f1\u56fd\u519c\u4e1a\u751f\u4ea7\u529b\u6709\u8f83\u5927\u8fdb\u6b65\nB. \u82f1\u56fd\u7684\u57ce\u5e02\u5316\u8fdb\u7a0b\u52a0\u5feb\nC. \u82f1\u56fd\u519c\u6751\u8d44\u672c\u4e3b\u4e49\u53d1\u5c55\u8fc5\u901f\nD. \u82f1\u56fd\u5c01\u5efa\u79df\u4f43\u5173\u7cfb\u5927\u91cf\u5b58\u5728\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3940619385479189, "itpossible/Chinese-Mistral-7B-v0.1": 0.28218339836013884, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e24\u6837\u672c\u5747\u6570\u6bd4\u8f83\u65f6\uff0c\u5206\u522b\u53d6\u4ee5\u4e0b\u68c0\u9a8c\u6c34\u51c6\uff0c\u4ee5()\u6240\u5bf9\u5e94\u7684\u7b2c\u4e8c\u7c7b\u9519\u8bef\u6700\u5c0f\u3002\nA. $\\alpha=0.01$\nB. $\\alpha=0.05$\nC. $\\alpha=0.10$\nD. $\\alpha=0.25$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u56db\u4f4d\u6765\u90fd\u662f\u6211\u56fd\u5510\u671d\u6770\u51fa\u7684\u8bd7\u4eba\uff0c\u5176\u4e2d\u53f7\u79f0\u201c\u8bd7\u5723\u201d\u7684\u662f\nA. \u674e\u767d\nB. \u767d\u5c45\u6613\nC. \u675c\u752b\nD. \u9646\u6e38\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.685918759524443, "HuggingFaceH4/zephyr-7b-beta": 0.43589454625904106, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u964d\u4f4e\u6162\u6027\u80ba\u6e90\u6027\u5fc3\u810f\u75c5\u80ba\u52a8\u8109\u9ad8\u538b\u7684\u5173\u952e\u6cbb\u7597\u662f\nA. \u5f3a\u5fc3\u5242\nB. \u6c27\u7597\nC. \u547c\u5438\u5174\u594b\u5242\nD. \u652f\u6c14\u7ba1\u6269\u5f20\u5242\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.31483005318115603, "HuggingFaceH4/zephyr-7b-beta": 0.4418927007410025, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5175034254922011, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7772159577163528}}, {"question": "\u9a6c\u514b\u601d\u4e3b\u4e49\u4ea7\u751f\u7684\u7ecf\u6d4e\u6839\u6e90\u662f\nA. \u5de5\u4e1a\u9769\u547d\nB. \u8d44\u672c\u4e3b\u4e49\u7ecf\u6d4e\u5371\u673a\nC. \u9636\u7ea7\u6597\u4e89\nD. \u8d44\u672c\u4e3b\u4e49\u793e\u4f1a\u751f\u4ea7\u529b\u548c\u751f\u4ea7\u5173\u7cfb\u7684\u77db\u76fe\u8fd0\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9249418024891244, "meta-math/MetaMath-Mistral-7B": 0.9836778356904811, "itpossible/Chinese-Mistral-7B-v0.1": 0.9228500054279898, "HuggingFaceH4/zephyr-7b-beta": 0.9985165070540758, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9284537599424082, "meta-llama/Meta-Llama-3-8B": 0.8957983869426328, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.927285888117938}}, {"question": "\u5728\u4e09\u6bb5\u8bba\u4e2d\uff0c\u5982\u679c\u524d\u63d0\u4e2d\u6709\u4e00\u4e2a\u662f\u5426\u5b9a\u547d\u9898\uff0c\u90a3\u4e48\u7ed3\u8bba\u5fc5\u5b9a\u662f\nA. \u771f\u547d\u9898\nB. \u5047\u547d\u9898\nC. \u80af\u5b9a\u547d\u9898\nD. \u5426\u5b9a\u547d\u9898\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u542c\u89c9\u4e2d\u67a2\u4f4d\u4e8e\nA. \u89d2\u56de\nB. \u989e\u6a2a\u56de\nC. \u4e2d\u592e\u540e\u56de\nD. \u4e2d\u592e\u524d\u56de\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4fdd\u9669\u4e8b\u6545\u7684\u9274\u5b9a\u4eba\u3001\u8bc1\u660e\u4eba\u3001\u8d22\u4ea7\u8bc4\u4f30\u4eba\u6545\u610f\u63d0\u4f9b\u865a\u5047\u8bc1\u660e\u6587\u4ef6\uff0c\u4e3a\u4ed6\u4eba\u9a97\u53d6\u4fdd\u9669\u91d1\u63d0\u4f9b\u6761\u4ef6\u7684\uff0c\uff08\uff09\u3002\nA. \u4ee5\u4fdd\u9669\u8bc8\u9a97\u7f6a\u7684\u5171\u72af\u8bba\u5904\nB. \u6784\u6210\u4f2a\u8bc1\u7f6a\nC. \u6784\u6210\u63d0\u4f9b\u865a\u5047\u8bc1\u660e\u6587\u4ef6\u7f6a\nD. \u6784\u6210\u63d0\u4f9b\u865a\u5047\u8bc1\u660e\u6587\u4ef6\u7f6a\u548c\u4fdd\u9669\u8bc8\u9a97\u7f6a\u4e24\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u81b3\u98df\u8c03\u67e5\u4f1a\u6807\u51c6\u5316\u7684\u6839\u672c\u76ee\u7684\u662f\nA. \u5728\u4e00\u5b9a\u7684\u8303\u56f4\u5185\u83b7\u5f97\u6700\u4f73\u79e9\u5e8f\u3002\nB. \u5728\u4e00\u5b9a\u7684\u8303\u56f4\u5185\u83b7\u6700\u5c0f\u7684\u635f\u5931\u3002\nC. \u5728\u4e00\u5b9a\u7684\u8303\u56f4\u5185\u83b7\u5f97\u6700\u4f73\u79e9\u5e8f\u548c\u6548\u76ca\u3002\nD. \u5728\u4e00\u5b9a\u7684\u8303\u56f4\u5185\u83b7\u5f97\u6700\u4f73\u6548\u76ca\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9104318760261568, "meta-math/MetaMath-Mistral-7B": 0.9868006825481296, "itpossible/Chinese-Mistral-7B-v0.1": 0.42796803990042037, "HuggingFaceH4/zephyr-7b-beta": 0.9966433158783116, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9431875332148099, "meta-llama/Meta-Llama-3-8B": 0.6496565745267675, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9899305832706037}}, {"question": "\u4fdd\u62a4\u75c5\u6bd2\u6838\u9178\u7684\u662f\nA. \u8863\u58f3\nB. \u6838\u9178\nC. \u6838\u8863\u58f3\nD. \u5305\u819c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u4e2a\u793e\u533a\u4e2d\u7684\u5404\u79cd\u7ec4\u7ec7\u6216\u5b50\u7cfb\u7edf\u5728\u672c\u793e\u533a\u5185\u90e8\u5f7c\u6b64\u4e4b\u95f4\u7684\u7ed3\u6784\u548c\u529f\u80fd\u5173\u7cfb\uff0c\u88ab\u79f0\u4e3a\nA. \u793e\u533a\u6761\u72b6\u683c\u5c40\nB. \u793e\u533a\u6a2a\u5411\u683c\u5c40\nC. \u793e\u533a\u5757\u72b6\u683c\u5c40\nD. \u793e\u533a\u7eb5\u5411\u683c\u5c40\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2731272040287072, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5408\u6210\u560c\u5464\u3001\u5627\u5576\u78b1\u57fa\u7684\u5171\u540c\u539f\u6599\u6709\nA. \u5929\u51ac\u6c28\u9178\nB. N10-\u7532\u9170\u56db\u6c22\u53f6\u9178\nC. \u7518\u6c28\u9178\nD. \u78f7\u9178\u6838\u7cd6\u7126\u78f7\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5206\u6790\u7ec4\u7ec7\u7684\u81ea\u6211\u671f\u5f85\u5f62\u8c61\u4e0e\u5b9e\u9645\u793e\u4f1a\u5f62\u8c61\u4e4b\u95f4\u7684\u73b0\u5b9e\u8ddd\u79bb\u65f6\u4f7f\u7528\nA. \u8bed\u4e49\u5dee\u5f02\u5206\u6790\u6cd5\nB. \u7ec4\u7ec7\u5f62\u8c61\u5730\u4f4d\u56fe\nC. \u5f62\u8c61\u8981\u7d20\u5dee\u8ddd\u56fe\nD. \u5f62\u8c61\u8981\u7d20\u8c03\u67e5\u8868\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.422651459308303, "meta-math/MetaMath-Mistral-7B": 0.4137189778658777, "itpossible/Chinese-Mistral-7B-v0.1": 0.51507733146936, "HuggingFaceH4/zephyr-7b-beta": 0.6037350659141397, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4953737351259782, "meta-llama/Meta-Llama-3-8B": 0.3672766845447511, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4389959694889549}}, {"question": "\u667a\u529b\u7684\u4e09\u7ef4\u7ed3\u6784\u8bf4\u7684\u63d0\u51fa\u8005\u662f\nA. \u585e\u65af\u767b\nB. \u52a0\u5fb7\u7eb3\nC. \u65af\u76ae\u4e50\u66fc\nD. \u57fa\u5c14\u798f\u7279\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u5355\u4f4d\u6709\u5341\u4f59\u4eba\u5403\u4e86\u67d0\u51b7\u996e\u5e97\u7684\u51b0\u68d2\u540e\uff0c\u611f\u67d3\u4e86\u75e2\u75be\uff0c\u540c\u65f6\u9632\u75ab\u7ad9\u4ece\u51b0\u68d2\u4e2d\u5206\u79bb\u5230\u75e2\u75be\u6746\u83cc\uff0c\u4ece\u800c\u786e\u5b9a\u51b0\u68d2\u662f\nA. \u4f20\u67d3\u6e90\nB. \u4f20\u64ad\u5a92\u4ecb\nC. \u4f20\u64ad\u9014\u5f84\nD. \u5e26\u83cc\u8005\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6696505803884577, "meta-math/MetaMath-Mistral-7B": 0.9546653925085565, "itpossible/Chinese-Mistral-7B-v0.1": 0.4003958580429553, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6408697123450648, "meta-llama/Meta-Llama-3-8B": 0.5859786989973702, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5492591664791899}}, {"question": "\u8fd0\u884c\u4e2d\u7684\u53d8\u538b\u5668\u53d1\u51fa\u8fde\u7eed\u7684\u3001\u5747\u5300\u7684\u55e1\u55e1\u58f0\u97f3\u5e94\nA. \u6b63\u5e38\u8fd0\u884c\nB. \u52a0\u5f3a\u76d1\u89c6\nC. \u7acb\u5373\u9000\u51fa\u8fd0\u884c\nD. \u51cf\u8d1f\u8377\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.576392673271229, "meta-math/MetaMath-Mistral-7B": 0.9795303242544335, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9996759181675269, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9415218836318865, "meta-llama/Meta-Llama-3-8B": 0.5612835042362445, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6364155201798049}}, {"question": "\u751f\u4ea7\u51fd\u6570\u7684\u957f\u671f\u662f\u6307\nA. 1\u5e74\u4ee5\u5185\nB. 5\u5e74\u4ee5\u4e0a\nC. 1\u5e74\u4ee5\u4e0a\nD. \u6240\u6709\u6295\u5165\u90fd\u53ef\u4ee5\u8c03\u6574\u7684\u65f6\u95f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9317794441730795, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.36926690478595164, "meta-llama/Meta-Llama-3-8B": 0.9359067777325104, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9634620009738553}}, {"question": "\u4e0b\u5217\u6280\u672f\u4e2d\uff0c\u4e0d\u80fd\u9884\u9632\u91cd\u653e\u653b\u51fb\u7684\u662f\nA. \u5e8f\u53f7\nB. \u660e\u6587\u586b\u5145\nC. \u65f6\u95f4\u6233\nD. nonce\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.46589233465755325, "HuggingFaceH4/zephyr-7b-beta": 0.5296255122680202, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5754009946438191, "meta-llama/Meta-Llama-3-8B": 0.5579784254483179, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.989851820625967}}, {"question": "\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u7269\u4f53\u7684\u6e29\u5ea6\u5347\u9ad8\uff0c\u5b83\u7684\u5206\u5b50\u70ed\u8fd0\u52a8\u4e00\u5b9a\u52a0\u5267\nB. \u6e29\u5ea6\u9ad8\u7684\u7269\u4f53\u4e00\u5b9a\u6bd4\u6e29\u5ea6\u4f4e\u7684\u7269\u4f53\u70ed\u91cf\u591a\nC. \u6e29\u5ea6\u9ad8\u7684\u7269\u4f53\u4e00\u5b9a\u6bd4\u6e29\u5ea6\u4f4e\u7684\u7269\u4f53\u5185\u80fd\u5927\nD. \u7269\u4f53\u7684\u6e29\u5ea6\u5347\u9ad8\uff0c\u4e00\u5b9a\u662f\u4ece\u5916\u754c\u5438\u6536\u4e86\u70ed\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.41450857214761916, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5950025697999892}}, {"question": "\u52a8\u7269\u56ed\u7684\u52a8\u7269\u9020\u6210\u4ed6\u4eba\u635f\u5bb3\uff0c\u52a8\u7269\u56ed\u4e0d\u627f\u62c5\u8d23\u4efb\u7684\u60c5\u5f62\u662f\u3002\nA. \u52a8\u7269\u56ed\u80fd\u591f\u8bc1\u660e\u635f\u5bb3\u7cfb\u7b2c\u4e09\u4eba\u7684\u8fc7\u9519\u9020\u6210\nB. \u52a8\u7269\u56ed\u80fd\u591f\u8bc1\u660e\u5c3d\u5230\u7ba1\u7406\u804c\u8d23\nC. \u52a8\u7269\u56ed\u80fd\u591f\u8bc1\u660e\u8be5\u81f4\u635f\u52a8\u7269\u5e76\u975e\u5371\u9669\u52a8\u7269\nD. \u52a8\u7269\u56ed\u80fd\u591f\u8bc1\u660e\u635f\u5bb3\u7cfb\u88ab\u4fb5\u6743\u4eba\u91cd\u5927\u8fc7\u5931\u9020\u6210\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30359886656533613, "meta-math/MetaMath-Mistral-7B": 0.6694333479551923, "itpossible/Chinese-Mistral-7B-v0.1": 0.30300686740596605, "HuggingFaceH4/zephyr-7b-beta": 0.8653561150548591, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8143244390359892, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.45669489601726887}}, {"question": "\u4f01\u4e1a\u901a\u8fc7\u6539\u8fdb\u5e7f\u544a\u5ba3\u4f20\u548c\u63a8\u9500\u5de5\u4f5c\u3001\u589e\u8bbe\u5546\u4e1a\u7f51\u70b9\u3001\u501f\u52a9\u591a\u6e20\u9053\u5c06\u540c\u4e00\u4ea7\u54c1\u9001\u8fbe\u540c\u4e00\u5e02\u573a\u3001\u77ed\u671f\u524a\u4ef7\u7b49\u63aa\u65bd\uff0c\u4ee5\u4fbf\u5728\u73b0\u6709\u5e02\u573a\u4e0a\u6269\u5927\u73b0\u6709\u4ea7\u54c1\u7684\u9500\u552e\u3002\u8fd9\u79cd\u65b0\u4e1a\u52a1\u53d1\u5c55\u6218\u7565\u5c5e\u4e8e\nA. \u5e02\u573a\u6269\u5f20\nB. \u5e02\u573a\u5f00\u53d1\nC. \u5e02\u573a\u6e17\u900f\nD. \u4ea7\u54c1\u5f00\u53d1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8708125226874842, "HuggingFaceH4/zephyr-7b-beta": 0.992415235739579, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ef4\u679c\u8328\u57fa\u63d0\u51fa\"\u6559\u5b66\u5e94\u8d70\u5728\u5f00\u5c55\u524d\u9762\u301e\u7684\u542b\u4e49\u662f\u3002\nA. \u6559\u5b66\u7684\u91cd\u8981\u4efb\u52a1\u662f\u521b\u9020\u6700\u8fd1\u5f00\u5c55\u533a\nB. \u6839\u636e\u5b66\u751f\u73b0\u6709\u7684\u6c34\u5e73\u8fdb\u5c55\u6559\u5b66\nC. \u6559\u5b66\u53ef\u4ee5\u4e0d\u8003\u8651\u513f\u7ae5\u73b0\u6709\u7684\u5f00\u5c55\u6c34\u5e73\nD. \u63d0\u524d\u8bb2\u6388\u4e0b\u4e00\u9636\u6bb5\u624d\u80fd\u638c\u63e1\u7684\u5185\u5bb9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5744124342209367, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5$l_x=1000(8-0.1 x)^{\\frac{1}{3}}\uff0c0 \\leqslant x \\leqslant 80$\uff0c\u82e5\u8bbeX\u4e3a\u65b0\u751f\u5a6a\u513f\u7684\u5269\u4f59\u5bff\u547d\uff0c\u5219$E(X \\mid X>$$20)=____$\u3002\nA. 65\nB. 30\nC. 40\nD. 25\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9815694591931282, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.35909976350004963, "meta-llama/Meta-Llama-3-8B": 0.3723801373943884, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4323369074645566}}, {"question": "\u7528\u73af\u5200\u6cd5\u68c0\u6d4b\u538b\u5b9e\u5ea6\u65f6\uff0c\u5982\u73af\u5200\u6253\u5165\u6df1\u5ea6\u8f83\u6d45\uff0c\u5219\u68c0\u6d4b\u7ed3\u679c\u4f1a\nA. \u504f\u5c0f\nB. \u504f\u5927\nC. \u51c6\u786e\nD. \u65e0\u89c4\u5f8b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u7ef4\u6301\u76f8\u540c\u7684\u4ea7\u91cf\u6c34\u5e73\u65f6\uff0c\u51cf\u5c11\u4e00\u79cd\u751f\u4ea7\u8981\u7d20\u7684\u6570\u91cf\u4e0e\u589e\u52a0\u7684\u53e6\u4e00\u79cd\u751f\u4ea7\u8981\u7d20\u7684\u6570\u91cf\u4e4b\u6bd4\u88ab\u79f0\u4e3a\nA. \u6280\u672f\u7cfb\u6570\nB. \u5f39\u6027\u7cfb\u6570\nC. \u8fb9\u9645\u6280\u672f\u66ff\u4ee3\u7387\nD. \u6069\u683c\u5c14\u7cfb\u6570\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7146660515577031, "meta-math/MetaMath-Mistral-7B": 0.9608750488823277, "itpossible/Chinese-Mistral-7B-v0.1": 0.677467852496571, "HuggingFaceH4/zephyr-7b-beta": 0.9929634915643839, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5662654366413752, "meta-llama/Meta-Llama-3-8B": 0.7759396303981159, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u82f1\u56fd\uff0c\u72ed\u7a84\u7684\u94c1\u8def\u8de8\u8fc7\u50cf\u7eff\u8272\u6d77\u6d0b\u4e00\u6837\u7684\u4e61\u95f4\uff0c\u628a\u6cbf\u9014\u88ab\u88c5\u8fdb\u706b\u8f66\u91cc\u7684\u82f1\u56fd\u4eba\u6c11\uff0c\u629b\u8fdb\u57ce\u95e8\u53e3\u8d8a\u6765\u8d8a\u7a20\u5bc6\u7684\u4eba\u7fa4\u4e4b\u4e2d\u3002\u8fd9\u4e00\u73b0\u8c61\u53cd\u6620\u4e86\u5de5\u4e1a\u9769\u547d\nA. \u4fc3\u8fdb\u519c\u4e1a\u5feb\u901f\u53d1\u5c55\nB. \u63a8\u52a8\u57ce\u5e02\u5316\u8fdb\u7a0b\nC. \u7f29\u5c0f\u57ce\u4e61\u5dee\u8ddd\nD. \u52a0\u901f\u519c\u6751\u57ce\u9547\u5316\u8fdb\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5951879406481483, "meta-math/MetaMath-Mistral-7B": 0.8548364431559664, "itpossible/Chinese-Mistral-7B-v0.1": 0.7497214969500128, "HuggingFaceH4/zephyr-7b-beta": 0.9978921115937731, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9447920399674757, "meta-llama/Meta-Llama-3-8B": 0.875389306698467, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9947620417895895}}, {"question": "\u201c\u5171\u548c\u5efa\u8bbe\u4e4b\u521d\uff0c\u6240\u4ee5\u8270\u96be\u4e0d\u6613\u73b0\u5b9e\uff0c\u5f80\u5f80\u590d\u53cd\u4e13\u5236\u6216\u5e1d\u5236\u4e4b\u7406\u7531\uff0c\u4e43\u56e0\u793e\u4f1a\u4e4b\u60f0\u529b\uff0c\u963b\u788d\u65b0\u6cd5\u4f7f\u4e0d\u6613\u884c\uff0c\u975e\u5171\u548c\u672c\u8eab\u4e4b\u7f6a\u4e5f\u3002\u5176\u963b\u529b\u6700\u5f3a\u8005\uff0c\u83ab\u5982\u5b88\u65e7\u4e4b\u6b66\u4eba\uff0c\u53ca\u5b66\u8005\u3002\u201d\u9648\u72ec\u79c0\u8fd9\u6bb5\u8bba\u8ff0\u8868\u660e\u4ed6\u8ba4\u4e3a\u8f9b\u4ea5\u9769\u547d\u540e\u51fa\u73b0\u5e1d\u5236\u590d\u8f9f\u73b0\u8c61\u4e3b\u8981\u662f\u7531\u4e8e\nA. \u5eb7\u6709\u4e3a\u7b49\u5b88\u65e7\u4fdd\u7687\u515a\u4eba\u7684\u652f\u6301\u63a8\u52a8\nB. \u8fd1\u4ee3\u4e2d\u56fd\u5b58\u5728\u590d\u8f9f\u5e1d\u5236\u7684\u793e\u4f1a\u57fa\u7840\nC. \u8f9b\u4ea5\u9769\u547d\u5e76\u672a\u771f\u6b63\u786e\u7acb\u6c11\u4e3b\u5171\u548c\u5236\nD. \u5e7f\u5927\u4eba\u6c11\u7fa4\u4f17\u6ca1\u6709\u771f\u6b63\u7406\u89e3\u6c11\u4e3b\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7188072295323058, "meta-math/MetaMath-Mistral-7B": 0.8814694434283952, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.902430880088046, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6384500959763671, "meta-llama/Meta-Llama-3-8B": 0.4439174343663388, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.915172626265153}}, {"question": "\u5728\u7ba1\u7406\u65b9\u683c\u7406\u8bba\u4e2d\uff0c\u9886\u5bfc\u8005\u65e2\u5173\u5fc3\u751f\u4ea7\uff0c\u53c8\u5173\u5fc3\u4eba\uff0c\u8fd9\u79cd\u9886\u5bfc\u7c7b\u578b\u88ab\u79f0\u4e4b\u4e3a\nA. \u6218\u6597\u96c6\u4f53\u578b\u9886\u5bfc\nB. \u4ff1\u4e50\u90e8\u578b\u9886\u5bfc\nC. \u4efb\u52a1\u578b\u9886\u5bfc\nD. \u8d2b\u4e4f\u578b\u9886\u5bfc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9611\u5c3e\nA. \u7531\u56de\u80a0\u52a8\u8109\u76f4\u63a5\u8425\u517b\nB. \u4e09\u6761\u7ed3\u80a0\u5e26\u5747\u5728\u9611\u5c3e\u6839\u90e8\u96c6\u4e2d\nC. \u5c5e\u4e8e\u8179\u819c\u5916\u4f4d\u5668\u5b98\nD. \u65e0\u7cfb\u819c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33873922196938544, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.42212162698346284, "meta-llama/Meta-Llama-3-8B": 0.325455072595945, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7069593884050269}}, {"question": "\u653b\u51fb\u8005\u901a\u8fc7\u5bf9\u76ee\u6807\u4e3b\u673a\u8fdb\u884c\u7aef\u53e3\u626b\u63cf\uff0c\u53ef\u4ee5\u76f4\u63a5\u83b7\u5f97\nA. \u76ee\u6807\u4e3b\u673a\u7684\u53e3\u4ee4\nB. \u76ee\u6807\u4e3b\u673a\u4f7f\u7528\u4e86\u4ec0\u4e48\u64cd\u4f5c\u7cfb\u7edf\nC. \u7ed9\u76ee\u6807\u4e3b\u673a\u79cd\u690d\u6728\u9a6c\nD. \u76ee\u6807\u4e3b\u673a\u5f00\u653e\u4e86\u54ea\u4e9b\u7aef\u53e3\u670d\u52a1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8713536234702927, "meta-math/MetaMath-Mistral-7B": 0.971095283766545, "itpossible/Chinese-Mistral-7B-v0.1": 0.8001058351741857, "HuggingFaceH4/zephyr-7b-beta": 0.976474688805784, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6642185438126035, "meta-llama/Meta-Llama-3-8B": 0.9610317874540725, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9521257107898169}}, {"question": "\u5b87\u5b99\u5fae\u6ce2\u80cc\u666f\u8f90\u5c04\u7684\u8c31\u7ebf\u8f6e\u5ed3\u5f62\u72b6\u662f\nA. \u6b63\u5f26\u66f2\u7ebf\nB. \u9ed1\u4f53\u8c31\nC. \u5e42\u5f8b\u8c31\nD. \u629b\u7269\u7ebf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.689670084400338, "meta-math/MetaMath-Mistral-7B": 0.7989058520105061, "itpossible/Chinese-Mistral-7B-v0.1": 0.3233250233605477, "HuggingFaceH4/zephyr-7b-beta": 0.993678144008088, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9232126142142416, "meta-llama/Meta-Llama-3-8B": 0.7201172036540483, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9450248201027825}}, {"question": "\u6211\u56fd\u53e4\u4ee3\u5148\u8d24\u4e3b\u5f20\u201c\u4ec1\u8005\u7231\u4eba\u201d\uff0c\u5f3a\u8c03\u201c\u5df1\u6240\u4e0d\u6b32\uff0c\u52ff\u65bd\u4e8e\u4eba\u201d\uff0c\u201c\u5df1\u6b32\u7acb\u800c\u7acb\u4eba\uff0c\u5df1\u6b32\u8fbe\u800c\u8fbe\u4eba\u201d\u3002\u8fd9\u4e9b\u8bdd\u4f53\u73b0\u4e86\u4e2d\u534e\u6c11\u65cf\u4f20\u7edf\u7f8e\u5fb7\u4e2d\nA. \u7231\u56fd\u5949\u732e\uff0c\u4ee5\u5929\u4e0b\u4e3a\u5df1\u4efb\u7684\u7cbe\u795e\nB. \u5fb7\u6027\u4fee\u517b\uff0c\u91cd\u89c6\u8eac\u884c\u614e\u72ec\u7684\u7cbe\u795e\nC. \u4e50\u7fa4\u8d35\u548c\uff0c\u5f3a\u8c03\u4eba\u9645\u548c\u8c10\u7684\u7cbe\u795e\nD. \u52e4\u52b3\u52c7\u6562\uff0c\u8ffd\u6c42\u81ea\u7531\u89e3\u653e\u7684\u7cbe\u795e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9245675064169254}}, {"question": "\u903b\u8f91\u4e3b\u4e49\u7684\u4ee3\u8868\u4eba\u7269\u662f\nA. \u54e5\u5fb7\u5c14\nB. \u5e03\u52b3\u5a01\u5c14\nC. \u5e0c\u5c14\u4f2f\u7279\nD. \u7f57\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3220565245827037, "meta-math/MetaMath-Mistral-7B": 0.43771444552546424, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.44898421970539165, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.48872035215527276}}, {"question": "\u5728\u67d0\u4e9b\u6606\u866b\u7684\u5e7c\u866b\u4e2d\uff0c\u591a\u7ebf\u67d3\u8272\u4f53\u4ea7\u751f\u7684\u539f\u56e0\u662f\nA. \u5728\u67d3\u8272\u4f53\u590d\u5236\u4e4b\u524d\uff0c\u7ec6\u80de\u5206\u88c2\u53d1\u751f\u4e86\u51e0\u6b21\nB. \u67d3\u8272\u4f53\u53d1\u751f\u4e86\u590d\u5236\uff0c\u4f46\u6ca1\u6709\u53d1\u751f\u76f8\u5bf9\u5e94\u7684\u6838\u5206\u88c2\nC. \u4e00\u6761\u67d3\u8272\u4f53\u53d1\u751f\u65ad\u88c2\uff0c\u4e0e\u540c\u4e00\u6761\u67d3\u8272\u4f53\u7684\u53e6\u4e00\u90e8\u4f4d\u91cd\u65b0\u8fde\u63a5\nD. \u5728\u8fdb\u884c\u975e\u7b49\u4f4d\u4ea4\u6362\u65f6\uff0c\u4e00\u6761\u67d3\u8272\u4f53\u7684\u4e00\u5c0f\u90e8\u5206\u53d1\u751f\u4e86\u590d\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u5173\u539f\u53d1\u6027\u9ad8\u8840\u538b\u7684\u53d9\u8ff0\uff0c\u9519\u8bef\u7684\u662f\nA. \u9ad8\u8840\u538b\u665a\u671f\u7684\u80be\u810f\u5e38\u4e3a\u9897\u7c92\u6027\u56fa\u7f29\u80be\nB. \u9ad8\u8840\u538b\u53ef\u7ee7\u53d1\u7cd6\u5c3f\u75c5\nC. \u9ad8\u8840\u538b\u75c5\u5e38\u5f15\u8d77\u5de6\u5fc3\u5ba4\u80a5\u5927\nD. \u9ad8\u8840\u538b\u5e38\u5f15\u8d77\u4e0b\u80a2\u574f\u75bd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4582312054001781, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5839648299427213, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8618359652308925}}, {"question": "\u5cfb\u4e3d\uff0c\u9aa8\u529b\u52b2\u5065\u201d\u5f62\u5bb9\u7684\u662f\u5386\u53f2\u4e0a\u54ea\u4f4d\u4e66\u6cd5\u5bb6\u7684\u5b57\nA. \u738b\u7fb2\u4e4b\nB. \u5434\u9053\u5b50\nC. \u67f3\u516c\u6743\nD. \u989c\u771f\u537f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2904324311152722, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2810882504428991, "HuggingFaceH4/zephyr-7b-beta": 0.3810959068071204, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e1c\u6c49\u738b\u5145\u5728\u300a\u8bba\u8861\u300b\u4e2d\u8bf4\uff1a\u201c\u8427\u4f55\u5165\u79e6\uff0c\u6536\u62fe\u6587\u4e66\uff08\u56fd\u5bb6\u6863\u6848\u6587\u732e\uff09\uff0c\u6c49\u6240\u4ee5\u80fd\u5236\u4e5d\u5dde\u8005\uff0c\u6587\u4e66\u4e4b\u529b\u4e5f\u3002\u201d\u5176\u610f\u5728\u8bf4\u660e\uff0c\u897f\u6c49\u6210\u529f\u5730\u5b9e\u73b0\u5bf9\u5168\u56fd\u7684\u7edf\u6cbb\uff0c\u662f\u56e0\u4e3a\u6c49\u521d\nA. \u719f\u77e5\u79e6\u671d\u5178\u7ae0\u5236\u5ea6\nB. \u5145\u5206\u53d1\u6325\u6587\u4e66\u529f\u80fd\nC. \u4e86\u79e6\u671d\u7684\u57fa\u672c\u5236\u5ea6\nD. \u4e86\u5d07\u5c1a\u5112\u5bb6\u7684\u653f\u7b56\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u753b\u5bb6\u5bf9\u8272\u5f69\u6709\u6d53\u539a\u7684\u5174\u8da3\uff0c\u97f3\u4e50\u5bb6\u5bf9\u73b0\u5b9e\u4e2d\u7684\u97f3\u54cd\u975e\u5e38\u654f\u611f\uff0c\u96d5\u5851\u5bb6\u5bf9 \u975e\u5e38\u654f\u611f\u3002\nA. \u8272\u5f69\u548c\u7ebf\u6761\nB. \u7528\u6599\u548c\u5de5\u5177\nC. \u5f62\u8c61\u548c\u9020\u578b\nD. \u4f53\u79ef\u548c\u7a7a\u95f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6838\u80fd\u7684\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u4eba\u7c7b\u83b7\u5f97\u6838\u80fd\u6709\u4e24\u79cd\u9014\u5f84\u201d\u5373\u91cd\u6838\u88c2\u53d8\u548c\u8f7b\u6838\u805a\u53d8\nB. \u7269\u8d28\u662f\u7531\u539f\u5b50\u6784\u6210\u7684\uff0c\u539f\u5b50\u4e2d\u6709\u539f\u5b50\u6838\uff0c\u6240\u4ee5\u5229\u7528\u4efbI\u53ef\u7269\u8d28\u90fd\u80fd\u5f97\u5230\u6838\u80fd\nC. \u539f\u5b50\u5f39\u548c\u6c22\u5f39\u90fd\u662f\u5229\u7528\u539f\u5b50\u6838\u88c2\u53d8\u548c\u805a\u53d8\u7684\u539f\u7406\u5236\u6210\u7684\nD. \u81ea\u7136\u754c\u53ea\u6709\u5728\u4eba\u4e3a\u7684\u6761\u4ef6\u4e0b\u624d\u4f1a\u53d1\u751f\u88c2\u53d8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5482601385401136, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8120698456973621, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5974127774477938, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7866641637231536}}, {"question": "\u4e0b\u5217\u54ea\u4e00\u53e5\u8bd7\u63cf\u5199\u7684\u573a\u666f\u6700\u9002\u5408\u91c7\u7528\u6c34\u58a8\u753b\u6765\u8868\u73b0\nA. \u843d\u971e\u4e0e\u5b64\u9e5c\u9f50\u98de\uff0c\u79cb\u6c34\u5171\u957f\u5929\u4e00\u8272\nB. \u8fd4\u666f\u5165\u6df1\u6797\uff0c\u590d\u7167\u9752\u82d4\u4e0a\nC. \u63a5\u5929\u83b2\u53f6\u65e0\u7a77\u78a7\uff0c\u6620\u65e5\u8377\u82b1\u522b\u6837\u7ea2\nD. \u5b64\u821f\u84d1\u7b20\u7fc1\uff0c\u72ec\u9493\u5bd2\u6c5f\u96ea\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.29863342676099575, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3509604826644438, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7acb\u5fb7\u7c89ZnS\u00b7BaSO4\uff08\u4e5f\u79f0\u950c\u94a1\u767d\uff09\uff0c\u662f\u4e00\u79cd\u5e38\u7528\u767d\u8272\u989c\u6599\uff0c\u707c\u70e7\u7acb\u5fb7\u7c89\u6837\u54c1\u65f6\uff0c\u94a1\u7684\u7130\u8272\u4e3a\nA. \u9ec4\u8272\nB. \u7d2b\u8272\nC. \u7ea2\u8272\nD. \u7eff\u8272\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u5b66\u751f\u5728\u56de\u7b54\u201c\u4ec0\u4e48\u662f\u5706\u7684\u201d\u7684\u65f6\u5019\uff0c\u8bf4\u51fa\u201c\u592a\u9633\u662f\u5706\u7684\u201d\u201c\u8f66\u8f6e\u662f\u5706\u7684\u201d\u201c\u6bb7\u79c0\u6885\u5531\u6b4c\u7684\u65f6\u5019\u53e3 \u578b\u662f\u5706\u7684\u201d\u3002\u8fd9\u4e3b\u8981\u4f53\u73b0\u4e86\u53d1\u6563\u601d\u7ef4\u7684\u54ea\u4e00\u7279\u6027\nA. \u6d41\u7545\u6027\nB. \u7efc\u5408\u6027\nC. \u72ec\u7279\u6027\nD. \u591a\u7ef4\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39730625215953935, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3589842189651603, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u662f\u4e00\u4e9b\u56e0\u7279\u7f51\u4e0a\u5e38\u89c1\u7684\u6587\u4ef6\u7c7b\u578b\uff0c\u4e00\u822c\u4ee3\u8868WWW\u9875\u9762\u7684\u6587\u4ef6\u6269\u5c55\u540d\u662f\nA. wav\nB. htm\nC. gif\nD. txt\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9512968535574428, "meta-math/MetaMath-Mistral-7B": 0.9959762928168272, "itpossible/Chinese-Mistral-7B-v0.1": 0.9159728908674947, "HuggingFaceH4/zephyr-7b-beta": 0.9986616163626859, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9573326151457586, "meta-llama/Meta-Llama-3-8B": 0.9361716004558859, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9946525108748446}}, {"question": "\u7ebf\u6027\u56de\u5f52\u7684\u57fa\u672c\u5047\u8bbe\u4e0d\u5305\u62ec\u54ea\u4e2a\nA. \u5bf9\u4e8e\u89e3\u91ca\u53d8\u91cf\u7684\u6240\u6709\u89c2\u6d4b\u503c\uff0c\u968f\u673a\u8bef\u5dee\u9879\u6709\u76f8\u540c\u7684\u65b9\u5dee\nB. \u968f\u673a\u8bef\u5dee\u9879\u662f\u4e00\u4e2a\u671f\u671b\u503c\u4e3a0\u7684\u968f\u673a\u53d8\u91cf\nC. \u968f\u673a\u8bef\u5dee\u9879\u670d\u4ece\u6b63\u6001\u5206\u5e03\nD. \u968f\u673a\u8bef\u5dee\u9879\u5f7c\u6b64\u76f8\u5173\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7832769666148198, "meta-math/MetaMath-Mistral-7B": 0.9258568423086337, "itpossible/Chinese-Mistral-7B-v0.1": 0.759621924519737, "HuggingFaceH4/zephyr-7b-beta": 0.9987891100318935, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8504782716928969, "meta-llama/Meta-Llama-3-8B": 0.9443083915825868, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9854293909630396}}, {"question": "\u4e24\u6027\u5fc3\u7406\u5b66\u662f\u5bf9\u4f5c\u4e3a\u4e2a\u4f53\u751f\u7269\u5b66\u6027\u72b6\u7684\u7537\u5973\u5728\u793e\u4f1a\u5316\u6210\u957f\u8fdb\u7a0b\u4e2d\u6240\u53cd\u6620\u51fa\u6765\u7684\u4e24\u6027\u5fc3\u7406\u540c\u5f02\u6f14\u8fdb\u548c\u4e24\u6027\u884c\u4e3a\u53cd\u6620\u89c4\u5f8b\u8fdb\u884c\nA. \u4e13\u9898\u7814\u7a76\u7684\u5b66\u95ee\nB. \u5177\u4f53\u5206\u6790\u7684\u79d1\u5b66\nC. \u7cfb\u7edf\u7814\u7a76\u7684\u79d1\u5b66\nD. \u4e2a\u6848\u7814\u7a76\u7684\u79d1\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6301549205279136, "meta-math/MetaMath-Mistral-7B": 0.7766713204978515, "itpossible/Chinese-Mistral-7B-v0.1": 0.5605492408053316, "HuggingFaceH4/zephyr-7b-beta": 0.9848702983191747, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7674683504455379, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.906235399289796}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\uff0c\u6570\u5b57\u8868\u793a\u5206\u6570\u7684\u4e00\u53e5\u662f\nA. \u51e1\u8a69\u8ce6\u767e\u516d\u5bb6\uff0c\u5343\u4e09\u767e\u4e00\u5341\u516b\u7bc7\u3002\nB. \u7a3b\u82d7\u9577\u516d\u4e03\u5bf8\uff0c\u9673\u8349\u5fa9\u8d77\u3002\nC. \u662f\u4e43\u5176\u6240\u4ee5\u5343\u842c\u81e3\u800c\u7121\u6578\u8005\u4e5f\u3002\nD. \u4ec0\u4e00\uff0c\u53bb\u95dc\u5e02\u4e4b\u5fb5\uff0c\u4eca\u8332\u672a\u80fd\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.41829520677068827, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9ad8\u538b\u9505\u8981\u7ecf\u5e38\u68c0\u67e5\u6392\u6c14\u5b54\u662f\u5426\u7545\u901a\uff0c\u4e00\u822c()\u4e2a\u6708\u66f4\u6362\u4e00\u6b21\u6613\u7194\u7247\nA. 3\nB. 6\nC. 5\nD. 4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4123811341645335, "itpossible/Chinese-Mistral-7B-v0.1": 0.40870097338816025, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38277148274919337, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5089978812314824}}, {"question": "\u4e13\u5bb6\u5206\u6790\u6cd5\u53c8\u79f0\u4e3a\u5fb7\u5c14\u83f2\u6cd5\uff0c\u5b83\u7684\u9996\u8f6e\u8c03\u7814\u662f\nA. \u5f00\u653e\u5f0f\nB. \u91cd\u7533\u5f0f\nC. \u5c01\u95ed\u5f0f\nD. \u8bc4\u4ef7\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3876534932027888, "meta-math/MetaMath-Mistral-7B": 0.455276541486513, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.43387328161351885, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5382465318343895, "meta-llama/Meta-Llama-3-8B": 0.31911523504877376, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u540c\u91cf\u7684\u4e0b\u5217\u7269\u8d28\u5728\u4f53\u5185\u7ecf\u5f7b\u5e95\u6c27\u5316\u540e\uff0c\u91ca\u653e\u80fd\u91cf\u6700\u591a\u7684\u662f\nA. \u8461\u8404\u7cd6\nB. \u86cb\u767d\u8d28\nC. \u7cd6\u539f\nD. \u8102\u80aa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30856495466412287, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.864719068026787, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8065966787682091, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6132717898387673}}, {"question": "\u77f3\u6cb9\u6c14\u5177\u6709\u4e00\u5b9a\u7684\u6bd2\u6027\uff0c\u5728\u7a7a\u6c14\u4e2d\u6d53\u5ea6\u5927\u4e8e\uff08\uff09\u65f6\uff0c\u5219\u6709\u4f7f\u4eba\u4e2d\u6bd2\u7684\u5371\u9669\u3002\nA. 5%\nB. 10%\nC. 15%\nD. 20%\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.32082675033835323, "meta-math/MetaMath-Mistral-7B": 0.47432231883442416, "itpossible/Chinese-Mistral-7B-v0.1": 0.3534716292209113, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.35286194820709277, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe$z=(3-i)/(1+2i)$,\u5219|z|=\nA. 2\nB. 1\nC. sqrt{3}\nD. sqrt{2}\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5c5e\u4e8e\u8f93\u51fa\u8bbe\u5907\u7684\u662f\nA. \u663e\u793a\u5668\nB. \u952e\u76d8\nC. \u9f20\u6807\nD. \u626b\u63cf\u4eea\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9756577368573977, "meta-math/MetaMath-Mistral-7B": 0.9971332134306576, "itpossible/Chinese-Mistral-7B-v0.1": 0.9717494642864569, "HuggingFaceH4/zephyr-7b-beta": 0.9999665576901123, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9856567621302639, "meta-llama/Meta-Llama-3-8B": 0.9352197927952421, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.98648845234414}}, {"question": "\u58f0\u6ce2\u7531\u9f13\u819c\u7ecf\u542c\u9aa8\u94fe\u4f20\u5411\u5375\u5706\u7a97\u65f6\u51fa\u73b0\u7684\u632f\u52a8\u53d8\u5316\u662f\nA. \u5e45\u5ea6\u51cf\u5c0f\uff0c \u538b\u5f3a\u51cf\u5c0f\nB. \u5e45\u5ea6\u589e\u5927\uff0c \u538b\u5f3a\u589e\u5927\nC. \u5e45\u5ea6\u589e\u5927\uff0c \u538b\u5f3a\u51cf\u5c0f\nD. \u5e45\u5ea6\u51cf\u5c0f\uff0c \u538b\u5f3a\u589e\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e01\u4e01\u60f3\u4e0a\u7f51\u770b\u65b0\u95fb\uff0c\u4ed6\u9009\u7528\u4e0b\u9762\u54ea\u4e2a\u8f6f\u4ef6\u6bd4\u8f83\u5408\u9002\nA. PowerPoint \nB. Word\nC. Internet Explorer\nD. ACDSee\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9785435972754724, "meta-math/MetaMath-Mistral-7B": 0.99871293897633, "itpossible/Chinese-Mistral-7B-v0.1": 0.9754258127830335, "HuggingFaceH4/zephyr-7b-beta": 0.9953802513301, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.995464958134634, "meta-llama/Meta-Llama-3-8B": 0.9836988691304713, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9995110924543918}}, {"question": "\u65e0\u73af\u9e1f\u82f7(ACV)\u6709\u6548\u63a7\u5236\u5355\u7eaf\u75b1\u75b9\u75c5\u6bd2\u7684\u4f5c\u7528\u673a\u7406\nA. \u6291\u5236\u75c5\u6bd2DNA\u591a\u805a\u9176\u548cDNA\u5408\u6210\nB. \u6291\u5236\u75c5\u6bd2RNA\u805a\u5408\u9176\nC. \u6291\u5236\u654f\u611f\u7ec6\u80deDNA\u590d\u5236\nD. \u4f5c\u7528\u4e8e\u654f\u611f\u7ec6\u80de\u8868\u9762\u53d7\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.49594953322752444, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8291409807007465, "HuggingFaceH4/zephyr-7b-beta": 0.9907422509746691, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5011196952617933, "meta-llama/Meta-Llama-3-8B": 0.7542432538855612, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9703648673310151}}, {"question": "\u7535\u529b\u8c10\u6ce2\u5bf9\u65cb\u8f6c\u7535\u673a\u7684\u4e3b\u8981\u5371\u5bb3\u4e4b\u4e00\u662f\nA. \u5bfc\u81f4\u65cb\u8f6c\u7535\u673a\u7684\u8f6c\u901f\u4e0a\u5347\nB. \u5bfc\u81f4\u7cfb\u7edf\u7535\u538b\u548c\u9891\u7387\u53d8\u5316\nC. \u5bfc\u81f4\u65cb\u8f6c\u7535\u673a\u7684\u8f6c\u901f\u4e0b\u964d\nD. \u5bfc\u81f4\u7535\u673a\u94c1\u635f\u548c\u94dc\u635f\u589e\u52a0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5987343351106765, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6409876476286802}}, {"question": "\u60c5\u7eea\u4ea7\u751f\u7684\u57fa\u672c\u8fc7\u7a0b\u662f\u523a\u6fc0\u60c5\u666f\u2014\u8bc4\u4f30\u2014\u60c5\u7eea\u3002\u8fd9\u4e2a\u89c2\u70b9\u662f ()\u63d0\u51fa\u7684\u3002\nA. \u963f\u8bfa\u5fb7\nB. \u8a79\u59c6\u65af\nC. \u574e\u519c\nD. \u62c9\u624e\u52d2\u65af\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u901a\u5e38\u5c06\u4eba\u5bff\u4fdd\u9669\u3001\u767e\u79d1\u5168\u4e66\u8fd9\u7c7b\u6d88\u8d39\u8005\u4e0d\u4e86\u89e3\u6216\u5373\u4fbf\u4e86\u89e3\u4e5f\u6ca1\u6709\u5174\u8da3\u8d2d\u4e70\u7684\u7269\u54c1\u79f0\u4e3a\nA. \u9009\u8d2d\u54c1\nB. \u975e\u6e34\u6c42\u54c1\nC. \u7279\u6b8a\u54c1\nD. \u4fbf\u5229\u54c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6168905425595853, "meta-math/MetaMath-Mistral-7B": 0.9878257995500649, "itpossible/Chinese-Mistral-7B-v0.1": 0.6831232502127743, "HuggingFaceH4/zephyr-7b-beta": 0.995845459360709, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9726080088627879, "meta-llama/Meta-Llama-3-8B": 0.9014204501913459, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9989340546936253}}, {"question": "\u5728\u9a6c\u514b\u601d\u4e3b\u4e49\u6c11\u65cf\u7406\u8bba\u53d1\u5c55\u53f2\u4e0a\uff0c\u63d0\u51fa\u7b2c\u4e00\u4e2a\u5b8c\u6574\u7684\u79d1\u5b66\u7684\u6c11\u65cf\u5b9a\u4e49\u7684\u4eba\u662f\nA. \u65af\u5927\u6797\nB. \u6069\u683c\u65af\nC. \u5217\u5b81\nD. \u9a6c\u514b\u601d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ed3\u80a0\u764c\u60a3\u8005\u8840\u6e05 CEA \u68c0\u6d4b\u7684\u4e34\u5e8a\u610f\u4e49\uff0c\u4e0b\u5217\u63cf\u8ff0\u9519\u8bef\u7684\u662f\nA. \u672f\u540e CEA \u6301\u7eed\u5347\u9ad8\u63d0\u793a\u80bf\u7624\u590d\u53d1\nB. CEA \u7684\u9633\u6027\u7387\u4e0e\u7ed3\u80a0\u764c\u7684\u5206\u671f\u6709\u5173\nC. \u672f\u540e CEA \u672a\u660e\u663e\u4e0b\u964d\u8bf4\u660e\u53ef\u80fd\u6709\u80bf\u7624\u7684\u6b8b\u7559\nD. \u672f\u524d CEA \u6307\u6807\u6b63\u5e38\u662f\u7f29\u5c0f\u5207\u9664\u8303\u56f4\u7684\u4f9d\u636e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5816502116738543, "meta-math/MetaMath-Mistral-7B": 0.8563908261589264, "itpossible/Chinese-Mistral-7B-v0.1": 0.49449571405614084, "HuggingFaceH4/zephyr-7b-beta": 0.9988798321635483, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6734166670229816, "meta-llama/Meta-Llama-3-8B": 0.5364815755964616, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8627731086632965}}, {"question": "\u5929\u4f53\u5468\u65e5\u89c6\u8fd0\u52a8\u8fc7\u7a0b\u4e2d\u8fbe\u5230\u201c\u4e0a\u4e2d\u5929\u201d\u65f6\uff0c\u5b83\u4e00\u5b9a\nA. \u65b9\u4f4d\u89d2\u4e3a180\u00b0\nB. \u5728\u5730\u5e73\u7ebf\u4ee5\u4e0a\nC. \u8d64\u7ecf\u4e3a0\nD. \u5730\u5e73\u9ad8\u5ea6\u8fbe\u5230\u6700\u5927\u503c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.365287830104264, "meta-math/MetaMath-Mistral-7B": 0.5925811640558988, "itpossible/Chinese-Mistral-7B-v0.1": 0.4489842344514253, "HuggingFaceH4/zephyr-7b-beta": 0.8834334601785, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5271313420761637, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6211\u56fd\uff0c\u6cd5\u5f8b\u76d1\u7763\u7684\u4e13\u95e8\u673a\u5173\u662f\nA. \u56fd\u5bb6\u76d1\u5bdf\u673a\u5173\nB. \u56fd\u5bb6\u68c0\u5bdf\u673a\u5173\nC. \u56fd\u5bb6\u5ba1\u5224\u673a\u5173\nD. \u56fd\u5bb6\u6743\u529b\u673a\u5173\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5212003867219582, "meta-math/MetaMath-Mistral-7B": 0.8956046239426322, "itpossible/Chinese-Mistral-7B-v0.1": 0.7750097092312159, "HuggingFaceH4/zephyr-7b-beta": 0.7242725650613496, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6383691308449692, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b66\u6821\u6559\u80b2\u4ea7\u751f\u4e8e\nA. \u539f\u59cb\u793e\u4f1a\nB. \u8d44\u672c\u4e3b\u4e49\u793e\u4f1a\nC. \u5c01\u5efa\u793e\u4f1a\nD. \u5974\u96b6\u793e\u4f1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5de5\u7a0b\u9879\u76ee\u57fa\u7840\u90e8\u4f4d\u7684\u8d28\u91cf\u9a8c\u6536\u8bc1\u8bc1\u660e\u5e94\u7531\u54ea\u4e2a\u62a5\u9001\u8d28\u91cf\u76d1\u7763\u673a\u6784\u5907\u6848\nA. \u76d1\u7406\u5355\u4f4d\nB. \u52d8\u5bdf\u5355\u4f4d\nC. \u65bd\u5de5\u5355\u4f4d\nD. \u5efa\u8bbe\u5355\u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3233250233605477, "itpossible/Chinese-Mistral-7B-v0.1": 0.388225909182026, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4101046147696095, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8de8\u56fd\u516c\u53f8\u4f26\u7406\u5173\u7cfb\u6574\u5408\u7684\u5173\u952e\u662f\nA. \u5316\u89e3\u77db\u76fe\u51b2\u7a81\nB. \u5316\u89e3\u6587\u5316\u51b2\u7a81\nC. \u5316\u89e3\u5229\u76ca\u51b2\u7a81\nD. \u5316\u89e3\u8d38\u6613\u51b2\u7a81\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4617968711091074, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5510\u8bd7\u201c\u6162\u675f\u7f57\u88d9\u534a\u9732\u80f8\u201d\u3001\u201c\u7eee\u7f57\u7ea4\u7f15\u89c1\u808c\u80a4\u201d\uff0c\u63cf\u5199\u4e86\u5510\u671d\u670d\u9970\u7684\u8273\u4e3d\u5962\u534e\u548c\u5f00\u653e\u3002\u4f46\u5b8b\u671d\u65f6\u7684\u670d\u9970\u5374\u7b80\u6d01\u8d28\u6734\uff0c\u5973\u88c5\u62d8\u8c28\u3001\u4fdd\u5b88\uff0c\u8272\u5f69\u6de1\u96c5\u606c\u9759\u3002\u5510\u5b8b\u670d\u9970\u7684\u6f14\u53d8\uff0c\u53cd\u6620\u4e86\u4eba\u4eec\u5ba1\u7f8e\u89c2\u7684\u53d8\u5316\uff0c\u6b64\u79cd\u5ba1\u7f8e\u89c2\u7684\u8f6c\u53d8\u4e3b\u8981\u662f\u7531\u4e8e\nA. \u5c01\u5efa\u7ecf\u6d4e\u7684\u8870\u9000\nB. \u5ba1\u7f8e\u610f\u8bc6\u7684\u89c9\u9192\nC. \u7a0b\u6731\u7406\u5b66\u601d\u60f3\u7684\u5f71\u54cd\nD. \u4e13\u5236\u96c6\u6743\u7684\u5de9\u56fa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u6cb9\u6f06\u751f\u4ea7\u4f01\u4e1a\u4e3a\u4e86\u751f\u4ea7\u7ecf\u8425\u7684\u9700\u8981\uff0c\u6536\u8d2d\u4e8620\u5bb6\u6cb9\u6f06\u7ecf\u9500\u5546\u5e97\uff0c\u8be5\u589e\u957f\u6218\u7565\u5c5e\u4e8e\nA. \u540e\u5411\u4e00\u4f53\u5316\nB. \u6c34\u5e73\u591a\u5143\u5316\nC. \u540c\u5fc3\u591a\u5143\u5316\nD. \u524d\u5411\u4e00\u4f53\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.45092008750945856, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7734734645681532}}, {"question": "\u5f3a\u8c03\u53d1\u5c55\u4e2d\u56fd\u5bb6\u7ecf\u6d4e\u4e2d\u7279\u6709\u7684\u5e02\u573a\u4e0d\u5b8c\u5168\u6027\u3001\u521a\u6027\u3001\u77ed\u7f3a\u3001\u8fc7\u5269\u3001\u4f4e\u4f9b\u7ed9\u5f39\u6027\u7b49\u56fa\u6709\u7ecf\u6d4e\u7279\u70b9\uff0c\u8fd9\u6b63\u662f\u53d1\u5c55\u4e2d\u56fd\u5bb6\u7ecf\u6d4e\u73b0\u5b9e\u4e0e\u897f\u65b9\u4f20\u7edf\u7ecf\u6d4e\u7406\u8bba\u7684\nA. \u57fa\u672c\u76f8\u540c\u4e4b\u5904\nB. \u91cd\u8981\u533a\u522b\nC. \u6beb\u4e0d\u76f8\u5e72\u7684\u8bba\u8ff0\nD. \u5185\u5728\u8054\u7cfb\u7684\u8868\u73b0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9256738930551837, "meta-math/MetaMath-Mistral-7B": 0.9910966005130059, "itpossible/Chinese-Mistral-7B-v0.1": 0.8661953382048043, "HuggingFaceH4/zephyr-7b-beta": 0.9952636932929064, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9156519784728733, "meta-llama/Meta-Llama-3-8B": 0.8142747141217006, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9497645437323002}}, {"question": "\u4e0b\u5217\u56e0\u7d20\u4e2d\uff0c\u4e0d\u4f1a\u5f71\u54cd\u5f2f\u6c89\u68c0\u6d4b\u503c\u5927\u5c0f\u7684\u56e0\u7d20\u662f\nA. \u516c\u8def\u7b49\u7ea7\nB. \u9762\u5c42\u548c\u57fa\u5c42\u7c7b\u578b\nC. \u6c14\u6e29\nD. \u6d4b\u8bd5\u8f66\u540e\u8f74\u91cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.34986939917036525, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u8fdb\u884cWilcoxon\u914d\u5bf9\u6cd5\u79e9\u548c\u68c0\u9a8c\u65f6\uff0c\u4ee5\u4e0b\u4f55\u79cd\u68c0\u9a8c\u5047\u8bbe\u662f\u6b63\u786e\u7684\nA. H_0:\u5dee\u503c\u7684\u603b\u4f53\u4e2d\u4f4d\u6570\u4e3a0\nB. H_0:\u4e24\u6837\u672c\u5bf9\u5e94\u603b\u4f53\u7684\u4e2d\u4f4d\u6570\u76f8\u540c\nC. H_0:\u4e24\u6837\u672c\u5747\u6570\u76f8\u540c\nD. H_0:\u4e24\u6837\u672c\u5bf9\u5e94\u7684\u603b\u4f53\u5206\u5e03\u76f8\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7343739775143555, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9114187530779742, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5185\u6e7f\u7684\u4ea7\u751f\u4e0e\u54ea\u4e00\u810f\u7684\u5173\u7cfb\u6700\u4e3a\u5bc6\u5207\nA. \u80ba\nB. \u809d\nC. \u80be\nD. \u813e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5579784254483179, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.45287282330589346, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4ee5\u4e0b\u98df\u7269\u4e2d\u9971\u548c\u8102\u80aa\u9178\u542b\u91cf\u6700\u4f4e\u7684\u6cb9\u8102\u662f\nA. \u725b\u6cb9\nB. \u9c7c\u6cb9\nC. \u732a\u6cb9\nD. \u7f8a\u6cb9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41407897985311565, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7000143059908333, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9270592212027667, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9997268329689524}}, {"question": "\u4e0b\u5217\u4e2d\u6210\u836f\u8054\u5408\u7528\u836f\u5408\u7406\u7684\u662f\nA. \u73cd\u83ca\u964d\u538b\u7247\u4e0e\u590d\u65b9\u6787\u6777\u7cd6\u6d46\nB. \u5929\u9ebb\u4e38\u4e0e\u5ddd\u8d1d\u6787\u6777\u80f6\u56ca\nC. \u82cf\u5408\u9999\u4e38\u4e0e\u80c6\u5b81\u7247\nD. \u4e8c\u9648\u4e38\u4e0e\u5e73\u80c3\u6563\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.4588123566081842, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u201c\u8863\u670d\u201d\u4e0d\u5b9a\u6307\u7684\u4e00\u9879\u662f\nA. \u6211\u7684\u8863\u670d\u5462\uff1f\nB. \u5c0f\u5fc3\uff0c\u522b\u628a\u8863\u670d\u5f04\u810f\u4e86\nC. \u8863\u670d\u6d17\u597d\u667e\u51fa\u53bb\u4e86\nD. \u5976\u5976\u9001\u7ed9\u5c0f\u6885\u4e24\u4ef6\u8863\u670d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.31712010892822357, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u660e\u672b\u6e05\u521d\uff0c\u738b\u592b\u4e4b\u4e3b\u5f20\u201c\u6709\u5176\u529b\u8005\u6cbb\u5176\u5730\u201d\uff0c\u9ec4\u5b97\u7fb2\u63d0\u51fa\u201c\u6388\u6c11\u4ee5\u7530\u201d\u201c\u7530\u571f\u5747\u4e4b\u201d\uff0c\u987e\u708e\u6b66\u529b\u4e3b\u201c\u5747\u7530\u201d\uff0c\u8fd9\u4e9b\u4e3b\u5f20\nA. \u76ee\u7684\u662f\u7ef4\u62a4\u6e05\u671d\u7edf\u6cbb\nB. \u8868\u8fbe\u4e86\u5bf9\u4e2a\u4f53\u5c0f\u519c\u7684\u5173\u6ce8\nC. \u4f53\u73b0\u4e86\u91cd\u519c\u6291\u5546\u601d\u60f3\nD. \u5177\u6709\u6d53\u539a\u7684\u590d\u53e4\u8272\u5f69\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6118359879614661, "meta-math/MetaMath-Mistral-7B": 0.8823734615588656, "itpossible/Chinese-Mistral-7B-v0.1": 0.46683855109474853, "HuggingFaceH4/zephyr-7b-beta": 0.9824163925715274, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5269670394864534, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1938\u5e741\u6708\uff0c\u56fd\u6c11\u653f\u5e9c\u4e3a\u7ba1\u7406\u5168\u56fd\u7ecf\u6d4e\u4e8b\u52a1\uff0c\u5c06\u5b9e\u4e1a\u90e8\u6539\u7ec4\u4e3a\u7ecf\u6d4e\u90e8\uff0c\u5176\u804c\u6743\u8303\u56f4\u5305\u62ec\u4e86\u6218\u65f6\u4e3b\u8981\u751f\u4ea7\u9886\u57df\u548c\u5546\u4e1a\u9886\u57df\uff0c\u4e0b\u5c5e\u673a\u6784\u4f17\u591a\u3002\u7ecf\u6d4e\u90e8\u7684\u8bbe\u7acb\nA. \u6807\u5fd7\u7740\u5de5\u4f5c\u91cd\u5fc3\u8f6c\u5411\u7ecf\u6d4e\u9886\u57df\nB. \u9002\u5e94\u4e86\u6218\u4e89\u6301\u4e45\u5316\u7684\u9700\u8981\nC. \u63d0\u9ad8\u4e86\u540e\u65b9\u767e\u59d3\u751f\u6d3b\u6c34\u5e73\nD. \u5bfc\u81f4\u6297\u6218\u65f6\u671f\u884c\u653f\u6548\u7387\u7684\u4f4e\u4e0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4343032251852025, "meta-math/MetaMath-Mistral-7B": 0.4929727322630045, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.75703533249083, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6f0f\u51fa\u6027\u51fa\u8840\u7684\u53d1\u751f\u673a\u5236\u662f\u7531\u4e8e\nA. \u8840\u7ba1\u58c1\u5468\u56f4\u75c5\u53d8\u4fb5\u72af\nB. \u6bdb\u7ec6\u8840\u7ba1\u58c1\u901a\u900f\u6027\u589e\u52a0\nC. \u7ec4\u7ec7\u7684\u6e17\u900f\u538b\u589e\u9ad8\nD. \u8840\u6d46\u80f6\u4f53\u6e17\u900f\u538b\u964d\u4f4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.485129053173459, "meta-math/MetaMath-Mistral-7B": 0.8496034578157672, "itpossible/Chinese-Mistral-7B-v0.1": 0.5436660948320693, "HuggingFaceH4/zephyr-7b-beta": 0.8282853585757359, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8175216824726387, "meta-llama/Meta-Llama-3-8B": 0.3860362588781417, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7533158802115971}}, {"question": "\u5173\u4e8e\u98df\u7ba1\u88c2\u5b54\nA. \u6709\u8ff7\u8d70\u795e\u7ecf\u901a\u8fc7\nB. \u5e73\u7b2c\u5341\u80f8\u690e\u6c34\u5e73\nC. \u6709\u80f8\u5bfc\u7ba1\u901a\u8fc7\nD. \u4f4d\u4e8e\u4e2d\u5fc3\u8171\u4e0a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.32082675033835323, "meta-math/MetaMath-Mistral-7B": 0.42344259394019673, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.44432870984215717, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3534716292209113, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9047\u4f24\u8005\u88ab\u538b\u4e8e\u8f66\u8f6e\u6216\u8d27\u7269\u4e0b\u65f6\uff0c\u9519\u8bef\u7684\u65b9\u6cd5\u662f\nA. \u62c9\u66f3\u4f24\u8005\u7684\u80a2\u4f53\nB. \u642c\u6389\u8d27\u7269\nC. \u8bbe\u6cd5\u79fb\u52a8\u8f66\u8f86\nD. \u91c7\u53d6\u76f8\u5e94\u7684\u6551\u62a4\u65b9\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3993599311464796, "meta-math/MetaMath-Mistral-7B": 0.9358193609139428, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.997858232708932, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4859710239932271}}, {"question": "\u5173\u4e8e\u6570\u7f6a\u5e76\u7f5a\uff0c\u4e0b\u5217\u8bf4\u6cd5\u9519\u8bef\u7684\u662f\nA. \u6570\u7f6a\u4e2d\u65e2\u6709\u88ab\u5224\u5904\u6709\u671f\u5f92\u5211\uff0c\u4e5f\u6709\u88ab\u5224\u5904\u7ba1\u5236\uff0c\u5728\u6709\u671f\u5f92\u5211\u6267\u884c\u5b8c\u6bd5\u540e\u7ee7\u7eed\u6267\u884c\u7ba1\u5236\nB. \u5728\u5211\u7f5a\u6267\u884c\u5b8c\u6bd5\u4ee5\u524d\u53c8\u72af\u7f6a\u7684\uff0c\u5e94\u5f53\u6309\u7167\u201c\u5148\u51cf\u540e\u5e76\u201d\u7684\u539f\u5219\u5b9e\u884c\u6570\u7f6a\u5e76\u7f5a\nC. \u5728\u5211\u7f5a\u6267\u884c\u5b8c\u6bd5\u4ee5\u524d\u53d1\u73b0\u6f0f\u7f6a\u7684\uff0c\u5e94\u5f53\u6309\u7167\u201c\u5148\u5e76\u540e\u51cf\u201d\u7684\u539f\u5219\u5b9e\u884c\u6570\u7f6a\u5e76\u7f5a\nD. \u5728\u5211\u7f5a\u6267\u884c\u5b8c\u6bd5\u4e4b\u540e\u53d1\u73b0\u6f0f\u7f6a\u7684\uff0c\u5e94\u5f53\u6309\u7167\u201c\u5148\u5e76\u540e\u51cf\u201d\u7684\u539f\u5219\u5b9e\u884c\u6570\u7f6a\u5e76\u7f5a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7563889565646142, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u8bf4\u6cd5\u6b63\u786e\u7684\u9009\u9879\u662f\nA. \u5730\u7403\u540c\u6b65\u901a\u8baf\u536b\u661f\u7684\u8f68\u9053\u53ef\u4ee5\u662f\u5706\u7684\u4e5f\u53ef\u4ee5\u662f\u692d\u5706\u7684\nB. \u7b2c\u4e00\u5b87\u5b99\u901f\u5ea6\u5927\u5c0f\u662f11.2\u339e/s\nC. \u5982\u679c\u9700\u8981\uff0c\u5730\u7403\u540c\u6b65\u901a\u8baf\u536b\u661f\u53ef\u4ee5\u5b9a\u70b9\u5728\u5730\u7403\u4e0a\u7a7a\u7684\u4efb\u4f55\u4e00\u70b9\nD. \u7b2c\u4e00\u5b87\u5b99\u901f\u5ea6\u662f\u4eba\u9020\u536b\u661f\u5728\u5730\u9762\u9644\u8fd1\u7ed5\u5730\u7403\u505a\u5300\u901f\u5706\u5468\u8fd0\u52a8\u6240\u5fc5\u987b\u5177\u6709\u7684\u901f\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39739914771396695, "meta-math/MetaMath-Mistral-7B": 0.42876488826381753, "itpossible/Chinese-Mistral-7B-v0.1": 0.555649968753704, "HuggingFaceH4/zephyr-7b-beta": 0.9864412606846574, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8277412945103777, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8472917650377111}}, {"question": "\u5173\u4e8e\u529f\u548c\u80fd\u7684\u8054\u7cfb\u548c\u533a\u522b\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u4e0d\u6b63\u786e\u7684\u662f\nA. \u529f\u662f\u8fc7\u7a0b\u91cf\uff0c\u80fd\u662f\u72b6\u6001\u91cf\nB. \u529f\u662f\u80fd\u91cf\u8f6c\u5316\u7684\u91cf\u5ea6\nC. \u505a\u529f\u7684\u8fc7\u7a0b\u603b\u5bf9\u5e94\u7740\u80fd\u91cf\u7684\u8f6c\u5316\u8fc7\u7a0b\nD. \u529f\u5c31\u662f\u80fd\uff0c\u80fd\u5c31\u662f\u529f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7662981379978866, "meta-math/MetaMath-Mistral-7B": 0.9517298264484855, "itpossible/Chinese-Mistral-7B-v0.1": 0.7869431304440002, "HuggingFaceH4/zephyr-7b-beta": 0.9987551895430657, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9536946532360767, "meta-llama/Meta-Llama-3-8B": 0.9317162895166567, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8704160386242369}}, {"question": "\u540c\u4e00\u79cd\u98df\u54c1\u5728\u8fdb\u884c\u70ed\u4ea4\u6362\u8fc7\u7a0b\u4e2d\uff0c\u70ed\u4ea4\u6362\u901f\u5ea6\u6700\u5feb\u7684\u662f\nA. \u4e0d\u53d7\u5f62\u72b6\u9650\u5236\nB. \u9897\u7c92\u578b\nC. \u8584\u7247\u72b6\nD. \u6761\u578b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.29068935354339714, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6429366559008652}}, {"question": "\u5728\u6c11\u65cf\u53d1\u5c55\u4e2d\uff0c\u201c\u5f02\u6e90\u540c\u6d41\u201d\u73b0\u8c61\u5c5e\u4e8e\nA. \u6c11\u65cf\u5206\u5316\nB. \u6c11\u65cf\u81ea\u7136\u540c\u5316\nC. \u6c11\u65cf\u5f3a\u8feb\u540c\u5316\nD. \u6c11\u65cf\u7ec4\u5408\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6078066170542094, "meta-math/MetaMath-Mistral-7B": 0.5947869525409518, "itpossible/Chinese-Mistral-7B-v0.1": 0.36727669637604776, "HuggingFaceH4/zephyr-7b-beta": 0.792055862366266, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6309\u7167\u793e\u4f1a\u53d8\u8fc1\u7684\u4eba\u4e3a\u53c2\u4e0e\u548c\u63a7\u5236\u7a0b\u5ea6\uff0c\u793e\u4f1a\u53d8\u8fc1\u53ef\u4ee5\u5206\u4e3a\nA. \u6574\u4f53\u53d8\u8fc1\u4e0e\u5c40\u90e8\u53d8\u8fc1\nB. \u81ea\u53d1\u53d8\u8fc1\u4e0e\u6709\u8ba1\u5212\u53d8\u8fc1\nC. \u793e\u4f1a\u8fdb\u6b65\u4e0e\u793e\u4f1a\u5012\u9000\nD. \u793e\u4f1a\u6539\u9769\u4e0e\u793e\u4f1a\u9769\u547d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7176399823040535, "meta-math/MetaMath-Mistral-7B": 0.7890440303859095, "itpossible/Chinese-Mistral-7B-v0.1": 0.5557113205012725, "HuggingFaceH4/zephyr-7b-beta": 0.9425989954304131, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8754137567888535, "meta-llama/Meta-Llama-3-8B": 0.538583667435727, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.758998895710661}}, {"question": "\u636e\u300a\u7d20\u95ee\u00b7\u4e3e\u75db\u8bba\u300b\uff0c\u60ca\u5219\nA. \u6c14\u7f13\nB. \u6c14\u4e0a\nC. \u6c14\u7ed3\nD. \u6c14\u4e71\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.371068906204966, "HuggingFaceH4/zephyr-7b-beta": 0.4422988238069945, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.30601362565976303, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u822c\u8ba4\u4e3a\uff0c\u793e\u4f1a\u5b66\u8005\u5bf9\u4e8e\u793e\u533a\u7684\u7c7b\u578b\u5b66\u7814\u7a76\u59cb\u4e8e\u5fb7\u56fd\u793e\u4f1a\u5b66\u5bb6\nA. \u6ed5\u5c3c\u65af\nB. \u4f2f\u6770\u65af\nC. \u97e6\u4f2f\nD. \u6797\u5fb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6372737911588504, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u9897\u4f4d\u4e8e\u67d0\u661f\u7cfb\uff08\u7ea61000kpc\uff09\u4e2d\u7684\u8d85\u5de8\u661f\u7684\u7edd\u5bf9\u661f\u7b49\u4e3a-8\uff0c\u90a3\u4e48\u5728\u5730\u7403\u4e0a\u89c2\u6d4b\u5176\u89c6\u661f\u7b49\u4e3a\nA. 18\nB. 20\nC. 8\nD. 10\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.37286418926012416, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u516c\u5171\u5173\u7cfb\u901a\u8fc7\u5411\u793e\u4f1a\u704c\u8f93\u5f3a\u70c8\u7684\uff08\uff09\u548c\u9ad8\u5ea6\u7684\u8d23\u4efb\u611f\uff0c\u4ee5\u589e\u8fdb\u793e\u4f1a\u4ea4\u5f80\u3001\u4fc3\u4f7f\u56e2\u4f53\u5408\u4f5c\u7b49\u6d3b\u52a8\u6765\u4fc3\u4f7f\u793e\u4f1a\u884c\u4e3a\u5f97\u5230\u534f\u8c03\u3002\nA. \u4e2a\u4eba\u610f\u8bc6\nB. \u96c6\u4f53\u610f\u8bc6\nC. \u793e\u4f1a\u610f\u8bc6\nD. \u73af\u5883\u610f\u8bc6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6cd5\u7684\u6eaf\u53ca\u529b\uff0c\u5404\u56fd\u91c7\u7528\u7684\u901a\u4f8b\u4e3a\nA. \u4ece\u65b0\u517c\u4ece\u91cd\nB. \u4ece\u65e7\u517c\u4ece\u8f7b\nC. \u4ece\u65b0\u517c\u4ece\u8f7b\nD. \u4ece\u65e7\u517c\u4ece\u91cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3828200001063987, "meta-math/MetaMath-Mistral-7B": 0.588191546041739, "itpossible/Chinese-Mistral-7B-v0.1": 0.3499320087587727, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.36463638986032215, "meta-llama/Meta-Llama-3-8B": 0.34031470637689565, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6605489471380294}}, {"question": "\u4e0b\u5217\u4e2d\u5b66\u5e38\u89c1\u5b9e\u9a8c\u7684\u73b0\u8c61\u6216\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u5411CuSO4\u6eb6\u6db2\u4e2d\u6ef4\u5165\u8fc7\u91cf\u7684NaOH\u6eb6\u6db2\u5145\u5206\u53cd\u5e94\u540e\uff0c\u5c06\u6df7\u5408\u6db2\u4f53\u5012\u5165\u84b8\u53d1\u76bf\u4e2d\u52a0\u70ed\u716e\u6cb8\u4e00\u4f1a\uff0c\u7136\u540e\u51b7\u5374\u3001\u8fc7\u6ee4\uff0c\u6ee4\u7eb8\u4e0a\u7684\u7269\u4f53\u4e3a\u201c\u84dd\u8272\u56fa\u4f53\u201d\nB. \u5236\u5907\u6c22\u6c27\u5316\u4e9a\u94c1\u65f6\uff0c\u5411\u786b\u9178\u4e9a\u94c1\u6eb6\u6db2\u4e2d\u6ef4\u52a0\u6c22\u6c27\u5316\u94a0\u6eb6\u6db2\uff0c\u8fb9\u52a0\u8fb9\u6405\u62cc\uff0c\u5373\u53ef\u5236\u5f97\u767d\u8272\u7684\u6c22\u6c27\u5316\u4e9a\u94c1\nC. \u68c0\u9a8c\u7ea2\u7816\u4e2d\u7684\u6c27\u5316\u94c1\u6210\u5206\uff0c\u5411\u7ea2\u7816\u7c89\u672b\u4e2d\u52a0\u5165\u76d0\u9178\uff0c\u5145\u5206\u53cd\u5e94\u540e\u53d6\u4e0a\u5c42\u6e05\u6db2\u4e8e\u8bd5\u7ba1\u4e2d\uff0c\u6ef4\u52a0KSCN\u6eb6\u6db22\uff5e3\u6ef4\uff0c\u6eb6\u6db2\u663e\u7ea2\u8272\u5373\u53ef\u8bc1\u660e\nD. \u8fc7\u91cf\u7684\u94c1\u6295\u5165\u5230\u4e00\u5b9a\u91cf\u7684\u7a00\u785d\u9178\u4e2d\uff0c\u5145\u5206\u53cd\u5e94\u540e\u53d6\u4e0a\u5c42\u6e05\u6db2\u4e8e\u8bd5\u7ba1\u4e2d\uff0c\u6ef4\u52a0KSCN\u6eb6\u6db2\uff0c\u6eb6\u6db2\u663e\u7ea2\u8272\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.30239379048437676, "HuggingFaceH4/zephyr-7b-beta": 0.5228570158904099, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.396720449250724, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2018\u5e74\u5168\u7403\u8303\u56f4\u5185\u53ef\u89c1\uff08\uff09\u6b21\u65e5\u504f\u98df\uff0c\u800c\u6211\u56fd\u53ef\u4ee5\u89c2\u6d4b\u5230\u5176\u4e2d\u7684\uff08\uff09\u6b21\nA. 3\uff0c1\nB. 2\uff0c0\nC. 4\uff0c2\nD. 2\uff0c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2022\u5e742\u670828\u65e5\uff0c\u56fd\u5bb6\u79d1\u5b66\u6280\u672f\u90e8\u9ad8\u6280\u672f\u7814\u7a76\u53d1\u5c55\u4e2d\u5fc3\u53d1\u5e03\u201c2021\u5e74\u5ea6\u4e2d\u56fd\u79d1\u5b66\u5341\u5927\u8fdb\u5c55\u201d\uff0c\u5176\u4e2d\u5929\u6587\u4e0e\u822a\u5929\u65b9\u9762\u7684\u9879\u76ee\u5171\u67094\u9879\u8fdb\u699c\u5355\uff0c\u8fd9\u4e0d\u5305\u62ec\nA. \u9ad8\u6d77\u62d4\u5b87\u5b99\u7ebf\u89c2\u6d4b\u7ad9\uff08LHAASO\uff09\u9996\u6b21\u53d1\u73b0\u8d85\u5343\u4e07\u4ebf\u7535\u5b50\u4f0f\u7684\u9ad8\u80fd\u5149\u5b50\nB. \u706b\u661f\u63a2\u6d4b\u4efb\u52a1\u5929\u95ee\u4e00\u53f7\u63a2\u6d4b\u5668\u6210\u529f\u7740\u9646\u706b\u661f\nC. \u5ae6\u5a25\u4e94\u53f7\u6708\u7403\u6837\u54c1\u63ed\u793a\u6708\u7403\u6f14\u5316\u5965\u79d8\nD. FAST\u6355\u83b7\u4e16\u754c\u6700\u5927\u5feb\u901f\u5c04\u7535\u66b4\u6837\u672c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3017444864199176, "itpossible/Chinese-Mistral-7B-v0.1": 0.3650642813490256, "HuggingFaceH4/zephyr-7b-beta": 0.4897017878809109, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6537734418294702, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.34966087665839624}}, {"question": "\u5546\u54c1\u751f\u4ea7\u8005\u8981\u83b7\u5f97\u66f4\u591a\u6536\u76ca\u5fc5\u987b\u4f7f\u751f\u4ea7\u5546\u54c1\u7684\nA. \u4e2a\u522b\u52b3\u52a8\u65f6\u95f4\u5c0f\u4e8e\u793e\u4f1a\u5fc5\u8981\u52b3\u52a8\u65f6\u95f4\nB. \u4e2a\u522b\u52b3\u52a8\u65f6\u95f4\u7b49\u4e8e\u500d\u52a0\u7684\u793e\u4f1a\u5fc5\u8981\u52b3\u52a8\u65f6\u95f4\nC. \u4e2a\u522b\u52b3\u52a8\u65f6\u95f4\u5927\u4e8e\u793e\u4f1a\u5fc5\u8981\u52b3\u52a8\u65f6\u95f4\nD. \u4e2a\u522b\u52b3\u52a8\u65f6\u95f4\u7b49\u4e8e\u793e\u4f1a\u5fc5\u8981\u52b3\u52a8\u65f6\u95f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53d1\u97f3\u5668\u5b98\u4e2d\u8d77\u5171\u9e23\u4f5c\u7528\u7684\u662f\nA. \u80ba\u548c\u6c14\u7ba1\nB. \u5589\u5934\u548c\u58f0\u5e26\nC. \u53e3\u8154\u548c\u9f3b\u8154\nD. \u80f8\u8154\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7954281909148398, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.527642960605542, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8c03\u8282\u4f26\u7406\u5173\u7cfb\u7684\u5f3a\u5236\u624b\u6bb5\u548c\u975e\u5f3a\u5236\u624b\u6bb5\u4e3b\u8981\u6307\u7684\u662f\nA. \u6cd5\u5f8b\u4e0e\u9053\u5fb7\nB. \u653f\u6cbb\u4e0e\u6cd5\u5f8b\nC. \u6cd5\u5f8b\u4e0e\u6587\u5316\nD. \u653f\u6cbb\u4e0e\u9053\u5fb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5009036027231616, "meta-math/MetaMath-Mistral-7B": 0.7524679670633587, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7753368361154985, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9218241222000616, "meta-llama/Meta-Llama-3-8B": 0.48390568165862735, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9629877070311865}}, {"question": "WHO\u63d0\u51fa\u7684\u6269\u5927\u514d\u75ab\u8ba1\u5212\uff0c\u4ee5\u9884\u9632\nA. \u767d\u5589\u3001\u767e\u65e5\u54b3\u3001\u7834\u4f24\u98ce\u3001\u6d41\u884c\u6027\u4e59\u578b\u8111\u708e\u3001\u6d41\u884c\u6027\u8111\u810a\u9ad3\u819c\u708e\u548c\u80ba\u7ed3\u6838\nB. \u767d\u5589\u3001\u767e\u65e5\u54b3\uff1b\u7834\u4f24\u98ce\u3001\u9ebb\u75b9\u548c\u6d41\u884c\u6027\u8111\u810a\u9ad3\u819c\u708e\nC. \u767d\u5589\u3001\u767e\u65e5\u54b3\u3001\u7834\u4f24\u98ce\u3001\u6d41\u884c\u6027\u4e59\u578b\u8111\u708e\u3001\u810a\u9ad3\u7070\u8d28\u708e\u548c\u80ba\u7ed3\u6838\nD. \u767d\u5589\u3001\u767e\u65e5\u54b3\u3001\u7834\u4f24\u98ce\u3001\u9ebb\u75b9\u3001\u810a\u9ad3\u7070\u8d28\u708e\u548c\u80ba\u7ed3\u6838\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33585118474755266, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3602912461121415, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5047\u6027\u80a5\u5927\u662f\u6307\u5b9e\u8d28\u7ec6\u80de\nA. \u840e\u7f29\nB. \u5316\u751f\nC. \u589e\u751f\nD. \u80a5\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3735450304159101, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3035988665653362, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u5c06\u5176\u6536\u85cf\u7684\u4e00\u5e45\u9f50\u767d\u77f3\u7684\u9057\u753b\u5356\u7ed9\u4e59\uff0c\u4ef7\u91d1\u4e3a5\u4e07\u5143\u3002\u7532\u5c06\u4ef7\u91d1\u503a\u6743\u8f6c\u8ba9\u7ed9\u4e19\u5e76\u901a\u77e5\u4e86\u4e59\u3002\u5c65\u884c\u671f\u5c4a\u81f3\u524d\uff0c\u8be5\u753b\u706d\u5931\u3002\u5219\nA. \u4e59\u4e0d\u5f97\u89e3\u9664\u5408\u540c\u5e76\u4e0d\u5f97\u62d2\u7edd\u4e19\u7684\u7ed9\u4ed8\u8bf7\u6c42\nB. \u4e59\u4e0d\u5f97\u89e3\u9664\u5408\u540c\u4f46\u5f97\u62d2\u7edd\u4e19\u7684\u7ed9\u4ed8\u8bf7\u6c42\nC. \u4e59\u5f97\u5bf9\u7532\u4e3b\u5f20\u89e3\u9664\u5408\u540c\uff0c\u4f46\u4e0d\u5f97\u62d2\u7edd\u4e19\u7684\u7ed9\u4ed8\u8bf7\u6c42\nD. \u4e59\u5f97\u89e3\u9664\u5408\u540c\u5e76\u62d2\u7edd\u4e19\u7684\u7ed9\u4ed8\u8bf7\u6c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.43430321574048925}}, {"question": "\u5728$\\bigtriangleup ABC$\u4e2d\uff0c\u82e5c=1\uff0c$a=\\sqrt{3}$\uff0c$\\angle A=\\frac{2\\pi}{3}$\uff0c\u5219b\u4e3a\nA. 1\nB. $\\frac{\\sqrt{10}}{2}$\nC. $\\sqrt{7}$\nD. 2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u79cd\u4ea7\u54c1\u4ef7\u683c\u7684\u5927\u5e45\u5ea6\u51cf\u5c11\uff0c\u5bfc\u81f4\u53e6\u4e00\u79cd\u4ea7\u54c1\u9500\u552e\u91cf\u7684\u63d0\u9ad8\uff0c\u8fd9\u4e24\u79cd\u4ea7\u54c1\u5c5e\u4e8e\nA. \u72ec\u7acb\u54c1\nB. \u4e92\u8865\u54c1\nC. \u66ff\u4ee3\u54c1\nD. \u4f4e\u6863\u54c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7774026225907019, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7008351274614482}}, {"question": "\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff0c\u8bbe\u5176\u89c2\u5bdf\u503c\u7a7a\u95f4\u4e3a \u72b6\u6001\u7a7a\u95f4\u4e3a \u5982\u679c\u7528\u7ef4\u7279\u6bd4\u7b97\u6cd5(Viterbi algorithm)\u8fdb\u884c\u89e3\u7801\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a\nA. O(NK)\nB. O(N^2K)\nC. \u4ee5\u4e0a\u90fd\u4e0d\u662f\nD. O(NK^2)\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u5c5e\u4e8e\u4ef7\u683c\u7b56\u7565\u4e2d\u7684\u9053\u5fb7\u539f\u5219\u7684\u662f\nA. \u9053\u5fb7\u5316\u5305\u88c5\nB. \u5e7f\u544a\u6b3a\u8bc8\nC. \u538b\u5f3a\u5f0f\u63a8\u9500\nD. \u6076\u6027\u4ef7\u683c\u6218\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u957f\u6c5f\u4e09\u5ce1\u5de5\u7a0b\u7684\u6821\u6838\u6d2a\u6c34\u6d2a\u5cf0\u6d41\u91cf\u548c\u8bbe\u8ba1\u6d2a\u6c34\u6d2a\u5cf0\u6d41\u91cf\u5206\u522b\u4e3a[]m^3/s\u3002\nA. 110000\u300198800\nB. 124300\u3001110000\nC. 124300\u300198800\nD. 110000\u300180000\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5332765201192919, "meta-math/MetaMath-Mistral-7B": 0.9342636877705471, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9791097618265406, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6212321414577082, "meta-llama/Meta-Llama-3-8B": 0.34040633907578005, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4f30\u7b97\u5f02\u82b1\u6388\u7c89\u690d\u7269\u5e7f\u4e49\u9057\u4f20\u7387\u65f6\uff0c\u53ef\u4ee5\u7528\u6765\u4f30\u8ba1\u6027\u72b6\u73af\u5883\u65b9\u5dee\u7684\u662f\nA. \u56de\u4ea4\u4e16\u4ee3\u7684\u8868\u73b0\u65b9\u5dee\nB. F1\u7684\u8868\u73b0\u65b9\u5dee\nC. F2\u7684\u8868\u73b0\u65b9\u5dee\nD. \u4eb2\u672c\u7684\u8868\u73b0\u65b9\u5dee\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8bb0\u8d26\u9519\u8bef\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bd5\u7b97\u5e73\u8861\u53d1\u73b0\u7684\u662f\nA. \u501f\u8d37\u65b9\u5411\u76f8\u53cd\nB. \u6f0f\u8bb0\u67d0\u9879\u7ecf\u6d4e\u4e1a\u52a1\nC. \u91cd\u8bb0\u67d0\u9879\u7ecf\u6d4e\u4e1a\u52a1\nD. \u501f\u8d37\u91d1\u989d\u4e0d\u4e00\u81f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3837616345579143, "HuggingFaceH4/zephyr-7b-beta": 0.9033294738403513, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6988583227191244, "meta-llama/Meta-Llama-3-8B": 0.29660173325630934, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u80c6\u56ca\u75be\u75c5\u4e2d\uff0c\u4e0e\u80c6\u56ca\u764c\u53d1\u75c5\u65e0\u5173\u7684\u662f\nA. \u80c6\u56ca\u80c6\u56fa\u9187\u606f\u8089\nB. \u80c6\u56ca\u7ed3\u77f3>2cm\nC. \u80c6\u56ca\u817a\u7624\nD. \u201c\u74f7\u5316\u201d\u80c6\u56ca\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5047\u8bbe\u67d0\u4eba\u5177\u6709A\u56fd\u56fd\u7c4d\uff0c\u5728C\u56fd\u6709\u82e5\u5e72\u52a8\u4ea7\uff0c\u5728B\u56fd\u5c45\u4f4f\u4e865\u5e74\u4ee5\u540e\u672a\u7acb\u9057\u5631\u6b7b\u4ea1\uff0c\u5728A\u56fd\u548c\u5728D\u56fd\u7684\u7ee7\u627f\u4eba\u53d1\u751f\u7ea0\u7eb7\u800c\u7531A\u56fd\u6cd5\u9662\u53d7\u7406\u6b64\u6848\u3002\u5982\u679c\u4e0a\u8ff0\u56db\u4e2a\u56fd\u5bb6\u90fd\u53c2\u52a0\u300a\u6b7b\u8005\u9057\u4ea7\u7ee7\u627f\u6cd5\u5f8b\u9002\u7528\u516c\u7ea6\u300b\uff0c\u90a3\u4e48\u6839\u636e\u8be5\u516c\u7ea6\uff0cA\u56fd\u6cd5\u9662\u89e3\u51b3\u6b64\u7ee7\u627f\u7ea0\u7eb7\u7684\u51c6\u636e\u6cd5\u539f\u5219\u4e0a\u5e94\u9002\u7528\nA. D\u56fd\u6cd5\nB. C\u56fd\u6cd5\nC. B\u56fd\u6cd5\nD. A\u56fd\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3900852433273489, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4432947163656107}}, {"question": "\u4e0b\u5217\u5404\u9879\u5236\u5ea6\u4e2d\u88ab\u8bb8\u591a\u4eba\u79f0\u4e3a\u56fd\u9645\u53f8\u6cd5\u4e2d\u7684\u201c\u5b89\u5168\u9600\u201d\u7684\u662f\nA. \u5148\u51b3\u95ee\u9898\nB. \u516c\u5171\u79e9\u5e8f\u4fdd\u7559\nC. \u53cd\u81f4\nD. \u8bc6\u522b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.37897238125478655, "HuggingFaceH4/zephyr-7b-beta": 0.38218165130723625, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.532725100490484, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8438241855323827}}, {"question": "\u4e0b\u5217\u56fd\u5bb6\u4e2d\uff0c\u5f53\u4e8b\u4eba\u672a\u6709\u7279\u522b\u7ea6\u5b9a\uff0c\u4e0d\u4ee5\u4ea4\u4ed8\u65f6\u95f4\u4f5c\u4e3a\u52a8\u4ea7\u4e70\u5356\u5408\u540c\u4e2d\u8d27\u7269\u6240\u6709\u6743\u8f6c\u79fb\u65f6\u95f4\u7684\u662f\nA. \u5fb7\u56fd\nB. \u6cd5\u56fd\nC. \u7f8e\u56fd\nD. \u4e2d\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.37480848372461606, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.45169543862159645, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u6570\u5b66\u4e0e\u731c\u60f3\u300b\u7684\u4f5c\u8005\u662f\nA. \u7f57\u7d20\nB. \u5468\u5317\u6d77\nC. \u91d1\u5cb3\u9716\nD. \u6ce2\u5229\u4e9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u591a\u5a92\u4f53\u8ba1\u7b97\u673a\u662f\u6307\nA. \u53ef\u4ee5\u542c\u97f3\u4e50\u7684\u8ba1\u7b97\u673a\nB. \u53ef\u4ee5\u901a\u7528\u7684\u8ba1\u7b97\u673a\nC. \u80fd\u5904\u7406\u6216\u63d0\u4f9b\u58f0\u97f3\u3001\u56fe\u50cf\u3001\u6587\u5b57\u7b49\u591a\u79cd\u4fe1\u606f\u5f62\u5f0f\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\nD. \u53ef\u4ee5\u770b\u7535\u89c6\u7684\u8ba1\u7b97\u673a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9830950930141634, "meta-math/MetaMath-Mistral-7B": 0.999151170732061, "itpossible/Chinese-Mistral-7B-v0.1": 0.9693984016024123, "HuggingFaceH4/zephyr-7b-beta": 0.9999460912442556, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9948701533316264, "meta-llama/Meta-Llama-3-8B": 0.9865439836567009, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9997353448523807}}, {"question": "1948\u5e74\uff0c\u62c9\u65af\u97e6\u5c14\u5728\u300a\u4f20\u64ad\u5728\u793e\u4f1a\u4e2d\u7684\u7ed3\u6784\u548c\u529f\u80fd\u300b\u4e00\u6587\u4e2d\u9996\u6b21\u63d0\u51fa\u4e86\nA. \u4f20\u64ad\u76845W\u6a21\u5f0f\nB. \u653f\u6cbb\u4f20\u64ad\u7684\u6982\u5ff5\nC. \u5927\u4f17\u4f20\u64ad\u5b66\u7684\u6982\u5ff5\nD. \u5185\u5bb9\u5206\u6790\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8915328085342475}}, {"question": "\u67a2\u6298\u6308\u89c1\u4e8e\u300a\u7d20\u95ee\u00b7\u75ff\u8bba\u300b\u4e2d\u75ff\u8bc1\u7684\nA. \u9aa8\u75ff\nB. \u8089\u75ff\nC. \u7b4b\u75ff\nD. \u8109\u75ff\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bc9\u8bf8\u60c5\u611f\u8bba\u8bc1\u7684\u7c7b\u578b\u4e0d\u5305\u62ec\nA. \u8bc9\u8bf8\u516c\u4f17\u8bba\u8bc1\nB. \u8bc9\u8bf8\u601c\u60af\u8bba\u8bc1\nC. \u4e0d\u9002\u5f53\u6743\u5a01\u8bba\u8bc1\nD. \u8bc9\u8bf8\u5a01\u529b\u8bba\u8bc1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4566949108066516, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4287649174612196, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.46183047452191517}}, {"question": "\u63d0\u51fa\u4e8610\u79cd\u5de5\u4f5c\u6709\u5173\u7684\u4ef7\u503c\u89c2\u7684\u5fc3\u7406\u5b66\u5bb6\u662f\nA. \u97e6\u7eb3\nB. \u970d\u5170\u5fb7\nC. \u7f57\u6770\u65af\nD. \u51ef\u8328\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6696\u950b\u96e8\u4e00\u822c\u8f83\u51b7\u950b\u96e8[ ]\u3002\nA. \u96e8\u5f3a\u5c0f\uff0c\u96e8\u533a\u8303\u56f4\u5927\uff0c\u964d\u96e8\u5386\u65f6\u957f\nB. \u96e8\u5f3a\u5927\uff0c\u96e8\u533a\u8303\u56f4\u5927\uff0c\u964d\u96e8\u5386\u65f6\u77ed\nC. \u96e8\u5f3a\u5c0f\uff0c\u96e8\u533a\u8303\u56f4\u5927\uff0c\u964d\u96e8\u5386\u65f6\u77ed\nD. \u96e8\u5f3a\u5927\uff0c\u96e8\u533a\u8303\u56f4\u5c0f\uff0c\u964d\u96e8\u5386\u65f6\u957f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5109596852532194, "meta-llama/Meta-Llama-3-8B": 0.31828968051251794, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7881654682304102}}, {"question": "\u201c\u7f8e\u4eba\u9c7c\u201d\u7684\u5f62\u8c61\u662f\u4e0b\u5217\u54ea\u79cd\u60f3\u8c61\u7684\u4ea7\u7269\nA. \u65e0\u610f\u60f3\u8c61\nB. \u518d\u9020\u60f3\u8c61\nC. \u521b\u9020\u60f3\u8c61\nD. \u5e7b\u60f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36703366206306104, "meta-math/MetaMath-Mistral-7B": 0.41181126583177446, "itpossible/Chinese-Mistral-7B-v0.1": 0.45092007969637743, "HuggingFaceH4/zephyr-7b-beta": 0.719351562255175, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4160513129390438, "meta-llama/Meta-Llama-3-8B": 0.5132584120809294, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5460335170744607}}, {"question": "\u5e02\u573a\u8425\u9500\u4fe1\u606f\u7cfb\u7edf\u7531\u56db\u4e2a\u5b50\u7cfb\u7edf\u6784\u6210\uff0c\u5176\u4e2d\u627f\u62c5\u201c\u641c\u96c6\u3001\u8bc4\u4f30\u3001\u4f20\u9012\u7ba1\u7406\u4eba\u5458\u5236\u5b9a\u51b3\u7b56\u6240\u5fc5\u987b\u7684\u5404\u79cd\u4fe1\u606f\u201d\u7684\u5b50\u7cfb\u7edf\u662f\nA. \u5185\u90e8\u62a5\u544a\u7cfb\u7edf\nB. \u5e02\u573a\u8425\u9500\u8c03\u7814\u7cfb\u7edf\nC. \u5e02\u573a\u8425\u9500\u5206\u6790\u7cfb\u7edf\nD. \u5e02\u573a\u8425\u9500\u60c5\u62a5\u7cfb\u7edf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3393943779895649, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.43598572185465745, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6559\u80b2\u6d3b\u52a8\u4e0e\u5176\u4ed6\u793e\u4f1a\u6d3b\u52a8\u6700\u6839\u672c\u7684\u533a\u522b\u5728\u4e8e\nA. \u662f\u5426\u4fc3\u8fdb\u4eba\u7684\u53d1\u5c55\nB. \u662f\u5426\u5177\u6709\u7ec4\u7ec7\u6027\u548c\u7cfb\u7edf\u6027\nC. \u662f\u5426\u4fc3\u8fdb\u793e\u4f1a\u53d1\u5c55\nD. \u662f\u5426\u6709\u76ee\u7684\u7684\u57f9\u517b\u4eba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u75c5\u7684\u53d1\u75c5\u7387\u662f\u6307\nA. \u67d0\u75c5\u5728\u67d0\u5e74\u5185\u53d1\u751f\u7684\u65b0\u75c5\u4f8b\u6570\u4e0e\u540c\u5e74\u66b4\u9732\u4eba\u53e3\u6570\u4e4b\u6bd4\nB. \u67d0\u79cd\u611f\u67d3\u5f15\u8d77\u7684\u75c5\u4f8b\u6570\nC. \u67d0\u79cd\u539f\u56e0\u5bfc\u81f4\u67d0\u75c5\u7684\u53d1\u75c5\u7387\nD. \u4efb\u4f55\u75be\u75c5\u7684\u53d1\u75c5\u6982\u7387\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.858310944838203, "meta-math/MetaMath-Mistral-7B": 0.970222371894609, "itpossible/Chinese-Mistral-7B-v0.1": 0.8226303115189229, "HuggingFaceH4/zephyr-7b-beta": 0.9997567799747468, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9553448057590739, "meta-llama/Meta-Llama-3-8B": 0.6615534006423989, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9850014396583442}}, {"question": "\u5eb7\u5fb7\u58f0\u79f0\u7406\u6027\u7684\u81ea\u7136\u76ee\u7684\u662f\nA. \u4ea7\u751f\u826f\u597d\u7684\u610f\u613f\nB. \u4ea7\u751f\u5feb\u611f\nC. \u4ea7\u751f\u77e5\u8bc6\nD. \u4ea7\u751f\u5e78\u798f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6159436143326027, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6cd5\u5f8b\u8d23\u4efb\u7684\u8868\u8ff0\uff0c\u6b63\u786e\u7684\u6709\nA. \u627f\u62c5\u6cd5\u5f8b\u8d23\u4efb\u7684\u6700\u7ec8\u4f9d\u636e\u662f\u6cd5\u5f8b\uff0c\u6cd5\u5f8b\u8d23\u4efb\u5177\u6709\u56fd\u5bb6\u5f3a\u5236\u6027\nB. \u6cd5\u5f8b\u8d23\u4efb\u7684\u76ee\u7684\u662f\u901a\u8fc7\u5176\u60e9\u7f5a\u7684\u65b9\u5f0f\u6765\u5b9e\u73b0\u7684\nC. \u5982\u679c\u6743\u5229\u6ca1\u6709\u53d7\u5230\u4fb5\u72af\uff0c\u5c31\u4e0d\u4f1a\u4ea7\u751f\u6cd5\u5f8b\u8d23\u4efb\nD. \u6ca1\u6709\u4ece\u4e8b\u8fdd\u6cd5\u884c\u4e3a\uff08\u5e7f\u4e49\uff09\u5c31\u4e0d\u53ef\u80fd\u627f\u62c5\u6cd5\u5f8b\u8d23\u4efb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6794076859801301, "meta-math/MetaMath-Mistral-7B": 0.9025737945917478, "itpossible/Chinese-Mistral-7B-v0.1": 0.6000738117537388, "HuggingFaceH4/zephyr-7b-beta": 0.9961192325533609, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.485540181339914, "meta-llama/Meta-Llama-3-8B": 0.8344029841035819, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9581542604069412}}, {"question": "\u53d1\u751f\u5728\u4e0b\u5217\u4e0d\u540c\u90e8\u4f4d\u7684\u7ea4\u7ef4\u86cb\u767d\u6027\u708e\u75c7\u4e2d\uff0c\u5c5e\u4e8e\u5047\u819c\u6027\u708e\u75c7\u7684\u662f\nA. \u80f8\u819c\nB. \u80a0\u9ecf\u819c\nC. \u5fc3\u5305\u819c\nD. \u5173\u8282\u6ed1\u819c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.29068935354339714, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3191152350487737}}, {"question": "\u300a\u804c\u4e1a\u5b89\u5168\u5065\u5eb7\u7ba1\u7406\u4f53\u7cfb\u5bfc\u5219\u300b\uff08ILO-OSH2001\uff09\u4e0e\u300a\u804c\u4e1a\u5065\u5eb7\u5b89\u5168\u7ba1\u7406\u4f53\u7cfb\u5bfc\u5219\u300b\uff08OHSAS18001\u7b26\u8fd0\u884c\u6a21\u5f0f\u5747\u4f53\u73b0\u4e86\uff08\uff09\u7684\u601d\u60f3\u3002\nA. \u6301\u7eed\u6539\u8fdb\nB. \u73b0\u4ee3\u7ba1\u7406\nC. \u4f20\u7edf\u7ba1\u7406\nD. \u5f3a\u5316\u76d1\u7763\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6613527559067245, "meta-math/MetaMath-Mistral-7B": 0.9595317278439129, "itpossible/Chinese-Mistral-7B-v0.1": 0.5458012431008373, "HuggingFaceH4/zephyr-7b-beta": 0.9923535336795632, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.890029572156384, "meta-llama/Meta-Llama-3-8B": 0.8549932904908898, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9248714618522712}}, {"question": "17\u4e16\u7eaa\u897f\u65b9\u5bf9\u4e1c\u65b9\u8fdb\u884c\u5546\u4e1a\u5784\u65ad\u8d38\u6613\u548c\u6b96\u6c11\u6269\u5f20\u4e2d\uff0c\u4e00\u4e9b\u56fd\u5bb6\u7eb7\u7eb7\u5efa\u7acb\u201c\u4e1c\u5370\u5ea6\u516c\u53f8\u201d\uff0c\u5176\u4e2d\u82f1\u56fd\u7684\u201c\u4e1c\u5370\u5ea6\u516c\u53f8\u201d\u6700\u4e3a\u4eba\u719f\u77e5\u3002\u4ee5\u4e0b\u56fd\u5bb6\u4e2d\uff0c\u4e5f\u5efa\u7acb\u201c\u4e1c\u5370\u5ea6\u516c\u53f8\u201d\u7684\u662f\nA. \u897f\u73ed\u7259\nB. \u5fb7\u56fd\nC. \u8377\u5170\nD. \u8461\u8404\u7259\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6377238610526251, "meta-math/MetaMath-Mistral-7B": 0.9622524815609821, "itpossible/Chinese-Mistral-7B-v0.1": 0.5062980607791807, "HuggingFaceH4/zephyr-7b-beta": 0.7710714420528668, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7480252487593255, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7278717774262637}}, {"question": "\u5fb7\u80b2\u8fc7\u7a0b\u7684\u4e3b\u8981\u77db\u76fe\u662f\nA. \u9053\u5fb7\u8ba4\u8bc6\u4e0e\u9053\u5fb7\u884c\u4e3a\u4e4b\u95f4\u7684\u77db\u76fe\nB. \u6559\u80b2\u8005\u63d0\u51fa\u7684\u9053\u5fb7\u8981\u6c42\u4e0e\u53d7\u6559\u80b2\u8005\u73b0\u6709\u9053\u5fb7\u6c34\u5e73\u4e4b\u95f4\u7684\u77db\u76fe\nC. \u5fb7\u80b2\u5185\u5bb9\u4e0e\u5fb7\u80b2\u65b9\u6cd5\u4e4b\u95f4\u7684\u77db\u76fe\nD. \u6559\u80b2\u8005\u4e0e\u53d7\u6559\u80b2\u8005\u4e4b\u95f4\u7684\u77db\u76fe\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37261446483990335, "meta-math/MetaMath-Mistral-7B": 0.3918192370385313, "itpossible/Chinese-Mistral-7B-v0.1": 0.5675496001959213, "HuggingFaceH4/zephyr-7b-beta": 0.6041952324500252, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5845217466020443, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9288158905566964}}, {"question": "\u5f71\u54cd\u9176\u4fc3\u53cd\u5e94\u901f\u5ea6\u7684\u56e0\u7d20\u4e0d\u5305\u62ec\nA. \u9176\u7684\u6d53\u5ea6\nB. \u53cd\u5e94\u73af\u5883\u7684PH\nC. \u53cd\u5e94\u6e29\u5ea6\nD. \u9176\u539f\u7684\u6d53\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.49309194331935696, "meta-math/MetaMath-Mistral-7B": 0.7069593921833606, "itpossible/Chinese-Mistral-7B-v0.1": 0.7774039847004791, "HuggingFaceH4/zephyr-7b-beta": 0.9431349830019745, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6981525772058953, "meta-llama/Meta-Llama-3-8B": 0.9462177892246169, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.579334751126242}}, {"question": "\u79fb\u52a8\u7528\u6237\u6709\u4e9b\u5c5e\u6027\u4fe1\u606f\u9700\u8981\u53d7\u5230\u4fdd\u62a4\uff0c\u8fd9\u4e9b\u4fe1\u606f\u4e00\u65e6\u6cc4\u9732\uff0c\u4f1a\u5bf9\u516c\u4f17\u7528\u6237\u7684\u751f\u547d\u8d22\u4ea7\u5b89\u5168\u9020\u6210\u5a01\u80c1\u3002\u4ee5\u4e0b\u5404\u9879\u4e2d\uff0c\u4e0d\u9700\u8981\u88ab\u4fdd\u62a4\u7684\u5c5e\u6027\u662f\nA. \u7528\u6237\u4f4d\u7f6e\u4fe1\u606f\nB. \u7528\u6237\u901a\u8bdd\u4fe1\u606f\nC. \u7ec8\u7aef\u8bbe\u5907\u4fe1\u606f\nD. \u516c\u4f17\u8fd0\u8425\u5546\u4fe1\u606f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7394581670240545, "meta-math/MetaMath-Mistral-7B": 0.9566572902465627, "itpossible/Chinese-Mistral-7B-v0.1": 0.6018035124746308, "HuggingFaceH4/zephyr-7b-beta": 0.9983891058446842, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9808523648514736, "meta-llama/Meta-Llama-3-8B": 0.8344029841035819, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6095988638575048}}, {"question": "\u8521\u67d0\u60f3\u505a\u751f\u610f\uff0c\u65e0\u5948\u624b\u4e2d\u7f3a\u94b1\uff0c\u67d0\u65e5\u8d81\u81ea\u5df1\u505a\u751f\u610f\u7684\u670b\u53cb\u5f8b\u67d0\u4e4b\u5b50\u5c0f\u4e1c\u653e\u5b66\u4e4b\u673a\uff0c\u9a97\u5176\u5230\u81ea\u5df1\u4e8b\u5148\u79df\u7528\u7684\u4e00\u6240\u623f\u5b50\u5185\uff0c\u6253\u7535\u8bdd\u8981\u631f\u5f8b\u67d0\u752820\u4e07\u5143\u6362\u5b69\u5b50\u3002\u5f8b\u67d0\u62a5\u8b66\uff0c\u8521\u67d0\u53d1\u73b0\u540e\u5c06\u5c0f\u4e1c\u6740\u6b7b\u3002\u5bf9\u8521\u67d0\u7684\u884c\u4e3a\u5e94\u5f53\nA. \u4ee5\u6572\u8bc8\u52d2\u7d22\u7f6a\u548c\u6545\u610f\u6740\u4eba\u7f6a\u5e76\u7f5a\nB. \u4ee5\u7ed1\u67b6\u7f6a\u548c\u6545\u610f\u6740\u4eba\u7f6a\u7684\u7275\u8fde\u72af\uff0c\u62e9\u4e00\u91cd\u7f6a\u5904\u65ad\nC. \u4ee5\u7ed1\u67b6\u7f6a\u548c\u6545\u610f\u6740\u4eba\u7f6a\u5e76\u7f5a\nD. \u4ee5\u7ed1\u67b6\u7f6a\u5904\u7f5a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6897\u6b7b\u7076\u7684\u8d28\u5730\u4e3b\u8981\u53d6\u51b3\u4e8e\nA. \u8840\u7ba1\u7684\u5206\u5e03\u65b9\u5f0f\nB. \u6897\u6b7b\u7076\u7684\u542b\u8840\u91cf\nC. \u574f\u6b7b\u7684\u7c7b\u578b\nD. \u6897\u6b7b\u7684\u539f\u56e0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7285651930995701, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e0b\u5217\u53e5\u6cd5\u7ed3\u6784\u4e2d\uff0c\u4e0e\u201c\u8db3\u7403\u6bd4\u8d5b\u201d\u6709\u540c\u6784\u5173\u7cfb\u7684\u662f\nA. \u5206\u6790\u95ee\u9898\nB. \u521b\u4f5c\u5b9e\u8df5\nC. \u8868\u793a\u540c\u610f\nD. \u5de5\u4f5c\u6df1\u5165 \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u54ea\u4e2a\u56fd\u5bb6\u72ec\u7acb\u5ba3\u544a\u4e86\u6301\u7eed100\u591a\u5e74\u7684\u6b96\u6c11\u4f53\u7cfb\u74e6\u89e3\nA. \u5357\u975e\nB. \u57c3\u585e\u4fc4\u6bd4\u4e9a\nC. \u7d22\u9a6c\u91cc\nD. \u7eb3\u7c73\u6bd4\u4e9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4596151485366816, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9146336051095227, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5e72\u6270\u7d20\u6297\u75c5\u6bd2\u4f5c\u7528\u7684\u673a\u5236\u662f\nA. \u963b\u6b62\u75c5\u6bd2\u7a7f\u5165\nB. \u8bf1\u5bfc\u7ec6\u80de\u4ea7\u751f\u6297\u75c5\u6bd2\u86cb\u767d\nC. \u6291\u5236\u75c5\u6bd2\u751f\u7269\u5408\u6210\nD. \u6291\u5236\u75c5\u6bd2\u5438\u9644\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4670166907590249, "meta-math/MetaMath-Mistral-7B": 0.5803063277457281, "itpossible/Chinese-Mistral-7B-v0.1": 0.5211775449077607, "HuggingFaceH4/zephyr-7b-beta": 0.999748980367357, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6951852606296997, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9814019863135761}}, {"question": "\u4e00\u4e2a\u7269\u4f53\u540c\u65f6\u53d7\u5230\u4e24\u4e2a\u529b\u7684\u4f5c\u7528\uff0c\u8fd9\u4e24\u4e2a\u529b\u7684\u4e09\u8981\u7d20\u5b8c\u5168\u76f8\u540c\uff0c\u90a3\u4e48\u8fd9\u4e24\u4e2a\u529b\nA. \u53ef\u80fd\u662f\u5e73\u8861\u529b\nB. \u4e00\u5b9a\u4e0d\u662f\u5e73\u8861\u529b\nC. \u65e0\u6cd5\u5224\u65ad\nD. \u4e00\u8d70\u662f\u5e73\u8861\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4204678737898294, "itpossible/Chinese-Mistral-7B-v0.1": 0.40202865642572594, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5364803484147718}}, {"question": "\u5bfc\u81f4\u7fa4\u4f17\u5bf9\u5178\u578b\u5ba3\u4f20\u4ea7\u751f\u4e0d\u771f\u5b9e\u611f\u7684\u4e3b\u8981\u539f\u56e0\u662f\nA. \u62a5\u9053\u6570\u91cf\u8fc7\u5927\nB. \u6709\u610f\u62d4\u9ad8\u5178\u578b\nC. \u62a5\u9053\u7bc7\u5e45\u957f\nD. \u62a5\u9053\u65f6\u95f4\u96c6\u4e2d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.310313193127302, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5269642307264539, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4762404232568518, "meta-llama/Meta-Llama-3-8B": 0.8081721397041021, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9062057129927819}}, {"question": "\u5df2\u77e5$T(0)$\u7684\u5206\u5e03\u4e3a:$F_0(t)=\\left\\{\\begin{array}{ll}t / 100\uff0c& 0<t \\leqslant 100 \\\\ 1\uff0c& t>100\\end{array}\\right.$\uff0c\u5219\u65b0\u751f\u5a74\u513f\u5728 30 \u5c81\u548c 50 \u5c81\u4e4b\u95f4\u6b7b\u4ea1\u7684\u6982\u7387\u4e3a ( )\u3002\nA. 0.7\nB. 0.5\nC. 0.2\nD. 0.6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2821833983601388, "HuggingFaceH4/zephyr-7b-beta": 0.4737435543170778, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.43186001532203844}}, {"question": "\u4e0b\u5217\u5bf9\u7ed3\u5408\u80c6\u7ea2\u7d20\u7684\u8bf4\u6cd5\u54ea\u4e00\u9879\u662f\u9519\u8bef\u7684\nA. \u4e0d\u6613\u900f\u8fc7\u751f\u7269\u819c\nB. \u4e0e\u91cd\u6c2e\u8bd5\u5242\u5448\u76f4\u63a5\u53cd\u5e94\u9633\u6027\nC. \u968f\u6b63\u5e38\u4eba\u5c3f\u6db2\u5927\u91cf\u6392\u51fa\nD. \u4e3b\u8981\u662f\u53cc\u8461\u8404\u5168\u7b97\u80c6\u7ea2\u7d20\u916f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33121905446883504, "meta-math/MetaMath-Mistral-7B": 0.5967953609995239, "itpossible/Chinese-Mistral-7B-v0.1": 0.3416622556274959, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5221145873208052, "meta-llama/Meta-Llama-3-8B": 0.5202594698638454, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6559\u80b2\u8981\u57f9\u517b\u6709\u6587\u5316\u3001\u6709\u4fee\u517b\u548c\u5177\u6709\u591a\u79cd\u624d\u80fd\u7684\u653f\u6cbb\u5bb6\u548c\u5546\u4eba\uff0c\u8fd9\u79cd\u6559\u80b2\u65b9\u5f0f\u51fa\u73b0\u5728()\nA. \u53e4\u4ee3\u57c3\u53ca\nB. \u53e4\u4ee3\u96c5\u5178\nC. \u53e4\u4ee3\u7f57\u9a6c\nD. \u53e4\u4ee3\u65af\u5df4\u8fbe\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4088038922836321, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.399093384069114, "meta-llama/Meta-Llama-3-8B": 0.5732746720377035, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9384011887009837}}, {"question": "\u8c03\u6574\u56fd\u9645\u8d38\u6613\u652f\u4ed8\u7684\u56fd\u9645\u60ef\u4f8b\u6709\nA. \u300a\u7ef4\u65af\u6bd4\u89c4\u5219\u300b\u548c\u300a\u8ddf\u5355\u4fe1\u7528\u8bc1\u7edf\u4e00\u60ef\u4f8b\u300b\nB. \u300a1967\u5e74\u5546\u4e1a\u5355\u636e\u6258\u6536\u7edf\u4e00\u89c4\u5219\u300b\u548c\u300a\u534e\u6c99\u2014\u725b\u6d25\u89c4\u5219\u300b\nC. \u300a1967\u5e74\u5546\u4e1a\u5355\u636e\u6258\u6536\u7edf\u4e00\u89c4\u5219\u300b\u548c\u300a\u8ddf\u5355\u4fe1\u7528\u8bc1\u7edf\u4e00\u60ef\u4f8b\u300b\nD. \u300a\u6d77\u7259\u89c4\u5219\u300b\u548c\u300a\u6c49\u5821\u89c4\u5219\u300bB\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7590311479141443, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5691376044166693, "HuggingFaceH4/zephyr-7b-beta": 0.995818232766588, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7095423436378792, "meta-llama/Meta-Llama-3-8B": 0.6072888357068648, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7959347608823574}}, {"question": "\u52a8\u673a\u4e0e\u5de5\u4f5c\u6548\u7387\u7684\u5173\u7cfb\u662f\nA. \u4e00\u79cd\u76f4\u7ebf\u5173\u7cfb\nB. \u5012U\u5f62\u66f2\u7ebf\u5173\u7cfb\nC. \u4e00\u79cd\u4ea4\u53c9\u5173\u7cfb\nD. \u4e00\u79cd\u7ebf\u6027\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.47939944413006647, "meta-math/MetaMath-Mistral-7B": 0.4776726622890207, "itpossible/Chinese-Mistral-7B-v0.1": 0.6837774311650493, "HuggingFaceH4/zephyr-7b-beta": 0.7792660961096893, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7279616160067545, "meta-llama/Meta-Llama-3-8B": 0.5792585154146437, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9674240446210057}}, {"question": "\u4e00\u4e2a\u4eba\u751f\u7406\u4e0a\u53d1\u80b2\u6210\u719f\uff0c\u5fc5\u7136\u5bf9\u6027\u7231\u4ea7\u751f\u6b63\u5e38\u7684\u6b32\u671b\uff0c\u8fd9\u79cd\u6b32\u671b\u5f97\u5230\u5408\u7406\u7684\u6ee1\u8db3\uff0c\u6709\u5229\u4e8e\u4eba\u7684\nA. \u8eab\u5fc3\u5065\u5eb7\nB. \u5fc3\u7406\u5065\u5eb7\nC. \u4f53\u9b44\u5065\u5eb7\nD. \u751f\u7406\u5065\u5eb7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.765505772784012, "HuggingFaceH4/zephyr-7b-beta": 0.9996249677684163, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8796067067462928, "meta-llama/Meta-Llama-3-8B": 0.7061102164860679, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7055604784291182}}, {"question": "\u5982\u679c\u2f9a\u6f6e\u53d1\u2f63\u7684\u6d77\u57df\uff0c\u6709\u5bd2\u51b7\u7684\u6cbf\u5cb8\u6d41\u7ecf\u8fc7\uff0c\u5176\u5bf9\u2f9a\u6f6e\u7684\u5f71\u54cd\u662f\u4e0b\u5217\u54ea\u4e9b\uff08\uff09\uff1aa\u4f7f\u6c61\u67d3\u533a\u57df\u66f4\u96c6\u4e2d\uff1bb\u52a0\u5267\u6c61\u67d3\u7a0b\u5ea6\uff1bc\u4f7f\u6c61\u67d3\u533a\u57df\u66f4\u5206\u6563\uff1bd\u51cf\u8f7b\u6c61\u67d3\u7a0b\u5ea6\nA. cd\nB. ab\nC. ad\nD. bc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2887476825929059, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3017444864199176, "meta-llama/Meta-Llama-3-8B": 0.34239623393788804, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.32659802886351824}}, {"question": "\u793e\u4f1a\u4e3b\u4e49\u6c11\u65cf\u5173\u7cfb\u786e\u7acb\u7684\u6807\u5fd7\u662f\nA. \u793e\u4f1a\u4e3b\u4e49\u5e02\u573a\u7ecf\u6d4e\u5236\u5ea6\u7684\u5b9e\u884c\nB. \u6539\u9769\u5f00\u653e\nC. \u6c11\u4e3b\u6539\u9769\u548c\u793e\u4f1a\u4e3b\u4e49\u6539\u9020\u7684\u5b8c\u6210\nD. \u65b0\u4e2d\u56fd\u7684\u6210\u7acb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.382971948780131, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5339239132881537, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7410187293397119}}, {"question": "\u6cd5\u5f8b\u4e0a\u89c4\u5b9a\u7684\u7528\u4ee5\u8fdb\u884c\u6cd5\u5f8b\u63a8\u7406\u7684\u51c6\u5219\uff0c\u79f0\u4e3a\nA. \u786e\u5b9a\u6027\u89c4\u5219\nB. \u51c6\u7528\u6027\u89c4\u5219\nC. \u6cd5\u5f8b\u539f\u5219\nD. \u6cd5\u5f8b\u6982\u5ff5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5704973567693387, "meta-math/MetaMath-Mistral-7B": 0.5382565360026652, "itpossible/Chinese-Mistral-7B-v0.1": 0.536944563822864, "HuggingFaceH4/zephyr-7b-beta": 0.9092201782071638, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4909237390016368, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u4e61\u571f\u8bd7\u4eba\u201d\u6307\u7684\u662f\nA. \u4f55\u5176\u82b3\nB. \u7530\u95f4\nC. \u81e7\u514b\u5bb6\nD. \u827e\u9752\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5b8c\u74a7\u5f52\u8d75\u201d\u8fd9\u4e2a\u6545\u4e8b\u53d1\u751f\u5728\nA. \u79e6\u6c49\u65f6\u671f\nB. \u5510\u5b8b\u65f6\u671f\nC. \u6625\u79cb\u6218\u56fd\nD. \u4e09\u56fd\u65f6\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3293945554888329, "meta-math/MetaMath-Mistral-7B": 0.43241138747909885, "itpossible/Chinese-Mistral-7B-v0.1": 0.5364816052401276, "HuggingFaceH4/zephyr-7b-beta": 0.535729619338768, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6831232636048101, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6868924894879682}}, {"question": "\u65ad\u8def\u5668\u989d\u5b9a\u7535\u538b\u6307\nA. \u6b63\u5e38\u5de5\u4f5c\u76f8\u7535\u538b\nB. \u6b63\u5e38\u5de5\u4f5c\u7ebf\u7535\u538b\u6700\u5927\u503c\nC. \u6b63\u5e38\u5de5\u4f5c\u7ebf\u7535\u538b\u6709\u6548\u503c\nD. \u65ad\u8def\u5668\u6b63\u5e38\u5de5\u4f5c\u7535\u538b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.34600580950320253, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3697865149374796}}, {"question": "\u5976\u725b\u7684\u4ea7\u5976\u91cf\u8fbe\u5230\u9ad8\u5cf0\u671f\u7684\u80ce\u6b21\u662f\nA. 4\u20147\u80ce\nB. 1\u20142\u80ce\nC. 8\u201410\u80ce\nD. 2\u20143\u80ce\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36010898878387526, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8440335050081356, "meta-llama/Meta-Llama-3-8B": 0.33075425723442, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8084544901555285}}, {"question": "\u6211\u56fd\u9972\u517b\u6807\u51c6\u6240\u91c7\u7528\u7684\u80fd\u91cf\u4f53\u7cfb\u4e2d\uff0c\u732a\u901a\u5e38\u7528\nA. \u6d88\u5316\u80fd\nB. \u603b\u80fd\nC. \u51c0\u80fd\nD. \u4ee3\u8c22\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6839\u636e\u8fd0\u8f93\u5408\u540c\u5f62\u6210\u7684\u6cd5\u5f8b\u5173\u7cfb\u4e2d\uff0c\u6258\u8fd0\u65b9\u548c\u627f\u8fd0\u65b9\u7684\u6743\u5229\u4e49\u52a1\u6240\u6307\u5411\u7684\u5bf9\u8c61\u662f\nA. \u8fd0\u8f93\u8d39\u7528\nB. \u88ab\u6258\u8fd0\u7684\u8d27\u7269\nC. \u8fd0\u8f93\u884c\u4e3a\nD. \u8fd0\u8f93\u65b9\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u4e2a\u5c5e\u4e8e\u6cd5\u5f8b\u6587\u5316\u6700\u5185\u5728\u7684\u6df1\u5c42\u6b21\u56e0\u7d20\uff1f\nA. \u6cd5\u5f8b\u8bbe\u65bd\nB. \u6cd5\u5f8b\u5236\u5ea6\nC. \u6cd5\u5f8b\u5b9e\u8df5\nD. \u6cd5\u5f8b\u610f\u8bc6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9196869919431131, "meta-math/MetaMath-Mistral-7B": 0.9884715761500917, "itpossible/Chinese-Mistral-7B-v0.1": 0.860867511726826, "HuggingFaceH4/zephyr-7b-beta": 0.9998060981153342, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.971935426279272, "meta-llama/Meta-Llama-3-8B": 0.9813504555909589, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9902082562999092}}, {"question": "\u5728\u7f16\u8f91\u4e00\u4e2a\u5df2\u5b58\u5728\u7684Word\u6587\u6863\u65f6\uff0c\u6267\u884c\u201c\u6587\u4ef6\u201d\u83dc\u5355\u4e2d\u7684\u201c\u4fdd\u5b58\u201d\u547d\u4ee4\u540e\nA. \u53ef\u4ee5\u5c06\u5f53\u524d\u6587\u6863\u5b58\u50a8\u5728\u5df2\u6709\u7684\u4efb\u610f\u6587\u4ef6\u5939\u5185\nB. \u53ef\u4ee5\u5148\u5efa\u7acb\u4e00\u4e2a\u65b0\u6587\u4ef6\u5939\uff0c\u518d\u5c06\u5f53\u524d\u6587\u6863\u5b58\u50a8\u5728\u8be5\u6587\u4ef6\u5185\nC. \u53ea\u80fd\u5c06\u5f53\u524d\u6587\u6863\u5b58\u50a8\u5728\u539f\u6587\u4ef6\u4e2d\nD. \u5c06\u6240\u6709\u6253\u5f00\u7684\u6587\u6863\u5b58\u76d8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4121571348789764, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u516c\u4f17\u79d1\u5b66\u662f\u6307\u5728\u79d1\u5b66\u516c\u5f00\u5316\u4e0e\u79d1\u5b66\u653f\u7b56\u516c\u5f00\u5316\u7684\u5fc5\u8981\u57fa\u7840\u4e0a\uff0c\u516c\u4f17\u76f4\u63a5\u53c2\u4e0e\u79d1\u5b66\u77e5\u8bc6\u4ea7\u51fa\u8fc7\u7a0b\u3002\u516c\u5171\u79d1\u5b66\u662f\u6307\u666e\u901a\u516c\u4f17\u53c2\u4e0e\u79d1\u5b66\u76f8\u5173\u7684\u79d1\u5b66\u51b3\u7b56\u5ba1\u8bae\u3001\u79d1\u5b66\u95ee\u9898\u8ba8\u8bba\u4ee5\u53ca\u79d1\u5b66\u6210\u679c\u8f6c\u5316\u7b49\u3002\u6839\u636e\u4e0a\u8ff0\u5b9a\u4e49\uff0c\u4e0b\u5217\u5c5e\u4e8e\u516c\u4f17\u79d1\u5b66\u7684\u662f\nA. \u67d0\u533b\u5b66\u4f26\u7406\u59d4\u5458\u4f1a\u5c31\u57fa\u56e0\u7f16\u8f91\u7684\u9053\u5fb7\u98ce\u9669\u5411\u79d1\u6280\u5de5\u4f5c\u8005\u5f81\u8be2\u610f\u89c1\nB. \u67d0\u8c03\u67e5\u516c\u53f8\u5c315G\u6280\u672f\u7684\u5546\u4e1a\u524d\u666f\u5728\u516c\u5171\u573a\u6240\u53d1\u653e\u8c03\u67e5\u95ee\u5377\nC. \u67d0\u533b\u836f\u516c\u53f8\u4e3a\u7f29\u77ed\u7814\u53d1\u7684\u6297\u764c\u65b0\u836f\u4e0a\u5e02\u5468\u671f\u800c\u62db\u52df\u8bd5\u7528\u5fd7\u613f\u8005\nD. \u67d0\u624b\u673a\u516c\u53f8\u7814\u53d1\u90e8\u95e8\u5c31\u662f\u5426\u53d6\u6d88\u7269\u7406\u6309\u952e\u516c\u5f00\u5f81\u8be2\u6d88\u8d39\u8005\u610f\u89c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u79ef\u6781\u5f15\u5bfc\u5b66\u751f\u5bb6\u957f\u8f6c\u53d8\u6559\u80b2\u65b9\u5f0f\uff0c\u5b9e\u73b0\u5bf9\u5b50\u5973\u7684\u54ea\u79cd\u7ba1\u7406\uff0c\u4f7f\u5b50\u5973\u5728\u8212\u9002\u3001\u5bbd\u677e\u7684\u5bb6\u5ead\u73af\u5883\u4e2d\u5065\u5eb7\u6210\u957f\nA. \u653e\u4efb\u5316\nB. \u6c11\u4e3b\u5316\nC. \u4e13\u5236\u5316\nD. \u6eba\u7231\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.37727078502384037, "itpossible/Chinese-Mistral-7B-v0.1": 0.8600987346350768, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4733159701518364, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6821\u98ce\u3001\u6559\u98ce\u548c\u5b66\u98ce\u662f\u5b66\u6821\u6587\u5316\u7684\u91cd\u8981\u6784\u6210\u90e8\u5206\uff0c\u5c31\u8bfe\u7a0b\u7c7b\u578b\u800c\u8a00\uff0c\u4ed6\u4eec\u5c5e\u4e8e\nA. \u6d3b\u52a8\u8bfe\u7a0b\nB. \u5b66\u79d1\u8bfe\u7a0b\nC. \u9690\u5f62\u8bfe\u7a0b\nD. \u663e\u6027\u8bfe\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5716036572086838, "meta-math/MetaMath-Mistral-7B": 0.7920585257563605, "itpossible/Chinese-Mistral-7B-v0.1": 0.5541352578976882, "HuggingFaceH4/zephyr-7b-beta": 0.9885975319621743, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8324345381052123, "meta-llama/Meta-Llama-3-8B": 0.5450990636079687, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4e8e\u6d77\u4e0a\u4fb5\u6743\u884c\u4e3a\u7684\u6cd5\u5f8b\u9002\u7528\uff0c\u6211\u56fd\u300a\u6d77\u5546\u6cd5\u300b\u89c4\u5b9a\uff1a\u8239\u8236\u5728\u516c\u6d77\u4e0a\u53d1\u751f\u78b0\u649e\u7684\u635f\u5bb3\u8d54\u507f\uff0c\u9002\u7528\nA. \u4fb5\u6743\u884c\u4e3a\u5730\u6cd5\nB. \u52a0\u5bb3\u8239\u8236\u7684\u65d7\u56fd\u6cd5\nC. \u53d7\u7406\u6848\u4ef6\u7684\u6cd5\u9662\u6240\u5728\u5730\u6cd5\nD. \u53d7\u5bb3\u8239\u8236\u7684\u65d7\u56fd\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8349\u4e66\u3001\u884c\u4e66\u3001\u6977\u4e66\u3001\u96b6\u4e66\u56db\u79cd\u5b57\u4f53\u5f53\u4e2d\u54ea\u4e00\u79cd\u662f\u5176\u4f59\u4e09\u79cd\u7684\u8d77\u6e90\nA. \u884c\u4e66\nB. \u96b6\u4e66\nC. \u6977\u4e66\nD. \u8349\u4e66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.45421441298381454, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7007208863715848, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u7c7b\u793e\u4f1a\u5386\u53f2\u53d1\u5c55\u7684\u51b3\u5b9a\u529b\u91cf\u662f\nA. \u4eba\u53e3\u56e0\u7d20\nB. \u5730\u7406\u6761\u4ef6\nC. \u793e\u4f1a\u610f\u8bc6\nD. \u751f\u4ea7\u65b9\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9199092316001213, "meta-math/MetaMath-Mistral-7B": 0.9909975006818482, "itpossible/Chinese-Mistral-7B-v0.1": 0.9059451588899862, "HuggingFaceH4/zephyr-7b-beta": 0.9974831412908682, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.89099858715379, "meta-llama/Meta-Llama-3-8B": 0.9790078982958733, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.81232195296503}}, {"question": "\u73ed\u4e3b\u4efb\u9648\u8001\u5e08\u901a\u8fc7\u751f\u674f\u7684\u9178\u6da9\u548c\u719f\u674f\u7684\u9999\u751c\u6765\u6559\u80b2\u4e00\u4f4d\u65e9\u604b\u7684\u521d\u4e09\u5973\u751f\uff0c\u544a\u8bc9\u5979\uff0c\u8c08\u604b\u7231\u548c\u5403\u674f\u5b50\u662f\u4e00\u6837\u7684\u9053\u7406\u3002\u4e2d\u5b66\u751f\u8fd8\u6ca1\u6709\u751f\u957f\u6210\u719f\uff0c\u6b64\u523b\u5047\u8bbe\u8c08\u604b\u7231\uff0c\u5c31\u5982\u540c\u5403\u751f\u674f\u5b50\u4e00\u822c\uff0c\u53ea\u80fd\u53c8\u82e6\u53c8\u6da9\uff1a\u53ea\u6709\u5230\u6210\u719f\u540e\u518d\u53bb\u54c1\u5c1d\uff0c\u624d\u4f1a\u9999\u751c\u53ef\u53e3\uff0c\u65e0\u6bd4\u5e78\u798f\u3002\u4ece\u800c\u4f7f\u8fd9\u4f4d\u5973\u751f\u4ece\u65e9\u604b\u4e2d\u8d70\u4e86\u51fa\u6765\u3002\u8fd9\u8868\u8fbe\u4e86\u5fb7\u80b2\u7684\u90a3\u4e00\u539f\u5219\nA. \u758f\u5bfc\u539f\u5219\nB. \u6709\u7684\u653e\u77e2\u539f\u5219\nC. \u77e5\u884c\u7edf\u4e00\u539f\u5219\nD. \u957f\u5584\u6551\u5931\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.32384540068229783, "meta-math/MetaMath-Mistral-7B": 0.310313193127302, "itpossible/Chinese-Mistral-7B-v0.1": 0.37047588928040154, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.42363828119916935, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4415437293991451}}, {"question": "\u6cd5\u897f\u65af\u7684\u65b0\u95fb\u4e8b\u4e1a\u5c5e\u4e8e\u4ec0\u4e48\u9636\u6bb5\nA. \u81ea\u7531\u4e3b\u4e49\nB. \u793e\u4f1a\u4e3b\u4e49\nC. \u96c6\u6743\u4e3b\u4e49\nD. \u96c6\u56e2\u5784\u65ad\u8d44\u672c\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6929629251571825, "meta-math/MetaMath-Mistral-7B": 0.9135066843010077, "itpossible/Chinese-Mistral-7B-v0.1": 0.6761376643804187, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7487331467808147, "meta-llama/Meta-Llama-3-8B": 0.5540336947166429, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9577738151745152}}, {"question": "\u5728\u7edf\u8ba1\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u901a\u5e38\u4ee5\u6982\u7387\u7684\u5f62\u5f0f\u63cf\u8ff0\u4efb\u610f\u8bed\u53e5\u7684\u53ef\u80fd\u6027\uff0c\u5229\u7528\u6700\u5927\u76f8\u4f3c\u5ea6\u4f30\u8ba1\u8fdb\u884c\u5ea6\u91cf\uff0c\u5bf9\u4e8e\u4e00\u4e9b\u4f4e\u9891\u8bcd\uff0c\u65e0\u8bba\u5982\u4f55\u6269\u5927\u8bad\u7ec3\u6570\u636e\uff0c\u51fa\u73b0\u7684\u9891\u5ea6\u4ecd\u7136\u5f88\u4f4e\uff0c\u4e0b\u5217\u54ea\u79cd\u65b9\u6cd5\u80fd\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\nA. \u6570\u636e\u5e73\u6ed1\nB. N\u5143\u6587\u6cd5\nC. \u4e00\u5143\u6587\u6cd5\nD. \u4e00\u5143\u5207\u5206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9178674868386584, "meta-math/MetaMath-Mistral-7B": 0.9681887429319896, "itpossible/Chinese-Mistral-7B-v0.1": 0.7471176172678912, "HuggingFaceH4/zephyr-7b-beta": 0.999862751896702, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.932954377066253, "meta-llama/Meta-Llama-3-8B": 0.9538954368436, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9858372336251703}}, {"question": "\u64ad\u97f3\u5458\u548c\u4e3b\u6301\u4eba\u5728\u9762\u4e34\u5076\u53d1\u56f0\u5883\u6216\u9700\u8981\u8f6c\u6362\u5bf9\u8c61\u65f6\uff0c\u80fd\u8f6c\u627f\u81ea\u7136\uff0c\u4e14\u8bed\u8a00\u4e30\u5bcc\u591a\u5f69\uff0c\u8fd9\u5c5e\u4e8e\u521b\u9020\u6027\u601d\u7ef4\u4e2d\u7684\nA. \u6d41\u7545\u6027\nB. \u72ec\u521b\u6027\nC. \u81ea\u7136\u6027\nD. \u7075\u6d3b\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4556045653209304, "meta-math/MetaMath-Mistral-7B": 0.7285651930995702, "itpossible/Chinese-Mistral-7B-v0.1": 0.6119753796813616, "HuggingFaceH4/zephyr-7b-beta": 0.9468632440751004, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8919373485904687, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f13\u89e3\u548c\u89e3\u51b3\u6211\u56fd\u5f53\u4ee3\u793e\u4f1a\u95ee\u9898\u7684\u6839\u672c\u9014\u5f84\u662f\nA. \u5b8c\u5584\u793e\u4f1a\u4fdd\u969c\u5236\u5ea6\u3002\nB. \u6539\u9769\u4e0a\u5c42\u5efa\u7b51\u548c\u610f\u8bc6\u5f62\u6001\u3002\nC. \u89e3\u653e\u548c\u53d1\u5c55\u751f\u4ea7\u529b\u3002\nD. \u53d1\u5c55\u79d1\u5b66\u6280\u672f\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5836921973700113, "meta-math/MetaMath-Mistral-7B": 0.8504195613495658, "itpossible/Chinese-Mistral-7B-v0.1": 0.37993597849898714, "HuggingFaceH4/zephyr-7b-beta": 0.9417984492391366, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.676003163101417, "meta-llama/Meta-Llama-3-8B": 0.7193708838028927, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6964101550773678}}, {"question": "\u65b0\u95fb\u7acb\u6cd5\u662f\u6307\nA. \u8fd0\u7528\u6cd5\u5f8b\u624b\u6bb5\u7ba1\u7406\u548c\u63a7\u5236\u65b0\u95fb\u4e8b\u4e1a\u7684\u7a0b\u5e8f\u539f\u5219\u548c\u673a\u5236\nB. \u7edf\u6cbb\u9636\u7ea7\u5236\u5b9a\u3001\u9881\u5e03\u548c\u4fee\u8ba2\u65b0\u95fb\u6cd5\u5f8b\u7684\u7a0b\u5e8f\u548c\u8fc7\u7a0b\nC. \u56fd\u5bb6\u5236\u5b9a\u548c\u9881\u5e03\u7684\u6709\u5173\u65b0\u95fb\u4f20\u64ad\u7684\u6cd5\u5f8b\u6761\u6587\nD. \u8fd0\u7528\u65b0\u95fb\u5a92\u4f53\u5bf9\u56fd\u5bb6\u7acb\u6cd5\u7684\u5ba3\u4f20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u903b\u8f91\u56de\u5f52\u8f93\u51fa\u4e0e\u76ee\u6807\u5bf9\u6bd4\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u4e0b\u8bc4\u4f30\u6307\u6807\u4e2d\u54ea\u4e00\u9879\u4e0d\u9002\u7528\uff1f\nA. \u51c6\u786e\u5ea6\nB. \u5747\u65b9\u8bef\u5dee\nC. AUC-ROC\nD. Logloss\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4525781786483538, "meta-math/MetaMath-Mistral-7B": 0.9125826209418042, "itpossible/Chinese-Mistral-7B-v0.1": 0.39008525750841133, "HuggingFaceH4/zephyr-7b-beta": 0.756682872749623, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8142673614186398, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7936986958880587}}, {"question": "\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u4e0d\u6b63\u786e\u7684\u662f\nA. \u7528\u6eb4\u6c34\u65e2\u53ef\u4ee5\u9274\u522b\u7532\u70f7\u548c\u4e59\u70ef\uff0c\u4e5f\u53ef\u4ee5\u9664\u53bb\u7532\u70f7\u4e2d\u7684\u4e59\u70ef\nB. \u6cb9\u8102\u3001\u6dc0\u7c89\u3001\u86cb\u767d\u8d28\u5728\u4e00\u5b9a\u6761\u4ef6\u4e0b\u90fd\u80fd\u53d1\u751f\u6c34\u89e3\u53cd\u5e94\nC. \u4e59\u70ef\u548c\u82ef\u5747\u80fd\u53d1\u751f\u6c27\u5316\u53cd\u5e94\uff0c\u8bf4\u660e\u4e59\u70ef\u548c\u82ef\u5206\u5b50\u4e2d\u5747\u6709\u78b3\u78b3\u53cc\u952e\nD. \u6709\u673a\u5316\u5408\u7269\u4e2d\u6bcf\u4e2a\u78b3\u539f\u5b50\u6700\u591a\u5f62\u62104\u4e2a\u5171\u4ef7\u952e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6664508062213516}}, {"question": "\u7ecf\u4e2d\u592e\u519b\u59d4\u4e3b\u5e2d\u4e60\u8fd1\u5e73\u6279\u51c6\uff0c\u89e3\u653e\u519b\u603b\u53c2\u8c0b\u90e8\u3001\u603b\u653f\u6cbb\u90e8\u3001\u603b\u540e\u52e4\u90e8\u3001\u603b\u88c5\u5907\u90e8\u8054\u5408\u5370\u53d1\u300a\u5389\u884c\u8282\u7ea6\u4e25\u683c\u7ecf\u8d39\u7ba1\u7406\u7684\u89c4\u5b9a\u300b\uff0c\u5173\u4e8e\u6b64\u300a\u89c4\u5b9a\u300b\u4e0b\u5217\u8bf4\u6cd5\u9519\u8bef\u7684\u662f\nA. \u6309\u6218\u6597\u529b\u6807\u51c6\u82b1\u94b1\u529e\u4e8b\nB. \u89c4\u8303\u5206\u6563\u91c7\u8d2d\u5206\u6563\u652f\u4ed8\nC. \u4ece\u4e25\u7ba1\u63a7\u4f1a\u8bae\u96c6\u8bad\u548c\u516c\u52a1\u63a5\u5f85\u5f00\u652f\nD. \u4e25\u683c\u7ecf\u8d39\u5206\u914d\u4e0e\u5ba1\u6279\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2777229469001534, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4991427664426008}}, {"question": "\u6c6a\u9759\u4e4b\u5c5e\u4e8e\nA. \u73b0\u4ee3\u6d3e\u8bd7\u4eba\nB. \u8c61\u5f81\u8bd7\u6d3e\u8bd7\u4eba\nC. \u6e56\u7554\u8bd7\u4eba\nD. \u5c0f\u8bd7\u8bd7\u4eba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.27697441697632474, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.47802183132919573}}, {"question": "\u6d77\u56e0\u91cc\u5e0c\u4e8b\u6545\u56e0\u679c\u8fde\u9501\u7406\u8bba\u5c06\u4f24\u5bb3\u4e8b\u6545\u7684\u76f4\u63a5\u539f\u56e0\u786e\u5b9a\u4e3a\nA. \u4eba\u7684\u4e0d\u5b89\u5168\u884c\u4e3a\u548c\u7269\u7684\u4e0d\u5b89\u5168\u72b6\u6001\nB. \u4eba\u7684\u7f3a\u70b9\nC. \u4e8b\u6545\nD. \u9057\u4f20\u53ca\u793e\u4f1a\u73af\u5883\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7877107825834443, "meta-math/MetaMath-Mistral-7B": 0.9597793667705414, "itpossible/Chinese-Mistral-7B-v0.1": 0.7344131918386652, "HuggingFaceH4/zephyr-7b-beta": 0.8912898181102757, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7514749853254261, "meta-llama/Meta-Llama-3-8B": 0.765159419847113, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7096296270213737}}, {"question": "\u5e73\u5747\u603b\u6210\u672c\u51cf\u53bb\u5e73\u5747\u53ef\u53d8\u6210\u672c\u7b49\u4e8e\nA. \u56fa\u5b9a\u6210\u672c\nB. \u8fb9\u9645\u6210\u672c\nC. \u53ef\u53d8\u6210\u672c\nD. \u5e73\u5747\u56fa\u5b9a\u6210\u672c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8fd0\u2f64\u7b49\u5f0f\u6027\u8d28\u8fdb\u2f8f\u7684\u53d8\u5f62\uff0c\u6b63\u786e\u7684\u662f\nA. \u82e52x=3y\uff0c\u5219x/2 = y/3 \nB. \u5982\u679cam=bm\uff0c\u90a3\u4e48a=b \nC. \u82e510x=5\uff0c\u5219x=2\nD. \u5982\u679c a/m=b/m\uff0c\u90a3\u4e48a=b \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41407896539198114, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9083925896858698, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5105908121188107, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u632a\u5a01\u5267\u4f5c\u5bb6\u6613\u535c\u751f\u7684\u300a\u73a9\u5076\u4e4b\u5bb6\u300b\u4ece\u9898\u6750\u5185\u5bb9\u6765\u770b\u5c5e\u4e8e\nA. \u793e\u4f1a\u95ee\u9898\u5267\nB. \u53f2\u8bd7\u5267\nC. \u7ae5\u8bdd\u5267\nD. \u5b97\u6559\u5267\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8873448794317625, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9719185769328069}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\uff0c\u542b\u6709\u540d\u8bcd\u4f5c\u72b6\u8bed\uff0c\u8868\u793a\u5bf9\u4eba\u7684\u6001\u5ea6\u7684\u4e00\u53e5\u662f\nA. \u7530\u55ae\u4e43\u8d77\uff0c\u5f15\u9084\uff0c\u6771\u9109\u5750\uff0c\u5e2b\u4e8b\u4e4b\u3002\nB. \u9d6c\u4e4b\u5f99\u65bc\u5357\u51a5\u4e5f\uff0c\u6c34\u6483\u4e09\u5343\u91cc\u3002\nC. \u9ece\u4e18\u4e4b\u9b3c\u52b9\u5176\u5b50\u4e4b\u72c0\uff0c\u6276\u800c\u9053\u82e6\u4e4b\u3002\nD. \u5bb6\u4eba\u8eca\u621f\u6b32\u5f80\u5c31\u91ab\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.319195801224909, "meta-llama/Meta-Llama-3-8B": 0.29539205153207015, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c11\u65cf\u540c\u5316\u5206\u4e3a\u5f3a\u8feb\u540c\u5316\u4e0e\nA. \u81ea\u89c9\u540c\u5316\nB. \u81ea\u613f\u540c\u5316\nC. \u81ea\u7531\u540c\u5316\nD. \u81ea\u7136\u540c\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5224\u65ad\u56e0\u679c\u8054\u7cfb\u65f6\u7684\u5fc5\u8981\u6761\u4ef6\u662f\nA. \u8054\u7cfb\u7684\u5f3a\u5ea6\nB. \u8054\u7cfb\u7684\u5408\u7406\u6027\nC. \u65f6\u95f4\u987a\u5e8f\uff0c\u5373\u5148\u201c\u56e0\u201d\u540e\u201c\u679c\u201d\nD. \u8054\u7cfb\u7684\u4e00\u81f4\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9228761409101862, "meta-math/MetaMath-Mistral-7B": 0.992650478831775, "itpossible/Chinese-Mistral-7B-v0.1": 0.8918944011133372, "HuggingFaceH4/zephyr-7b-beta": 0.9995329205322587, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9526870137081016, "meta-llama/Meta-Llama-3-8B": 0.6536630724531365, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9902585028603463}}, {"question": "\u6c5f\u6cfd\u6c11\u66fe\u7ecf\u6307\u51fa\uff0c\u628a\u7956\u56fd\u548c\u4eba\u6c11\u7684\u5229\u76ca\u653e\u5728\u9996\u4f4d\uff0c\u4e3a\u7956\u56fd\u7684\u72ec\u7acb\u548c\u5bcc\u5f3a\u3001\u4e3a\u4eba\u6c11\u7684\u89e3\u653e\u548c\u5e78\u798f\u8d21\u732e\u6bd5\u751f\u7cbe\u529b\uff0c\u4ee5\u6b64\u4f5c\u4e3a\u4eba\u751f\u7684\u6700\u9ad8\u4ef7\u503c\uff0c\u8fd9\u662f\u4e2d\u56fd\u77e5\u8bc6\u5206\u5b50\u6210\u957f\u7684\u6b63\u786e\u9053\u8def\u3002\u8fd9\u5f3a\u8c03\u7684\u662f\nA. \u4e00\u4e2a\u4eba\u5bf9\u793e\u4f1a\u7684\u4ef7\u503c\u4e3b\u8981\u770b\u4ed6\u4ece\u793e\u4f1a\u4e2d\u53d6\u5f97\u4e86\u4ec0\u4e48\nB. \u8ffd\u6c42\u548c\u5b9e\u73b0\u7406\u60f3\u662f\u4e00\u4e2a\u8270\u82e6\u594b\u6597\u7684\u8fc7\u7a0b\nC. \u4e2d\u534e\u6c11\u65cf\u5bcc\u4e8e\u7231\u56fd\u4e3b\u4e49\u7684\u5149\u8363\u4f20\u7edf\nD. \u7231\u56fd\u4e3b\u4e49\u662f\u8c31\u5199\u58ee\u4e3d\u4eba\u751f\u7684\u529b\u91cf\u6e90\u6cc9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8446072766044381, "meta-math/MetaMath-Mistral-7B": 0.6655611789083795, "itpossible/Chinese-Mistral-7B-v0.1": 0.8514947140431106, "HuggingFaceH4/zephyr-7b-beta": 0.9955394320822702, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8851347231444935, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6532050213689634}}, {"question": "\u5e02\u653f\u5e9c\u5de5\u4f5c\u4eba\u5458\u7532\u63a5\u53d7\u8bf7\u6258\u4eba\u4e59\u7684 30 \u4e07\u5143\uff0c\u901a\u8fc7\u59b9\u592b\u5218\u67d0\uff08\u5e02\u516c\u5b89\u5c40\u5e72\u8b66\uff09\u8fdd\u89c4\u64a4\u9500\u4e86\u5bf9\u4e59\u7684\u7f51\u4e0a\u8ffd\u9003\u4fe1\u606f\u3002\u7532\u7684\u884c\u4e3a\u5e94\u8ba4\u5b9a\u4e3a\nA. \u53d7\u8d3f\u7f6a\nB. \u5229\u7528\u5f71\u54cd\u529b\u53d7\u8d3f\u7f6a\nC. \u6ee5\u7528\u804c\u6743\u7f6a\nD. \u4ecb\u7ecd\u8d3f\u8d42\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6709\u5173\u53d9\u8ff0\u6b63\u786e\u7684\u662f\nA. H2SO4\u3001Na2CO3\u3001FeCl3\u3001NaOH\u4e2d\u80fd\u4e24\u4e24\u53d1\u751f\u53cd\u5e94\u7684\u7269\u8d28\u5171\u67094\u7ec4\nB. \u5411FeI2\u6eb6\u6db2\u4e2d\u52a0\u5165\u76d0\u9178\u548c\u8fc7\u91cf\u7684H2O2\uff1a2I^\uff0d\uff0bH2O2\uff0b2H^\uff0b===I2\uff0b2H2O\nC. \u5411AlCl3\u6eb6\u6db2\u4e2d\u52a0\u5165\u6c28\u6c34\u5236Al(OH)3\uff1aAl^3\uff0b\uff0b3OH\uff0d===Al(OH)3\nD. \u5411CuCl2\u6eb6\u6db2\u4e2d\u52a0\u5165\u5c11\u91cf\u6c28\u6c34\uff1aCu2\uff0b\uff0b2OH\uff0d===Cu(OH)2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4088840916677829, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u8ba1\u7b97\u673a\u4e2d\u6570\u636e\u5b58\u50a8\u7684\u4fe1\u606f\u91cf\u5927\u5c0f\u7684\u57fa\u672c\u5355\u4f4d\u662f\nA. \u5b57\u8282\nB. \u5b57\u957f\nC. \u6247\u533a\nD. \u4e8c\u8fdb\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9768390309720104, "meta-math/MetaMath-Mistral-7B": 0.9994459166746964, "itpossible/Chinese-Mistral-7B-v0.1": 0.9349746507109644, "HuggingFaceH4/zephyr-7b-beta": 0.9992223729540993, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9852113522916963, "meta-llama/Meta-Llama-3-8B": 0.9647814141661648, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9996665425038086}}, {"question": "\u4e0b\u5217\u5bf9\u8bca\u65ad\u809d\u786c\u5316\u95e8\u8109\u9ad8\u538b\u75c7\u6700\u6709\u4ef7\u503c\u7684\u4f53\u5f81\u662f\nA. \u809d\u810f\u8d28\u5730\u575a\u786c\nB. \u813e\u810f\u80bf\u5927\nC. \u8718\u86db\u75e3\nD. \u8179\u58c1\u9759\u8109\u66f2\u5f20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6349274356806696, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6889519917284614, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.49513544832043754}}, {"question": "\u610f\u5927\u5229\u5f71\u7247\u300a\u5077\u81ea\u884c\u8f66\u7684\u4eba\u300b\u5c5e\u4e8e\nA. \u5de6\u5cb8\u6d3e\u7535\u5f71\nB. \u65b0\u73b0\u5b9e\u4e3b\u4e49\u7535\u5f71\nC. \u65b0\u6d6a\u6f6e\u7535\u5f71\nD. \u8868\u73b0\u4e3b\u4e49\u7535\u5f71\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2885095257630687, "meta-math/MetaMath-Mistral-7B": 0.3932534368794713, "itpossible/Chinese-Mistral-7B-v0.1": 0.4097932749405994, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.49077802894097217, "meta-llama/Meta-Llama-3-8B": 0.4181874540337694, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5973\u6027\uff0c25\u5c81\uff0c\u534a\u4e2a\u6708\u6765\u6015\u70ed\uff0c\u5fc3\u60b8\uff0c\u591a\u6c57\uff0c\u4f53\u91cd\u4e0b\u964d6kg\u3002\u67e5\u4f53\uff1a\u8840\u538b120/65mmHg\uff0c\u65e0\u7a81\u773c\uff0c\u7532\u72b6\u817a\u8f7b\u5ea6\u5f25\u6f2b\u6027\u80bf\u5927\uff0c\u53ef\u95fb\u53ca\u8840\u7ba1\u6742\u97f3\uff0c\u5fc3\u7387120\u6b21/\u5206\uff0c\u5f8b\u9f50\u3002\u82e5\u60a3\u8005\u672a\u6108\u800c\u53d1\u751f\u65e9\u5b55\uff0c\u6700\u4f73\u7684\u6cbb\u7597\u65b9\u6cd5\u662f\nA. \u53e3\u670d\u4e19\u786b\u6c27\u5627\u5576\nB. \u7532\u72b6\u817a\u624b\u672f\nC. 131I\u6cbb\u7597\nD. \u53e3\u670d\u7532\u5def\u54aa\u5511\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3976839562230386, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2731272040287072, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6385722724440872}}, {"question": "\u201c\u4e07\u9686\u4f1a\u8bae\u7684\u6210\u529f\u5bf9\u4e8e\u4e2d\u56fd\u7684\u91cd\u8981\u610f\u4e49\u53ef\u4ee5\u5206\u4e3a\u56fd\u5bb6\u95f4\u5173\u7cfb\u548c\u601d\u60f3\u610f\u8bc6\u4e24\u4e2a\u5c42\u6b21\u3002\u5e94\u8be5\u8bf4\uff0c\u4e07\u9686\u4f1a\u8bae\u662f\u4e2d\u56fd\u7b2c\u4e8c\u6b21\u5efa\u4ea4\u9ad8\u6f6e\u7684\u8d77\u70b9\u3002\u201d\u8fd9\u8bf4\u660e\u4e07\u9686\u4f1a\u8bae\nA. \u5426\u5b9a\u4e86\u201c\u4e00\u8fb9\u5012\u201d\u5916\u4ea4\u7684\u5408\u7406\u6027\nB. \u6709\u52a9\u4e8e\u7ed3\u675f\u610f\u8bc6\u5f62\u6001\u7684\u5236\u7ea6\nC. \u63a8\u52a8\u4e86\u897f\u65b9\u56fd\u5bb6\u4e0e\u4e2d\u56fd\u7684\u5efa\u4ea4\u70ed\nD. \u5229\u4e8e\u53d1\u5c55\u4e0e\u4e2d\u4e1c\u7684\u53cb\u597d\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u4ece\u738b\u67d0\u5904\u501f\u5f97\u4e00\u8f86\u4ef7\u503c10\u4e07\u5143\u7684\u7ade\u8d5b\u7528\u81ea\u884c\u8f66\uff0c\u56e0\u6025\u9700\u7528\u94b1\uff0c\u7532\u5c06\u8be5\u8f66\u8d28\u62bc\u7ed9\u5178\u5f53\u884c\uff0c\u5f97\u6b3e6\u4e07\u5143\u3002\u5728\u738b\u67d0\u7d22\u8981\u65f6\uff0c\u7532\u65e0\u529b\u8d4e\u56de\u8be5\u8f66\uff0c\u53c8\u5411\u674e\u67d0\u501f\u5f97\u4e00\u8f86\u4ef7\u503c15\u4e07\u5143\u7684\u7ade\u8d5b\u7528\u81ea\u884c\u8f66\uff0c\u5c06\u8be5\u8f66\u8d28\u62bc\u7ed9\u540c\u4e00\u5178\u5f53\u884c\uff0c\u5f97\u6b3e10\u4e07\u5143\u540e\u8d4e\u56de\u738b\u67d0\u7684\u81ea\u884c\u8f66\u3002\u7532\u5c06\u81ea\u884c\u8f66\u4ea4\u8fd8\u7ed9\u738b\u67d0\u540e\u6f5c\u9003\uff0c\u5bfc\u81f4\u674e\u67d0\u7684\u81ea\u884c\u8f66\u671f\u6ee1\u672a\u8d4e\u3002\u7532\u8bc8\u9a97\u7684\u91d1\u989d\u4e3a\nA. 6\u4e07\u5143\nB. 10\u4e07\u5143\nC. 25\u4e07\u5143\nD. 15\u4e07\u5143\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7257348721592318, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e91\u5357\u5730\u533a\u50a3\u65cf\u3001\u5fb7\u6602\u65cf\u3001\u963f\u660c\u65cf\u3001\u5e03\u6717\u65cf\u3001\u4f64\u65cf\u7684\u4fe1\u5f92\uff0c\u4fe1\u4ef0\u7684\u5b97\u6559\u4e3b\u8981\u662f\nA. \u5927\u4e58\u4f5b\u6559\nB. \u4f0a\u65af\u5170\u6559\nC. \u5c0f\u4e58\u4f5b\u6559\nD. \u85cf\u4f20\u4f5b\u6559\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.44910846613661165, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6547762393567108, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6cd5\u5f8b\u7684\u5f3a\u5236\u5b9e\u65bd\u90fd\u662f\u901a\u8fc7\u6cd5\u5b9a\u65f6\u95f4\u4e0e\u6cd5\u5b9a\u7a7a\u95f4\u4e0a\u7684\u6b65\u9aa4\u548c\u65b9\u5f0f\u800c\u5f97\u4ee5\u8fdb\u884c\u7684\uff0c\u6b64\u5373\u6240\u8c13\nA. \u6cd5\u7684\u7a0b\u5e8f\u6027\nB. \u6cd5\u7684\u5229\u5bfc\u6027\nC. \u6cd5\u7684\u56fd\u5bb6\u6027\nD. \u6cd5\u7684\u610f\u5fd7\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9234764044525482, "meta-math/MetaMath-Mistral-7B": 0.990965553490942, "itpossible/Chinese-Mistral-7B-v0.1": 0.719584993641575, "HuggingFaceH4/zephyr-7b-beta": 0.9998769451258436, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9944762723512239, "meta-llama/Meta-Llama-3-8B": 0.6533484061804418, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9719354321729163}}, {"question": "\u54ea\u4e2a\u56fd\u5bb6\u6700\u65e9\u53d1\u884c\u4f53\u80b2\u5f69\u7968\nA. \u57c3\u53ca\nB. \u5e0c\u814a\nC. \u5fb7\u56fd\nD. \u610f\u5927\u5229\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.36078609119776345, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u5b66\u751f\u5bb6\u957f\u7ed9\u4f59\u8001\u5e08\u9001\u793c\uff0c\u60f3\u8ba9\u4f59\u8001\u5e08\u7ed9\u5176\u5b69\u5b50\u5b89\u6392\u6700\u597d\u7684\u5ea7\u4f4d\uff0c\u4f59\u8001\u5e08\u62d2\u7edd\u4e86\u3002\u4f59\u8001\u5e08\u7684\u505a\u6cd5\nA. \u6b63\u786e\uff0c\u7b26\u5408\u4e25\u6148\u76f8\u6d4e\u7684\u8981\u6c42\nB. \u4e0d\u6b63\u786e\uff0c\u4e0d\u5229\u4e8e\u53d6\u5f97\u5bb6\u957f\u4fe1\u4efb\nC. \u4e0d\u6b63\u786e\uff0c\u4e0d\u7b26\u5408\u5bb6\u6821\u6c9f\u901a\u8981\u6c42\nD. \u6b63\u786e\uff0c\u7b26\u5408\u5ec9\u6d01\u4ece\u6559\u7684\u8981\u6c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8219902266636842, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u73b0\u4ee3\u5bb4\u4f1a\u793c\u4eea\u4e0d\u5305\u62ec\nA. \u5bb4\u4f1a\u996e\u9152\u793c\u4eea\nB. \u5403\u751c\u54c1\u793c\u4eea\nC. \u5bb4\u4f1a\u996e\u98df\u65b9\u793c\u4eea\nD. \u996e\u8336\u793c\u4eea\u793c\u4eea\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.49708930713115246, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4042843481163547, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4576403302121437}}, {"question": "\u5f53\u6742\u5408\u57fa\u56e0\u5bf9\u6570\u4e0e\u4ea4\u914d\u4ee3\u6570\u76f8\u540c\u65f6\uff0c\u56de\u4ea4\u4e0e\u81ea\u4ea4\u540e\u4ee3\u7fa4\u4f53\u5177\u6709\u76f8\u540c\u7684\nA. \u7eaf\u5408\u4f53\u6bd4\u4f8b\nB. \u6027\u72b6\u5747\u503c\nC. \u5355\u79cd\u57fa\u56e0\u578b\u7eaf\u5408\u8fdb\u5ea6\nD. \u7eaf\u5408\u57fa\u56e0\u578b\u79cd\u7c7b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3416622501780412, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.504969215730835, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u628a\u4e0b\u5217\u53e5\u5b50\u7ec4\u6210\u8bed\u610f\u8fde\u8d2f\u7684\u8bed\u6bb5\uff0c\u6392\u5e8f\u6700\u6070\u5f53\u7684\u4e00\u9879\u662f\uff08 \uff09i.\u4ece\u7a97\u5185\u5f80\u5916\u770b\u65f6\uff0c\u90a3\u4e00\u6735\u767d\u83b2\u5df2\u7ecf\u8c22\u4e86\uff0c\u767d\u74e3\u513f\u5c0f\u8239\u822c\u6563\u98d8\u5728\u6c34\u9762\u3002ii.\u90a3\u4e00\u6735\u7ea2\u83b2\uff0c\u6628\u591c\u8fd8\u662f\u542b\u82de\u7684\uff0c\u4eca\u6668\u5374\u5f00\u6ee1\u4e86\uff0c\u4ead\u4ead\u5730\u5728\u7eff\u53f6\u4e2d\u95f4\u7acb\u7740\u3002iii.\u6897\u4e0a\u53ea\u7559\u4e2a\u5c0f\u5c0f\u7684\u83b2\u84ec\uff0c\u548c\u51e0\u6839\u6de1\u9ec4\u8272\u7684\u82b1\u987b\u3002vi.\u534a\u591c\u91cc\u542c\u89c1\u7e41\u6742\u7684\u96e8\u58f0\uff0c\u65e9\u8d77\u662f\u6d53\u9634\u7684\u5929\uff0c\u6211\u89c9\u5f97\u6709\u4e9b\u70e6\u95f7\u3002\nA. i.iii.ii.vi. \nB. vi.i.iii.ii.\nC. vi.i.ii.iii. \nD. i.ii.vi.iii. \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.31283638571410965, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6307\u4ee4\u7cfb\u7edf\u4e2d\u91c7\u2f64\u4e0d\u540c\u5bfb\u5740\u2f45\u5f0f\u7684\u2f6c\u7684\u4e3b\u8981\u662f\nA. \u5b9e\u73b0\u5b58\u50a8\u7a0b\u5e8f\u548c\u7a0b\u5e8f\u63a7\u5236\nB. \u53ef\u4ee5\u76f4\u63a5\u8bbf\u95ee\u5916\u5b58\nC. \u7f29\u77ed\u6307\u4ee4\u2ed3\u5ea6\uff0c\u6269\u2f24\u5bfb\u5740\u7a7a\u95f4\uff0c\u63d0\u2fbc\u7f16\u7a0b\u7075\u6d3b\u6027\nD. \u63d0\u4f9b\u6269\u5c55\u64cd\u4f5c\u7801\u7684\u53ef\u80fd\u5e76\u964d\u4f4e\u8bd1\u7801\u96be\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8571088137352657, "meta-math/MetaMath-Mistral-7B": 0.968565304771062, "itpossible/Chinese-Mistral-7B-v0.1": 0.6643600348012851, "HuggingFaceH4/zephyr-7b-beta": 0.9702906021312379, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8974461006943917, "meta-llama/Meta-Llama-3-8B": 0.9244749914735, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9927477404522806}}, {"question": "\u201c\u4f46\u613f\u4eba\u957f\u4e45\uff0c\u5343\u91cc\u5171\u5a75\u5a1f\u201d\uff0c\u82cf\u4e1c\u5761\u501f\u8fd9\u9996\u8bcd\u8868\u8fbe\u4e86\u5bf9\u8c01\u7684\u601d\u5ff5\u4e4b\u60c5\nA. \u59bb\u5b50\nB. \u670b\u53cb\nC. \u7236\u4eb2\nD. \u5144\u5f1f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f9d\u636e\u5a01\u5ec9.\u5927\u5185Z\u7406\u8bba\uff0c\u5178\u578b\u7684\u65e5\u672c\u4f01\u4e1a\u7ba1\u7406\u6a21\u5f0f\u5373J\u578b\u7ba1\u7406\u6a21\u5f0f\u7684\u7279\u70b9\u662f\nA. \u7ec8\u8eab\u96c7\u4f63\u5236\u3001\u7f13\u6162\u8bc4\u4ef7\u4e0e\u664b\u5347\u3001\u5f62\u5f0f\u5316\u7684\u63a7\u5236\u65b9\u5f0f\u3001\u96c6\u4f53\u51b3\u7b56\u4e0e\u96c6\u4f53\u8d1f\u8d23\nB. \u77ed\u671f\u96c7\u4f63\u5236\u3001\u7f13\u6162\u8bc4\u4ef7\u4e0e\u664b\u5347\u3001\u542b\u84c4\u548c\u5185\u5728\u7684\u63a7\u5236\u3001\u4e2a\u4eba\u51b3\u7b56\u4e0e\u4e2a\u4eba\u8d1f\u8d23\nC. \u7ec8\u8eab\u96c7\u4f63\u5236\u3001\u7f13\u6162\u8bc4\u4ef7\u4e0e\u664b\u5347\u3001\u542b\u84c4\u548c\u5185\u5728\u7684\u63a7\u5236\u3001\u4e2a\u4eba\u51b3\u7b56\u4e0e\u4e2a\u4eba\u8d1f\u8d23\nD. \u7ec8\u8eab\u96c7\u4f63\u5236\u3001\u7f13\u6162\u8bc4\u4ef7\u4e0e\u664b\u5347\u3001\u542b\u84c4\u548c\u5185\u5728\u7684\u63a7\u5236\u3001\u96c6\u4f53\u51b3\u7b56\u4e0e\u96c6\u4f53\u8d1f\u8d23\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7790652617087002, "meta-math/MetaMath-Mistral-7B": 0.7889559114880483, "itpossible/Chinese-Mistral-7B-v0.1": 0.36815856345072534, "HuggingFaceH4/zephyr-7b-beta": 0.9999556003305148, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47374354820769543, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u7ee7\u627f\u6cd5\u300b\u7b2c16\u6761\u7b2c2\u6b3e\u89c4\u5b9a\uff1a\u201c\u516c\u6c11\u53ef\u4ee5\u7acb\u9057\u5631\u5c06\u4e2a\u4eba\u8d22\u4ea7\u6307\u5b9a\u7531\u6cd5\u5b9a\u7ee7\u627f\u4eba\u7684\u4e00\u4eba\u6216\u8005\u6570\u4eba\u7ee7\u627f\u3002\u201d\u4ece\u6cd5\u7684\u89c4\u8303\u4f5c\u7528\u770b\uff0c\u8be5\u9879\u89c4\u5b9a\u5c5e\u4e8e\u4e0b\u5217\u54ea\u4e00\u60c5\u51b5\uff1f\nA. \u786e\u5b9a\u7684\u6307\u5f15\nB. \u4e2a\u522b\u6307\u5f15\nC. \u975e\u89c4\u8303\u6027\u6307\u5f15\nD. \u6709\u9009\u62e9\u7684\u6307\u5f15\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9496004307315348, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3926712330267577}}, {"question": "\u4e00\u6bb5\u7ef3\u5b50\uff0c\u7b2c\u4e00\u6b21\u526a\u53bb\u5168\u957f\u76841/5\uff0c\u7b2c\u4e8c\u6b21\u526a\u53bb\u5168\u957f\u76841/2\uff0c\u8fd8\u526930\u7c73\uff0c\u8fd9\u6839\u7ef3\u5b50\u5168\u957f\nA. 90\nB. 100\nC. 80\nD. 160\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4668385765046321}}, {"question": "\u4e00\u7ec4\u4e0d\u4e00\u81f4\u7684\u524d\u63d0()\u63a8\u5bfc\u51fa\u4e0d\u4e00\u81f4\u7684\u7ed3\u8bba\u3002\nA. \u53ef\u80fd\nB. \u4e0d\u4f1a\nC. \u5fc5\u7136\nD. \u4e0d\u786e\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.371855153616996, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u8bba\u8bed\u00b7\u516b\u4f7e\u300b\u8bb0\u5f55\u5b54\u5b50\u7684\u8bdd\uff1a\u201c\u5468\u76d1\u4e8e\u4e8c\u4ee3\uff0c\u90c1\u90c1\u4e4e\u6587\u54c9\uff01\u543e\u4ece\u5468\u3002\u201d\u300a\u8340\u5b50\u300b\u8bf4\uff1a\u201c\u7531\u58eb\u4ee5\u4e0a\u5219\u5fc5\u4ee5\u793c\u8282\u4e4b\u3002\u201d\u5bf9\u4ee5\u4e0a\u6750\u6599\u7406\u89e3\u51c6\u786e\u7684\u662f\uff1aa\u201c\u5468\u76d1\u4e8e\u4e8c\u4ee3\u201d\u4e2d\u7684\u201c\u4e8c\u4ee3\u201d\u662f\u6307\u590f\u5546\u4e24\u4ee3\uff1bb\u201c\u543e\u4ece\u5468\u201d\u6307\u5b54\u5b50\u5c0a\u5d07\u5468\u793c\uff1bc\u590f\u5546\u653f\u6cbb\u6587\u5316\u5df2\u6210\u719f\uff1bd\u300a\u8340\u5b50\u300b\u6307\u51fa\u4e86\u793c\u4e50\u5236\u5728\u7ef4\u62a4\u5206\u5c01\u5236\u3001\u5b97\u6cd5\u5236\u65b9\u9762\u7684\u91cd\u5927\u4f5c\u7528\nA. abc\nB. abd\nC. bcd\nD. acd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u6839\u3001\u830e\u3001\u79cd\u5b50\u7c7b\u4e3a\u4e3b\u7684\u6c64\u5242\uff0c\u714e\u716e\u524d\u9700\u6d78\u6ce1\nA. 10-15\u5206\u949f\nB. 60\u5206\u949f\nC. 20-30\u5206\u949f\nD. 30-45\u5206\u949f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6b66\u5668\u662f\u6218\u4e89\u80dc\u8d1f\u7684\nA. \u51b3\u5b9a\u529b\u91cf\nB. \u4e00\u822c\u529b\u91cf\nC. \u91cd\u8981\u529b\u91cf\nD. \u5236\u80dc\u529b\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.40861556016427, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5688808453941943, "HuggingFaceH4/zephyr-7b-beta": 0.9873929973584569, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4158830357742739, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9172082927102067}}, {"question": "\u8499\u53e4\u65cf\u559c\u98df\u7684\u3001\u62db\u5f85\u5c0a\u8d35\u5ba2\u4eba\u7684\u6700\u4e3a\u4e30\u76db\u548c\u6700\u4e3a\u8bb2\u7a76\u7684\u4e00\u79cd\u4f20\u7edf\u5bb4\u5e2d\u662f\nA. \u5168\u72d7\u5e2d\nB. \u5168\u9a6c\u5e2d\nC. \u5168\u725b\u5e2d\nD. \u5168\u7f8a\u5e2d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4685110143602662, "meta-math/MetaMath-Mistral-7B": 0.7910196038595715, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9996716148964736, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4226697335403775, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8501443058094469}}, {"question": "\u4e0b\u5217\u4e0d\u5c5e\u4e8e\u5546\u4e1a\u8d3f\u8d42\u65b9\u5f0f\u7684\u662f\nA. \u63d0\u4f9b\u5404\u79cd\u4f63\u91d1\u6216\u8d39\u7528\u3001\u7ed9\u4e88\u56de\u6263\nB. \u8d60\u9001\u5b9e\u7269\nC. \u8d60\u9001\u8d2d\u7269\u5361\u3001\u8d60\u9001\u6709\u4ef7\u8bc1\u5238(\u5305\u62ec\u80a1\u7968\u3001\u503a\u5238\u7b49)\nD. \u8d60\u9001\u8282\u65e5\u5361\u7247\u5e76\u5fae\u4fe1\u53d1\u9001\u795d\u798f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8857417156667575, "meta-math/MetaMath-Mistral-7B": 0.9980357001339237, "itpossible/Chinese-Mistral-7B-v0.1": 0.6799657320431312, "HuggingFaceH4/zephyr-7b-beta": 0.999971490396767, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9941306169526135, "meta-llama/Meta-Llama-3-8B": 0.77807263654179, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8748709099051837}}, {"question": "\u5728\u5145\u5206\u9700\u6c42\u7684\u60c5\u51b5\u4e0b\uff0c\u5e02\u573a\u8425\u9500\u7ba1\u7406\u7684\u4efb\u52a1\u662f\nA. \u7ef4\u6301\u5e02\u573a\u8425\u9500\nB. \u523a\u6fc0\u5e02\u573a\u8425\u9500\nC. \u6539\u53d8\u5e02\u573a\u8425\u9500\nD. \u534f\u8c03\u5e02\u573a\u8425\u9500\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.353703093539192, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3588823130168717, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u201c\u4e00\u4e2a\u4eba\u5bf9\u5176\u4ed6\u4e00\u5207\u4eba\u201d\u7684\u5f62\u5f0f\u8868\u73b0\u51fa\u6765\u7684\u6cd5\u5f8b\u5173\u7cfb\u6709\nA. \u76f8\u90bb\u5173\u7cfb\nB. \u7269\u6743\u5173\u7cfb\nC. \u7ee7\u627f\u5173\u7cfb\nD. \u503a\u6743\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.47139482437594743, "itpossible/Chinese-Mistral-7B-v0.1": 0.371068906204966, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4394042026589986, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8ba1\u7b97\u673a\u7684\u4e09\u7c7b\u603b\u7ebf\u4e2d\uff0c\u4e0d\u5305\u62ec\nA. \u63a7\u5236\u603b\u7ebf\nB. \u5730\u5740\u603b\u7ebf\nC. \u4f20\u8f93\u603b\u7ebf\nD. \u6570\u636e\u603b\u7ebf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4589629238350974, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6126103789494174, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u624b\u90e8\u89e6\u7535\u540e\u65e0\u6cd5\u677e\u8131\u5e26\u7535\u4f53\u7684\u539f\u56e0\u662f\nA. \u624b\u90e8\u808c\u8089\u75c9\u631b\nB. \u5931\u53bb\u77e5\u89c9\nC. \u7cbe\u795e\u5931\u5e38\nD. \u60ca\u614c\u8fc7\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3713329068544684, "meta-math/MetaMath-Mistral-7B": 0.718541758191147, "itpossible/Chinese-Mistral-7B-v0.1": 0.8781990782311176, "HuggingFaceH4/zephyr-7b-beta": 0.7953256854636601, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5292563273920333, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ea4\u611f\u795e\u7ecf\u7cfb\u7edf\u5174\u594b\u65f6\uff0c\u5f15\u8d77\u7684\u751f\u7406\u6548\u5e94\u662f\nA. \u77b3\u5b54\u5f00\u5927\u808c\u6536\u7f29\nB. \u652f\u6c14\u7ba1\u5e73\u6ed1\u808c\u6536\u7f29\nC. \u80c3\u80a0\u8fd0\u52a8\u589e\u5f3a\nD. \u4fc3\u8fdb\u80f0\u5c9b\u7d20\u7684\u5206\u6ccc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.37569982832624943, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe\u67d0\u968f\u673a\u53d8\u91cf$X$\u7684\u751f\u5b58\u51fd\u6570\u4e3a:$S(x)=a x^3+b\uff0c0<=x<=k$\u3002\u82e5E(X)=45\uff0c\u5219$\\operatorname{Var}(X)=$\u3002\nA. 120\nB. 90\nC. 135\nD. 450\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3216592693751747, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2989226068355507, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.29539205153207015, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u54fa\u4e73\u671f\u5987\u5973\u6bcf\u65e5\u6444\u5165\u7684\u86cb\u767d\u8d28\u5e94\u6bd4\u975e\u598a\u5a20\u5987\u5973\u591a\u6444\u5165\u591a\u5c11\u514b\u81b3\u98df\u86cb\u767d\u8d28\uff1f\nA. 10\nB. 20\nC. 25\nD. 15\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4028736874644724, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.47098022470269785}}, {"question": "\u793e\u4f1a\u4e3b\u4e49\u7684\u6839\u672c\u76ee\u7684\u5728\u4e8e\nA. \u9547\u538b\u8d44\u4ea7\u9636\u7ea7\u7684\u53cd\u6297\nB. \u6d88\u706d\u5265\u524a\u3001\u6d88\u9664\u4e24\u6781\u5206\u5316\uff0c\u6700\u7ec8\u8fbe\u5230\u5171\u540c\u5bcc\u88d5\nC. \u5efa\u7acb\u65e0\u4ea7\u9636\u7ea7\u4e13\u653f \nD. \u5de9\u56fa\u5171\u4ea7\u515a\u7684\u9886\u5bfc \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8439743629846208, "meta-math/MetaMath-Mistral-7B": 0.9910371387064367, "itpossible/Chinese-Mistral-7B-v0.1": 0.9246452568959929, "HuggingFaceH4/zephyr-7b-beta": 0.9983570660976421, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9392260831888972, "meta-llama/Meta-Llama-3-8B": 0.9747015394201898, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.996116046974529}}, {"question": "\u7ed8\u5236\u7edf\u8ba1\u56fe\u65f6\uff0c\u8981\u80fd\u6e05\u695a\u5730\u8868\u793a\u6570\u91cf\u589e\u51cf\u53d8\u5316\u7684\u60c5\u51b5\uff0c\u5e94\u9009\u7528\nA. AB\nB. \u6247\u5f62\u7edf\u8ba1\u56fe\nC. \u6298\u7ebf\u7edf\u8ba1\u56fe\nD. \u6761\u5f62\u7edf\u8ba1\u56fe\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5860914659133937, "meta-math/MetaMath-Mistral-7B": 0.8514948295101389, "itpossible/Chinese-Mistral-7B-v0.1": 0.5382584234801103, "HuggingFaceH4/zephyr-7b-beta": 0.9933377747273396, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8711309543384564, "meta-llama/Meta-Llama-3-8B": 0.3867179772068803, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u5230\u4e2d\u5e74\uff0c\u592b\u59bb\u6027\u7231\u8868\u8fbe\u5e94\u5f53\u662f\nA. \u76f8\u77e5\u9ed8\u5951\u7684\u6700\u4f73\u5339\u914d\u9636\u6bb5\nB. \u4ee5\u7cbe\u795e\u4e4b\u7231\u66ff\u4ee3\u8089\u6b32\nC. \u653e\u5f03\nD. \u514b\u5236\u8282\u6b32\u4e3a\u4e3b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41942772178129184, "meta-math/MetaMath-Mistral-7B": 0.43998825292819, "itpossible/Chinese-Mistral-7B-v0.1": 0.8159027919725951, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8830036079142622, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7657682572532684}}, {"question": "\u5212\u5206\u90e8\u95e8\u6cd5\u7b2c\u4e00\u4f4d\u7684\u6807\u51c6\u662f\nA. \u6cd5\u5f8b\u8c03\u6574\u673a\u5236\nB. \u793e\u4f1a\u5173\u7cfb\u4ee5\u53ca\u6cd5\u5f8b\u8c03\u6574\u90fd\u662f\u5ba2\u89c2\u7684\nC. \u6cd5\u5f8b\u6240\u8c03\u6574\u7684\u793e\u4f1a\u5173\u7cfb\u79cd\u7c7b\nD. \u4e2d\u56fd\u7279\u8272\u793e\u4f1a\u4e3b\u4e49\u6cd5\u5f8b\u4f53\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39739913344026007, "meta-math/MetaMath-Mistral-7B": 0.3825322550539375, "itpossible/Chinese-Mistral-7B-v0.1": 0.41010460035012436, "HuggingFaceH4/zephyr-7b-beta": 0.9804972218392605, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4740698349299186, "meta-llama/Meta-Llama-3-8B": 0.714997584061957, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.756785366897121}}, {"question": "\u7ba1\u7406\u662f\u4e00\u79cd\u793e\u4f1a\u73b0\u8c61\uff0c\u5b83\u8d77\u6e90\u4e8e\nA. \u79c1\u6709\u5236\u7684\u51fa\u73b0\nB. \u4eba\u7c7b\u7684\u5171\u540c\u52b3\u52a8\nC. \u56fd\u5bb6\u7684\u4ea7\u751f\nD. \u5de5\u5546\u4e1a\u7684\u5174\u8d77\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7832769362604091, "meta-math/MetaMath-Mistral-7B": 0.9567839749192754, "itpossible/Chinese-Mistral-7B-v0.1": 0.32205625344145955, "HuggingFaceH4/zephyr-7b-beta": 0.9558609441158069, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4486432138495854, "meta-llama/Meta-Llama-3-8B": 0.7734649101497252, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.86132287725942}}, {"question": "\u4e0b\u5217\u9879\u76ee\u7ba1\u7406\u76f8\u5173\u8d44\u6599\u4e2d\uff0c\u80fd\u591f\u53cd\u6620\u9879\u76ee\u7ae3\u5de5\u9a8c\u6536\u4fe1\u606f\u7684\u662f\nA. \u65bd\u5de5\u5b89\u5168\u8bbe\u65bd\u9a8c\u6536\u8bb0\u5f55\u8868\nB. \u5e74\u5ea6\u5b8c\u6210\u5de5\u4f5c\u5206\u6790\u8868\nC. \u5355\u4f4d\u5de5\u7a0b\u4ea4\u5de5\u8d28\u91cf\u6838\u5b9a\u8868\nD. \u9879\u76ee\u6210\u672c\u504f\u5dee\u5206\u6790\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8735927963776479, "meta-math/MetaMath-Mistral-7B": 0.9488427551370076, "itpossible/Chinese-Mistral-7B-v0.1": 0.9476575848523009, "HuggingFaceH4/zephyr-7b-beta": 0.9995718172720731, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9421661217808239, "meta-llama/Meta-Llama-3-8B": 0.9142884941456334, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9966100081823955}}, {"question": "\u534a\u5bfc\u4f53\u53ea\u8bfb\u5b58\u50a8\u5668\uff08ROM\uff09\u4e0e\u534a\u5bfc\u4f53\u968f\u673a\u5b58\u50a8\u5668\uff08RAM\uff09\u7684\u4e3b\u8981\u533a\u522b\u5728\u4e8e\nA. ROM\u6389\u7535\u540e\uff0c\u4fe1\u606f\u4f1a\u4e22\u5931\uff0cRAM\u5219\u4e0d\u4f1a\nB. ROM\u662f\u5185\u5b58\u50a8\u5668\uff0cRAM\u662f\u5916\u5b58\u50a8\u5668\nC. RAM\u662f\u5185\u5b58\u50a8\u5668\uff0cROM\u662f\u5916\u5b58\u50a8\u5668\nD. ROM\u53ef\u4ee5\u6c38\u4e45\u4fdd\u5b58\u4fe1\u606f\uff0cRAM\u5728\u6389\u7535\u540e\u4fe1\u606f\u4f1a\u4e22\u5931\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5079005081667411, "meta-math/MetaMath-Mistral-7B": 0.8720635222966914, "itpossible/Chinese-Mistral-7B-v0.1": 0.7916506636563405, "HuggingFaceH4/zephyr-7b-beta": 0.9997820828674852, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5903270571328658, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1949\u5e74\u540e\uff0e\u4e2d\u56fd\u79ef\u6781\u4e3b\u52a8\u5730\u878d\u4eba\u4e16\u754c\u3002\u5c06\u4e0b\u5217\u91cd\u5927\u5916\u4ea4\u4e8b\u4ef6\u6309\u65f6\u95f4\u5148\u540e\u987a\u5e8f\u6392\u5217\uff0c\u6b63\u786e\u7684\u662f\u3002a\u91cd\u8fd4\u8054\u5408\u56fd\uff1bb\u4e2d\u7f8e\u5173\u7cfb\u6b63\u5e38\u5316\uff1bc\u53c2\u52a0\u4e07\u9686\u4f1a\u8bae\uff1bd\u52a0\u5165\u4e16\u754c\u8d38\u6613\u7ec4\u7ec7\nA. cabd\nB. cbad\nC. abcd\nD. bacd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.43693093449896003}}, {"question": "\u4e0b\u5217\u5b9e\u9a8c\u64cd\u4f5c\u6216\u8005\u7ed3\u8bba\u6b63\u786e\u7684\u662f\nA. \u914d\u5236\u4e00\u5b9a\u7269\u8d28\u7684\u91cf\u6d53\u5ea6\u7684\u6eb6\u6db2\u65f6\uff0c\u5bb9\u91cf\u74f6\u5fc5\u987b\u6d01\u51c0\u3001\u5e72\u71e5\u3001\u4e0d\u6f0f\u6c34\nB. \u5b9e\u9a8c\u5ba4\u914d\u5236500 mL 0.2 mol\u00b7L^\uff0d1\u7684\u786b\u9178\u4e9a\u94c1\u6eb6\u6db2\uff0c\u5176\u64cd\u4f5c\u662f\uff1a\u7528\u5929\u5e73\u79f027.8 g\u7eff\u77fe\uff0c\u653e\u5165500 mL\u5bb9\u91cf\u74f6\uff0c\u52a0\u6c34\u6eb6\u89e3\u3001\u7a00\u91ca\u3001\u5b9a\u5bb9\u3001\u6447\u5300\nC. \u5b9e\u9a8c\u5ba4\u91cc\u9700\u8981480 mL 2.0 mol\u00b7L^\uff0d1\u7684\u6c22\u6c27\u5316\u94a0\u6eb6\u6db2\uff0c\u914d\u5236\u6eb6\u6db2\u65f6\u5148\u79f0\u91cf\u6c22\u6c27\u5316\u94a0\u56fa\u4f5338.4 g\uff0c\u7136\u540e\u518d\u6309\u7167\u6eb6\u89e3\u3001\u51b7\u5374\u3001\u79fb\u6db2\u3001\u5b9a\u5bb9\u3001\u6447\u5300\u7b49\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c\nD. 25 \u00b0C\u65f6\uff0c\u7528\u60f0\u6027\u7535\u6781\u7535\u89e3\u67d0\u6d53\u5ea6\u7684NaOH\u6eb6\u6db2\uff0c\u4e00\u5c0f\u6bb5\u65f6\u95f4\u540e\uff0cNaOH\u6eb6\u6db2\u7684\u6d53\u5ea6\u53ef\u80fd\u589e\u5927\uff0c\u4e5f\u53ef\u80fd\u4e0d\u53d8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u7ec4\u7ec7\u4e2d\u64cd\u4f5c\u6216\u4e8b\u52a1\u5904\u7406\u6d41\u7a0b\u7684\u4e00\u79cd\u63cf\u8ff0\u3001\u8ba1\u5212\u4e0e\u89c4\u5b9a\uff0c\u88ab\u79f0\u4e3a\nA. \u7a0b\u5e8f\nB. \u4e60\u60ef\nC. \u76ee\u6807\nD. \u6743\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6772021578787777, "meta-math/MetaMath-Mistral-7B": 0.995474616227922, "itpossible/Chinese-Mistral-7B-v0.1": 0.9262712843554439, "HuggingFaceH4/zephyr-7b-beta": 0.8120698707124843, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3588823130168717, "meta-llama/Meta-Llama-3-8B": 0.9483873160727244, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9790987166500321}}, {"question": "\u4e0b\u5217\u54ea\u79cd\u9176\u662f\u7cd6\u9175\u89e3\u548c\u7cd6\u5f02\u751f\u9014\u5f84\u4e2d\u5171\u6709\u7684\nA. \u7518\u6cb9\u919b-3-\u78f7\u9178\u9176\nB. \u4e19\u916e\u9178\u7fa7\u5316\u9176\nC. \u5df2\u7cd6\u6fc0\u9176\nD. \u4e19\u916e\u9178\u6fc0\u9176\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2801288226217134, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.37905047623827515}}, {"question": "\u5206\u5b50\u5f0f\u4e3aC5H12O\u7684\u9187\u4e0e\u548c\u5b83\u76f8\u5bf9\u5206\u5b50\u8d28\u91cf\u76f8\u540c\u7684\u4e00\u5143\u7fa7\u9178\u8fdb\u884c\u916f\u5316\u53cd\u5e94\uff0c\u751f\u6210\u7684\u916f\u5171\u6709\u7684\u79cd\u6570(\u4e0d\u8003\u8651\u7acb\u4f53\u5f02\u6784)\u4e3a\nA. 16\nB. 15\nC. 18\nD. 17\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.28850952576306876, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u6211\u65e7\u8bf4\u4ee5\u5fe0\u5b5d\u8282\u4e49\u8303\u675f\u5168\u56fd\u4e4b\u4eba\u5fc3\uff0c\u4e00\u5207\u6cd5\u5ea6\u7eb2\u7eaa\u7ecf\u6570\u5343\u5e74\u5723\u54f2\u6240\u521b\u5782\uff0c\u7adf\u6beb\u65e0\u53ef\u8d35\uff1f\u4f55\u5fc5\u5148\u81ea\u8f7b\u8d31\uff0c\u4e00\u95fb\u65b0\u8bf4\uff0c\u9042\u5c06\u6570\u5343\u5e74\u6240\u5c0a\u4fe1\u6301\u5faa\u8005\u5f03\u7edd\u4e0d\u503c\u4e00\u987e\uff0c\u5bf9\u4e8e\u65b0\u4eba\u7269\u6709\u81ea\u60ed\u5f62\u79fd\u55eb\u5685\u4e0d\u6562\u8a00\u4e4b\u6982\uff0c\u751a\u6216\u8fce\u5408\u65b0\u4eba\u7269\u6bc1\u9a82\u5148\u4ee3\u9057\u4f20\uff0c\u8bdf\u8fb1\u81ea\u5bb6\u5b66\u7406\u3002\u5c82\u56fd\u5bb6\u6570\u767e\u5e74\u6761\u6559\u6240\u9881\u4ee5\u53ca\u543e\u4eba\u80dc\u8863\u5c31\u5085\u6570\u5341\u5e74\u671d\u65af\u5915\u65af\u8005\uff0c\u5168\u5c5e\u865a\u4f2a\u65e0\u7269\u4e4e\u201d\u3002\u8fd9\u79cd\u89c2\u70b9\u6ca1\u6709\nA. \u8bf4\u660e\u4f5c\u8005\u7684\u601d\u60f3\u987d\u56fa\u5b88\u65e7\nB. \u8868\u8fbe\u5bf9\u4e8e\u65b0\u6587\u5316\u8fd0\u52a8\u7684\u4e0d\u6ee1\nC. \u4f1a\u53d7\u5230\u5f53\u65f6\u4e3b\u6d41\u6587\u5316\u7684\u6392\u65a5\nD. \u4f53\u73b0\u4e86\u6c11\u65cf\u6587\u5316\u7684\u81ea\u4fe1\u5fc3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f8e\u56fd\u9996\u5148\u63d0\u51fa\u628a\u4eba\u6743\u4f5c\u4e3a\u5916\u4ea4\u653f\u7b56\u57fa\u672c\u539f\u5219\u7684\u662f\u54ea\u4e00\u4efb\u653f\u5e9c\nA. \u5361\u7279\u653f\u5e9c\nB. \u80af\u5c3c\u8fea\u653f\u5e9c\nC. \u514b\u6797\u987f\u653f\u5e9c\nD. \u827e\u68ee\u8c6a\u5a01\u5c14\u653f\u5e9c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u57fa\u7763\u6559\u6700\u521d\u4f20\u5230\u7f57\u9a6c\u5e1d\u56fd\u65f6\uff0c\u53ea\u88ab\u4ec0\u4e48\u4eba\u63a5\u53d7\u3002\nA. \u7687\u65cf\nB. \u8d35\u65cf\nC. \u4e0b\u5c42\u4eba\u6c11\nD. \u516c\u6c11\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44333826795256104, "meta-math/MetaMath-Mistral-7B": 0.7332870557410595, "itpossible/Chinese-Mistral-7B-v0.1": 0.325455072595945, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7195326395038985, "meta-llama/Meta-Llama-3-8B": 0.5724807789405023, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9632838066830359}}, {"question": "\u6559\u5e08\u7684\u8868\u7387\u4f5c\u7528\u4e3b\u8981\u4f53\u73b0\u5728\nA. \u8863\u7740\u6574\u6cbb\nB. \u4e2a\u4eba\u9b45\u529b\nC. \u8a00\u884c\u4e00\u81f4\nD. \u4e3e\u6b62\u7aef\u5e84\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7516312932470274, "meta-math/MetaMath-Mistral-7B": 0.9708484351952241, "itpossible/Chinese-Mistral-7B-v0.1": 0.7687640793714905, "HuggingFaceH4/zephyr-7b-beta": 0.989946563913792, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6595640683475087, "meta-llama/Meta-Llama-3-8B": 0.9029685035165912, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9835838858886564}}, {"question": "\u6700\u51fa\u540d\u7684\u964d\u7ef4\u7b97\u6cd5\u662f PCA \u548c t-SNE\u3002\u5c06\u8fd9\u4e24\u4e2a\u7b97\u6cd5\u5206\u522b\u5e94\u7528\u5230\u6570\u636e\u300cX\u300d\u4e0a\uff0c\u5e76\u5f97\u5230\u6570\u636e\u96c6\u300cX_projected_PCA\u300d\uff0c\u300cX_projected_tSNE\u300d\u3002\u4e0b\u9762\u54ea\u4e00\u9879\u5bf9\u300cX_projected_PCA\u300d\u548c\u300cX_projected_tSNE\u300d\u7684\u63cf\u8ff0\u662f\u6b63\u786e\u7684\nA. \u4e24\u4e2a\u90fd\u5728\u6700\u8fd1\u90bb\u7a7a\u95f4\u80fd\u5f97\u5230\u89e3\u91ca\nB. X_projected_PCA \u5728\u6700\u8fd1\u90bb\u7a7a\u95f4\u80fd\u5f97\u5230\u89e3\u91ca\nC. \u4e24\u4e2a\u90fd\u4e0d\u80fd\u5728\u6700\u8fd1\u90bb\u7a7a\u95f4\u5f97\u5230\u89e3\u91ca\nD. X_projected_tSNE \u5728\u6700\u8fd1\u90bb\u7a7a\u95f4\u80fd\u5f97\u5230\u89e3\u91ca\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2017\u5e749\u670815\u65e5\uff0c\u5361\u897f\u5c3c\u53f7\u63a2\u6d4b\u5668\u4ee5\u5760\u5165\u54ea\u4e00\u4e2a\u661f\u7403\u5927\u6c14\u5c42\u7684\u65b9\u5f0f\u7ed3\u675f\u4e86\u5b83\u7684\u63a2\u6d4b\u4efb\u52a1\nA. \u6728\u661f\nB. \u91d1\u661f\nC. \u571f\u661f\nD. \u6d77\u738b\u661f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.514040625288414, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.495517173388281}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6350\u52a9\u6cd5\u4eba\u7684\u8868\u8ff0\u6b63\u786e\u7684\u662f\u3002\nA. \u6350\u52a9\u6cd5\u4eba\u7ec8\u6b62\u65f6\u5e94\u5c06\u5269\u4f59\u8d22\u4ea7\u8fd4\u8fd8\u7ed9\u6350\u52a9\u4eba\nB. \u6350\u52a9\u6cd5\u4eba\u65e0\u9700\u8bbe\u7acb\u51b3\u7b56\u673a\u6784\nC. \u6350\u52a9\u4eba\u662f\u6350\u52a9\u6cd5\u4eba\u7684\u6cd5\u5b9a\u4ee3\u8868\u4eba\nD. \u6350\u52a9\u6cd5\u4eba\u662f\u4e3a\u516c\u76ca\u76ee\u7684\u8bbe\u7acb\u7684\u975e\u8425\u5229\u6cd5\u4eba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8769530828718273, "meta-math/MetaMath-Mistral-7B": 0.9940201212568341, "itpossible/Chinese-Mistral-7B-v0.1": 0.6592302966062901, "HuggingFaceH4/zephyr-7b-beta": 0.9997871165850114, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9767428736135438, "meta-llama/Meta-Llama-3-8B": 0.9567839764596234, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9916709522207359}}, {"question": "2013\u5e745\u6708\uff0c\u67d0\u5916\u56fd\u4eba\u5728\u6211\u56fd\u5883\u5185\u8fd0\u8f93\u6bd2\u54c12\u5343\u514b\uff0c\u4eba\u6c11\u6cd5\u9662\u4f9d\u636e\u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u5211\u6cd5\u300b\u5224\u5904\u5176\u65e0\u671f\u5f92\u5211\u3002\u8be5\u6848\u6240\u4f53\u73b0\u7684\u6211\u56fd\u6cd5\u5f8b\u6548\u529b\u7684\u539f\u5219\u662f\nA. \u4fdd\u62a4\u4e3b\u4e49\nB. \u5c5e\u4eba\u4e3b\u4e49\nC. \u5c5e\u5730\u4e3b\u4e49\nD. \u6298\u4e2d\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6333905234466792, "meta-math/MetaMath-Mistral-7B": 0.8945453776944468, "itpossible/Chinese-Mistral-7B-v0.1": 0.3984585186882418, "HuggingFaceH4/zephyr-7b-beta": 0.9978629456438793, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9210051014832511, "meta-llama/Meta-Llama-3-8B": 0.34686667406054084, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5633482771460928}}, {"question": "\u8fdb\u4e00\u6b65\u6253\u7834\u884c\u653f\u6027\u5784\u65ad\u548c\u5730\u533a\u5c01\u9501\uff0c\u5065\u5168\u6211\u56fd\u7edf\u4e00\u5f00\u653e\u7684\u5e02\u573a\uff0c\u5f00\u5c55\u7ade\u4e89\uff1aa\u4f1a\u4fc3\u4f7f\u4f01\u4e1a\u6539\u8fdb\u6280\u672f\u3001\u63d0\u9ad8\u52b3\u52a8\u751f\u4ea7\u7387\uff1bb\u5fc5\u7136\u5e26\u6765\u793e\u4f1a\u8d44\u6e90\u7684\u4f18\u5316\u914d\u7f6e\uff1bc\u6709\u5229\u4e8e\u4fc3\u8fdb\u56fd\u6c11\u7ecf\u6d4e\u6574\u4f53\u7d20\u8d28\u7684\u63d0\u9ad8\uff1bd\u53ef\u4ee5\u4fdd\u8bc1\u4ef7\u683c\u53d8\u5316\u7684\u7075\u654f\u6027\uff0c\u4f7f\u4f9b\u6c42\u5173\u7cfb\u5c3d\u5feb\u5f97\u5230\u8c03\u6574\nA. bcd\nB. acd\nC. abd\nD. abc \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.325455072595945, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u751f\u6001\u5ea6\u5c5e\u4e8e\u4eba\u751f\u89c2\u7684\u8303\u7574\uff0c\u662f\u6307\nA. \u4eba\u751f\u5b9e\u8df5\u6d3b\u52a8\u7684\u603b\u76ee\u6807\nB. \u4eba\u7684\u5b9e\u8df5\u5bf9\u4e8e\u793e\u4f1a\u3001\u4ed6\u4eba\u548c\u81ea\u8eab\u6240\u5177\u6709\u7684\u610f\u4e49\nC. \u4eba\u4eec\u901a\u8fc7\u751f\u6d3b\u5b9e\u8df5\u6240\u5f62\u6210\u7684\u5bf9\u4eba\u751f\u95ee\u9898\u7684\u4e00\u79cd\u7a33\u5b9a\u7684\u5fc3\u7406\u503e\u5411\u548c\u57fa\u672c\u610f\u56fe\nD. \u4eba\u4eec\u5728\u5b9e\u8df5\u4e2d\u5f62\u6210\u7684\u5bf9\u4eba\u751f\u76ee\u7684\u548c\u610f\u4e49\u7684\u6839\u672c\u770b\u6cd5\u548c\u6001\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9197188234872963, "meta-math/MetaMath-Mistral-7B": 0.9853455997410715, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9996081955943568, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9122495075790371, "meta-llama/Meta-Llama-3-8B": 0.7149052341869634, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "19\u4e16\u7eaa60\u5e74\u4ee3\uff0c\u6709\u4f5c\u5bb6\u8d5e\u53f9\u5f53\u65f6\u901a\u822a\u7684\u67d0\u8fd0\u6cb3\u662f\u201c\u5728\u4e00\u4e2a\u67095000\u5e74\u6587\u660e\u7684\u56fd\u5bb6\u5f00\u901a\u7684\uff0c\u4e1c\u65b9\u4f1f\u5927\u4e4b\u822a\u9053\u201d\u3002\u4ed6\u63cf\u8ff0\u7684\u662f\nA. \u4f0a\u5229\u8fd0\u6cb3\nB. \u66fc\u5f7b\u65af\u7279\u8fd0\u6cb3\nC. \u5df4\u62ff\u9a6c\u8fd0\u6cb3\nD. \u82cf\u4f0a\u58eb\u8fd0\u6cb3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.331100931216185, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8080710900816808, "HuggingFaceH4/zephyr-7b-beta": 0.927968074252662, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\uff0c\u52a0\u7740\u91cd\u53f7\u7684\u5b57\u4e0e\u62ec\u53f7\u4e2d\u7684\u5b57\u662f\u5047\u501f\u5173\u7cfb\u7684\u4e00\u53e5\u662f\nA. \u6c11\u4e0d\u77e5\u79ae\uff0c\u672a\u751f\u5176\u5171(\u606d)\u3002\nB. \u5176\u5fa1\u4e4b\u59bb\u5f9e\u9580\u9593\u800c\u95d5(\u7aba)\u3002\nC. \u5c0f\u4eba\u4e5f\u8005\uff0c\u75be(\u6025)\u7232\u8a95\u800c\u6b32\u4eba\u4e4b\u4fe1\u5df1\u4e5f\u3002\nD. \u4ea1\u4e0d\u8d8a\u7adf(\u5883)\uff0c\u53cd\u4e0d\u8a0e\u8cca\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.32871459371036227, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.333183235354062, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u75c5\u56e0\u542b\u4e49\u6700\u786e\u5207\u7684\u8868\u8fbe\u662f\nA. \u7269\u7406\u56e0\u7d20\nB. \u51e1\u80fd\u4f7f\u4eba\u4eec\u53d1\u75c5\u6982\u7387\u589e\u52a0\u7684\u56e0\u7d20\nC. \u5316\u5b66\u56e0\u7d20\nD. \u5fc3\u7406\u56e0\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.916749437701761, "meta-math/MetaMath-Mistral-7B": 0.9850023629989786, "itpossible/Chinese-Mistral-7B-v0.1": 0.6813928055347347, "HuggingFaceH4/zephyr-7b-beta": 0.9970587251868546, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9725973741230547, "meta-llama/Meta-Llama-3-8B": 0.7814802762780727, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9795426892733978}}, {"question": "\u201c\u628a\u843d\u53f6\u770b\u6210\u7f8e\u5999\u7684\u97f3\u7b26\uff0c\u5c31\u6ca1\u6709\u4e86\u60b2\u79cb\u4e4b\u611f\u3002\u201d\u4f53\u73b0\u7684\u54f2\u5b66\u9053\u7406\u662f\nA. \u610f\u8bc6\u5177\u6709\u76ee\u7684\u6027\u548c\u8ba1\u5212\u6027\nB. \u610f\u8bc6\u5bf9\u4eba\u4f53\u751f\u7406\u6d3b\u52a8\u5177\u6709\u8c03\u8282\u548c\u63a7\u5236\u4f5c\u7528\nC. \u610f\u8bc6\u5177\u6709\u79d1\u5b66\u9884\u89c1\u6027\nD. \u610f\u8bc6\u5177\u6709\u4e3b\u52a8\u521b\u9020\u6027\u548c\u81ea\u89c9\u9009\u62e9\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\u62ec\u53f7\u5de6\u8fb9\u7684\u5b57\u4e0e\u62ec\u53f7\u4e2d\u7684\u5b57\u662f\u5206\u5316\u5b57\u5173\u7cfb\u7684\u4e00\u53e5\u662f\nA. \u5c11\u76ca\u8006(\u55dc)\u98df\uff0c\u548c\u65bc\u8eab\u4e5f\u3002\nB. \u6216\u4ee4\u5b7a\u5b50\u61f7\u9322\u6308\u58fa\u58c5(\u74ee)\u800c\u5f80\u9164\u3002\nC. \u4ee5\u5b98\u670d\u4e8b\uff0c\u4ee5\u52de\u6bbf(\u5960)\u8cde\uff0c\u91cf\u529f\u800c\u5206\u7984\u3002\nD. \u70ba\u9152\u751a\u7f8e\uff0c\u7e23(\u61f8)\u5e5f\u751a\u9ad8\uff0c\u7136\u800c\u4e0d\u552e\uff0c\u9152\u9178\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6977\u4e66\u6700\u65e9\u4ea7\u751f\u4e8e\nA. \u9b4f\u664b\nB. \u6c49\u4ee3\nC. \u5b8b\u4ee3\nD. \u5510\u4ee3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4124578775795746, "meta-math/MetaMath-Mistral-7B": 0.6474756562217, "itpossible/Chinese-Mistral-7B-v0.1": 0.4279733126632047, "HuggingFaceH4/zephyr-7b-beta": 0.7987504848532639, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5262386497518335, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4804299094883311}}, {"question": "\u5173\u4e8e\u529f\u7387\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\uff1a\nA. \u4ece\u516c\u5f0fP\uff1dFv\u53ef\u77e5\uff0c\u6c7d\u8f66\u7684\u53d1\u52a8\u673a\u529f\u7387\u5c31\u662f\u6307\u5408\u5916\u529b\u7684\u529f\u7387\nB. \u4ece\u516c\u5f0fP\uff1dFv\u53ef\u77e5\uff0c\u6c7d\u8f66\u7684\u53d1\u52a8\u673a\u529f\u7387\u53ef\u4ee5\u968f\u901f\u5ea6\u7684\u4e0d\u65ad\u589e\u5927\u800c\u589e\u5927\nC. \u529f\u7387\u662f\u63cf\u8ff0\u529b\u5bf9\u7269\u4f53\u505a\u529f\u5feb\u6162\u7684\u7269\u7406\u91cf\nD. \u529b\u5bf9\u7269\u4f53\u505a\u7684\u529f\u8d8a\u591a\uff0c\u529b\u505a\u529f\u7684\u529f\u7387\u8d8a\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37463354581534514, "meta-math/MetaMath-Mistral-7B": 0.6190893546178163, "itpossible/Chinese-Mistral-7B-v0.1": 0.552947259895356, "HuggingFaceH4/zephyr-7b-beta": 0.8636709558410047, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5982184225708296, "meta-llama/Meta-Llama-3-8B": 0.4209626922472216, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9163316244400217}}, {"question": "\u4e0b\u5217\u5404\u5f0f\u4e2d\uff0c\u79ef\u6700\u2f29\u7684\u7b97\u5f0f\u662f\nA. 16.5*0.24\nB. 1.65*24.8\nC. 16.5*0.024 \nD. 16.5*24.8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6008196602100531, "meta-math/MetaMath-Mistral-7B": 0.9348738685002549, "itpossible/Chinese-Mistral-7B-v0.1": 0.7717598196637816, "HuggingFaceH4/zephyr-7b-beta": 0.9980451936694362, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8439357472335508, "meta-llama/Meta-Llama-3-8B": 0.6882563170969853, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7991657096217735}}, {"question": "\u60a3\u8005\uff0c\u7537\uff0c65\u5c81\u3002\u8fd1\u671f\u8868\u73b0\u80a2\u4f53\u5173\u8282\u75bc\u75db\uff0c\u6e38\u8d70\u4e0d\u5b9a\uff0c\u5c48\u4f38\u4e0d\u5229\uff0c\u4f34\u6076\u98ce\u53d1\u70ed\uff0c\u82d4\u8584\u767d\uff0c\u8109\u6d6e\u7b49\u75c7\u72b6\u3002\u6839\u636e\u516d\u6deb\u81f4\u75c5\u7279\u70b9\uff0c\u4e0a\u8ff0\u75c5\u8bc1\u4f53\u73b0\u4e86\u8be5\u90aa\u6c14\u4ec0\u4e48\u81f4\u75c5\u7279\u70b9\nA. \u5584\u884c\nB. \u5f00\u6cc4\nC. \u5e72\u6da9\nD. \u51dd\u6ede\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u996e\u7528\u8c46\u6d46\u5e94\u6ce8\u610f\u4ec0\u4e48\nA. \u4ee5\u4e0a\u90fd\u662f\nB. \u8c46\u6d46\u4e2d\u4e0d\u80fd\u51b2\u5165\u9e21\u86cb\nC. \u8c46\u6d46\u5f7b\u5e95\u716e\u5f00\uff0c\u4e0d\u8981\u7528\u4fdd\u6e29\u74f6\u50a8\u5b58\u8c46\u6d46\nD. \u4e0d\u8981\u7a7a\u8179\u996e\u8c46\u6d46\uff0c\u4e0d\u8981\u8fc7\u91cf\u996e\u8c46\u6d46\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9260327025566587, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u793e\u4f1a\u4e3b\u4e49\u7684\u6839\u672c\u4efb\u52a1\u662f\nA. \u8fdb\u884c\u653f\u6cbb\u4f53\u5236\u6539\u9769\u3001\u7ecf\u6d4e\u4f53\u5236\u6539\u9769\nB. \u6539\u9769\u751f\u4ea7\u8d44\u6599\u6240\u6709\u5236\nC. \u8fdb\u884c\u9636\u7ea7\u6597\u4e89\nD. \u89e3\u653e\u751f\u4ea7\u529b\u3001\u53d1\u5c55\u751f\u4ea7\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8120701373592093, "meta-math/MetaMath-Mistral-7B": 0.9761366738603405, "itpossible/Chinese-Mistral-7B-v0.1": 0.9132503086545704, "HuggingFaceH4/zephyr-7b-beta": 0.9908582913853481, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7614473965534341, "meta-llama/Meta-Llama-3-8B": 0.7585740344032252, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5149098255113425}}, {"question": "\u5728IE\u6d4f\u89c8\u5668\u7684\u5730\u5740\u680f\u91cc\u8f93\u5165http://www.sinA.Com.cn\uff0c\u5b83\u8868\u793a\nA. \u672c\u5730\u786c\u76d8\u4e2d\u7684\u4e00\u4e2a\u6587\u4ef6\u6216\u6587\u4ef6\u5939\u5730\u5740\nB. \u7f51\u5740\nC. \u7535\u5b50\u4fe1\u7bb1\u5730\u5740\nD. Windows\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9817662554831822, "meta-math/MetaMath-Mistral-7B": 0.9978834212819819, "itpossible/Chinese-Mistral-7B-v0.1": 0.9790666822453533, "HuggingFaceH4/zephyr-7b-beta": 0.9999848990936075, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9982230591017109, "meta-llama/Meta-Llama-3-8B": 0.9817794822662246, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9992283106404725}}, {"question": "\u7ecf\u6d4e\u5168\u7403\u5316\u5bf9\u6211\u56fd\u7ecf\u6d4e\u53d1\u5c55\u7684\u610f\u4e49\u4e3b\u8981\u5728\u4e8e\nA. \u5b83\u4f7f\u6211\u56fd\u519c\u4ea7\u54c1\u7684\u51fa\u53e3\u5904\u4e8e\u66f4\u52a0\u6709\u5229\u7684\u5730\u4f4d\nB. \u5b83\u80fd\u4f7f\u6211\u56fd\u4fe1\u606f\u5316\u3001\u5de5\u4e1a\u5316\u53d8\u4e3a\u73b0\u5b9e\nC. \u5b83\u662f\u6211\u56fd\u7ecf\u6d4e\u4f53\u5236\u6539\u9769\u7684\u6839\u672c\u76ee\u6807\nD. \u5b83\u4e3a\u6211\u56fd\u5229\u7528\u5916\u56fd\u8d44\u91d1\u3001\u6280\u672f\u53d1\u5c55\u7ecf\u6d4e\u521b\u9020\u4e86\u6761\u4ef6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7431727821686612, "meta-math/MetaMath-Mistral-7B": 0.9206597080416067, "itpossible/Chinese-Mistral-7B-v0.1": 0.6009130325141256, "HuggingFaceH4/zephyr-7b-beta": 0.9850358475940381, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9450505160512817, "meta-llama/Meta-Llama-3-8B": 0.8879773219067105, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u6839\u7ef3\u5b50\u957f5\u7c73\uff0c\u7b2c\u4e00\u6b21\u7528\u53bb1/5\uff0c\u7b2c\u4e8c\u6b21\u7528\u53bb1/5\u7c73\uff0c\u8fd8\u5269\u4e0b\nA. 4\u7c73\nB. 12/5\u7c73\nC. 3\u7c73\nD. 2/5\u7c73\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u8f93\u8840\u7684\u77e5\u8bc6\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u4e00\u9879\u662f\nA. \u6210\u5206\u8f93\u8840\u5177\u6709\u7597\u6548\u597d\uff0c\u526f\u4f5c\u7528\u5c0f\uff0c\u8282\u7ea6\u8840\u6db2\u8d44\u6e90\u4ee5\u53ca\u4fbf\u4e8e\u4fdd\u5b58\u548c\u8fd0\u8f93\u7b49\u4f18\u70b9\nB. \u5728\u5927\u51fa\u8840\u7684\u7d27\u6025\u60c5\u51b5\u4e0b\uff0c\u53ef\u4f9d\u636e\u732e\u8840\u8005\u6240\u62a5\u8840\u578b\uff0c\u76f4\u63a5\u8f93\u8840\nC. \u8f93\u8840\u524d\u7ecf\u8fc7\u68c0\u75ab\u5c31\u80fd\u591f\u907f\u514d\u53d7\u8840\u8005\u611f\u67d3\u8840\u6db2\u4f20\u64ad\u6027\u75be\u75c5\nD. \u8f93\u8840\u53ef\u4ee5\u4e3a\u75c5\u4eba\u589e\u52a0\u62b5\u6297\u529b\uff0c\u8865\u5145\u8425\u517b\uff0c\u6240\u4ee5\u624b\u672f\u75c5\u4eba\u5e94\u5e38\u89c4\u8f93\u8840\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6522270926906072, "meta-math/MetaMath-Mistral-7B": 0.9355418963534637, "itpossible/Chinese-Mistral-7B-v0.1": 0.7809462502055788, "HuggingFaceH4/zephyr-7b-beta": 0.9676958931191698, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8583369779697134, "meta-llama/Meta-Llama-3-8B": 0.3671700228158849, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5536200190701807}}, {"question": "\u4e0b\u9762\u5173\u4e8eWindows2000\u6587\u4ef6\u540d\u7684\u53d9\u8ff0\u4e2d\uff0c\u9519\u8bef\u7684\u662f\nA. \u6587\u4ef6\u540d\u4e2d\u5141\u8bb8\u4f7f\u7528\u591a\u4e2a\u5706\u70b9\u5206\u9694\u7b26\nB. \u6587\u4ef6\u540d\u4e2d\u5141\u8bb8\u4f7f\u7528\u7ad6\u7ebf\u201c|\u201d\nC. \u6587\u4ef6\u540d\u4e2d\u5141\u8bb8\u4f7f\u7528\u6c49\u5b57 \nD. \u6587\u4ef6\u540d\u4e2d\u5141\u8bb8\u4f7f\u7528\u7a7a\u683c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41634420631281843, "meta-math/MetaMath-Mistral-7B": 0.5674426107837454, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9915978948941798, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4547849306349041}}, {"question": "\u4e0a\u6d77\u67d0\u5546\u53a6\u5c4b\u9876\u4e0a\u94fa\u6ee1\u4e86\u2f00\u4e2a\u4e2a\u79cd\u690d\u7bb1\uff0c\u8fd9\u2fa5\u88ab\u79f0\u4e3a\u201c\u5c4b\u9876\u519c\u5e84\u201d\u3002\u5546\u5bb6\u4e13\u2ed4\u4ece\u2ed3\u2f69\u2f2d\u8fd0\u6765\u4f18\u8d28\u2f1f\u58e4\uff0c\u5438\u5f15\u9644\u8fd1\u5c45\u2ea0\u79df\u501f\u79cd\u690d\u7bb1\u79cd\u690d\u852c\u679c\u3002\u5c4b\u9876\u519c\u5e84\u8fd0\u4f5c\u4ee5\u6765\uff0c\u6574\u680b\u5546\u53a6\u590f\u5b63\u7a7a\u8c03\u2f64\u7535\u91cf\u4e0b\u964d...... \u5546\u5bb6\u5728\u7ecf\u8425\u5c4b\u9876\u519c\u5e84\u65f6\uff0c\u91cd\u70b9\u5173\u6ce8\u7684\u662f\nA. \u5730\u5f62\nB. \u2f1f\u58e4\nC. \u5e02\u573a\nD. \u52b3\u52a8\u2f12\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7201578723308697, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u5f53\u4ee3\u4e2d\u56fd\uff0c\u5e73\u7b49\u662f\u793e\u4f1a\u4e3b\u4e49\u6c11\u65cf\u5173\u7cfb\u7684\u57fa\u7840\uff0c\u53ea\u6709\uff08\uff09\u624d\u80fd\u5b9e\u73b0\u5404\u6c11\u65cf\u771f\u6b63\u7684\u5e73\u7b49\uff1b\u53ea\u6709\u5b9e\u73b0\u5404\u6c11\u65cf\u7684\u5e73\u7b49\uff0c\u56fd\u5bb6\u624d\u80fd\u957f\u671f\u7a33\u5b9a\u548c\u53d1\u5c55\u3002\nA. \u548c\u8c10\nB. \u56e2\u7ed3\nC. \u53d1\u5c55\nD. \u4e92\u52a9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2002\u5e749\u6708\uff0c\u7f8e\u56fd\u5728\u90a3\u4e00\u4efd\u6587\u4ef6\u4e2d\u660e\u786e\u63d0\u51fa\u8981\u5bf9\u6050\u6016\u4efd\u5b50\u53ca\u5176\u652f\u6301\u8005\u8fdb\u884c\u201c\u5148\u53d1\u5236\u4eba\u201d\u7684\u519b\u4e8b\u6253\u51fb\nA. \u300a\u7f51\u7edc\u4e2d\u5fc3\u6218\u62a5\u544a\u300b\nB. \u300a\u7f8e\u56fd\u5b89\u5168\u6218\u7565\u62a5\u544a\u300b\nC. \u300a\u53cd\u5bfc\u6761\u7ea6\u300b\nD. \u300a\u6838\u6001\u52bf\u8bc4\u4f30\u62a5\u544a\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.8188844346993447, "itpossible/Chinese-Mistral-7B-v0.1": 0.48186381109648174, "HuggingFaceH4/zephyr-7b-beta": 0.9653361495568236, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6094058524852267, "meta-llama/Meta-Llama-3-8B": 0.7198350060309758, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9584519914636341}}, {"question": "\u674e\u5b50\u52cb\u8001\u5e08\u5728\u5173\u4e8e\u201c\u81ea\u6211\u201d\u7684\u8bb2\u89e3\u4e2d\u63d0\u5230\u8102\u80aa\u65f6\u6240\u6301\u7684\u89c2\u70b9\u4e0d\u5305\u62ec\nA. \u8ffd\u6c42\u5851\u5f62\u5c31\u662f\u60f3\u5c3d\u4e00\u5207\u529e\u6cd5\u51cf\u5c11\u8102\u80aa\nB. \u6784\u6210\u5927\u8111\u3001\u795e\u7ecf\u7cfb\u7edf\u3001\u9176\u7cfb\u7edf\u4e3b\u8981\u662f\u8102\u80aa\nC. \u4eba\u7684\u9762\u5bb9\u8010\u4e0d\u8010\u770b\u53d6\u51b3\u4e8e\u8102\u80aa\u5bf9\u6211\u4eec\u7684\u5851\u5f62\nD. \u8102\u80aa\u662f\u751f\u547d\u4e2d\u6700\u5177\u7075\u6027\u7684\u7ec4\u7ec7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3253699102351613, "meta-math/MetaMath-Mistral-7B": 0.4235793196586464, "itpossible/Chinese-Mistral-7B-v0.1": 0.35286194820709277, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8228818456145353, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3889367446193995}}, {"question": "\u5728\u6211\u56fd\u73b0\u884c\u7684\u6cd5\u5f8b\u89e3\u91ca\u4f53\u5236\u4e2d\uff0c\u6709\u6743\u8fdb\u884c\u884c\u653f\u89e3\u91ca\u7684\u4e3b\u4f53\u662f\nA. \u5168\u56fd\u4eba\u5927\u5e38\u59d4\u4f1a\nB. \u56fd\u52a1\u9662\u6cd5\u5236\u529e\u516c\u5ba4\nC. \u56fd\u52a1\u9662\u53ca\u5176\u4e3b\u7ba1\u90e8\u95e8\nD. \u56fd\u52a1\u9662\u529e\u516c\u5385\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34530064363589513, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7494318585327056, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8691224859544684}}, {"question": "\u4e3a\u8c03\u67e5\u6050\u6016\u6d3b\u52a8\u5acc\u7591\uff0c\u7ecf\u6709\u5173\u673a\u5173\u6279\u51c6\uff0c\u53ef\u4ee5\u6839\u636e\u5176\u5371\u9669\u7a0b\u5ea6\uff0c\u8d23\u4ee4\u6050\u6016\u6d3b\u52a8\u5acc\u7591\u4eba\u5458\u9075\u5b88\u4e0b\u5217\u4e00\u9879\u6216\u8005\u591a\u9879\u7ea6\u675f\u63aa\u65bd\u3002\u5176\u4e2d\u4e0d\u5305\u62ec\nA. \u4e0d\u5f97\u4e0e\u7279\u5b9a\u7684\u4eba\u5458\u4f1a\u89c1\u6216\u8005\u901a\u4fe1\nB. \u672a\u7ecf\u516c\u5b89\u673a\u5173\u6279\u51c6\u4e0d\u5f97\u79bb\u5f00\u6240\u5c45\u4f4f\u7684\u5e02\u3001\u53bf\u6216\u8005\u6307\u5b9a\u7684\u5904\u6240\nC. \u5b9a\u671f\u5411\u516c\u5b89\u673a\u5173\u62a5\u544a\u6d3b\u52a8\u60c5\u51b5\nD. \u4e24\u5e74\u5185\u4e0d\u5f97\u53c2\u52a0\u5927\u578b\u7fa4\u4f17\u6027\u6d3b\u52a8\u6216\u8005\u4ece\u4e8b\u7279\u5b9a\u7684\u6d3b\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.309493990256606, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.37238045729739705, "HuggingFaceH4/zephyr-7b-beta": 0.42568098717745234, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.41172531682861635, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.39406190645844597}}, {"question": "\u8f85\u9176NADP+\u5206\u5b50\u4e2d\u542b\u6709\u4e0b\u5217\u54ea\u79cdB\u65cf\u7ef4\u751f\u7d20\nA. \u70df\u9170\u80fa\nB. \u6838\u9ec4\u7d20\nC. \u53f6\u9178\nD. \u5421\u54c6\u919b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4185564512048971, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f01\u4e1a\u8d54\u507f\u8d23\u4efb\u7684\u6839\u636e\u5728\u4e8e\uff08\uff09\u7ecf\u6d4e\u4f26\u7406\u51c6\u5219\nA. \u8bda\u5b9e\u4e0d\u6b3a\nB. \u516c\u5e73\u4e92\u5229\nC. \u6d88\u8d39\u8005\u81ea\u6211\u4fdd\u62a4\nD. \u7b49\u4ef7\u4ea4\u6362\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3548205732608228, "meta-math/MetaMath-Mistral-7B": 0.5661394068176863, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6884396958659744}}, {"question": "\u4fe1\u606f\u5316\u6218\u4e89\uff0c\u662f\u4eba\u7c7b\u793e\u4f1a\u653f\u6cbb\u3001\u7ecf\u6d4e\u3001()\u548c\u6218\u4e89\u5b9e\u8df5\u53d1\u5c55\u5230\u4e00\u5b9a\u9636\u6bb5\u7684\u5fc5\u7136\u4ea7\u7269\u3002\nA. \u6587\u5316\nB. \u8ba1\u7b97\u673a\u6280\u672f\nC. \u79d1\u5b66\u6280\u672f\nD. \u822a\u5929\u6280\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3469506764321279, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4678022005791944, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9006\u8f6c\u5f55\u75c5\u6bd2\u7684\u6838\u9178\u7c7b\u578b\u4e3a\nA. \u5355\u80a1\u6b63\u94feRNA\nB. \u5355\u80a1\u6b63\u94feDNA\nC. \u5355\u80a1\u6b63\u94feRNA \u4e8c\u805a\u4f53\nD. \u5355\u80a1\u8d1f\u94feDNA\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3608710589800533, "meta-math/MetaMath-Mistral-7B": 0.6505306617196189, "itpossible/Chinese-Mistral-7B-v0.1": 0.4957405044145814, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "Windows\u7684\u6587\u4ef6\u5939\u4e0b\nA. \u4e0d\u80fd\u5b58\u653e\u4efb\u4f55\u4e1c\u897f\nB. \u53ea\u80fd\u5b58\u653e\u6587\u4ef6\nC. \u53ef\u4ee5\u5b58\u653e\u6587\u4ef6\u6216\u6587\u4ef6\u5939\nD. \u53ea\u80fd\u5b58\u653e\u6587\u4ef6\u5939\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9045536444172638, "meta-math/MetaMath-Mistral-7B": 0.987025469935265, "itpossible/Chinese-Mistral-7B-v0.1": 0.8517546858689024, "HuggingFaceH4/zephyr-7b-beta": 0.9996834895317919, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8914872268351296, "meta-llama/Meta-Llama-3-8B": 0.94433252419465, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9987027183452448}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6211\u56fd\u53f8\u6cd5\u6743\u7684\u8868\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u53f8\u6cd5\u6743\u7684\u4e13\u5c5e\u6027\u8981\u6c42\u53f8\u6cd5\u6743\u53ea\u80fd\u7531\u56fd\u5bb6\u5404\u7ea7\u5ba1\u5224\u673a\u5173\u7edf\u4e00\u884c\u4f7f\nB. \u53f8\u6cd5\u6743\u7684\u7ec8\u5c40\u6027\u610f\u5473\u7740\u4e00\u5207\u7ea0\u7eb7\u6700\u7ec8\u90fd\u5e94\u7531\u53f8\u6cd5\u673a\u5173\u4f5c\u51fa\u88c1\u51b3\nC. \u53f8\u6cd5\u6743\u72ec\u7acb\u610f\u5473\u7740\u53f8\u6cd5\u6743\u4e0d\u53d7\u4e00\u5207\u673a\u5173\u548c\u4e2a\u4eba\u7684\u76d1\u7763\nD. \u6211\u56fd\u53f8\u6cd5\u6743\u5305\u62ec\u5ba1\u5224\u6743\u548c\u68c0\u5bdf\u6743\u4e24\u79cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3579084833373272, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9573228124950606, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8276511307807662, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u4e2d\u5b66\u66fe\u8001\u5e08\uff0c\u6bcf\u6b21\u5e03\u7f6e\u8bfe\u540e\u4f5c\u4e1a\u540e\uff0c\u90fd\u53ea\u662f\u5728\u4e0b\u6b21\u8bfe\u5802\u4e0a\u4e3a\u5b66\u751f\u6838\u5bf9\u4e00\u4e0b\u7b54\u6848\uff0c\u66fe\u8001\u5e08\u7684\u505a\u6cd5\nA. \u5408\u7406\uff0c\u53ef\u4ee5\u4fc3\u8fdb\u5b66\u751f\u81ea\u5b66\nB. \u4e0d\u5408\u7406\uff0c\u6559\u5e08\u5e94\u8ba4\u771f\u6279\u6539\u4f5c\u4e1a\nC. \u4e0d\u5408\u7406\uff0c\u589e\u52a0\u4e86\u5b66\u751f\u8bfe\u540e\u8d1f\u62c5\nD. \u5408\u7406\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6559\u5b66\u6548\u7387\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4733159998692773, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u813e\u4e4b\u534e\u5728\nA. \u6bdb\nB. \u53d1\nC. \u9762\nD. \u5507\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u529f\u7387\u7684\u8bf4\u6cd5\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u7269\u4f53\u505a\u529f\u8d8a\u591a\uff0c\u529f\u7387\u8d8a\u5927\nB. \u7269\u4f53\u505a\u529f\u65f6\u95f4\u8d8a\u77ed\uff0c\u529f\u7387\u8d8a\u5927\nC. \u7269\u4f53\u505a\u529f\u8d8a\u5feb\uff0c\u529f\u7387\u8d8a\u5927\nD. \u7269\u4f53\u505a\u529f\u65f6\u95f4\u8d8a\u957f\uff0c\u529f\u7387\u8d8a\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5567495648742405, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7694875001360125}}, {"question": "\u57fa\u7763\u6559\u4e8e\u4ec0\u4e48\u65f6\u5019\u88ab\u7f57\u9a6c\u5e1d\u56fd\u627f\u8ba4\u662f\u5408\u6cd5\u5b97\u6559\u3002\nA. \u516c\u514342\u5e74\nB. \u516c\u514364\u5e74\nC. \u516c\u5143250\u5e74\nD. \u516c\u5143313\u5e74\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9200599475379488, "meta-math/MetaMath-Mistral-7B": 0.9962553906393465, "itpossible/Chinese-Mistral-7B-v0.1": 0.8765921638121826, "HuggingFaceH4/zephyr-7b-beta": 0.975514911680622, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5663881986703987, "meta-llama/Meta-Llama-3-8B": 0.7889560529835358, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9995545266417647}}, {"question": "\u67d0\u6d41\u57df\u7684\u4e00\u573a\u6d2a\u6c34\u4e2d\uff0c\u5730\u9762\u5f84\u6d41\u7684\u6d88\u9000\u901f\u5ea6\u4e0e\u5730\u4e0b\u5f84\u6d41\u7684\u76f8\u6bd4\uff0c\u5176\u60c5\u51b5\u662f[ ]\u3002\nA. \u524d\u8005\u5927\u4e8e\u540e\u8005\nB. \u524d\u8005\u5c0f\u4e8e\u540e\u8005\nC. \u524d\u8005\u5c0f\u4e8e\u7b49\u4e8e\u540e\u8005\nD. \u4e8c\u8005\u76f8\u7b49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3838049430956765, "meta-math/MetaMath-Mistral-7B": 0.3966311141649463, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3941364379880996, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7389\u7c73T\u578b\u4e0d\u80b2\u7cfb\u662f\u5c5e\u4e8e\u5b62\u5b50\u4f53\u4e0d\u80b2\u578b\uff0c\u7528S\uff08rr\uff09\u96cc\u6027\u00d7N\uff08RR\uff09\u96c4\u6027\uff0cF2\u80b2\u6027\u8868\u73b0\u4e3a\nA. 3/4\u53ef\u80b2\u690d\u682a:1/4\u4e0d\u80b2\u690d\u682a\nB. \u5168\u90e8\u690d\u682a\u82b1\u7c89\u53ef\u80b2\nC. \u540c\u4e00\u690d\u682a\u4e0a3/4\u82b1\u7c89\u53ef\u80b2:1/4\u82b1\u7c89\u4e0d\u80b2\nD. 3/4\u690d\u682a\u8868\u73b0\u7a57\u4e0a\u82b1\u7c89\u80b2\u6027\u5206\u79bb:1/4\u690d\u682a\u5b8c\u5168\u4e0d\u80b2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2854833233458694, "meta-math/MetaMath-Mistral-7B": 0.60791374755879, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4300758428768765, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u53ec\u6811\u5c6f\u548c\u5357\u6728\u8bfa\u5a1c\u300b\u662f\u54ea\u4e2a\u6c11\u65cf\u7684\u6c11\u95f4\u821e\u8e48\nA. \u82d7\nB. \u767d\nC. \u50a3\nD. \u85cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.34239623393788804, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.33544561004293627, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7963047843142508}}, {"question": "\u300a\u6700\u9ad8\u4eba\u6c11\u6cd5\u9662\u5173\u4e8e\u8d2f\u5f7b\u6267\u884c\u3008\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u6c11\u6cd5\u901a\u5219\u3009\u82e5\u5e72\u95ee\u9898\u7684\u610f\u89c1\uff08\u8bd5\u884c\uff09\u300b\u7b2c184\u6761\u89c4\u5b9a\uff1a\u201c\u5916\u56fd\u6cd5\u4eba\u4ee5\u5176\u6ce8\u518c\u767b\u8bb0\u5730\u56fd\u5bb6\u7684\u6cd5\u5f8b\u4e3a\u5176\u672c\u56fd\u6cd5\uff0c\u6cd5\u4eba\u7684\u6c11\u4e8b\u884c\u4e3a\u80fd\u529b\u4f9d\u5176\u672c\u56fd\u6cd5\u786e\u5b9a\u3002\u5916\u56fd\u6cd5\u4eba\u5728\u6211\u56fd\u9886\u57df\u5185\u8fdb\u884c\u7684\u6c11\u4e8b\u6d3b\u52a8\uff0c\u5fc5\u987b\u7b26\u5408\u6211\u56fd\u7684\u6cd5\u5f8b\u89c4\u5b9a\u3002\u201d\u8be5\u6761\u6b3e\u6240\u4f53\u73b0\u7684\u662f\u54ea\u4e00\u539f\u5219\uff1f\nA. \u5c5e\u5730\u4e3b\u4e49\u539f\u5219\nB. \u4fdd\u62a4\u4e3b\u4e49\u539f\u5219\nC. \u5c5e\u4eba\u4e3b\u4e49\u539f\u5219\nD. \u5c5e\u4eba\u4e3b\u4e49\u548c\u5c5e\u5730\u4e3b\u4e49\u4e4b\u6298\u4e2d\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3230435121167668, "itpossible/Chinese-Mistral-7B-v0.1": 0.5937452201987418, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u7535\u6e90\u7535\u538b\u4e00\u5b9a\u65f6\uff0c\u4e0e\u8d1f\u8f7d\u7535\u6d41\u5927\u5c0f\u65e0\u5173\u7684\u53d8\u538b\u5668\u635f\u8017\u662f\nA. \u6ca1\u6709\nB. \u94c1\u635f\nC. \u603b\u7684\u635f\u8017\nD. \u94dc\u635f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.310313193127302, "meta-math/MetaMath-Mistral-7B": 0.4720305127304022, "itpossible/Chinese-Mistral-7B-v0.1": 0.4045257140067991, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3938024279923704, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6708\u7ecf\u5468\u671f\u4e2d\u96cc\u6fc0\u7d20\u5206\u6ccc\u51fa\u73b0\u7b2c\u4e8c\u6b21\u9ad8\u5cf0\u7684\u76f4\u63a5\u539f\u56e0\u662f\nA. \u5375\u6ce1\u523a\u6fc0\u7d20\u7684\u4f5c\u7528\nB. \u96cc\u6fc0\u7d20\u7684\u6b63\u53cd\u9988\u4f5c\u7528\nC. \u5b55\u6fc0\u7d20\u7684\u6b63\u53cd\u9988\u4f5c\u7528\nD. \u9ec4\u4f53\u751f\u6210\u7d20\u7684\u4f5c\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6cd5\u5f8b\u90e8\u95e8\u7684\u5212\u5206\u9700\u8981\u5728\u9075\u5faa\u5ba2\u89c2\u6807\u51c6\u7684\u540c\u65f6\u575a\u6301\u6b63\u786e\u7684\u539f\u5219\uff0c\u4e0b\u5217\u5bf9\u4e8e\u6cd5\u5f8b\u90e8\u95e8\u5212\u5206\u539f\u5219\u7684\u7406\u89e3\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u9002\u5f53\u5e73\u8861\u539f\u5219\u4e3b\u8981\u662f\u6307\u5404\u6cd5\u5f8b\u90e8\u95e8\u5305\u542b\u7684\u6cd5\u5f8b\u6cd5\u89c4\u5728\u6570\u91cf\u4e0a\u5927\u81f4\u5e73\u8861\nB. \u76f8\u5bf9\u7a33\u5b9a\u539f\u5219\u8981\u6c42\u6cd5\u5f8b\u90e8\u95e8\u5212\u5206\u5e94\u5f53\u6709\u4e00\u5b9a\u7684\u524d\u77bb\u6027\uff0c\u4e0d\u80fd\u9891\u7e41\u53d8\u52a8\u6cd5\u5f8b\u90e8\u95e8\u7684\u5185\u5bb9\u3001\u7ed3\u6784\nC. \u5ba2\u89c2\u6027\u539f\u5219\u8981\u6c42\u5212\u5206\u6cd5\u5f8b\u90e8\u95e8\u5e94\u4ee5\u6cd5\u5f8b\u89c4\u8303\u7684\u5185\u5728\u7ed3\u6784\u548c\u6548\u529b\u4f4d\u9636\u4e3a\u57fa\u7840\nD. \u5f53\u540c\u4e00\u90e8\u6cd5\u5f8b\u53ef\u4ee5\u88ab\u5212\u5206\u4e8e\u51e0\u4e2a\u4e0d\u540c\u7684\u6cd5\u5f8b\u90e8\u95e8\u65f6\uff0c\u5e94\u91c7\u7528\u4e3b\u6b21\u539f\u5219\u5bf9\u5176\u5212\u5f52\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u7ba1\u5236\u7684\u63cf\u8ff0\uff0c\u4e0d\u6b63\u786e\u7684\u662f\uff08 \uff09\u3002\nA. \u5728\u52b3\u52a8\u4e2d\u540c\u5de5\u540c\u916c\nB. \u4f9d\u6cd5\u5b9e\u884c\u793e\u533a\u77eb\u6b63\nC. \u53ef\u540c\u65f6\u9002\u7528\u7981\u6b62\u4ee4\nD. \u5211\u671f\u4ece\u5224\u51b3\u5ba3\u544a\u4e4b\u65e5\u8d77\u8ba1\u7b97\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3208267503383532, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4061526466284296, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.46997671254146767, "meta-llama/Meta-Llama-3-8B": 0.38181831864725896, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5929\u65f6\u4e0d\u5982\u5730\u5229\uff0c\u5730\u5229\u4e0d\u5982\u4eba\u548c\u201d\u51fa\u81ea\nA. \u300a\u5b5f\u5b50\u300b\nB. \u300a\u8bba\u8bed\u300b\nC. \u300a\u5e84\u5b50\u300b\nD. \u300a\u6625\u79cb\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2957129572682819, "meta-math/MetaMath-Mistral-7B": 0.31779524536129716, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38176024012366444, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe\u968f\u673a\u53d8\u91cf$X_1 \u3001 X_2$\u76f8\u4e92\u72ec\u7acb\uff0c\u5b83\u4eec\u7684\u5206\u5e03\u5217\u5206\u522b\u4e3a:$$X_1 \\sim(\\begin{array}{ccc}0 & 1 & 2 \\\\0.5 & 0.3 & 0.2\\end{array})\uff0cX_2 \\sim(\\begin{array}{cccc}0 & 1 & 2 & 3 \\\\0.4 & 0.3 & 0.2 & 0.1\\end{array})$$\u4ee4$S=X_1+X_2$\uff0c\u5219$P_S(2)=$\u3002\nA. 0.07\nB. 0.20\nC. 0.17\nD. 0.27\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bfa\u8d1d\u5c14\u5956\u662f\u6839\u636e\u5316\u5b66\u5bb6\u8bfa\u8d1d\u5c14\u9057\u5631\u8bbe\u7acb\u7684\uff0c\u5305\u62ec\u81ea\u7136\u79d1\u5b66\u548c\u4eba\u6587\u79d1\u5b66\u7684\u7efc\u5408\u6027\u3002\u56fd\u9645\u6027\u548c\u6c38\u4e45\u6027\u7cfb\u5217\u5956\u9879\uff0c\u4e3a\u56fd\u9645\u6700\u9ad8\u8363\u8a89\u5956\u9879\u3002\u8bfa\u8d1d\u5c14\u7684\u56fd\u7c4d\u662f\nA. \u745e\u5178\nB. \u745e\u58eb\nC. \u82f1\u56fd\nD. \u5fb7\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9426116898129979, "meta-math/MetaMath-Mistral-7B": 0.9954305871982061, "itpossible/Chinese-Mistral-7B-v0.1": 0.9198932603856375, "HuggingFaceH4/zephyr-7b-beta": 0.9999391160439779, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9906201500377568, "meta-llama/Meta-Llama-3-8B": 0.9758876450989701, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9957202673742049}}, {"question": "\u97e6\u4f2f\u7684\u4e3b\u8981\u8457\u4f5c\u6709\nA. \u300a\u5b97\u6559\u751f\u6d3b\u7684\u57fa\u672c\u5f62\u5f0f\u300b\nB. \u300a\u793e\u4f1a\u5206\u5de5\u8bba\u300b\nC. \u300a\u793e\u4f1a\u9759\u529b\u5b66\u300b\nD. \u300a\u65b0\u6559\u4f26\u7406\u4e0e\u8d44\u672c\u4e3b\u4e49\u7cbe\u795e\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.40322670951905004, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5221145873208052, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67cf\u62c9\u56fe\u7684\u7406\u5ff5\u8bba\u5177\u6709\u5f88\u5f3a\u7684\uff08\uff09\u8272\u5f69\u3002\nA. \u4f26\u7406\nB. \u795e\u8bdd\nC. \u653f\u6cbb\nD. \u79d1\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.470139614369339, "HuggingFaceH4/zephyr-7b-beta": 0.6306977673253236, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8736397923821929}}, {"question": "\u9176\u4e0e\u4e00\u822c\u50ac\u5316\u5242\u5177\u6709\u4ee5\u4e0b\u5171\u6027\uff0c\u4f8b\u5916\u7684\u662f\nA. \u4e0d\u80fd\u6539\u53d8\u53cd\u5e94\u5e73\u8861\u70b9\nB. \u964d\u4f4e\u53cd\u5e94\u6d3b\u5316\u80fd\nC. \u53cd\u5e94\u524d\u540e\u81ea\u8eab\u6ca1\u6709\u8d28\u4e0e\u91cf\u7684\u6539\u53d8\nD. \u7531\u7279\u5b9a\u6784\u60f3\u7684\u6d3b\u6027\u4e2d\u5fc3\u53d1\u6325\u4f5c\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9707\u540e\u6551\u4eba\u65f6\u5bf9\u5904\u4e8e\u9ed1\u6697\u7a92\u606f\u3001\u9965\u6e34\u72b6\u6001\u4e0b\u57cb\u538b\u8fc7\u4e45\u7684\u4eba\uff0c\u6b63\u786e\u7684\u62a4\u7406\u65b9\u6cd5\u662f\nA. \u5c3d\u5feb\u6551\u51fa\u6765\uff0c\u5c3d\u5feb\u8fdb\u98df\nB. \u5c3d\u5feb\u6551\u51fa\u6765\uff0c\u5c3d\u5feb\u89c1\u5149\u4eae\nC. \u5c3d\u5feb\u6551\u51fa\u6765\uff0c\u5c3d\u5feb\u8f93\u6c27\nD. \u8499\u4e0a\u773c\u775b\u6551\u51fa\u6765\uff0c\u6162\u6162\u547c\u5438\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u671f\u672b\u7ed3\u8d26\u540e\uff0c\u8d39\u7528\u7c7b\u8d26\u6237\u7684\u4f59\u989d\nA. \u4e00\u5b9a\u5728\u501f\u65b9\nB. \u7b49\u4e8e\u96f6\nC. \u4e00\u5b9a\u5728\u8d37\u65b9\nD. \u6709\u65f6\u5728\u501f\u65b9\uff0c\u6709\u65f6\u5728\u8d37\u65b9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4279733272551463, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u98df\u54c1\u4e2d\uff0c\u54ea\u4e9b\u5c5e\u7981\u6b62\u751f\u4ea7\u7ecf\u8425\u7684\nA. \u65e0\u6807\u7b7e\u7684\u9884\u5305\u88c5\u98df\u54c1\nB. \u8d85\u8fc7\u4fdd\u8d28\u671f\u7684\u98df\u54c1\nC. \u4ee5\u4e0a\u90fd\u662f\nD. \u8425\u517b\u6210\u5206\u4e0d\u7b26\u5408\u98df\u54c1\u5b89\u5168\u6807\u51c6\u7684\u4e13\u4f9b\u5a74\u5e7c\u513f\u548c\u5176\u4ed6\u7279\u5b9a\u4eba\u7fa4\u7684\u4e3b\u8f85\u98df\u54c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4478229463913712, "meta-math/MetaMath-Mistral-7B": 0.697621611067203, "itpossible/Chinese-Mistral-7B-v0.1": 0.3606485908051371, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8548362099935166, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6379225570053081}}, {"question": "\u513f\u7ae5\u5728\u6027\u6e38\u620f\u4e2d\nA. \u5b66\u4e60\u6027\u6d3b\u52a8\nB. \u6ee1\u8db3\u6027\u6b32\u671b\nC. \u5b66\u4e60\u548c\u6a21\u4eff\u6210\u4eba\u699c\u6837\nD. \u4eab\u53d7\u6027\u5feb\u611f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6184469845382073, "meta-math/MetaMath-Mistral-7B": 0.7669811854551682, "itpossible/Chinese-Mistral-7B-v0.1": 0.5277410314170397, "HuggingFaceH4/zephyr-7b-beta": 0.9955383183063907, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.680575892622364, "meta-llama/Meta-Llama-3-8B": 0.7848511175203898, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9634755564921311}}, {"question": "\u201c\u592b\u793c\u4e4b\u521d\uff0c\u59cb\u8bf8\u996e\u98df\u201d\u8fd9\u53e5\u8bdd\u51fa\u81ea\nA. \u300a\u793c\u8bb0\u300b\nB. \u300a\u5468\u793c\u300b\nC. \u300a\u4eea\u793c\u300b\nD. \u300a\u8bba\u8bed\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3550009601731296, "meta-math/MetaMath-Mistral-7B": 0.5893012202001756, "itpossible/Chinese-Mistral-7B-v0.1": 0.435985751168481, "HuggingFaceH4/zephyr-7b-beta": 0.5084498970926947, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7303838957075778, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6251333510109354}}, {"question": "\u714e\u8377\u5305\u86cb\u65f6\uff0c\u5728\u70ed\u9505\u91cc\u52a0\u4ec0\u4e48\u53ef\u4ee5\u9632\u6b62\u6cb9\u7684\u98de\u6e85\nA. \u6dc0\u7c89\nB. \u6e29\u6c34\nC. \u9171\u6cb9\nD. \u76d0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f5b\u6559\u56db\u5927\u5929\u738b\u4e2d\uff0c\u897f\u65b9\u5e7f\u76ee\u5929\u738b\u624b\u6301\u7684\u5175\u5668\u662f\nA. \u7435\u7436\nB. \u5b9d\u5251\nC. \u96e8\u4f1e\nD. \u7ef3\u7d22\u6216\u9f99\u86c7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3919625941466883, "HuggingFaceH4/zephyr-7b-beta": 0.713791602350665, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3528619536675027, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4988507755423443}}, {"question": "\u534e\u5317\u5730\u533a\u4e0d\u80fd\u683d\u57f9\u67d1\u6854\u3002\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\u662f\nA. \u6d3b\u52a8\u79ef\u6e29\nB. \u8d8a\u51ac\u6700\u4f4e\u6e29\u5ea6\nC. \u6709\u6548\u79ef\u6e29\nD. \u6700\u70ed\u6708\u6e29\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4053276424169019, "meta-math/MetaMath-Mistral-7B": 0.6235777978593002, "itpossible/Chinese-Mistral-7B-v0.1": 0.5530762562180135, "HuggingFaceH4/zephyr-7b-beta": 0.9905070762890198, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9560129343425213, "meta-llama/Meta-Llama-3-8B": 0.5152277915574198, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9305277262223639}}, {"question": "\u4ee5\u4e0b\u5173\u4e8e\u6559\u5e08\u4e0e\u5b66\u751f\u4e4b\u95f4\u6cd5\u5f8b\u5173\u7cfb\u7684\u8bf4\u6cd5\uff0c\u4e0d\u6b63\u786e\u7684\u9009\u9879\u662f\nA. \u7ba1\u7406\u4e0e\u88ab\u7ba1\u7406\u7684\u5173\u7cfb\nB. \u4fdd\u62a4\u4e0e\u88ab\u4fdd\u62a4\u7684\u5173\u7cfb\nC. \u63a7\u5236\u4e0e\u88ab\u63a7\u5236\u7684\u5173\u7cfb\nD. \u6559\u80b2\u4e0e\u88ab\u6559\u80b2\u7684\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8240766599913387, "meta-math/MetaMath-Mistral-7B": 0.9578548060676986, "itpossible/Chinese-Mistral-7B-v0.1": 0.7748804863475893, "HuggingFaceH4/zephyr-7b-beta": 0.9990554604701446, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8269126345760738, "meta-llama/Meta-Llama-3-8B": 0.4528728233058935, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8831081806412672}}, {"question": "\u8ba1\u7b97\u67d0\u4e00\u65f6\u671f\u5185\u53d1\u75c5\u7387\u65f6\u8981\u8003\u8651\u65b0\u53d1\u75c5\u4f8b\u6570\u548c\nA. \u75c5\u4f8b\u6309\u65f6\u95f4\u5206\u5e03\u3001\u66b4\u9732\u4eba\u53e3\u6570\nB. \u66b4\u9732\u4eba\u53e3\u6570\u7684\u5e74\u9f84\u3001\u6027\u522b\u5206\u5e03\nC. \u75c5\u4f8b\u6309\u6027\u522b\u5e74\u9f84\u5206\u5e03\u3001\u66b4\u9732\u4eba\u53e3\u6570\nD. \u75c5\u4f8b\u7684\u5730\u533a\u5206\u5e03\u3001\u5404\u5730\u533a\u66b4\u9732\u4eba\u53e3\u6570\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e9a\u91cc\u58eb\u591a\u5fb7\u63d0\u51fa\u7684\u8bba\u8bc1\u8bc4\u4ef7\u4e09\u6807\u51c6\u4e0d\u5305\u542b\nA. \u5206\u6790\u6807\u51c6\nB. \u4fee\u8f9e\u6807\u51c6\nC. \u8bba\u8fa9\u6807\u51c6\nD. \u903b\u8f91\u6807\u51c6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4403307162059198, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u725b\u5976\u4e0d\u5b9c\u4e0e\u4e0b\u5217\u54ea\u4e9b\u6c34\u679c\u540c\u5403\nA. \u9999\u8549\nB. \u82f9\u679c\nC. \u6a58\u5b50\nD. \u68a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3990012287478204, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.41031354508388346, "HuggingFaceH4/zephyr-7b-beta": 0.4626430517885697, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6159436381473287, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5$\\tan\\alpha=1/4$, $\\tan(\\alpha-\\beta)=1/3$\uff0c\u5219$\\tan\\beta=$\nA. -1/13\nB. 1/13\nC. -11/7\nD. 7/11\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4614726445129184}}, {"question": "\u201c\u4e24\u4eba\u624b\u6301\u4e00\u79cd\u85e4\u3001\u76ae\u3001\u68c9\u5236\u4f5c\u7684\u77ed\u68d2\u4f3c\u7684\u5668\u68b0\uff0c\u572816\u5e02\u5c3a\u76f4\u5f84\u7684\u5706\u5f62\u573a\u5730\u5185\uff0c\u6309\u7167\u4e00\u5b9a\u89c4\u5219\u4f7f\u7528\u5288\u3001\u780d\u3001\u523a\u3001\u5d29\u3001\u70b9\u3001\u65a9\u7b49\u65b9\u6cd5\u4ee5\u51b3\u80dc\u8d1f\u201d\uff0c\u5177\u4f53\u6765\u8bb2\u8fd9\u662f\u4e0b\u5217\u54ea\u79cd\u8fd0\u52a8\u3002\nA. \u640f\u6597\u8fd0\u52a8\nB. \u77ed\u5175\nC. \u51fb\u5251\nD. \u5668\u68b0\u5bf9\u7ec3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4043454556636844, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6c49\u5b57\u90fd\u662f\u8c61\u5f62\u5b57\u7684\u662f\nA. \u9a6c\u3001\u6c34\u3001\u706b\nB. \u4ea6\u3001\u65e5\u3001\u5bf8\nC. \u4ece\u3001\u672c\u3001\u6728\nD. \u5317\u3001\u5203\u3001\u6708\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4523982422198261, "meta-math/MetaMath-Mistral-7B": 0.29258944112561125, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8197839411511323, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5987341664399921, "meta-llama/Meta-Llama-3-8B": 0.5340639057355574, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7921826338501606}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6c11\u4e3b\u3001\u6cd5\u6cbb\u3001\u4eba\u6cbb\u3001\u5fb7\u6cbb\u7684\u8868\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u4f9d\u7167\u4eba\u6cbb\u7684\u7406\u8bba\uff0c\u6cbb\u7406\u56fd\u5bb6\u4e0d\u9700\u8981\u6cd5\u5f8b\nB. \u897f\u65b9\u201c\u6c11\u4e3b\u201d\u4e00\u8bcd\u6e90\u4e8e\u53e4\u7f57\u9a6c\nC. \u6cd5\u6cbb\u4e0e\u5fb7\u6cbb\u662f\u4e24\u79cd\u6839\u672c\u5bf9\u7acb\u7684\u6cbb\u56fd\u65b9\u7565\nD. \u4f9d\u7167\u4f20\u7edf\u7684\u5fb7\u6cbb\u7406\u8bba\uff0c\u56fd\u5bb6\u4e3b\u8981\u901a\u8fc7\u9053\u5fb7\u6559\u5316\u8fdb\u884c\u6cbb\u7406\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4310339313236754, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5372592105744849, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7a7a\u95f4\u7269\u4f53\u53d1\u5c04\u56fd\u5e94\u5c06\u5176\u53d1\u5c04\u7684\u7269\u4f53\u62a5\u544a\u7ed9\nA. \u8054\u5408\u56fd\u7ecf\u6d4e\u53ca\u793e\u4f1a\u7406\u4e8b\u4f1a\nB. \u8054\u5408\u56fd\u5916\u7a7a\u59d4\u5458\u4f1a\nC. \u8054\u5408\u56fd\u5b89\u5168\u7406\u4e8b\u4f1a\nD. \u8054\u5408\u56fd\u79d8\u4e66\u957f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4575160312128929, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5730\u7403\u8868\u2faf\u80fd\u91cf\u4e3b\u8981\u6765\u6e90\u4e8e\nA. \u2f24\u2f53\u53cd\u5c04\nB. \u5730\u7403\u5185\u90e8\nC. \u592a\u9633\u8f90\u5c04\nD. \u2f24\u2f53\u8f90\u5c04\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9324548770190584, "meta-math/MetaMath-Mistral-7B": 0.9968306443919462, "itpossible/Chinese-Mistral-7B-v0.1": 0.8537713115995371, "HuggingFaceH4/zephyr-7b-beta": 0.9991734388712523, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9656022509745987, "meta-llama/Meta-Llama-3-8B": 0.9381863379289329, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9956563573896151}}, {"question": "\u4e0d\u662f\u7ee7\u53d1\u6027\u80ba\u7ed3\u6838\u7684\u597d\u53d1\u90e8\u4f4d\nA. \u53f3\u4e2d\u53f6\u6216\u5de6\u820c\u53f6\nB. \u4e0b\u53f6\u540e\u57fa\u5e95\u6bb5\nC. \u4e0a\u53f6\u5c16\u540e\u6bb5\nD. \u4e0b\u53f6\u80cc\u6bb5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4117253102680101, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u82f1\u56fd18\u4e16\u7eaa\u4eba\u53e3\u6b7b\u4ea1\u7387\u660e\u663e\u4e0b\u964d\uff0c\u4f461816\u5e74\u540e\u6b7b\u4ea1\u7387\u4e0a\u5347\uff0c1831-1841\u5e74\uff0c\u5de5\u5382\u96c6\u4e2d\u7684\u4f2f\u660e\u7ff0\u6bcf\u5e74\u4eba\u6b7b\u4ea1\u7387\u753114.6\u4e0a\u5347\u523027.2\uff0c\u5229\u7269\u6d66\u753121\u4e0a\u5347\u523034.8\uff0c\u5bfc\u81f4\u4e0a\u8ff0\u60c5\u51b5\u53d1\u751f\u7684\u91cd\u8981\u539f\u56e0\u662f\nA. \u4eba\u53e3\u81a8\u80c0\u98df\u7269\u77ed\u7f3a\nB. \u533b\u7597\u6280\u672f\u6c34\u5e73\u4e0b\u964d\nC. \u57ce\u5e02\u73af\u5883\u6781\u5176\u6076\u5316\nD. \u5316\u5b66\u5de5\u4e1a\u6c61\u67d3\u4e25\u91cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7217812988848032, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8323123301754398, "HuggingFaceH4/zephyr-7b-beta": 0.9440936960983541, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7575048490818288, "meta-llama/Meta-Llama-3-8B": 0.9282824443568612, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8803135778062989}}, {"question": "\u5316\u5b66\u6027\u75bc\u75db\u4e0d\u5177\u5907\u4ee5\u4e0b\u54ea\u9879\u8868\u73b0\u5f62\u5f0f\uff1f\nA. \u6301\u7eed\u75db\nB. \u591c\u95f4\u75db\nC. \u95f4\u6b47\u75bc\u75db\nD. \u4f11\u606f\u75db\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2821833983601388, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9488\u5bf9\u4f01\u4e1a\u8425\u9500\u6d3b\u52a8\u7684\u6027\u8d28\u63d0\u51fa\u7684\u4ea4\u6613\u4e2d\u7684\u57fa\u672c\u4fe1\u6761\u662f\nA. \u516c\u5e73\u7ade\u4e89\nB. \u4f18\u8d28\u670d\u52a1\nC. \u4e92\u5229\u4e0e\u5408\u4f5c\nD. \u8425\u9500\u9053\u5fb7\u539f\u5219\u7684\u5236\u5ea6\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.45700661919012875, "meta-math/MetaMath-Mistral-7B": 0.6178756482632423, "itpossible/Chinese-Mistral-7B-v0.1": 0.5094952310073941, "HuggingFaceH4/zephyr-7b-beta": 0.7843643538258469, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.854581367163579}}, {"question": "\u67d0\u4ea7\u54c1\u7684\u5bff\u547d\u751f\u5b58\u51fd\u6570\u4e3a$S(x)=1-0.0025 x^2\uff0c0 \\leqslant x \\leqslant 20$\uff0c\u5219\u8be5\u4ea7\u54c1\u4e2d\u503c\u5e74\u9f84\u65f6\u7684\u672b\u6765\u671f\u671b\u5bff\u547d\u4e3a ( ) \u3002\nA. 2.0965\nB. 12.142\nC. 3.0966\nD. 1.0965\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3157387487413704, "meta-math/MetaMath-Mistral-7B": 0.4227611043096916, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6384888027205364, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.36150852560561064, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u8111\u5185\uff0cNH3\u7684\u4e3b\u8981\u50a8\u5b58\u548c\u8fd0\u8f93\u5f62\u5f0f\u662f\nA. \u5c3f\u7d20\nB. \u8c37\u6c28\u9170\u80fa\nC. \u5929\u51ac\u6c28\u9178\nD. \u4e19\u6c28\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37566161190965525, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6543091387558159, "HuggingFaceH4/zephyr-7b-beta": 0.8654028723538472, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.37185516753938336, "meta-llama/Meta-Llama-3-8B": 0.39008524332734884, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7173140554566617}}, {"question": "\u4e2a\u4eba\u5730\u4f4d\u83b7\u5f97\u6a21\u5f0f\u5f3a\u8c03\u5bf9\u83b7\u5f97\u65b0\u804c\u4e1a\u5177\u6709\u91cd\u8981\u610f\u4e49\u7684\u56e0\u7d20\u662f\nA. \u672c\u4eba\u53d7\u6559\u80b2\u6c34\u5e73\nB. \u672c\u4eba\u7b2c\u4e00\u4e2a\u804c\u4e1a\nC. \u73b0\u804c\u4e1a\nD. \u7236\u4eb2\u7684\u804c\u4e1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2867722825175887, "meta-math/MetaMath-Mistral-7B": 0.4907780365772522, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.608530794259424, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38812073416459414, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3788721386989683}}, {"question": "\u4e0e\u547c\u5438\u5173\u7cfb\u6700\u5bc6\u5207\u7684\u4e24\u810f\u662f\nA. \u80ba\u4e0e\u80be\nB. \u80ba\u4e0e\u5fc3\nC. \u80ba\u4e0e\u809d\nD. \u813e\u4e0e\u80ba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6d41\u57df\u7684\u603b\u84b8\u53d1\u5305\u62ec[ ]\u3002\nA. \u6c34\u9762\u84b8\u53d1\u3001\u571f\u58e4\u84b8\u53d1\u3001\u9646\u9762\u84b8\u6563\u53d1\nB. \u9646\u9762\u84b8\u53d1\u3001\u690d\u7269\u84b8\u6563\u53d1\u3001\u571f\u58e4\u84b8\u53d1\nC. \u6c34\u9762\u84b8\u53d1\u3001\u9646\u9762\u84b8\u53d1\u3001\u690d\u7269\u84b8\u6563\u53d1\nD. \u6c34\u9762\u84b8\u53d1\u3001\u690d\u7269\u84b8\u6563\u53d1\u3001\u571f\u58e4\u84b8\u53d1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3329283826715201, "meta-math/MetaMath-Mistral-7B": 0.45739273913525497, "itpossible/Chinese-Mistral-7B-v0.1": 0.3423962339378881, "HuggingFaceH4/zephyr-7b-beta": 0.9985687871686313, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.649401332483259, "meta-llama/Meta-Llama-3-8B": 0.38060176574458326, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u8d85\u5e02\u9500\u552e\u7684A\u4ea7\u54c1\u7684\u4f9b\u8d27\u6210\u672c\u662f50\u5143\uff0c\u5e02\u573a\u96f6\u552e\u4ef7\u662f80\u5143\uff0c\u5982\u679c\u7528\u96f6\u552e\u4ef7\u683c\u6765\u8861\u91cf\uff0cA\u4ea7\u54c1\u7684\u52a0\u6210\u7387R\u5e94\u8be5\u4e3a\nA. 160%\nB. 60%\nC. 63%\nD. 37.50%\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u82e5\u8102\u2f64\u53cc\u7b26\u53f7\u4f4d\uff0c\u5219\u53d1\u2f63\u6b63\u6ea2\u7684\u7279\u5f81\u662f\uff1a\u53cc\u7b26\u53f7\u4f4d\u4e3a\nA. 1 0\nB. 1 1\nC. 0 1\nD.  0 0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b0\u8bbe\u4f01\u4e1a\u6536\u5230\u6295\u8d44\u4eba\u6295\u5165\u7684\u8d27\u5e01\u8d44\u91d1\uff0c\u5e94\u8d37\u8bb0\u7684\u79d1\u76ee\u662f\nA. \u201c\u5176\u4ed6\u5e94\u4ed8\u6b3e\u201d\nB. \u201c\u5b9e\u6536\u8d44\u672c\u201d\nC. \u201c\u9884\u6536\u8d26\u6b3e\u201d\nD. \u201c\u5e94\u4ed8\u8d26\u6b3e\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.810309427215727, "meta-math/MetaMath-Mistral-7B": 0.9791386331084805, "itpossible/Chinese-Mistral-7B-v0.1": 0.7423642994879258, "HuggingFaceH4/zephyr-7b-beta": 0.9921444567380572, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9818154546960786, "meta-llama/Meta-Llama-3-8B": 0.8563908023347615, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9545759010309943}}, {"question": "\u9009\u51fa\u4e0b\u5217\u53e5\u2f26\u4e2d\u6ca1\u6709\u8bed\u75c5\u7684\u2f00\u9879\nA. \u5982\u679c\u6ca1\u6709\u8fd9\u4e9b\u2f63\u7075\uff0c\u2f24\u2f83\u7136\u624d\u4f1a\u5931\u53bb\u4e86\u2f8a\u5f69\u3002\nB. \u968f\u7740\u79d1\u5b66\u2f47\u65b0\u2f49\u5f02\u7684\u53d1\u5c55\uff0c\u4f7f\u7535\u8111\u5df2\u6210\u4e3a\u2f08\u4eec\u4e0d\u53ef\u7f3a\u5c11\u7684\u2f2f\u5177\u3002\nC. \u53c2\u52a0\u56fd\u9645\u2ee2\u62c9\u677e\u2f50\u8d5b\u7684\u8fd0\u52a8\u5458\u5728\u516c\u8def\u4e0a\u2edc\u5feb\u5730\u5954\u8dd1\u3002 \nD. \u80fd\u5426\u987a\u5229\u5f00\u5c55\u2f24\u8bfe\u95f4\u6d3b\u52a8\uff0c\u662f\u63d0\u2fbc\u5b66\u2f63\u8eab\u4f53\u7d20\u8d28\u7684\u91cd\u8981\u4fdd\u969c\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.39196259414668827, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5408762846295914}}, {"question": "\u6700\u65e9\u7814\u7a76\u8ba1\u7b97\u673a\u7f51\u7edc\u7684\u76ee\u7684\u662f\u4ec0\u4e48\uff1f\nA. \u76f4\u63a5\u7684\u4e2a\u4eba\u901a\u4fe1\nB. \u5171\u4eab\u786c\u76d8\u7a7a\u95f4\u3001\u6253\u5370\u673a\u7b49\u8bbe\u5907\nC. \u5927\u91cf\u7684\u6570\u636e\u4ea4\u6362\nD. \u5171\u4eab\u8ba1\u7b97\u8d44\u6e90\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5630125866665702, "meta-math/MetaMath-Mistral-7B": 0.6773229276564237, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9991792245580963, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5387490318854252, "meta-llama/Meta-Llama-3-8B": 0.3982518517054595, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7541772155373094}}, {"question": "\u5df2\u77e5$l_x=1000(8-0.1 x)^{\\frac{1}{3}}\uff0c0 \\leqslant x \\leqslant 80$\uff0c\u5219 20 \u5c81\u4eba\u7684\u5269\u4f59\u5bff\u547d\u7684\u65b9\u5dee\u4e3a () \u3002\nA. 46\nB. 289.3\nC. 45\nD. 47.7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.29727732270820134, "meta-math/MetaMath-Mistral-7B": 0.31483005318115603, "itpossible/Chinese-Mistral-7B-v0.1": 0.28850952576306876, "HuggingFaceH4/zephyr-7b-beta": 0.44102705170224143, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3728641892601242, "meta-llama/Meta-Llama-3-8B": 0.3638582843838116, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7070476085373751}}, {"question": "\u755c\u79bd\u91c7\u98df\u751f\u8c46\u997c\u540e\u51fa\u73b0\u62c9\u7a00\uff0c\u5f71\u54cd\u86cb\u767d\u8d28\u7684\u5229\u7528\uff0c\u5176\u91cd\u8981\u539f\u56e0\u662f\u751f\u8c46\u997c\u4e2d\u542b\u6709\nA. \u6297\u7cd6\u5316\u9176\nB. \u6bd2\u7d20\nC. \u6297\u8102\u80aa\u9176\nD. \u6297\u80f0\u86cb\u767d\u9176\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3865080349581478, "meta-math/MetaMath-Mistral-7B": 0.31911523504877376, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9887673615926561, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "30\u5e74\u4ee3\u8d75\u4e39\u548c\u5468\u7487\u4e3b\u6f14\u7684\u5f71\u7247\uff08\uff09\u662f\u6211\u56fd\u7535\u5f71\u827a\u672f\u57fa\u672c\u6210\u719f\u7684\u6700\u91cd\u8981\u7684\u6807\u5fd7\u3002\nA. \u300a\u5c0f\u57ce\u4e4b\u6625\u300b\nB. \u300a\u9a6c\u8def\u5929\u4f7f\u300b\nC. \u300a\u4e00\u6c5f\u6625\u6c34\u5411\u4e1c\u6d41\u300b\nD. \u300a\u6e14\u5149\u66f2\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4252779334112031, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4489842197053917, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8336637824420108}}, {"question": "\u4e32\u2f8f\u63a5\u2f1d\u662f\u6307\nA. \u63a5\u2f1d\u4e0e\u7cfb\u7edf\u603b\u7ebf\u4e4b\u95f4\u5e76\u2f8f\u4f20\u9001\uff0c\u63a5\u2f1d\u4e0eI/O\u8bbe\u5907\u4e4b\u95f4\u5e76\u2f8f\u4f20\u9001\nB. \u63a5\u2f1d\u4e0e\u7cfb\u7edf\u603b\u7ebf\u4e4b\u95f4\u5e76\u2f8f\u4f20\u9001\uff0c\u63a5\u2f1d\u4e0eI/O\u8bbe\u5907\u4e4b\u95f4\u4e32\u2f8f\u4f20\u9001\nC. \u63a5\u2f1d\u4e0e\u7cfb\u7edf\u603b\u7ebf\u4e4b\u95f4\u4e32\u2f8f\u4f20\u9001\uff0c\u63a5\u2f1d\u4e0eI/O\u8bbe\u5907\u4e4b\u95f4\u5e76\u2f8f\u4f20\u9001\nD. \u63a5\u2f1d\u4e0e\u7cfb\u7edf\u603b\u7ebf\u4e4b\u95f4\u4e32\u2f8f\u4f20\u9001\uff0c\u63a5\u2f1d\u4e0eI/O\u8bbe\u5907\u4e4b\u95f4\u4e32\u2f8f\u4f20\u9001\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4713383340670756, "meta-math/MetaMath-Mistral-7B": 0.8016700166707951, "itpossible/Chinese-Mistral-7B-v0.1": 0.6561504849489735, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3256774323330271, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8104847505507687}}, {"question": "\u5df2\u77e5\u5728\u4e00\u4e2a\u591a\u51cf\u56e0\u6a21\u578b\u4e2d\uff0c\u6b7b\u4ea1\u529b\u6ee1\u8db3:$\\mu_{x+t}^{(k)}=\\frac{1}{n+1} \\cdot \\frac{k}{60-x-t}\uff0ct<60-x ; k=1\uff0c2\uff0c\\cdots\uff0cn$\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u6709$(\uff09\u3002$(1)${ }_t p_x^{(\\tau)}=\\frac{n}{2(60-x-t)}$(2)$f(t\uff0cj)=\\frac{j(60-x-t)^{\\frac{n}{2}-1}}{(n+1)(60-x)^{\\frac{n}{2}}}$;(3)$g(t)=\\frac{n(60-x-t)^{\\frac{n}{2}-1}}{2(60-x)^{\\frac{\\pi}{2}}}$;(4)$h(2 \\mid T=4)=\\frac{1}{n(n+1)}$\u3002\nA. (1) (2) (3) (4)\nB. (1) (2) (4)\nC. $(1)(3)(4)$\nD. (2) (3)\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.29660173325630934, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f9b\u7ed9\u7684\u53d8\u52a8\u662f\u6307\uff08\uff09\u4e0d\u53d8\u6761\u4ef6\u4e0b\uff0c\u5176\u4ed6\u56e0\u7d20\u53d8\u52a8\u6240\u5f15\u8d77\u7684\u4f9b\u7ed9\u7684\u53d8\u52a8\u3002\nA. \u672c\u8eab\u7684\u4ef7\u683c\nB. \u504f\u597d\nC. \u6280\u672f\nD. \u4ea7\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u652f\u6c14\u7ba1\u54ee\u5598\u6025\u6027\u53d1\u4f5c\u9996\u9009\u7684\u6cbb\u7597\u65b9\u6cd5\u662f\nA. \u96fe\u5316\u5438\u5165\u6c99\u4e01\u80fa\u9187\nB. \u9759\u8109\u4f7f\u7528\u7cd6\u76ae\u8d28\u6fc0\u7d20\nC. \u96fe\u5316\u5438\u5165\u5f02\u4e19\u6258\u6eb4\u94f5\nD. \u9759\u8109\u6ce8\u5c04\u6c28\u8336\u78b1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5992248618697807, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5294278660517604}}, {"question": "16\u6c7d\u4fee\uff082\uff09\u73ed\u603b\u2f08\u6570\u662f50\uff0c\u5176\u4e2d\u559c\u6b22\u84dd\u7403\u7684\u670921\u2f08\uff0c\u559c\u6b22\u2f7b\u2f51\u7403\u7684\u670919\u2f08\uff0c\u65e2\u4e0d\u559c\u6b22\u7bee\u7403\u2f1c\u4e0d\u559c\u6b22\u2f7b\u2f51\u7403\u7684\u670915\u2f08\uff0c\u90a3\u4e48\u65e2\u559c\u6b22\u7bee\u7403\u2f1c\u559c\u6b22\u2f7b\u2f51\u7403\u7684\u6709\u2f0f\u2f08\nA. 5\u2f08\nB. 7\u2f08\nC. 6\u2f08\nD. 4\u2f08\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2866128117777278, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u516c\u5171\u5173\u7cfb\u610f\u8bc6\u662f\u6307\u4e00\u4e2a\u7ec4\u7ec7\u5728\u4ee5\uff08\uff09\u5229\u76ca\u548c\u793e\u4f1a\u8d23\u4efb\u4e3a\u524d\u63d0\u7684\u57fa\u7840\u4e0a\u4ea7\u751f\u7684\uff0c\u4e3b\u52a8\u4e3a\u516c\u4f17\u670d\u52a1\u7684\u610f\u8bc6\u3001\u6c9f\u901a\u7684\u610f\u8bc6\u3001\u534f\u8c03\u7684\u610f\u8bc6\u3001\u4fe1\u8a89\u7684\u610f\u8bc6\u3001\u5f62\u8c61\u7684\u610f\u8bc6\u3001\u73af\u4fdd\u7684\u610f\u8bc6\u3001\u672a\u6765\u7684\u610f\u8bc6\u548c\u6539\u5584\u5173\u7cfb\u7684\u5404\u79cd\u53ef\u6301\u7eed\u53d1\u5c55\u7684\u610f\u8bc6\u3002\nA. \u793e\u4f1a\nB. \u7fa4\u4f17\nC. \u516c\u4f17\nD. \u4f01\u4e1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7173143384496009, "meta-math/MetaMath-Mistral-7B": 0.7115961595717684, "itpossible/Chinese-Mistral-7B-v0.1": 0.7265333534661474, "HuggingFaceH4/zephyr-7b-beta": 0.9987863061317865, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5905829471208283, "meta-llama/Meta-Llama-3-8B": 0.5561948256005164, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6307190657776388}}, {"question": "\u4e0b\u5217\u63aa\u65bd\u4e2d\uff0c\u5c5e\u4e8e\u6cd5\u5f8b\u5236\u88c1\u7684\u662f\nA. \u9a7e\u9a76\u5458\u4e59\u5f00\u8f66\u65f6\u4e0d\u5c0f\u5fc3\u5c06\u4e00\u884c\u4eba\u649e\u6210\u8f7b\u5fae\u4f24\uff0c\u4e59\u4e3b\u52a8\u8d54\u507f\u4e86\u8be5\u884c\u4eba300\u5143\u94b1\nB. \u516c\u6c11\u4e19\u72af\u76d7\u7a83\u7f6a\uff0c\u4f46\u56e0\u6709\u7acb\u529f\u8868\u73b0\uff0c\u6cd5\u9662\u5224\u51b3\u514d\u9664\u5176\u6cd5\u5f8b\u8d23\u4efb\nC. \u56fd\u5bb6\u516c\u52a1\u5458\u7532\u56e0\u8fdd\u6cd5\u5931\u804c\u53d7\u5230\u964d\u7ea7\u5904\u5206\nD. \u515a\u5458\u4e01\u56e0\u8fdd\u53cd\u515a\u7eaa\u53d7\u5230\u515a\u5185\u4e25\u91cd\u8b66\u544a\u5904\u5206\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8498975456370527, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.563482676013778, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5535821240782043}}, {"question": "\u4f9d2005\u5e74\u7684\u300a\u4e2d\u56fd\u56fd\u9645\u7ecf\u6d4e\u8d38\u6613\u4ef2\u88c1\u59d4\u5458\u4f1a\u4ef2\u88c1\u89c4\u5219\u300b\uff0c\u4ef2\u88c1\u88c1\u51b3\u6709\nA. \u7ec8\u5c40\u88c1\u51b3\nB. \u7ec8\u5c40\u88c1\u51b3\u3001\u4e2d\u95f4\u88c1\u51b3\u3001\u90e8\u5206\u88c1\u51b3\nC. \u7ec8\u5c40\u88c1\u51b3\u3001\u9a73\u56de\u4ef2\u88c1\u7533\u8bf7\u7684\u88c1\u51b3\nD. \u7ec8\u5c40\u88c1\u51b3\u3001\u4e2d\u95f4\u88c1\u51b3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5116954005038535, "meta-math/MetaMath-Mistral-7B": 0.5768093237093421, "itpossible/Chinese-Mistral-7B-v0.1": 0.6965837159364825, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5935373245563008, "meta-llama/Meta-Llama-3-8B": 0.47406982006883397, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u518d\u522b\u5eb7\u6865\u300b\u4e2d\uff0c\u201c\u5915\u9633\u4e2d\u7684\u65b0\u5a18\u201d\u2f50\u55bb\u7684\u662f\nA. \u2ec4\u5929\u7684\u4e91\u5f69\nB. \u5eb7\u6cb3\u7684\u67d4\u6ce2\nC. \u8f6f\u6ce5\u4e0a\u7684\u2ed8\u8347\nD. \u6cb3\u7554\u7684\u2fa6\u67f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3661763986529049, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8032615310011809, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.47189919054326046}}, {"question": "\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. $-\\frac{x+y}{3}-2x$\u4e0d\u662f\u6574\u5f0f\nB. $5a^{2}b^{2}$\u7684\u6b21\u6570\u662f5\nC. x\u662f\u5355\u9879\u5f0f\nD. $4xy^{3}+3x^{2}y$\u7684\u6b21\u6570\u662f7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2896633838187122, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7535\u5bb9\u5668\u7684\u7535\u6d41i=C*du/dt\uff0c\u5f53u>0\uff0cdu/dt>0\u65f6\uff0c\u5219\u8868\u660e\u7535\u5bb9\u5668\u6b63\u5728\nA. \u53cd\u65b9\u5411\u5145\u7535\nB. \u53cd\u65b9\u5411\u653e\u7535\nC. \u5145\u7535\nD. \u653e\u7535\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3885978497153787, "meta-math/MetaMath-Mistral-7B": 0.7390730736178928, "itpossible/Chinese-Mistral-7B-v0.1": 0.34239623393788804, "HuggingFaceH4/zephyr-7b-beta": 0.6119741321948607, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6814523621232792, "meta-llama/Meta-Llama-3-8B": 0.35347162922091135, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u201c\u6211\u4e70\u7684\u4e66\u201d\u8fd9\u4e2a\u7ed3\u6784\u4e2d\uff0c\u5171\u5305\u542b\nA. \u4e24\u79cd\u7ed3\u6784\nB. \u56db\u79cd\u7ed3\u6784\nC. \u4e00\u79cd\u7ed3\u6784 \nD. \u4e09\u79cd\u7ed3\u6784\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e24\u53f0\u5e76\u8054\u8fd0\u884c\u7684\u673a\u7ec4\u4e4b\u95f4\u65e0\u529f\u529f\u7387\u7684\u589e\u91cf\u6309\u4e0b\u5217\u54ea\u79cd\u89c4\u5f8b\u5206\u914d\nA. \u4e0e\u673a\u7ec4\u7684\u539f\u6599\u6d88\u8017\u6210\u53cd\u6bd4\nB. \u4e0e\u673a\u7ec4\u7684\u5bb9\u91cf\u6210\u6b63\u6bd4\nC. \u4e0e\u673a\u7ec4\u7684\u65e0\u529f\u7279\u6027\u7684\u8c03\u5dee\u7cfb\u6570\u6210\u53cd\u6bd4\nD. \u4e0e\u673a\u7ec4\u7684\u5bb9\u91cf\u6210\u53cd\u6bd4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41714169488097963, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3436615088034303, "HuggingFaceH4/zephyr-7b-beta": 0.6252977991607325, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.43095011912865144, "meta-llama/Meta-Llama-3-8B": 0.3164901664353555, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7023747532101147}}, {"question": "\u7532\u6570\u76842/3\u7b49\u4e8e\u4e59\u6570\u76843/4\uff0c\u5219\u7532\u6570\uff08\uff09\u4e59\u6570\nA. \u5c0f\u4e8e\nB. \u5927\u4e8e\nC. \u7b49\u4e8e\nD. \u65e0\u6cd5\u786e\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e24\u6c49\u5b9e\u884c\u5dde\u90e1\u63a8\u8350\uff0c\u671d\u5ef7\u8003\u8bd5\u4efb\u7528\u7684\u5bdf\u4e3e\u5236\uff1b\u7ecf\u9b4f\u664b\u4e5d\u54c1\u4e2d\u6b63\u5236\uff0c\u81f3\u968b\u5510\u6f14\u53d8\u4e3a\u81ea\u7531\u6295\u8003\uff0c\u5dee\u989d\u5f55\u7528\u7684\u79d1\u4e3e\u5236\uff0c\u79d1\u4e3e\u5236\u66f4\u6709\u5229\u4e8e\nA. \u63d0\u5347\u793e\u4f1a\u6587\u5316\u6c34\u5e73\nB. \u9274\u522b\u5b98\u5458\u9053\u5fb7\u6c34\u5e73\nC. \u6392\u9664\u4e16\u5bb6\u5b50\u5f1f\u5165\u4ed5\nD. \u9009\u62d4\u6700\u4f18\u79c0\u7684\u5b98\u540f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u7c7b\u8fdb\u4eba 21 \u4e16\u7eaa\uff0c\u82f1\u56fd\u5e7f\u64ad\u516c\u53f8\uff08 BBs \uff09\u5728\u5168\u7403\u8303\u56f4\u5185\u8fdb\u884c\u201c\u5343\u5e74\u601d\u60f3\u5bb6\u201d\u7f51\u8bc4\uff0c\u540d\u5217\u699c\u9996\u7684\u662f\nA. \u725b\u987f\nB. \u8fbe\u5c14\u6587\nC. \u9a6c\u514b\u601d\nD. \u7231\u56e0\u65af\u5766\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7359621217694734, "meta-math/MetaMath-Mistral-7B": 0.9351704547341622, "itpossible/Chinese-Mistral-7B-v0.1": 0.6647959920010039, "HuggingFaceH4/zephyr-7b-beta": 0.9081967663342064, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5025449849459913, "meta-llama/Meta-Llama-3-8B": 0.7687638967728131, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7687175161798957}}, {"question": "\u8bbe $y=y(x)$ \u662f\u4e8c\u9636\u5e38\u7cfb\u6570\u5fae\u5206\u65b9\u7a0b $y^{\\prime \\prime}+p y^{\\prime}+q y=\\mathrm{e}^{3 x}$ \u6ee1\u8db3\u521d\u59cb\u6761\u4ef6 $y(0)=y^{\\prime}(0)=0$ \u7684\u7279\u89e3\uff0c \u5219\u5f53 $x \\rightarrow 0$ \u65f6\uff0c \u51fd\u6570 $\\frac{\\ln \\left(1+x^2\\right)}{y(x)}$ \u7684\u6781\u9650\uff08\uff09.\nA. \u4e0d\u5b58\u5728\nB. \u7b49\u4e8e 2\nC. \u7b49\u4e8e 1\nD. \u7b49\u4e8e 3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u628a0.8\u4ebf\u6539\u5199\u6210\u7528\u201c\u4e07\u201d\u4f5c\u5355\u4f4d\u7684\u6570\u662f\nA. 80000\u4e07\nB. 8000\u4e07\nC. 80000000\u4e07\nD. 0.8\u4e07\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3164509086136633, "meta-math/MetaMath-Mistral-7B": 0.47733964925679356, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5878419105765741, "meta-llama/Meta-Llama-3-8B": 0.3810959124279104, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9175\u6bcd\u6291\u5236\u578b\u5c0f\u83cc\u843d\u7a81\u53d8\u7c7b\u578b\u4e0e\u6b63\u5e38\u83cc\u843d\u6742\u4ea4\u540e\u4ee3\u4e2d\nA. \u5168\u90e8\u4e3a\u6b63\u5e38\u83cc\u843d\nB. \u5168\u90e8\u4e3a\u5c0f\u83cc\u843d\nC. \u7b49\u6bd4\u4f8b\u7684\u6b63\u5e38\u83cc\u843d\u548c\u5c0f\u83cc\u843d\nD. \u4e0d\u5b9a\u6bd4\u4f8b\u7684\u6b63\u5e38\u83cc\u843d\u548c\u5c0f\u83cc\u843d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5556877882451451, "itpossible/Chinese-Mistral-7B-v0.1": 0.40486120275724136, "HuggingFaceH4/zephyr-7b-beta": 0.9383804723228468, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6213512102057225, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u5316\u5986\u54c1\u5236\u9020\u5546\u5c06\u5176\u4ea7\u54c1\u9500\u552e\u7ed9\u6279\u53d1\u5546\uff0c\u6279\u53d1\u5546\u518d\u9500\u552e\u7ed9\u8d85\u7ea7\u5e02\u573a\uff0c\u8be5\u5316\u5986\u54c1\u5236\u9020\u5546\u6240\u91c7\u7528\u6e20\u9053\u7684\u9636\u6570\u662f\nA. \u4e00\u9636\nB. \u4e8c\u9636\nC. \u96f6\u9636\nD. \u4e09\u9636\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.43357605626879403, "meta-math/MetaMath-Mistral-7B": 0.597694820158018, "itpossible/Chinese-Mistral-7B-v0.1": 0.5105908419077619, "HuggingFaceH4/zephyr-7b-beta": 0.5914348907373274, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6561505118446185, "meta-llama/Meta-Llama-3-8B": 0.6184469704732808, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7410187097144011}}, {"question": "\u8bb0\u8d26\u51ed\u8bc1\u8d26\u52a1\u5904\u7406\u7a0b\u5e8f\u7684\u4f18\u70b9\u662f\nA. \u4fbf\u4e8e\u6838\u5bf9\u8d26\u76ee\u548c\u8fdb\u884c\u8bd5\u7b97\u5e73\u8861\nB. \u6709\u5229\u4e8e\u4f1a\u8ba1\u6838\u7b97\u7684\u65e5\u5e38\u5206\u5de5\nC. \u603b\u5206\u7c7b\u8d26\u53cd\u6620\u7ecf\u6d4e\u4e1a\u52a1\u8f83\u8be6\u7ec6\nD. \u51cf\u8f7b\u4e86\u767b\u8bb0\u603b\u5206\u7c7b\u8d26\u7684\u5de5\u4f5c\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u5b9e\u65bd\u8d22\u52a1\u76d1\u7763\u65f6\uff0c\u4ee5\u4f1a\u8ba1\u51ed\u8bc1\u3001\u8d26\u7c3f\u3001\u62a5\u8868\u7b49\u4f1a\u8ba1\u8d44\u6599\u4e3a\u5bf9\u8c61\u8fdb\u884c\u68c0\u67e5\u7684\u65b9\u6cd5\u88ab\u79f0\u4e3a\nA. \u73b0\u573a\u8c03\u67e5\nB. \u8d22\u52a1\u5206\u6790\nC. \u8d22\u52a1\u68c0\u67e5\nD. \u5b9e\u7269\u68c0\u67e5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8941174027113005, "meta-math/MetaMath-Mistral-7B": 0.9944616145795504, "itpossible/Chinese-Mistral-7B-v0.1": 0.9240536439336591, "HuggingFaceH4/zephyr-7b-beta": 0.9990374880260315, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8996454873430885, "meta-llama/Meta-Llama-3-8B": 0.5751870025025523, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7051407497764441}}, {"question": "\u4e0b\u5217\u6709\u5173\u4fa6\u5bdf\u4e0e\u76d1\u89c6\u6280\u672f\u7684\u53d1\u5c55\u8d8b\u52bf\u4e0d\u5bf9\u7684\u7684\u4e00\u9879\u662f\nA. \u64cd\u4f5c\u4e0a\u7684\u6a21\u5f0f\u5316\nB. \u624b\u6bb5\u4e0a\u7684\u7efc\u5408\u5316\nC. \u901f\u5ea6\u4e0a\u7684\u5b9e\u65f6\u5316\nD. \u7a7a\u95f4\u4e0a\u7684\u7acb\u4f53\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.36694472144562074, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.371068906204966}}, {"question": "\u8d44\u4ea7\u9636\u7ea7\u65b0\u95fb\u81ea\u7531\u601d\u60f3\u7684\u7406\u8bba\u57fa\u7840\u662f\nA. \u5929\u8d4b\u4eba\u6743\u8bf4\nB. \u4eba\u9053\u4e3b\u4e49\nC. \u672c\u80fd\u8bf4\nD. \u4eba\u672c\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.696362635488482, "meta-math/MetaMath-Mistral-7B": 0.9090797292079228, "itpossible/Chinese-Mistral-7B-v0.1": 0.44827807575923106, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8820142034941768, "meta-llama/Meta-Llama-3-8B": 0.7497662732324736, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.48075358919298516}}, {"question": "\u4e0b\u5217\u6d3b\u52a8\u7528\u4e0d\u5230\u591a\u5a92\u4f53\u6280\u672f\u7684\u662f\nA. \u7f51\u4e0a\u641c\u7d22\u8d44\u6599\nB. \u89c6\u9891\u4f1a\u8bae\u7cfb\u7edf\nC. \u8fdc\u7a0b\u6559\u5b66\u7cfb\u7edf\nD. \u7535\u89c6\u5b9e\u51b5\u4f20\u64ad\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41050578355368295, "meta-math/MetaMath-Mistral-7B": 0.7958993881279712, "itpossible/Chinese-Mistral-7B-v0.1": 0.3941848058979588, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7292951343758534, "meta-llama/Meta-Llama-3-8B": 0.4551316031872074, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u63d0\u51fa\u201c\u9700\u6c42\u5c42\u6b21\u8bba\u201d\u7684\u4eba\u672c\u4e3b\u4e49\u5fc3\u7406\u5b66\u5bb6\u662f\nA. \u7f57\u6d1b\u6885\nB. \u7f57\u6770\u65af\nC. \u5f17\u5170\u514b\nD. \u9a6c\u65af\u6d1b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9750467689506441, "meta-math/MetaMath-Mistral-7B": 0.9976423757200669, "itpossible/Chinese-Mistral-7B-v0.1": 0.9702713268577944, "HuggingFaceH4/zephyr-7b-beta": 0.9999898662389755, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9872120396501437, "meta-llama/Meta-Llama-3-8B": 0.9908170140131141, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9969045501123416}}, {"question": "\u76f8\u90bb\u8425\u517b\u7ea7\u4e4b\u95f4\u7684\u80fd\u91cf\u8f6c\u5316\u6548\u7387\u5927\u7ea6\u662f\nA. 30%\nB. 1/5\nC. 1/10\nD. 1/20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37742802246227186, "meta-math/MetaMath-Mistral-7B": 0.44350412064414846, "itpossible/Chinese-Mistral-7B-v0.1": 0.3088936322941584, "HuggingFaceH4/zephyr-7b-beta": 0.8568151501029034, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47776354226150874, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e3a\u4e86\u7f29\u77ed\u6307\u4ee4\u4e2d\u67d0\u4e2a\u5730\u5740\u6bb5\u7684\u4f4d\u6570\uff0c\u6709\u6548\u7684\u2f45\u6cd5\u662f\u91c7\u53d6\nA. \u5bc4\u5b58\u5668\u5bfb\u5740\nB. \u2f74\u5373\u5bfb\u5740\nC. \u95f4\u63a5\u5bfb\u5740\nD. \u53d8\u5740\u5bfb\u5740\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.33490177722883935, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4967046426016897, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8106\u6027X\u67d3\u8272\u4f53\u662f\u7531\u4e8e\u67d3\u8272\u4f53\u4e0a\u7684FMR1\u57fa\u56e0\u51fa\u73b0\u8fc7\u91cf\u7684CGG//GCC\u91cd\u590d\u5e8f\u5217\uff0c\u5bfc\u81f4DNA\u4e0e\u86cb\u767d\u8d28\u7ed3\u5408\u5f02\u5e38\uff0c\u4ece\u800c\u51fa\u73b0\u201c\u7f22\u6c9f\u201d\uff0c\u67d3\u8272\u4f53\u6613\u4e8e\u4ece\u201c\u7f22\u6c9f\u201d\u5904\u65ad\u88c2\u3002\u4e0b\u5217\u5206\u6790\u9519\u8bef\u7684\u662f\nA. \u7537\u6027\u4e0e\u5973\u6027\u4f53\u7ec6\u80de\u4e2d\u51fa\u73b0X\u67d3\u8272\u4f53\u201c\u7f22\u6c9f\u201d\u7684\u6982\u7387\u4e0d\u540c\nB. \u8106\u6027X\u67d3\u8272\u4f53\u51fa\u73b0\u7684\u6839\u672c\u539f\u56e0\u662f\u57fa\u56e0\u7a81\u53d8\nC. \u8106\u6027X\u67d3\u8272\u4f53\u66f4\u6613\u53d1\u751f\u67d3\u8272\u4f53\u7684\u7ed3\u6784\u53d8\u5f02\nD. \u7531\u4e8e\u5b58\u5728\u8f83\u591a\u7684GC\u91cd\u590d\u5e8f\u5217\uff0c\u8106\u6027X\u67d3\u8272\u4f53\u7ed3\u6784\u66f4\u7a33\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7413594188534195, "meta-math/MetaMath-Mistral-7B": 0.8375486433106744, "itpossible/Chinese-Mistral-7B-v0.1": 0.7143252117466141, "HuggingFaceH4/zephyr-7b-beta": 0.9885936083941091, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9055250417304759, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7398567727568421}}, {"question": "\u5982\u679c\u4e00\u4e2a\u4e09\u89d2\u5f62\u7684\u4e24\u4e2a\u5185\u89d2\u5ea6\u6570\u7684\u548c\u7b49\u4e8e\u7b2c\u4e09\u4e2a\u5185\u89d2\u7684\u5ea6\u6570\uff0c\u90a3\u4e48\u8fd9\u4e2a\u4e09\u89d2\u5f62\u662f\nA. \u9510\u89d2\u4e09\u89d2\u5f62\nB. \u65e0\u6cd5\u5224\u65ad\nC. \u76f4\u89d2\u4e09\u89d2\u5f62\nD. \u949d\u89d2\u4e09\u89d2\u5f62\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3626062801770275, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.333183235354062, "HuggingFaceH4/zephyr-7b-beta": 0.6824601402028627, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4252043365499811, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3543879727206675}}, {"question": "\u4ee5\u4e0b\u4eba\u9020\u5929\u4f53\u4e2d\u8f68\u9053\u9ad8\u5ea6\u6700\u9ad8\u7684\u662f\nA. \u5730\u7403\u540c\u6b65\u8f68\u9053\u536b\u661f\nB. \u592a\u9633\u540c\u6b65\u8f68\u9053\u536b\u661f\nC. \u56fd\u9645\u7a7a\u95f4\u7ad9\nD. \u54c8\u52c3\u7a7a\u95f4\u671b\u8fdc\u955c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3395893495876965, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6cd5\u5f8b\u89c4\u5219\u7684\u8bba\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u6211\u56fd\u5baa\u6cd5\u89c4\u5b9a\uff1a\u7981\u6b62\u975e\u6cd5\u62d8\u7981\u548c\u4ee5\u5176\u4ed6\u65b9\u6cd5\u975e\u6cd5\u5265\u593a\u6216\u8005\u9650\u5236\u516c\u6c11\u7684\u4eba\u8eab\u81ea\u7531\uff0c\u7981\u6b62\u975e\u6cd5\u641c\u67e5\u516c\u6c11\u7684\u8eab\u4f53\u3002\u8be5\u89c4\u5219\u5c5e\u4e8e\u547d\u4ee4\u6027\u89c4\u5219\nB. \u6211\u56fd\u7acb\u6cd5\u6cd5\u89c4\u5b9a\uff1a\u5168\u56fd\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u901a\u8fc7\u7684\u6cd5\u5f8b\u7531\u56fd\u5bb6\u4e3b\u5e2d\u7b7e\u7f72\u4e3b\u5e2d\u4ee4\u4e88\u4ee5\u516c\u5e03\u3002\u8be5\u89c4\u5219\u5c5e\u4e8e\u51c6\u7528\u6027\u89c4\u5219\nC. \u6cd5\u5f8b\u89c4\u5219\u53ef\u5206\u4e3a\u516c\u7406\u6027\u89c4\u5219\u548c\u653f\u7b56\u6027\u89c4\u5219\u3002\u516c\u7406\u6027\u89c4\u5219\u7531\u6cd5\u5f8b\u539f\u7406\u6784\u6210\uff0c\u800c\u653f\u7b56\u6027\u89c4\u5219\u5219\u662f\u57fa\u4e8e\u4e00\u5b9a\u7684\u653f\u7b56\u8003\u91cf\u5236\u5b9a\nD. \u6211\u56fd\u5f8b\u5e08\u6cd5\u89c4\u5b9a\uff1a\u6cd5\u5f8b\u63f4\u52a9\u7684\u5177\u4f53\u529e\u6cd5\uff0c\u7531\u56fd\u52a1\u9662\u53f8\u6cd5\u884c\u653f\u90e8\u95e8\u5236\u5b9a\uff0c\u62a5\u56fd\u52a1\u9662\u6279\u51c6\u3002\u8be5\u89c4\u5219\u5c5e\u4e8e\u59d4\u4efb\u6027\u89c4\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1989\u5e74\u4ee5\u6765\uff0c\u4e2d\u56fd\u5148\u540e\u53c2\u52a0\u4e8622\u9879\u7ef4\u548c\u884c\u52a8\uff0c\u7d2f\u8ba1\u6d3e\u51fa\u7ef4\u548c\u5b98\u517520000\u591a\u4eba\u6b21\uff0c\u5176\u4e2d\uff0c\u67093\u540d\u519b\u4e8b\u89c2\u5bdf\u5458\u548c6\u540d\u7ef4\u548c\u58eb\u5175\u5728\u6267\u884c\u4efb\u52a1\u65f6\u82f1\u52c7\u727a\u7272\u3002\u8fd9\u8bf4\u660e\u5f53\u524d\u6211\u56fd\nA. \u79ef\u6781\u53c2\u52a0\u4ee5\u8054\u5408\u56fd\u4e3a\u4e2d\u5fc3\u7684\u591a\u8fb9\u5916\u4ea4\nB. \u79ef\u6781\u53c2\u4e0e\u5236\u5b9a\u56fd\u9645\u653f\u6cbb\u7ecf\u6d4e\u79e9\u5e8f\u89c4\u5219\nC. \u5efa\u7acb\u65b0\u578b\u533a\u57df\u5408\u4f5c\u7ec4\u7ec7\nD. \u5df2\u6210\u4e3a\u4e16\u754c\u653f\u6cbb\u683c\u5c40\u591a\u6781\u5316\u4e2d\u7684\u4e00\u6781\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6635567757659967, "meta-math/MetaMath-Mistral-7B": 0.9377971058110611, "itpossible/Chinese-Mistral-7B-v0.1": 0.7336078414521197, "HuggingFaceH4/zephyr-7b-beta": 0.7968745793040931, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8971306280536386, "meta-llama/Meta-Llama-3-8B": 0.7961421929653374, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9251936766728855}}, {"question": "\u201c\u52a8\u4ea7\u7ee7\u627f\u9002\u7528\u88ab\u7ee7\u627f\u4eba\u4f4f\u6240\u5730\u6cd5\u201d\uff0c\u8fd9\u6761\u51b2\u7a81\u89c4\u8303\u4e2d\u7684\u201c\u4f4f\u6240\u5730\u201d\u5728\u56fd\u9645\u53f8\u6cd5\u4e0a\u79f0\u4f5c\nA. \u51c6\u636e\u6cd5\nB. \u8fde\u63a5\u70b9\nC. \u6307\u5b9a\u539f\u56e0\nD. \u8303\u56f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.681750400848062, "meta-math/MetaMath-Mistral-7B": 0.9609798665303102, "itpossible/Chinese-Mistral-7B-v0.1": 0.5379982286652013, "HuggingFaceH4/zephyr-7b-beta": 0.9854277194449147, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.981217179128252, "meta-llama/Meta-Llama-3-8B": 0.5486066189172235, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u673a\u4f53\u5404\u810f\u8151\u7ec4\u7ec7\u8d77\u7740\u63a8\u52a8\u6e29\u7166\u4f5c\u7528\u7684\u4e3b\u8981\u662f\nA. \u813e\u6c14\nB. \u80be\u9633\nC. \u5fc3\u9633\nD. \u80ba\u6c14\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28595819752074036, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b0\u4e2d\u56fd\u6210\u7acb\u65f6\uff0c\u4e16\u754c\u5df2\u8fdb\u5165\u6838\u65f6\u4ee3\u3002\u66fe\u957f\u671f\u9886\u5bfc\u6838\u79d1\u5b66\u4e8b\u4e1a\u7684\u8042\u8363\u81fb\u5143\u5e05\u56de\u5fc6\u9053\uff1a\u201c\u6211\u4eec\u56fd\u5bb6\uff0c\u4e0d\u53ef\u80fd\u9760\u8d2d\u4e70\u6b66\u5668\u6765\u652f\u6491\u56fd\u9632\u2026\u2026\u552f\u4e00\u7684\u51fa\u8def\u53ea\u6709\u5c3d\u53ef\u80fd\u5438\u53d6\u56fd\u5916\u5148\u8fdb\u6210\u679c\uff0c\u8d70\u81ea\u5df1\u7814\u5236\u7684\u9053\u8def\u3002\u201d\u4e2d\u56fd\u8de8\u5165\u539f\u5b50\u80fd\u65f6\u4ee3\u5f00\u59cb\u4e8e\nA. 20\u4e16\u7eaa50\u5e74\u4ee3\nB. 20\u4e16\u7eaa60\u5e74\u4ee3\nC. 20\u4e16\u7eaa70\u5e74\u4ee3\nD. 20\u4e16\u7eaa40\u5e74\u4ee3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.325455072595945, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.40322669517609055, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u7684\u4eba\u9047\u4e8b\u603b\u662f\u4e3e\u68cb\u4e0d\u5b9a\uff0c\u4f18\u67d4\u5be1\u65ad\u3002\u8fd9\u8bf4\u660e\u4ed6\u4eec\u7684\u610f\u5fd7\u7f3a\u4e4f\nA. \u679c\u65ad\u6027\nB. \u81ea\u89c9\u6027\nC. \u575a\u6301\u6027\nD. \u81ea\u5236\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39167922293690677, "meta-math/MetaMath-Mistral-7B": 0.7804888156519003, "itpossible/Chinese-Mistral-7B-v0.1": 0.39325343687947123, "HuggingFaceH4/zephyr-7b-beta": 0.4878776333651383, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5096420131943611, "meta-llama/Meta-Llama-3-8B": 0.5140406412171048, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9973433943740333}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\uff08\uff09\u4e0d\u5c5e\u4e8e\u6211\u56fd\u793e\u4f1a\u4e3b\u4e49\u6cd5\u7684\u6b63\u5f0f\u610f\u4e49\u4e0a\u7684\u6e0a\u6e90\u3002\nA. \u56fd\u9645\u6761\u7ea6\nB. \u7279\u522b\u884c\u653f\u533a\u6cd5\u5f8b\nC. \u515a\u7684\u653f\u7b56\nD. \u519b\u4e8b\u6cd5\u89c4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6281558915090725}}, {"question": "\u9752\u6625\u671f\u7684\u53d1\u5c55\u4e2d\u4e00\u4e2a\u7a81\u51fa\u7684\u7279\u5f81\u6216\u8005\u8bf4\u662f\u7f3a\u9677\u5728\u4e8e\nA. \u5fc3\u7406\u7684\u53d1\u5c55\u901f\u5ea6\u5927\u5927\u8d85\u8fc7\u4e86\u751f\u7406\u548c\u793e\u4f1a\u7684\u6210\u719f\u5ea6\nB. \u751f\u7406\u7684\u53d1\u5c55\u901f\u5ea6\u5927\u5927\u8d85\u8fc7\u4e86\u5fc3\u7406\u548c\u793e\u4f1a\u7684\u6210\u719f\u5ea6\nC. \u793e\u4f1a\u7684\u6210\u719f\u5ea6\u5927\u5927\u8d85\u8fc7\u4e86\u751f\u7406\u548c\u5fc3\u7406\u7684\u53d1\u5c55\u901f\u5ea6\nD. \u751f\u7406\u3001\u5fc3\u7406\u548c\u793e\u4f1a\u53d1\u5c55\u540c\u6b65\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7022274760138737}}, {"question": "\u4ee5\u4e0b\u4e0d\u5c5e\u4e8e\u5171\u540c\u7684\u8bba\u8bc1\u8bc4\u4ef7\u6807\u51c6\u7684\u662f\nA. \u5206\u6790\u6807\u51c6\nB. \u8bba\u8fa9\u6807\u51c6\nC. \u903b\u8f91\u6807\u51c6\nD. \u4fee\u8f9e\u6807\u51c6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.44797806334939255, "itpossible/Chinese-Mistral-7B-v0.1": 0.37354501646787847, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.39241039047576015, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u54ea\u4e2a\u4e0d\u662f\u5370\u5237\u5a92\u4ecb\u7684\u7279\u70b9\nA. \u957f\u671f\u4fdd\u5b58\nB. \u7a0d\u7eb5\u5373\u901d\nC. \u8fc5\u901f\u5927\u91cf\u751f\u4ea7\nD. \u5bb9\u7eb3\u4fe1\u606f\u591a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7121347909888686, "meta-math/MetaMath-Mistral-7B": 0.9568517976789043, "itpossible/Chinese-Mistral-7B-v0.1": 0.310313193127302, "HuggingFaceH4/zephyr-7b-beta": 0.969972789600446, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9416280428336501, "meta-llama/Meta-Llama-3-8B": 0.4725527910764746, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9370634014910547}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u690e\u95f4\u5b54\u7684\u6b63\u786e\u53d9\u8ff0\u662f\nA. \u662f\u76f8\u90bb\u690e\u9aa8\u7684\u4e0a\u3001\u4e0b\u5207\u8ff9\u56f4\u6210\u7684\u5b54\nB. \u662f\u690e\u5f13\u548c\u690e\u4f53\u56f4\u6210\u7684\u5b54\nC. \u662f\u76f8\u90bb\u7684\u690e\u5f13\u56f4\u6210\u7684\u5b54\nD. \u5bb9\u7eb3\u810a\u9ad3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7184974345090873, "meta-math/MetaMath-Mistral-7B": 0.8401133603986994, "itpossible/Chinese-Mistral-7B-v0.1": 0.6024170715000717, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.501115075717943, "meta-llama/Meta-Llama-3-8B": 0.37047587865663084, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9333327451039333}}, {"question": "\u7532\u4e59\u5206\u522b\u4e3a\u56fd\u6709\u516c\u53f8\u603b\u7ecf\u7406\u548c\u526f\u603b\u7ecf\u7406\u4e8c\u4eba\u632a\u7528\u5355\u4f4d 100 \u4e07\u5143\u516c\u6b3e\u7ed9\u5f20\u4e09\u4ece\u4e8b\u671f\u8d27\u4ea4\u6613\u6536\u53d7\u5e76\u5e73\u5206\u4e86\u5f20\u4e0915\u4e07\u5143\u56de\u6263\uff0c\u5e76\u63ed\u53d1\u4e59\u4e5f\u6536\u53d7\u4e8615 \u4e07\u5143\u56de\u6263\uff0c\u672c\u6848\u4e2d\nA. \u7532\u5728\u53d7\u8d3f\u7f6a\u4e0a\u6210\u7acb\u81ea\u9996\nB. \u7532\u5728\u53d7\u8d3f\u7f6a\u884c\u4e0a\u6210\u7acb\u81ea\u9996\u548c\u7acb\u529f\nC. \u7532\u5728\u632a\u7528\u516c\u6b3e\u548c\u53d7\u8d3f\u7f6a\u4e0a\u5747\u6210\u7acb\u81ea\u9996\u548c\u7acb\u529f\nD. \u7532\u5728\u632a\u7528\u516c\u6b3e\u4e0a\u6210\u7acb\u7acb\u529f\u5728\u53d7\u8d3f\u7f6a\u4e0a\u6210\u7acb\u81ea\u9996\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.609004679819649, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.31283638571410965, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u73b0\u8c61\u53d1\u2f63\u7684\u8fc7\u7a0b\u4e2d\uff0c\u653e\u51fa\u70ed\u91cf\u7684\u2f00\u7ec4\u662f A\u51b0\u96ea\u6d88\u878d B\u79ef\u2f54\u2f32\u6db8 C\u6ef4\u2f54\u6210\u51b0 D\u971c\u6ee1\u679d\u5934\nA. AB\nB. BD\nC. AC\nD. CD\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u670d\u7528\u8865\u76ca\u836f\u671f\u95f4\u5076\u9047\u5916\u611f\uff0c\u4e3a\u9632\u201c\u95ed\u95e8\u7559\u5bc7\u201d\uff0c\u6700\u597d\nA. \u589e\u52a0\u670d\u836f\u6b21\u6570\nB. \u505c\u670d\nC. \u51cf\u8f7b\u836f\u91cf\nD. \u51cf\u5c11\u670d\u836f\u6b21\u6570\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4141910113721942, "meta-math/MetaMath-Mistral-7B": 0.5731662979611334, "itpossible/Chinese-Mistral-7B-v0.1": 0.55930872732656, "HuggingFaceH4/zephyr-7b-beta": 0.9221109746117793, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.715451517045689, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ed9\u5b9a\u751f\u5b58\u5206\u5e03\u51fd\u6570\u4e3a:$S(x)=\\frac{80-x}{80}\uff0c0 \\leqslant x \\leqslant 80$\uff0c\u5219${ }_6 m_{20}=( )$\u3002\nA. 1/54\nB. 1/52\nC. 1/57\nD. 1/59\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28937433699268383, "meta-math/MetaMath-Mistral-7B": 0.29660173325630934, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.41270331149012474, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6b63\u5348\u65f6\uff0c\u5730\u7403\u4e0a\u5782\u76f4\u7684\u7269\u4f53\u59cb\u7ec8\u6709\u5f71\u2f26\u4e14\u5176\u5f71\u2f26\u53ea\u671d\u5357\u7684\u5730\u533a\u5728\nA. \u2f9a\u9053\u4ee5\u5357\u7684\u5730\u533a\nB. \u5317\u56de\u5f52\u7ebf\u4ee5\u5317\u7684\u5730\u533a\nC. \u5357\u56de\u5f52\u7ebf\u4ee5\u5357\u2f84\u5357\u6781\u5708\u4ee5\u5317\u4e4b\u95f4\u7684\u5730\u533a\nD. \u5317\u56de\u5f52\u7ebf\u4ee5\u5317\u2f84\u5317\u6781\u5708\u4ee5\u5357\u4e4b\u95f4\u7684\u5730\u533a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.544637155799303, "meta-math/MetaMath-Mistral-7B": 0.9066200996200503, "itpossible/Chinese-Mistral-7B-v0.1": 0.31283638571410965, "HuggingFaceH4/zephyr-7b-beta": 0.963636938128395, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7583248094494377, "meta-llama/Meta-Llama-3-8B": 0.3818183045785905, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.383353196415313}}, {"question": "\u6559\u80b2\u7684\u5fc3\u7406\u8d77\u6e90\u8bf4\u7684\u4ee3\u8868\u4eba\u7269\u662f\u7f8e\u56fd\u6559\u80b2\u5bb6\u5b5f\u7984\uff0c\u4ed6\u8ba4\u4e3a ()\nA. \u4f20\u7edf\u6559\u80b2\u7684\u65b9\u5f0f\u4e3b\u8981\u662f\u65e5\u5e38\u751f\u6d3b\u4e2d\u513f\u7ae5\u5bf9\u6210\u4eba\u7684\u65e0\u610f\u6a21\u4eff\nB. \u4eba\u7c7b\u6559\u80b2\u8d77\u6e90\u4e8e\u52b3\u52a8\u6216\u52b3\u52a8\u8fc7\u7a0b\u4e2d\u6240\u4ea7\u751f\u7684\u9700\u8981\nC. \u6559\u80b2\u4e0e\u5176\u4ed6\u4e07\u4e8b\u4e07\u7269\u4e00\u6837\uff0c\u90fd\u662f\u7531\u4eba\u683c\u5316\u7684\u795e\u521b\u9020\u7684\nD. \u6559\u80b2\u6d3b\u52a8\u4e0d\u4ec5\u5b58\u5728\u4e8e\u4eba\u7c7b\u793e\u4f1a\u4e4b\u4e2d\uff0c\u800c\u4e14\u5b58\u5728\u4e8e\u4eba\u7c7b\u793e\u4f1a\u4e4b\u5916\uff0c\u751a\u81f3\u5b58\u5728\u4e8e\u52a8\u7269\u754c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9992017240339992, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6038207619032859, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6613527675446964}}, {"question": "\u4e00\u4e2a\u75319\u4eba\u7ec4\u6210\u7684\u51b3\u7b56\u7fa4\u4f53\u5bf9A\u3001B\u3001C\u65b9\u6848\u8fdb\u884c\u6295\u7968\uff0c\u7ed3\u679c\u4e3a\uff1aA\u65b9\u68484\u7968\uff0cB\u65b9\u68483\u7968\uff0cC\u65b9\u68482\u7968\uff0c\u6700\u540eA\u65b9\u6848\u80dc\u51fa\u3002\u8fd9\u8bf4\u660e\u8be5\u51b3\u7b56\u7fa4\u4f53\u91c7\u7528\u7684\u662f\nA. \u5168\u4f53\u4e00\u81f4\u89c4\u5219\nB. \u7edd\u5bf9\u591a\u6570\u89c4\u5219\nC. \u7b80\u5355\u591a\u6570\u89c4\u5219\nD. \u8fc7\u534a\u6570\u89c4\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4354979592796837, "meta-math/MetaMath-Mistral-7B": 0.7676778137554507, "itpossible/Chinese-Mistral-7B-v0.1": 0.3749196223738778, "HuggingFaceH4/zephyr-7b-beta": 0.9039385471681446, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.563482676013778, "meta-llama/Meta-Llama-3-8B": 0.49670465162226823, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5005757148705979}}, {"question": "\u5c0f\u8bf4\u300a\u5728\u5176\u9999\u5c45\u8336\u9986\u91cc\u300b\u4e2d\uff0c\u90a2\u5e7a\u5435\u5435\u4e0e\u65b9\u6cbb\u56fd\u4e89\u5435\u76f8\u9a82\u66b4\u9732\u51fa\u6765\u7684\u4e3b\u8981\u793e\u4f1a\u95ee\u9898\u662f\nA. \u8d4b\u7a0e\u5236\u7684\u8150\u8d25\nB. \u5b97\u6cd5\u5236\u5ea6\u7684\u4e0d\u5408\u7406\nC. \u5175\u5f79\u7fce\u7684\u8150\u8d25\nD. \u5a5a\u59fb\u5236\u5ea6\u7684\u4e0d\u5408\u7406\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe\u5929\u9876\u8ddd\u4e3az\uff0c\u5728\u8fdc\u79bb\u5730\u5e73\u5708\u7684\u5929\u533a\uff0c\u5927\u6c14\u6d88\u5149\u91cf\u8fd1\u4f3c\u6b63\u6bd4\u4e8e\nA. cosz\nB. 1/cosz\nC. 1/tanz\nD. sinz\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3525272826242582, "meta-math/MetaMath-Mistral-7B": 0.4065058740298266, "itpossible/Chinese-Mistral-7B-v0.1": 0.4249498721138753, "HuggingFaceH4/zephyr-7b-beta": 0.6163611534969456, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4345819400480277, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u590f\u5929\u665a\u4e0a\u6606\u866b\u5411\u706f\u5149\u7fa4\u96c6\u8fd9\u662f\u4ec0\u4e48\u5f15\u8d77\u7684?\nA. \u7269\u6d41\nB. \u4fe1\u606f\u6d41\nC. \u4ef7\u503c\u6d41\nD. \u80fd\u6d41\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4540363998471267, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4460818980111562}}, {"question": "\u4eba\u662f\u4f17\u56e0\u7f18\u548c\u5408\u800c\u751f\uff0c\u6545\u540d\nA. \u4eba\u751f\nB. \u65c1\u751f\nC. \u4f17\u751f\nD. \u5316\u751f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6308803751577879, "meta-math/MetaMath-Mistral-7B": 0.9504829633895076, "itpossible/Chinese-Mistral-7B-v0.1": 0.39196260835213925, "HuggingFaceH4/zephyr-7b-beta": 0.9922501184182518, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6505629222006098, "meta-llama/Meta-Llama-3-8B": 0.7696050288478636, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9961988497612747}}, {"question": "\u4e0b\u5217\u76f8\u5f53\u4e8e\u8d38\u6613\u6298\u6263\u7684\u662f\nA. \u4fc3\u9500\u6298\u6263\nB. \u6570\u91cf\u6298\u6263\nC. \u73b0\u91d1\u6298\u6263\nD. \u529f\u80fd\u6298\u6263\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6d88\u8d39\u8005\u6536\u5165\u53d8\u5316\u5bf9\u6d88\u8d39\u8005\u9884\u7b97\u7ea6\u675f\u7ebf\u7684\u5f71\u54cd\u662f\nA. \u4f7f\u9884\u7b97\u7ea6\u675f\u7ebf\u4fdd\u6301\u4e0d\u53d8\nB. \u4f7f\u9884\u7b97\u7ea6\u675f\u7ebf\u53d1\u751f\u65cb\u8f6c\nC. \u4f7f\u9884\u7b97\u7ea6\u675f\u7ebf\u659c\u7387\u6539\u53d8\nD. \u4f7f\u9884\u7b97\u7ea6\u675f\u7ebf\u53d1\u751f\u5e73\u79fb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7615135637327459, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7936986653596346, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7518\u6cb9-3-\u78f7\u9178\u7a7f\u68ad\u673a\u5236\u4e2d\uff0c 3-\u78f7\u9178\u7518\u6cb9\u8131\u6c22\u9176\u5728\u7ebf\u7c92\u4f53\u4e2d\u7684\u8f85\u57fa\u662f\nA. NAD+\nB. FMN\nC. NADP+\nD. FAD\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.381902052120576, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4e8e\u7ed3\u5408\u9176\u6765\u8bf4\uff0c\u51b3\u5b9a\u53cd\u5e94\u7279\u5f02\u6027\u7684\u662f\nA. \u8f85\u9176\nB. \u5168\u9176\nC. \u9176\u86cb\u767d\nD. \u91d1\u5c5e\u79bb\u5b50\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36692135433291434, "meta-math/MetaMath-Mistral-7B": 0.5667617383545978, "itpossible/Chinese-Mistral-7B-v0.1": 0.3883451836116358, "HuggingFaceH4/zephyr-7b-beta": 0.5120091574672435, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6235091628389408, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e2d\u533b\u89c2\u5bdf\u820c\u8c61\u7684\u76ee\u7684\u4e2d\u54ea\u9879\u4e0d\u5bf9\nA. \u4e86\u89e3\u75c5\u4f4d\u6df1\u6d45\nB. \u5224\u65ad\u6b63\u6c14\u76db\u8870\nC. \u8fa8\u522b\u75c5\u4f4d\u6df1\u6d45\nD. \u533a\u522b\u75c5\u90aa\u6027\u8d28\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.36253010780209793, "itpossible/Chinese-Mistral-7B-v0.1": 0.2885095257630687, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.44653093309784625}}, {"question": "\u82e5A\u4e0eB\u76f8\u4f3c\uff0c\u5219\nA. |A|=|B|\nB. A\uff0cB\u90fd\u548c\u540c\u4e00\u5bf9\u89d2\u77e9\u9635\u76f8\u4f3c\nC. A\uff0cB\u90fd\u6709\u76f8\u540c\u7684\u7279\u5f81\u5411\u91cf\nD. $A-\\lambda E=B-\\lambda E$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6162\u6027\u80ba\u5fc3\u75c5\u5fc3\u529f\u80fd\u4ee3\u507f\u671f\u9664\u4e86\u6709COPD\u7684\u4e34\u5e8a\u8868\u73b0\u5916\uff0c\u8fd8\u53ef\u6709\u7684\u4f53\u5f81\u662f\nA. \u4e09\u5c16\u74e3\u533a\u8212\u5f20\u671f\u6742\u97f3\nB. \u8179\u8154\u79ef\u6db2\nC. \u5251\u7a81\u4e0b\u5fc3\u810f\u640f\u52a8\u589e\u5f3a\nD. \u809d\u9888\u9759\u8109\u56de\u6d41\u5f81\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2656046866868781, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5016048901030241, "HuggingFaceH4/zephyr-7b-beta": 0.5612835042362446, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9879\u76ee\u7ae3\u5de5\u9a8c\u6536\u524d\uff0c\u65bd\u5de5\u4f01\u4e1a\u6309\u7167\u5408\u540c\u7ea6\u5b9a\u5bf9\u5df2\u5b8c\u6210\u5de5\u7a0b\u548c\u8bbe\u5907\u91c7\u53d6\u5fc5\u8981\u7684\u4fdd\u62a4\u63aa\u65bd\u6240\u53d1\u751f\u7684\u8d39\u7528\u5e94\u8ba1\u5165\nA. \u63aa\u65bd\u9879\u76ee\u8d39\nB. \u5176\u4ed6\u9879\u76ee\u8d39\nC. \u4f01\u4e1a\u7ba1\u7406\u8d39\nD. \u603b\u627f\u5305\u7ba1\u7406\u8d39\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33284604038304694, "meta-math/MetaMath-Mistral-7B": 0.4215367297385213, "itpossible/Chinese-Mistral-7B-v0.1": 0.3881207341645941, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3534716292209113, "meta-llama/Meta-Llama-3-8B": 0.44910846435799384, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u4f01\u4e1a\u5168\u5e74\u8425\u4e1a\u6536\u5165\u4e3a140 000\u5143\uff0c\u6d41\u52a8\u8d44\u4ea7\u5e73\u5747\u5360\u7528\u989d\u4e3a70 000\u5143\uff0c\u82e5\u4e00\u5e74\u6309360\u5929\u8ba1\u7b97\uff0c\u5219\u8be5\u4f01\u4e1a\u6d41\u52a8\u8d44\u4ea7\u5468\u8f6c\u5929\u6570\u4e3a\nA. 180\u5929\nB. 60\u5929\nC. 30\u5929\nD. 90\u5929\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.2885095257630687, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6828997532815543, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u524d\u6444\u6291\u5236\u4e0e\u5012\u6444\u6291\u5236\u8bc1\u5b9e\u4e86\u9057\u5fd8\u7684\u3002\nA. \u75d5\u8ff9\u8870\u9000\u8bf4\nB. \u52a8\u673a\u8bf4\nC. \u63d0\u53d6\u5931\u8d25\u8bf4\nD. \u5e72\u6270\u8bf4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3469506857495183, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u73b0\u4ee3\u4e3b\u4e49\u6587\u5b66\u601d\u6f6e\u4e2d\u7684\u7b2c\u4e00\u4e2a\u6587\u5b66\u6d41\u6d3e\u662f\nA. \u8fbe\u8fbe\u4e3b\u4e49\nB. \u9ed1\u8272\u5e7d\u9ed8\nC. \u8868\u73b0\u4e3b\u4e49\nD. \u8c61\u5f81\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53e4\u5e0c\u814a\u7684\u96c5\u5178\u6559\u80b2\u6bd4\u8f83\u91cd\u89c6\u5bf9\u5e74\u8f7b\u4e00\u4ee3\u8fdb\u884c\nA. \u519b\u4e8b\u4f53\u64cd\u6559\u80b2\nB. \u5929\u6587\u6570\u5b66\u6559\u80b2\nC. \u591a\u65b9\u9762\u53d1\u5c55\u6559\u80b2\nD. \u653f\u6cbb\u54f2\u5b66\u6559\u80b2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5226927731913321, "meta-math/MetaMath-Mistral-7B": 0.7559102424196683, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9027999196423898, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4125156818135334, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u54f2\u5b66\u201d\u4e00\u8bcd\u662f\u9ec4\u5b97\u5baa\u4ece\u54ea\u91cc\u5f15\u4ecb\u5230\u4e2d\u56fd\u5b66\u672f\u754c\nA. \u7f8e\u56fd\nB. \u82f1\u56fd\nC. \u65e5\u672c\nD. \u5fb7\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.40650589978597995}}, {"question": "\u90b1\u8001\u5e08\u5728\u5de5\u4f5c\u65e5\u5fd7\u4e2d\u5199\u9053\uff1a\u201c\u5728\u4eca\u5929\u7684\u6821\u7814\u4f1a\u4e0a\uff0c\u6211\u8bf4\u505a\u6559\u7814\u8ddf\u5199\u8bba\u6587\u7684\u65b9\u6cd5\u662f\u4e00\u6837\u7684\uff0c\u5c45\u7136\u6ca1\u6709\u5f97\u5230\u8ba4\u53ef\u3002\u662f\u6211\u9519\u4e86\uff1f\u8fd8\u662f\u5927\u5bb6\u4e0d\u7406\u89e3\u6211\uff1f\u6211\u5f97\u628a\u8fd9\u4e2a\u95ee\u9898\u641e\u6e05\u695a\u3002\u201d\u8fd9\u8bf4\u660e\u90b1\u8001\u5e08\nA. \u7f3a\u4e4f\u63a2\u7d22\u7cbe\u795e\nB. \u5584\u4e8e\u81ea\u6211\u53cd\u601d\nC. \u7f3a\u4e4f\u95ee\u9898\u610f\u8bc6\nD. \u8fab\u4e8e\u81ea\u6211\u6697\u793a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.8575666074958306, "itpossible/Chinese-Mistral-7B-v0.1": 0.4063965311423763, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8733796623097208, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9963753837238244}}, {"question": "5\u670817\u65e5\u201418\u65e5\uff0c\u7b2c19\u5c4a\u4fc4\u7f57\u65af\u2014\u6b27\u76df\u9996\u8111\u4f1a\u8bae\u5728\u4fc4\u5357\u90e8\u57ce\u5e02\uff08\uff09\u4e3e\u884c\uff0c\u4fc4\u603b\u7edf\u666e\u4eac\uff0c\u6b27\u76df\u59d4\u5458\u4f1a\u6ce8\u610f\u5df4\u7f57\u4f50\u3001\u6b27\u76df\u8f6e\u503c\u4e3b\u5e2d\u5fb7\u56fd\u603b\u7406\u9ed8\u514b\u5c14\u7b49\u51fa\u5e2d\u4e86\u8be5\u4f1a\u8bae\nA. \u4e4c\u62c9\u5c14\u65af\u514b\nB. \u963f\u62c9\u6728\u56fe\nC. \u963f\u5c14\u5e93\u8328\u514b\nD. \u8428\u9a6c\u62c9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.44797806334939255, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd55\u4e2a\u5c11\u6570\u6c11\u65cf\u4e2d\uff0c\u76ee\u524d\u901a\u7528\u6c49\u8bed\u7684\u6c11\u65cf\u662f\nA. \u6ee1\u3001\u58ee\nB. \u56de\u3001\u58ee\nC. \u6ee1\u3001\u56de\nD. \u6ee1\u3001\u8499\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3458685957611387, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.36078609119776345, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5226734428690251}}, {"question": "\u793e\u4f1a\u4e3b\u4e49\u96c6\u4f53\u4e3b\u4e49\u539f\u5219\u5f3a\u8c03\u96c6\u4f53\u5229\u76ca\u548c\u4e2a\u4eba\u5229\u76ca\u7684\u8fa9\u8bc1\u7edf\u4e00\u3002\u4e0b\u5217\u5173\u4e8e\u793e\u4f1a\u4e3b\u4e49\u96c6\u4f53\u4e3b\u4e49\u7684\u8bf4\u6cd5\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u96c6\u4f53\u4e3b\u4e49\u662f\u5bf9\u4e2a\u4eba\u7684\u538b\u5236\u548c\u5bf9\u4e2a\u6027\u7684\u675f\u7f1a\nB. \u4e3a\u4e86\u4e2a\u4eba\u5229\u76ca\u53ef\u4ee5\u727a\u7272\u96c6\u4f53\u5229\u76ca\nC. \u4e2a\u4eba\u5229\u76ca\u4e0e\u96c6\u4f53\u5229\u76ca\u4e4b\u95f4\u4e0d\u4f1a\u53d1\u751f\u77db\u76fe\nD. \u91cd\u89c6\u4e2a\u4eba\u7684\u6b63\u5f53\u5229\u76ca\u548c\u81ea\u89c9\u521b\u9020\u7cbe\u795e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.93871526918693, "meta-math/MetaMath-Mistral-7B": 0.9943749079817197, "itpossible/Chinese-Mistral-7B-v0.1": 0.9251661646615859, "HuggingFaceH4/zephyr-7b-beta": 0.9998417857495409, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9690829473539616, "meta-llama/Meta-Llama-3-8B": 0.9571598423528658, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9428402115501304}}, {"question": "\u5df2\u77e5 $A\uff0c B\uff0c C$ \u4e3a\u968f\u673a\u4e8b\u4ef6\uff0c $A$ \u4e0e $B$ \u76f8\u4e92\u72ec\u7acb\uff0c $P(C)=1$\uff0c \u5219\u4e0b\u5217\u4e8b\u4ef6\u4e2d\u4e0d\u76f8\u4e92 \u72ec\u7acb\u7684\u662f ( )\nA. $A\uff0c B\uff0c A C$\nB. $A\uff0c B\uff0c A-C$\nC. $A\uff0c B\uff0c \\bar{A} \\bar{C}$\nD. $A\uff0c B\uff0c A+C$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2658638974902097, "meta-math/MetaMath-Mistral-7B": 0.3222217097460757, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6cd5\u6240\u5177\u6709\u7684\u89c4\u5b9a\u4eba\u4eec\u884c\u4e3a\u7684\u6a21\u5f0f\u3001\u6307\u5bfc\u4eba\u4eec\u884c\u4e3a\u7684\u6027\u8d28\u7684\u7279\u5f81\u662f\u6307\nA. \u6cd5\u7684\u7a0b\u5e8f\u6027\nB. \u6cd5\u7684\u666e\u904d\u6027\nC. \u6cd5\u7684\u4e00\u822c\u6027\nD. \u6cd5\u7684\u89c4\u8303\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.42836608533023607, "meta-math/MetaMath-Mistral-7B": 0.32770783487659344, "itpossible/Chinese-Mistral-7B-v0.1": 0.6804114186757476, "HuggingFaceH4/zephyr-7b-beta": 0.8502825933008414, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.48464259282740424, "meta-llama/Meta-Llama-3-8B": 0.9685696364662739, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9832679450356241}}, {"question": "\u4e2d\u5b66\u751f\u5c0f\u5b59\u8fd1\u671f\u5fc3\u7406\u5f88\u77db\u76fe\uff0c\u89c9\u5f97\u672a\u6765\u7684\u81ea\u5df1\u5e94\u8be5\u662f\u4e00\u540d\u79d1\u5b66\u5bb6\uff0c\u4f46\u53c8\u89c9\u5f97\u80fd\u529b\u6709\u9650\uff0c\u9065\u4e0d\u53ef\u53ca\u3002\u6839\u636e\u57c3\u91cc\u514b\u68ee\u7684\u4eba\u683c\u53d1\u5c55\u9636\u6bb5\u8bba\uff0c\u5f53\u524d\u4ed6\u7684\u4e3b\u8981\u53d1\u5c55\u4efb\u52a1\u662f\nA. \u5efa\u7acb\u540c\u4e00\u6027\nB. \u83b7\u5f97\u52e4\u594b\u611f\nC. \u514b\u670d\u5185\u759a\u611f\nD. \u9632\u6b62\u5b64\u72ec\u611f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3454901351726419, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u81ea\u6027\u7b80\u5355\u6765\u8bf4\u5c31\u662f\u5bf9\u81ea\u5df1\u8eab\u4f53\u7684\u629a\u6170\uff0c\u4e0b\u5217\u5173\u4e8e\u81ea\u6027\u7684\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f\nA. \u8001\u5e74\u671f\u6ca1\u6709\u81ea\u6211\u53d6\u60a6\u7684\u5feb\u611f\nB. \u201c\u6ce1\u811a\u201d\u90fd\u662f\u81ea\u6211\u5173\u6000\u548c\u60a6\u7eb3\u7684\u65b9\u6cd5\nC. \u81ea\u6027\u662f\u61c2\u5f97\u5982\u4f55\u5173\u7231\u548c\u53d6\u60a6\u81ea\u5df1\nD. \u4eba\u4e00\u751f\u7684\u593415-20\u5e74\u90fd\u662f\u81ea\u6211\u6027\u6b32\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5677228619165313, "meta-math/MetaMath-Mistral-7B": 0.8838743385613244, "itpossible/Chinese-Mistral-7B-v0.1": 0.35658152023113504, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5154994880779628, "meta-llama/Meta-Llama-3-8B": 0.7034605196113604, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u5b50\u4e2d\u6ca1\u6709\u8bed\u75c5\u7684\u4e00\u9879\u662f\uff08\uff09\nA. \u4eba\u95f4\u6ca1\u6709\u6c38\u6052\u7684\u591c\u665a\uff0c\u4e16\u754c\u6ca1\u6709\u6c38\u6052\u7684\u51ac\u5929\u3002\nB. \u7535\u89c6\u673a\u91cc\u4f20\u51fa\u96c4\u58ee\u6709\u529b\u7684\u6218\u58eb\u4eec\u7684\u6b4c\u58f0\u3002\nC. \u7ecf\u8fc7\u5bf9\u8fd9\u7bc7\u8457\u4f5c\u7684\u9605\u8bfb\uff0c\u4f7f\u6211\u66f4\u6df1\u523b\u5730\u4e86\u89e3\u4e86\u4e2d\u56fd\u8fd1\u4ee3\u5c48\u8fb1\u7684\u5386\u53f2\u3002\nD. \u4ed6\u90a3\u5d07\u9ad8\u7684\u9769\u547d\u54c1\u8d28\uff0c\u7ecf\u5e38\u6d6e\u73b0\u5728\u6211\u7684\u8111\u6d77\u4e2d\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u98df\u54c1\u5e72\u5236\u8fc7\u7a0b\u7684\u6838\u5fc3\u95ee\u9898\u662f\nA. \u54c1\u8d28\u53d8\u5316\nB. \u6c34\u5206\u8f6c\u79fb\nC. \u6e7f\u70ed\u8f6c\u79fb\nD. \u6c34\u5206\u68af\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6211\u56fd\uff0c\u9759\u6001\u610f\u4e49\u4e0a\u7684\u6cd5\u5236\u662f\u6307\nA. \u6709\u6cd5\u53ef\u4f9d\u3001\u6709\u6cd5\u5fc5\u4f9d\u3001\u6267\u6cd5\u5fc5\u4e25\u3001\u8fdd\u6cd5\u5fc5\u7a76\nB. \u7acb\u6cd5\u3001\u6267\u6cd5\u3001\u5b88\u6cd5\u548c\u5bf9\u6cd5\u5f8b\u5b9e\u65bd\u7684\u76d1\u7763\nC. \u6cd5\u5f8b\u5236\u5ea6\nD. \u4f9d\u6cd5\u529e\u4e8b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.32939455548883284, "meta-math/MetaMath-Mistral-7B": 0.36260628017702756, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9661244786290751, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.44391743028409275, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6d6e\u529b\u7684\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u6d78\u5728\u6db2\u4f53\u91cc\u7684\u7269\u4f53\u53d7\u5230\u7684\u6d6e\u529b\u603b\u7b49\u4e8e\u7269\u4f53\u6392\u5f00\u6c34\u7684\u91cd\u529b\nB. \u7269\u4f53\u6d78\u6ca1\u5728\u6c34\u4e2d\u8d8a\u6df1\uff0c\u6240\u53d7\u5230\u7684\u6d6e\u529b\u8d8a\u5927\nC. \u94c1\u5757\u5728\u6c34\u4e2d\u4e0b\u6c89\uff0c\u8868\u660e\u94c1\u5757\u5728\u6c34\u4e2d\u4e0d\u53d7\u5230\u6d6e\u529b\u7684\u4f5c\u7528\nD. \u5730\u9762\u4e0a\u7684\u94c5\u7403\u4e5f\u53d7\u5230\u6d6e\u529b\u7684\u4f5c\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5164723629699796, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u67d0\u5e02\u653f\u5e9c\u5efa\u65b0\u529e\u516c\u5927\u697c\uff0c\u5de5\u7a0b\u7531\u67d0\u5efa\u7b51\u516c\u53f8\u627f\u5305\u3002\u5de5\u7a0b\u6309\u671f\u7ae3\u5de5\u5e76\u9a8c\u6536\u5408\u683c\u540e\uff0c\u5e02\u653f\u5e9c\u7531\u4e8e\u8d22\u653f\u56f0\u96be\uff0c\u90e8\u5206\u5de5\u7a0b\u6b3e\u4e00\u76f4\u672a\u6309\u671f\u4ed8\u7ed9\u5efa\u7b51\u516c\u53f8\uff0c\u81f4\u8be5\u516c\u53f8\u9677\u5165\u4e25\u91cd\u7ecf\u6d4e\u56f0\u96be\u3002\u5efa\u7b51\u516c\u53f8\u8ba4\u4e3a\u81ea\u5df1\u7684\u5408\u6cd5\u6743\u76ca\u88ab\u4fb5\u72af\uff0c\u9042\u8bc9\u81f3\u6cd5\u9662\u3002\u672c\u6848\u4e2d\uff0c\u5e02\u653f\u5e9c\u5e94\u8d1f\nA. \u8fdd\u7ea6\u8d23\u4efb\nB. \u884c\u653f\u8d23\u4efb\nC. \u4fb5\u6743\u8d23\u4efb\nD. \u7ecf\u6d4e\u8d23\u4efb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5149177897604843, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u7269\u6743\u5173\u7cfb\u7684\u6cd5\u5f8b\u9002\u7528\u4e0a\uff0c\u4e3b\u5f20\u4e0d\u5206\u52a8\u4ea7\u548c\u4e0d\u52a8\u4ea7\u4e00\u5f8b\u9002\u7528\u7269\u4e4b\u6240\u5728\u5730\u6cd5\u662f\u4ece\nA. 19\u4e16\u7eaa\u5f00\u59cb\u7684\nB. 18\u4e16\u7eaa\u5f00\u59cb\u7684\nC. 13\u300114\u4e16\u7eaa\u5f00\u59cb\u7684\nD. 17\u4e16\u7eaa\u5f00\u59cb\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.30359886656533613, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7239361221431285}}, {"question": "\u516c\u53f8\u7ecf\u7406\u7532\u5229\u7528\u804c\u52a1\u4e0a\u7684\u4fbf\u5229\uff0c\u4fb5\u541e\u672c\u5355\u4f4d\u8d22\u7269\u6570\u989d\u8f83\u5927\uff0c\u5bf9\u6b64\u72af\u7f6a\u6211\u56fd\u5211\u6cd5\u89c4\u5b9a\u7684\u6cd5\u5b9a\u5211\u662f\u201c\u5904\u4e94\u5e74\u4ee5\u4e0a\u6709\u671f\u5f92\u5211\uff0c\u53ef\u4ee5\u5e76\u5904\u6ca1\u6536\u8d22\u4ea7\u3002\u201d\u6839\u636e\u672c\u6761\u5bf9\u7532\u91cf\u5211\nA. \u6700\u9ad8\u53ef\u5224\u590425\u5e74\u6709\u671f\u5f92\u5211\uff0c\u4e0d\u5e76\u5904\u6ca1\u6536\u8d22\u4ea7\nB. \u6700\u9ad8\u53ef\u5224\u590420\u5e74\u6709\u671f\u5f92\u5211\uff0c\u5e76\u5904\u6ca1\u6536\u8d22\u4ea7\nC. \u5982\u679c\u5224\u59045\u5e74\u6709\u671f\u5f92\u5211\uff0c\u5219\u4e0d\u80fd\u5e76\u5904\u6ca1\u6536\u8d22\u4ea7\nD. \u6700\u9ad8\u53ef\u5224\u590415\u5e74\u6709\u671f\u5f92\u5211\uff0c\u5e76\u5904\u6ca1\u6536\u8d22\u4ea7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8665713653565577, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5256911586235454}}, {"question": "\u6cd5\u7684\u793e\u4f1a\u4f5c\u7528\u7684\u6838\u5fc3\u662f\nA. \u6267\u884c\u793e\u4f1a\u516c\u5171\u4e8b\u52a1\nB. \u7ef4\u62a4\u9636\u7ea7\u7edf\u6cbb\nC. \u5236\u88c1\u8fdd\u6cd5\u72af\u7f6a\nD. \u89c4\u8303\u4eba\u4eec\u7684\u884c\u4e3a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bca\u65ad\u8179\u8154\u9694\u5ba4\u7efc\u5408\u5f81(abdominalcompartment syndrome)\u65f6\u8180\u80f1\u5185\u6d4b\u5f97\u7684\u538b\u529b\u5e94\u5927\u4e8e\nA. 20 mmHg\nB. 10 mmHg\nC. 25 mmHg\nD. 15 mmHg\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33201172912095256, "meta-math/MetaMath-Mistral-7B": 0.37185515361699595, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9712118905384652, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6304846460756168, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4200051239257073}}, {"question": "\u6709\u5173\u5b63\u2edb\u7684\u6b63\u786e\u53d9\u8ff0\u662f\nA. \u6211\u56fd\u53ea\u53d7\u4e1c\u5357\u5b63\u2edb\u7684\u5f71\u54cd\nB. \u5b63\u2edb\u2f53\u5019\u90fd\u662f\u590f\u5b63\u2fbc\u6e29\u591a\u2fac\uff0c\u51ac\u5b63\u5bd2\u51b7\u2f32\u71e5\u3002\nC. \u5b63\u2edb\u73af\u6d41\u4e0d\u5c5e\u4e8e\u2f24\u2f53\u73af\u6d41\nD. \u6d77\u9646\u70ed\u2f12\u6027\u8d28\u7684\u5dee\u5f02\u662f\u5f62\u6210\u5b63\u2edb\u7684\u91cd\u8981\u56e0\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.707201571209355, "meta-math/MetaMath-Mistral-7B": 0.8809153513154411, "itpossible/Chinese-Mistral-7B-v0.1": 0.7101651240244498, "HuggingFaceH4/zephyr-7b-beta": 0.9949848811857321, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9382397522138817, "meta-llama/Meta-Llama-3-8B": 0.8789965138897159, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u6cd5\u5f8b\u4e0e\u9053\u5fb7\u7684\u5173\u7cfb\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u6709\nA. \u6cd5\u5f8b\u4e0e\u9053\u5fb7\u662f\u4e24\u7c7b\u91cd\u8981\u7684\u8c03\u6574\u4eba\u4eec\u884c\u4e3a\u7684\u89c4\u8303\uff0c\u90fd\u662f\u968f\u7740\u9636\u7ea7\u7684\u5206\u5316\u548c\u56fd\u5bb6\u7684\u5efa\u7acb\u800c\u51fa\u73b0\u7684\nB. \u6709\u65f6\u4e00\u4e2a\u884c\u4e3a\u53ef\u80fd\u5408\u4e4e\u60c5\u7406\uff0c\u4f46\u5374\u4e0d\u5408\u6cd5\uff08\u6cd5\u5f8b\u4e0d\u5141\u8bb8\u6216\u8005\u4e0d\u53d7\u6cd5\u5f8b\u4fdd\u62a4\uff09\u3002\u53cd\u4e4b\uff0c\u4e5f\u53ef\u80fd\u51fa\u73b0\u4e00\u4e2a\u53d7\u6cd5\u5f8b\u4fdd\u62a4\u7684\u884c\u4e3a\uff0c\u5374\u4e0d\u7b26\u5408\u9053\u5fb7\u89c4\u8303\u7684\u8981\u6c42\nC. \u6cd5\u5f8b\u4ee5\u9053\u5fb7\u4e3a\u57fa\u7840\uff0c\u4f46\u5728\u8c03\u6574\u8303\u56f4\u4e0a\u5c0f\u4e8e\u9053\u5fb7\u7684\u8c03\u6574\u8303\u56f4\uff0c\u6cd5\u5f8b\u901a\u5e38\u53ea\u5bf9\u5176\u4e2d\u4e25\u91cd\u7684\u3001\u9700\u8981\u52a8\u7528\u56fd\u5bb6\u5f3a\u5236\u529b\u7684\u884c\u4e3a\u4f5c\u51fa\u53cd\u5e94\u3002\u56e0\u6b64\uff0c\u5c5e\u4e8e\u6cd5\u5f8b\u8c03\u6574\u8303\u56f4\u7684\u5fc5\u7136\u5c5e\u4e8e\u9053\u5fb7\u8c03\u6574\u8303\u56f4\nD. \u7531\u4e8e\u6cd5\u5f8b\u548c\u9053\u5fb7\u90fd\u5177\u6709\u591a\u5143\u6027\uff0c\u56e0\u6b64\uff0c\u6cd5\u5f8b\u548c\u9053\u5fb7\u603b\u662f\u5b58\u5728\u4e00\u5b9a\u7684\u51b2\u7a81\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4283660707349313, "meta-math/MetaMath-Mistral-7B": 0.6513924583736448, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5905830794787831, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8533228951551598}}, {"question": "\u7ecf\u8425\u540c\u4e00\u54c1\u724c\u4ea7\u54c1\u7684\u4e0d\u540c\u8d85\u7ea7\u5e02\u573a\u3001\u767e\u8d27\u5e97\u4e0e\u4fbf\u5229\u5e97\u4e4b\u95f4\u7684\u51b2\u7a81\u5c5e\u4e8e\nA. \u6f5c\u5728\u51b2\u7a81\nB. \u5782\u76f4\u6e20\u9053\u51b2\u7a81\nC. \u6c34\u5e73\u6e20\u9053\u51b2\u7a81\nD. \u591a\u6e20\u9053\u51b2\u7a81\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4634050595472996, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.47139483922833675, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8223373204667369}}, {"question": "\u51b3\u5b9a\u98df\u54c1\u5b89\u5168\u7684\u76f8\u5bf9\u6027\u7684\u56e0\u7d20\u4e0d\u5305\u62ec\nA. \u98df\u7528\u8005\u672c\u8eab\nB. \u98df\u54c1\u672c\u8eab\nC. \u5916\u6765\u4e0d\u826f\u56e0\u7d20\nD. \u751f\u6001\u73af\u5883\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u538b\u529b\u8fc7\u5927\u53ef\u80fd\u4f1a\u4fc3\u4f7f\u4eba\u4eec\u6ce8\u91cd\u77ed\u671f\u76ee\u6807\u800c\u5ffd\u89c6\nA. \u793e\u4f1a\u8fdb\u6b65\nB. \u4f01\u4e1a\u53d1\u5c55\nC. \u957f\u671f\u5229\u6da6\nD. \u5229\u76ca\u76f8\u5173\u8005\u5229\u76ca\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6177599630041029, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9053\u5bb6\u601d\u60f3\u5728\u6211\u56fd\u5f71\u54cd\u6df1\u8fdc\uff0c\u8bf7\u95ee\u5386\u53f2\u4e2d\u7684\u54ea\u4e00\u65f6\u671f\u6700\u63a5\u8fd1\u9053\u5bb6\u6240\u4e3b\u5f20\u7684\u65e0\u4e3a\u800c\u6cbb\nA. \u5149\u6b66\u4e2d\u5174\nB. \u5f00\u65e0\u76db\u4e16\nC. \u8d1e\u89c2\u4e4b\u6cbb\nD. \u6587\u666f\u4e4b\u6cbb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3390741764209328, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5744691360642968, "HuggingFaceH4/zephyr-7b-beta": 0.6463713397875831, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u79d1\u5b66\u5bb6\u505a\u4e86\u4e00\u4e2a\u4e3a\u671f8\u5468\u7684\u5b9e\u9a8c\uff1a\u4e09\u6279\u5b9e\u9a8c\u9f20\u5728\u767d\u5929\u706f\u5149\u7167\u5c0416\u5c0f\u65f6\u540e\uff0c\u518d\u5728\u9ed1\u591c\u91cc\u5206\u522b\u5904\u4e8e\u5168\u9ed1\u3001\u6697\u5149\u548c\u5f00\u706f\u72b6\u60018\u5c0f\u65f6\uff0c\u6bcf\u65e5\u5982\u6b64\u3002\u5b9e\u9a8c\u671f\u95f4\uff0c\u6240\u6709\u5b9e\u9a8c\u9f20\u7684\u98df\u7269\u7c7b\u522b\u53ca\u98df\u91cf\u90fd\u5b8c\u5168\u76f8\u540c\u3002\u7ed3\u679c\u53d1\u73b0\uff0c\u591c\u95f4\u5904\u4e8e\u6697\u5149\u73af\u5883\u3001\u5f00\u706f\u73af\u5883\u7684\u8001\u9f20\u90fd\u51fa\u73b0\u4f53\u91cd\u589e\u52a0\u7684\u73b0\u8c61\u3002\u636e\u6b64\uff0c\u7814\u7a76\u4eba\u5458\u5f97\u51fa\u7ed3\u8bba\uff1a\u897f\u65b9\u4eba\u7684\u666e\u904d\u80a5\u80d6\u4e0e\u665a\u4e0a\u706f\u706b\u901a\u660e\u7684\u8857\u666f\u548c\u7535\u8111\u3001\u7535\u89c6\u673a\u7684\u5149\u5bc6\u5207\u76f8\u5173\u3002\u4ee5\u4e0b\u5404\u9879\u5982\u679c\u4e3a\u771f\uff0c\u6700\u80fd\u8d28\u7591\u4e0a\u8ff0\u7ed3\u8bba\u7684\u662f\nA. \u5b9e\u9a8c\u65f6\u95f4\u4ec58\u5468\uff0c\u5bf9\u5e7c\u9f20\u6765\u8bf4\u592a\u8fc7\u77ed\u6682\uff0c\u5e94\u5ef6\u957f\u5b9e\u9a8c\u65f6\u95f4\nB. \u9ed1\u591c\u5904\u4e8e\u6697\u5149\u3001\u5f00\u706f\u73af\u5883\u7684\u5b9e\u9a8c\u9f20\u548c\u9ed1\u591c\u5904\u4e8e\u9ed1\u6697\u73af\u5883\u7684\u5b9e\u9a8c\u9f20\u4e0d\u4e00\u6837\uff0c\u524d\u8005\u5728\u591c\u95f4\u8fdb\u98df\uff0c\u800c\u591c\u665a\u9f20\u7c7b\u65b0\u9648\u4ee3\u8c22\u7387\u4f4e\uff0c\u80fd\u91cf\u6d88\u8017\u5c11\uff0c\u5bb9\u6613\u589e\u91cd\nC. \u636e\u7edf\u8ba1\uff0c\u5728\u665a\u4e0a\u8857\u666f\u706f\u706b\u901a\u660e\u7684\u897f\u65b9\u57ce\u5e02\u4e2d\uff0c\u90a3\u4e9b\u7ecf\u5e38\u63a5\u53d7\u7535\u8111\u3001\u7535\u89c6\u673a\u5149\u7167\u5c04\u7684\u4eba\u5927\u90e8\u5206\u5e76\u4e0d\u80a5\u80d6\nD. \u897f\u65b9\u4eba\u5e76\u4e0d\u662f\u666e\u904d\u80a5\u80d6\uff0c\u4e2d\u7b49\u53ca\u4ee5\u4e0a\u6536\u5165\u7684\u4eba\u7fa4\u975e\u5e38\u91cd\u89c6\u4f53\u91cd\u95ee\u9898\uff0c\u65f6\u5e38\u8fdb\u884c\u5065\u8eab\u7b49\u8eab\u6750\u7ba1\u7406\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.297239488123602, "meta-math/MetaMath-Mistral-7B": 0.33075425723442, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9974620087538346, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8172223656657541, "meta-llama/Meta-Llama-3-8B": 0.3926712710229656, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9883884000245818}}, {"question": "\u7532\u3001\u4e59\u4e8c\u4eba\u7ea6\u5b9a\uff0c\u5982\u679c30\u5929\u5185\u4e0b\u96e8\uff0c\u7532\u5c31\u9001\u7ed9\u4e59\u4e00\u628a\u96e8\u4f1e\u3002\u8be5\u7ea6\u5b9a\nA. \u65e2\u4e0d\u662f\u9644\u6761\u4ef6\u4e5f\u4e0d\u662f\u9644\u671f\u9650\u7684\u7ea6\u5b9a\nB. \u5c5e\u4e8e\u9644\u89e3\u9664\u6761\u4ef6\u7684\u6c11\u4e8b\u6cd5\u5f8b\u884c\u4e3a\nC. \u5c5e\u4e8e\u9644\u5ef6\u7f13\u6761\u4ef6\u7684\u6c11\u4e8b\u6cd5\u5f8b\u884c\u4e3a\nD. \u5c5e\u4e8e\u9644\u671f\u9650\u7684\u6c11\u4e8b\u6cd5\u5f8b\u884c\u4e3a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u4e0b\u56e0\u7d20\u4e2d\u4e0d\u4f1a\u5bfc\u81f4\u81ea\u52a8\u6b65\u67aa\u4e0d\u53d1\u706b\u7684\u662f\nA. \u51fb\u9488\u635f\u574f\nB. \u51fb\u9524\u5f39\u529b\u5c40\u9650\u6027\nC. \u5b50\u5f39\u6216\u5f39\u5323\u53e3\u8fc7\u810f\nD. \u5b50\u5f39\u5e95\u706b\u5931\u6548\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2893743369926839, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.29863342676099575, "HuggingFaceH4/zephyr-7b-beta": 0.5847717609975924, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.423442608492013, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.693195522178266}}, {"question": "\u8fd1\u4ee3\u65b0\u95fb\u4e8b\u4e1a\u8bde\u751f\u7684\u65f6\u95f4\u6807\u5fd7\u662f\nA. \u53e3\u8bed\u65b0\u95fb\u51fa\u73b0\nB. \u624b\u6284\u65b0\u95fb\u51fa\u73b0\nC. \u65b0\u95fb\u4e66\u7684\u95ee\u4e16\nD. \u5370\u5237\u65b0\u95fb\u7eb8\u95ee\u4e16\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8949310781016813, "meta-math/MetaMath-Mistral-7B": 0.9908515354879996, "itpossible/Chinese-Mistral-7B-v0.1": 0.8129212610903115, "HuggingFaceH4/zephyr-7b-beta": 0.9999421662889848, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.959313893502308, "meta-llama/Meta-Llama-3-8B": 0.9402323211636964, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8795692115842715}}, {"question": "\u5927\u578b\u6b4c\u821e\u300a\u4e91\u5357\u6620\u50cf\u300b\u7684\u7f16\u5bfc\u53ca\u4e3b\u6f14\u662f\u5f53\u4ee3\u8457\u540d\u821e\u8e48\u5bb6\nA. \u5200\u7f8e\u5170\nB. \u8d75\u9752\nC. \u9648\u7231\u83b2\nD. \u6768\u4e3d\u840d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6923079918599259, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9053\u5fb7\u4f5c\u4e3a\u7279\u6b8a\u7684\u793e\u4f1a\u610f\u8bc6\u5f62\u6001\u5bf9\u4e8e\u793e\u4f1a\u53d1\u5c55\u5177\u6709\u591a\u79cd\u529f\u80fd\u3002\u901a\u8fc7\u8bc4\u4ef7\u7b49\u65b9\u5f0f\uff0c\u6307\u5bfc\u548c\u7ea0\u6b63\u4eba\u4eec\u7684\u884c\u4e3a\u548c\u5b9e\u9645\u6d3b\u52a8\uff0c\u534f\u8c03\u4eba\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u662f\u9053\u5fb7\u7684\nA. \u6c9f\u901a\u529f\u80fd\nB. \u6fc0\u52b1\u529f\u80fd\nC. \u8ba4\u8bc6\u529f\u80fd\nD. \u8c03\u8282\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8769131439226125, "meta-math/MetaMath-Mistral-7B": 0.817062651990721, "itpossible/Chinese-Mistral-7B-v0.1": 0.5777517929053557, "HuggingFaceH4/zephyr-7b-beta": 0.970314008717393, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8525157017879105, "meta-llama/Meta-Llama-3-8B": 0.9468221746042054, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.98331377289772}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u975e\u516c\u6709\u5236\u7ecf\u6d4e\u7684\u8bba\u8ff0\uff0c\u6b63\u786e\u7684\u6709a\u975e\u516c\u6709\u5236\u7ecf\u6d4e\u662f\u7ecf\u6d4e\u53d1\u5c55\u7684\u91cd\u8981\u63a8\u52a8\u529b b\u53d1\u5c55\u975e\u516c\u6709\u5236\u7ecf\u6d4e\u6709\u5229\u4e8e\u6269\u5927\u5c31\u4e1a c\u53d1\u5c55\u975e\u516c\u6709\u5236\u7ecf\u6d4e\u80fd\u66f4\u597d\u5730\u6ee1\u8db3\u4eba\u6c11\u7684\u751f\u4ea7\u548c\u751f\u6d3b\u9700\u8981 d\u4fc3\u8fdb\u975e\u516c\u6709\u5236\u7ecf\u6d4e\u7684\u53d1\u5c55\uff0c\u662f\u5b8c\u5584\u793e\u4f1a\u4e3b\u4e49\u5e02\u573a\u7ecf\u6d4e\u4f53\u5236\u7684\u91cd\u8981\u5185\u5bb9\nA. ab\nB. abc\nC. abcd\nD. a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.38733735460863067, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4511926590231526, "meta-llama/Meta-Llama-3-8B": 0.6487062204267993, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8965343070503468}}, {"question": "\u8425\u5229\u6cd5\u4eba\u4f9d\u6cd5\u89e3\u6563\u8fdb\u884c\u6e05\u7b97\u671f\u95f4\uff0c\u8425\u5229\u6cd5\u4eba\nA. \u4e3b\u4f53\u8d44\u683c\u4e0d\u6d88\u706d\uff0c\u4f46\u4e0d\u5f97\u4ece\u4e8b\u4e0e\u6e05\u7b97\u65e0\u5173\u7684\u6d3b\u52a8\nB. \u4e3b\u4f53\u8d44\u683c\u6d88\u706d\uff0c\u4e0d\u80fd\u8fdb\u884c\u4efb\u4f55\u6c11\u4e8b\u6d3b\u52a8\nC. \u4e3b\u4f53\u8d44\u683c\u6d88\u706d\uff0c\u4f46\u53ef\u4ee5\u4ece\u4e8b\u4e0e\u6e05\u7b97\u6709\u5173\u7684\u6d3b\u52a8\nD. \u4e3b\u4f53\u8d44\u683c\u4e0d\u6d88\u706d\uff0c\u53ef\u4ee5\u8fdb\u884c\u5404\u79cd\u6c11\u4e8b\u6d3b\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5431959471454935, "meta-math/MetaMath-Mistral-7B": 0.8413970774284444, "itpossible/Chinese-Mistral-7B-v0.1": 0.38440464789131273, "HuggingFaceH4/zephyr-7b-beta": 0.993030818255234, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8170329851414571, "meta-llama/Meta-Llama-3-8B": 0.5441962514559905, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.762668191366817}}, {"question": "\u6469\u5c14\u6839\u8ba4\u4e3a\u4eba\u7c7b\u793e\u4f1a\u7684\u4e09\u4e2a\u9636\u6bb5\u4e2d\u7b2c\u4e8c\u4e2a\u9636\u6bb5\u662f\uff1a\nA. \u6587\u660e\u671f\nB. \u91ce\u86ee\u671f\nC. \u8499\u6627\u671f\nD. \u5236\u5ea6\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4703069149672875, "itpossible/Chinese-Mistral-7B-v0.1": 0.550166767216909, "HuggingFaceH4/zephyr-7b-beta": 0.8025188008939002, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5303059713287406, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.47894784513039607}}, {"question": "\u98ce\u673a\u6c34\u6cf5\u7684\u7cfb\u7edf\u8c03\u8282\u65b9\u5f0f\u9009\u62e9\u4e2d\u5e94\u6ce8\u610f\u7684\u95ee\u9898\u6709\nA. \u6cf5\u6216\u98ce\u673a\u7cfb\u7edf\u5de5\u4f5c\u6d41\u91cf\u7684\u53d8\u5316\u89c4\u5f8b\u4e0d\u662f\u51b3\u5b9a\u8c03\u8282\u88c5\u7f6e\u7c7b\u578b\u7684\u57fa\u672c\u4f9d\u636e\nB. \u6cf5\u4e0e\u98ce\u673a\u8f74\u529f\u7387\u5927\u5c0f\u4e5f\u4e0e\u9009\u62e9\u8c03\u8282\u88c5\u7f6e\u7684\u7c7b\u578b\u65e0\u5173\nC. \u6cf5\u4e0e\u98ce\u673a\u8c03\u901f\u88c5\u7f6e\u7684\u6548\u7387\u548c\u529f\u7387\u56de\u6536\nD. \u6cf5\u6216\u98ce\u673a\u7cfb\u7edf\u7ba1\u8def\u6027\u80fd\u66f2\u7ebf\u4e2d\u9759\u626c\u7a0b(\u9759\u538b)\u6240\u5360\u6bd4\u4f8b\u7684\u5927\u5c0f\uff0c\u4e0e\u53d8\u901f\u88c5\u7f6e\u8282\u80fd\u6548\u679c\u7684\u5927\u5c0f\u65e0\u5173\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9590713921613772, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8464621213886434, "meta-llama/Meta-Llama-3-8B": 0.47164719525062626, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7673920541617063}}, {"question": "\u4e0d\u80fd\u6fc0\u52b1\u627f\u5305\u4eba\u52aa\u529b\u964d\u4f4e\u6210\u672c\u548c\u7f29\u77ed\u5de5\u671f\u7684\u5408\u540c\u5f62\u5f0f\u662f\nA. \u6700\u5927\u6210\u672c\u52a0\u8d39\u7528\u5408\u540c\nB. \u6210\u672c\u52a0\u5956\u91d1\u5408\u540c\nC. \u6210\u672c\u52a0\u56fa\u5b9a\u6bd4\u4f8b\u8d39\u7528\u5408\u540c\nD. \u6210\u672c\u52a0\u56fa\u5b9a\u8d39\u7528\u5408\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33482387498194033, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ece\u672c\u8d28\u4e0a\u8bb2\uff0c\u8ba1\u7b97\u673a\u75c5\u6bd2\u662f\u2f00\u79cd\nA. \u7ec6\u83cc\nB. \u7a0b\u5e8f\nC. \u2f42\u672c\nD. \u5fae\u2f63\u7269\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9068711134213194, "meta-math/MetaMath-Mistral-7B": 0.9944123713964349, "itpossible/Chinese-Mistral-7B-v0.1": 0.9532258837615116, "HuggingFaceH4/zephyr-7b-beta": 0.9994770807515829, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9845700172700278, "meta-llama/Meta-Llama-3-8B": 0.746456729651657, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9994726707950637}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u201c\u56de\u6536\u7ad9\u201d\u7684\u53d9\u8ff0\u6b63\u786e\u7684\u662f\nA. \u201c\u56de\u6536\u7ad9\u201d\u4e5f\u53ef\u4ee5\u5220\u9664\nB. \u201c\u56de\u6536\u7ad9\u201d\u5176\u5b9e\u662f\u786c\u76d8\u4e2d\u7684\u4e00\u5757\u533a\u57df\nC. \u201c\u56de\u6536\u7ad9\u201d\u4e2d\u7684\u6587\u4ef6\u4e0d\u80fd\u6062\u590d\nD. \u201c\u56de\u6536\u7ad9\u201d\u5176\u5b9e\u662f\u5185\u5b58\u4e2d\u7684\u4e00\u5757\u533a\u57df\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.790235253301522, "meta-math/MetaMath-Mistral-7B": 0.967628897470259, "itpossible/Chinese-Mistral-7B-v0.1": 0.42844847344692183, "HuggingFaceH4/zephyr-7b-beta": 0.9983395764496212, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.822300255029047, "meta-llama/Meta-Llama-3-8B": 0.9532516876196322, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9990617772297142}}, {"question": "\u5973\u6027\uff0c65\u5c81\u3002\u80c6\u56ca\u7ed3\u77f310\u5e74\uff0c\u8fd1\u6765\u53cd\u590d\u53d1\u4f5c\u80c6\u7ede\u75db\uff0c\u8981\u6c42\u624b\u672f\u6cbb\u7597\u3002\u6709\u9ad8\u8840\u538b\u3001\u7cd6\u5c3f\u75c5\u75c5\u53f2\uff0c\u670d\u836f\u8840\u538b\u7ef4\u6301\u5728150-160/85-95mmHg\uff0c\u7a7a\u8179\u8840\u7cd67.3mmol/L\u3002\u60a3\u8005\u672f\u524d\u5904\u7406\u63aa\u65bd\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u8840\u538b\u3001\u8840\u7cd6\u5747\u6b63\u5e38\u540e\u518d\u624b\u672f\nB. \u8840\u7cd6\u964d\u81f36.1mmol/L\u518d\u624b\u672f\nC. \u65e0\u9700\u7279\u6b8a\u5904\u7406\uff0c\u53ef\u624b\u672f\nD. \u8840\u538b\u964d\u81f3130/90mmHg\u518d\u624b\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.907631494722809, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5976948474882556, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5267\u70c8\u8fd0\u52a8\u65f6\u5c11\u5c3f\u7684\u4e3b\u8981\u539f\u56e0\u662f\nA. \u6297\u5229\u5c3f\u6fc0\u7d20\u5206\u6ccc\u589e\u591a\nB. \u80be\u5c0f\u7403\u6bdb\u7ec6\u8840\u7ba1\u8840\u538b\u589e\u9ad8\nC. \u919b\u56fa\u916e\u5206\u6ccc\u589e\u591a\nD. \u80be\u5c0f\u52a8\u8109\u6536\u7f29\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31815811094261875, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7512007300137884, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8c37\u7c7b\u4e2d\uff0c\u8102\u80aa\u542b\u91cf\u6700\u9ad8\u7684\u662f\nA. \u835e\u9ea6\nB. \u5c0f\u9ea6\nC. \u5927\u7c73\nD. \u9ad8\u7cb1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5404\u7ec4\u5b57\uff0c\u5168\u90fd\u662f\u8c61\u5f62\u5b57\u7684\u4e00\u7ec4\u662f\nA. \u9f4a\u679c\u5411\nB. \u83ab\u96de\u7709\nC. \u661f\u9010\u9e7f\nD. \u7384\u9f52\u77e2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.29660173325630934, "meta-math/MetaMath-Mistral-7B": 0.5513389852637862, "itpossible/Chinese-Mistral-7B-v0.1": 0.3468666740605408, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4792568976060359, "meta-llama/Meta-Llama-3-8B": 0.2963332999770349, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.38277148274919337}}, {"question": "\u8bd9\u8c10\u3001\u5e7d\u9ed8\u7684\u8868\u6f14\u5f62\u5f0f\uff0c\u8868\u73b0\u4e86\u52b3\u52a8\u4eba\u6c11\u7684\u751f\u6d3b\u573a\u666f\uff0c\u5bcc\u6709\u6d53\u539a\u7684\u4e61\u571f\u6c14\u606f\uff0c\u662f\u4e0b\u5217\u54ea\u4e00\u4e2a\u5e90\u5267\u5267\u76ee?\nA. \u300a\u738b\u5a46\u9a82\u9e21\u300b\nB. \u300a\u5c0f\u8f9e\u5e97\u300b\nC. \u300a\u6253\u6851\u300b\nD. \u300a\u6253\u7076\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3354456100429363, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u7f51\u5740www.sina.com\u4e2d.com\u662f\u6307\nA. \u516c\u5171\u7c7b\nB. \u5546\u4e1a\u7c7b\nC. \u6559\u80b2\u7c7b\nD. \u653f\u5e9c\u7c7b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5531771438132309, "meta-math/MetaMath-Mistral-7B": 0.8791150181798699, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6505757806413096, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9684540523451204}}, {"question": "\u54c1\u724c\u662f\u4e00\u4e2a\u96c6\u5408\u6982\u5ff5\uff0c\u82e5\u4e00\u4e2a\u54c1\u724c\u5df2\u83b7\u5f97\u4e13\u7528\u6743\u5e76\u53d7\u6cd5\u5f8b\u4fdd\u62a4\uff0c\u5219\u6307\u7684\u662f\nA. \u54c1\u724c\u540d\u79f0\nB. \u54c1\u724c\u6807\u5fd7\nC. \u5546\u6807\nD. \u54c1\u724c\u8d44\u4ea7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5482601488847574, "meta-math/MetaMath-Mistral-7B": 0.7476878557209593, "itpossible/Chinese-Mistral-7B-v0.1": 0.4606798815186379, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7079514150700609, "meta-llama/Meta-Llama-3-8B": 0.8706174950667596, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9957955315245569}}, {"question": "\u5173\u4e8e\u5f39\u529b\uff0c\u4e0b\u5217\u8bf4\u6cd5\u9519\u8bef\u7684\u662f\nA. \u76f8\u4e92\u63a5\u89e6\u7684\u7269\u4f53\u95f4\u4e0d\u4e00\u5b9a\u4ea7\u751f\u5f39\u529b\nB. \u538b\u529b\u3001\u652f\u6301\u529b\u3001\u62c9\u529b\u90fd\u5c5e\u4e8e\u5f39\u529b\nC. \u5f39\u529b\u4ec5\u4ec5\u662f\u6307\u5f39\u7c27\u5f62\u53d8\u65f6\u5bf9\u5176\u4ed6\u7269\u4f53\u7684\u4f5c\u7528\nD. \u5f39\u529b\u662f\u6307\u53d1\u751f\u5f39\u6027\u5f62\u53d8\u7684\u7269\u4f53\uff0c\u7531\u4e8e\u8981\u6062\u590d\u539f\u72b6\uff0c\u5bf9\u63a5\u89e6\u5b83\u7684\u7269\u4f53\u4ea7\u751f\u7684\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36959451779581176, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.38109590680712035, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.334240003630352, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5676280735677326}}, {"question": "\u4e0b\u9762\u6700\u6709\u53ef\u80fd\u6210\u4e3a\u7a00\u7f3a\u7269\u54c1\u7684\u662f\nA. \u9633\u5149\nB. \u7a7a\u6c14\nC. \u8fd0\u9001\u5230\u90d1\u5dde\u7684\u6d77\u6c34\nD. \u590f\u5a01\u5937\u7684\u6d77\u6c34\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3520821795518118, "meta-math/MetaMath-Mistral-7B": 0.5290611440040508, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.44746601790678886, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6844532998996045}}, {"question": "\u6837\u65b9\u4e2d\u79cd\u7fa4\u4e2a\u4f53\u5206\u6563\u5ea6(S2)\u5927\u4e8e\u5e73\u5747\u6570(m)\uff0c\u8fd9\u79cd\u5206\u5e03\u683c\u5c40\u79f0\u4e3a\nA. \u5747\u5300\u578b\nB. \u6210\u7fa4\u578b\nC. \u968f\u673a\u578b\nD. \u751f\u6001\u578b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5113634294185699, "meta-math/MetaMath-Mistral-7B": 0.8156002601280453, "itpossible/Chinese-Mistral-7B-v0.1": 0.3331832353540621, "HuggingFaceH4/zephyr-7b-beta": 0.8876136405443363, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8241587235636779, "meta-llama/Meta-Llama-3-8B": 0.41829519824338984, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5288379371730567}}, {"question": "\u5c06\u4e00\u5bf9\u7b49\u4f4d\u57fa\u56e0\u7684\u6742\u5408\u4f53\u505a\u4eb2\u672c\u9010\u4ee3\u81ea\u4ea4\u4e09\u6b21\uff0cF3\u4ee3\u7eaf\u5408\u4f53\u7684\u6bd4\u4f8b\u4e3a\nA. 3/4\nB. 15/16\nC. 7/8\nD. 1/8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33284604038304694, "meta-math/MetaMath-Mistral-7B": 0.5367333910342541, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6784312859864856, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4084129141920774, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65e2\u5c5e\u4e8e\u6211\u56fd\u6cd5\u5f8b\u6e0a\u6e90\u53c8\u5c5e\u4e8e\u90e8\u95e8\u6cd5\u7684\u662f\nA. \u884c\u653f\u6cd5\nB. \u5baa\u6cd5\nC. \u884c\u653f\u6cd5\u89c4\nD. \u6cd5\u5f8b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3994863046503028, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2a\u4f53\u751f\u7269\u5b66\u6027\u72b6\u7684\u7537\u5973\uff0c\u65e2\u5305\u62ec\u751f\u7269\u6027\u4e2a\u4f53\u4e5f\u5305\u62ec\nA. \u6587\u5316\u6027\u7fa4\u4f53\nB. \u793e\u4f1a\u6027\u7fa4\u4f53\nC. \u8ba4\u77e5\u6027\u7fa4\u4f53\nD. \u751f\u7269\u6027\u7fa4\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4551316031872074, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9126710861606316, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u6c34\u7a3b\u5bf9\u6e29\u3001\u5149\u6761\u4ef6\u7684\u53cd\u5e94\uff0c\u6c34\u7a3b\u662f\nA. \u4f4e\u6e29\u3001\u957f\u65e5\u4f5c\u7269\nB. \u9ad8\u6e29\u3001\u77ed\u65e5\u4f5c\u7269\nC. \u9ad8\u6e29\u3001\u957f\u65e5\u4f5c\u7269\nD. \u4f4e\u6e29\u3001\u77ed\u65e5\u4f5c\u7269\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.28012882262171335, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3582379679317443, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bc6\u522b\u771f\u74f6\u5047\u9152\u7684\u6b63\u786e\u505a\u6cd5\u662f\nA. \u770b\u5305\u88c5\uff0c\u771f\u74f6\u5047\u9152\u7684\u5305\u88c5\u76d2\u4e0a\u6709\u78e8\u635f\u7684\u75d5\u8ff9\uff0c\u9632\u4f2a\u5546\u6807\u6ca1\u6709\u7acb\u4f53\u611f\nB. \u4ee5\u4e0a\u505a\u6cd5\u90fd\u6b63\u786e\nC. \u6447\u9152\u74f6\uff0c\u91cc\u9762\u7684\u6db2\u4f53\u5f00\u59cb\u53d8\u6df7\u6d4a\uff0c\u6ca1\u6709\u771f\u9152\u900f\u4eae\nD. \u51d1\u8fd1\u9152\u74f6\uff0c\u80fd\u95fb\u5230\u7531\u4e8e\u74f6\u76d6\u5bc6\u5c01\u4e0d\u4e25\u800c\u6563\u53d1\u51fa\u7684\u9152\u9999\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5667450635900629, "meta-math/MetaMath-Mistral-7B": 0.8937571763839606, "itpossible/Chinese-Mistral-7B-v0.1": 0.6032070326160707, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8576279165686839, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8142214761551307}}, {"question": "\u6700\u80fd\u5207\u4e2d\u300a\u4e94\u4ee3\u53f2\u4f36\u5b98\u4f20\u5e8f\u300b\u2f00\u2f42\u4e2d\u2f3c\u8bba\u70b9\u7684\u2f00\u7ec4\u5bf9\u5e94\u8bcd\u8bed\u662f\nA. \u5fe7\u52b3\u4e0e\u9038\u8c6b\nB. \u5929\u547d\u4e0e\u2f08\u4e8b \nC. \u5174\u76db\u4e0e\u8870\u8d25\nD. \u2f83\u6ee1\u4e0e\u8c26\u865a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.34239623393788804, "itpossible/Chinese-Mistral-7B-v0.1": 0.39845853297483913, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2885095257630687, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f5c\u4e3a\u6c11\u65cf\u98ce\u5473\u98df\u54c1\u7684\u201c\u51b7\u9762\u201d\uff0c\u5c5e\nA. \u50a3\u65cf\u83dc\nB. \u5f5d\u65cf\u83dc\nC. \u82d7\u65cf\u83dc\nD. \u671d\u9c9c\u65cf\u83dc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6172709091345574, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2821833983601387, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u79e6\u5f8b\u300b\u7ec6\u5bc6\u4e25\u82db\uff0c\u5b9e\u884c\u201c\u8f7b\u7f6a\u91cd\u7f5a\u201d\uff1b\u53e4\u7f57\u9a6c\u300a\u6c11\u6cd5\u5927\u5168\u300b\u89c4\u5b9a\uff1a\u201c\u2026\u2026\u4e0d\u5f97\u57fa\u4e8e\u6000\u7591\u800c\u60e9\u7f5a\u4efb\u4f55\u4eba\u3002\u2026\u2026\u4e0e\u5176\u5224\u5904\u65e0\u7f6a\u4e4b\u4eba\uff0c\u4e0d\u5982\u5bb9\u8bb8\u7f6a\u72af\u9003\u8131\u60e9\u7f5a\u3002\u201d\u8fd9\u8868\u660e\u4e8c\nA. \u7acb\u6cd5\u7684\u76ee\u7684\u4e00\u81f4\nB. \u7acb\u6cd5\u7406\u5ff5\u4e0d\u540c\nC. \u90fd\u662f\u7ef4\u62a4\u5c01\u5efa\u738b\u671d\u7684\u5de5\u5177\nD. \u90fd\u8fdd\u80cc\u4e86\u516c\u5e73\u516c\u5f00\u516c\u6b63\u7684\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8240446439975473, "meta-math/MetaMath-Mistral-7B": 0.8495800609875055, "itpossible/Chinese-Mistral-7B-v0.1": 0.7160347582771457, "HuggingFaceH4/zephyr-7b-beta": 0.9991609304081777, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9455792011104652, "meta-llama/Meta-Llama-3-8B": 0.762449586959667, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.824639165687409}}, {"question": "\u6559\u5e08\u5728\u8bfe\u5802\u4e0a\u63d0\u95ee\u4e00\u4e9b\u6709\u96be\u5ea6\u7684\u95ee\u9898\u65f6\uff0c\u901a\u5e38\u4f1a\u4e0d\u7531\u81ea\u4e3b\u5730\u5c06\u773c\u5149\u505c\u7559\u5728\u90a3\u4e9b\u4f18\u79c0\u7684\u5b66\u751f\u8eab\u4e0a\u3002\u8fd9\u79cd\u73b0\u8c61\u53cd\u6620\u7684\u662f\nA. \u6295\u5c04\u6548\u5e94\nB. \u4ece\u4f17\u6548\u5e94\nC. \u6728\u6876\u6548\u5e94\nD. \u671f\u5f85\u6548\u5e94\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.47807600301307, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.532375080957042}}, {"question": "\u519c\u4e1a\u751f\u4ea7\u5916\u5ef6\u7684\u6269\u5927\u518d\u751f\u4ea7\u662f\uff08\uff09\u5f15\u8d77\u7684\u751f\u4ea7\u89c4\u6a21\u7684\u6269\u5927\nA. \u7ba1\u7406\u6c34\u5e73\u7684\u63d0\u9ad8\nB. \u79d1\u5b66\u6280\u672f\u6c34\u5e73\u7684\u63d0\u9ad8\nC. \u751f\u4ea7\u8981\u7d20\u6570\u91cf\u7684\u589e\u52a0\nD. \u52b3\u52a8\u751f\u4ea7\u7387\u7684\u63d0\u9ad8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3567047194030858, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5881915741820898, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4527911590776765, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9421734600678805}}, {"question": "\u67d0\u5973\uff0c65\u5c81\u3002\u54b3\u559815\u5e74\uff0c\u6bcf\u81f3\u51ac\u5b63\u52a0\u91cd\uff0c\u9762\u8272\u6de1\u767d\uff0c\u54b3\u58f0\u65e0\u529b\uff0c\u52a8\u5219\u6c14\u5598\uff0c\u75f0\u6e05\u7a00\u8272\u767d\uff0c\u56db\u80a2\u8f7b\u5ea6\u6d6e\u80bf\uff0c\u820c\u6de1\u82d4\u767d\uff0c\u8109\u5f31\u3002\u4e34\u5e8a\u8bca\u65ad\u6700\u53ef\u80fd\u662f\nA. \u80be\u4e0d\u7eb3\u6c14\u8bc1\nB. \u5fc3\u80ba\u6c14\u865a\u8bc1\nC. \u80ba\u6c14\u865a\u8bc1\nD. \u80ba\u80be\u6c14\u865a\u8bc1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u9762\u54ea\u4e2a\u529f\u80fd\u5c5e\u4e8e\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u529f\u80fd\nA. \u5bf9\u8ba1\u7b97\u673a\u7528\u6237\u8bbf\u95ee\u7cfb\u7edf\u548c\u8d44\u6e90\u7684\u60c5\u51b5\u8fdb\u884c\u8bb0\u5f55\nB. \u5b9e\u73b0\u4e3b\u673a\u548c\u5916\u8bbe\u7684\u5e76\u884c\u5904\u7406\u4ee5\u53ca\u5f02\u5e38\u60c5\u51b5\u7684\u5904\u7406\nC. \u4fdd\u62a4\u7cfb\u7edf\u7a0b\u5e8f\u548c\u4f5c\u4e1a\uff0c\u7981\u6b62\u4e0d\u5408\u8981\u6c42\u7684\u5bf9\u7a0b\u5e8f\u548c\u6570\u636e\u7684\u8bbf\u95ee\nD. \u63a7\u5236\u7528\u6237\u7684\u4f5c\u4e1a\u6392\u5e8f\u548c\u8fd0\u884c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9452409472031106, "meta-math/MetaMath-Mistral-7B": 0.9979979997388808, "itpossible/Chinese-Mistral-7B-v0.1": 0.921678309385809, "HuggingFaceH4/zephyr-7b-beta": 0.9997221362158039, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9755149051405414, "meta-llama/Meta-Llama-3-8B": 0.87956921316271, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9872410228096172}}, {"question": "\u5973\u6027\uff0c34 \u5c81\uff0c1 \u4e2a\u6708\u6765\u53d1\u70ed\uff0c\u4e4f\u529b\uff0c\u54b3\u55fd\uff0c1 \u5929\u6765\u5de6\u773c\u7a81\u7136\u5931\u660e\u3002\u67e5\u4f53\uff1aT37.9 \u5ea6\uff0c\u8109\u7387 96\u6b21/\u5206\uff0c\u8840\u538b 128/75mmHg\uff0c\u5de6\u773c\u89c6\u529b\u6d88\u5931\uff0c\u5fc3\u5c16\u90e8\u95fb\u53ca 3/6 \u7ea7\u6536\u7f29\u671f\u6742\u97f3\uff0c\u813e\u808b\u4e0b\u53ef\u53ca\u3002\u5316\u9a8c\uff1aHb 96g/L\uff0cWBC 10.1\u00d7109/L\uff0c\u5c3f\u86cb\u767d(+)\u3002\u786e\u8bca\u8be5\u60a3\u8005\u75be\u75c5\u6700\u91cd\u8981\u7684\u68c0\u67e5\u662f\nA. \u80be\u6d3b\u68c0\nB. \u6297 dsDNA \u6297\u4f53\nC. \u8840\u57f9\u517b\nD. \u8d85\u58f0\u5fc3\u52a8\u56fe\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b0\u6d6a\u7f512010\u5e7412\u670827\u65e5\u62a5\u9053\uff0c\u6709\u5173\u4e13\u5bb6\u8ba4\u4e3a\uff0c2011\u5e74\u4eba\u6c11\u5e01\u53ef\u80fd\u5c0f\u5e45\u5347\u503c\uff0c\u4f46\u5e76\u4e0d\u5f71\u54cd\u4eba\u6c11\u5e01\u7684\u7a33\u5b9a\u6027\u3002\u4eba\u6c11\u5e01\u5347\u503c\u610f\u5473\u7740 a\u4eba\u6c11\u5e01\u6c47\u7387\u4e0a\u5347 b\u4eba\u6c11\u5e01\u6c47\u7387\u4e0b\u964d c\u6709\u5229\u4e8e\u6211\u56fd\u7684\u5546\u54c1\u51fa\u53e3\uff0c\u4f46\u4e0d\u5229\u4e8e\u8fdb\u53e3\u5916\u56fd\u5546\u54c1 d\u6211\u56fd\u51fa\u53e3\u5546\u54c1\u7684\u7ade\u4e89\u529b\u4e0b\u964d\nA. bc\nB. bd\nC. ac\nD. ad\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2821833983601388, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ea7\u7ed2\u7684\u7f8a\u79cd\u662f\nA. \u7ec6\u6bdb\u7f8a\nB. \u534a\u6bdb\u7f8a\nC. \u5c71\u7f8a\nD. \u8089\u7f8a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4329196469892127, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\u4f30\u8ba1\u7ecf\u5e38\u4f7f\u7528MLE\uff08\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff09\u3002\u9762\u4e34\u7684\u4e00\u4e2a\u95ee\u9898\u662f\u6ca1\u6709\u51fa\u73b0\u7684\u9879\u6982\u7387\u4e3a0\uff0c\u8fd9\u6837\u4f1a\u5bfc\u81f4\u8bed\u8a00\u6a21\u578b\u7684\u6548\u679c\u4e0d\u597d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4f7f\u7528\uff08\uff09\nA. \u589e\u52a0\u767d\u566a\u97f3\nB. \u5e73\u6ed1\nC. \u968f\u673a\u63d2\u503c\nD. \u53bb\u566a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9854925307066114, "meta-math/MetaMath-Mistral-7B": 0.9995640769865417, "itpossible/Chinese-Mistral-7B-v0.1": 0.806745102504497, "HuggingFaceH4/zephyr-7b-beta": 0.9999661432348715, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9975948365145809, "meta-llama/Meta-Llama-3-8B": 0.9815191433800692, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9998820609328215}}, {"question": "\u300a\u65e5\u51fa\u5370\u8c61\u300b\u7684\u4f5c\u8005\u662f\u6cd5\u56fd\u8457\u540d\u5370\u8c61\u6d3e\u753b\u5bb6\nA. \u9ad8\u66f4\nB. \u83ab\u5948\nC. \u96f7\u8bfa\u963f\nD. \u51e1\u9ad8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9037800975732925, "meta-math/MetaMath-Mistral-7B": 0.9890548363815607, "itpossible/Chinese-Mistral-7B-v0.1": 0.8158315492946424, "HuggingFaceH4/zephyr-7b-beta": 0.9984623230568587, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8554900137745982, "meta-llama/Meta-Llama-3-8B": 0.6159436072826524, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u75c5\u6bd2\u4e2d\uff0c\u5c5e\u4e8e\u662f DNA \u75c5\u6bd2\u7684\u662f\nA. \u65b0\u57ce\u75ab\u75c5\u6bd2\nB. \u9a6c\u7acb\u514b\u6c0f\u75c5\u75c5\u6bd2\nC. \u53e3\u8e44\u75ab\u75c5\u6bd2\nD. \u79bd\u6d41\u611f\u75c5\u6bd2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.43589457557147204, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8690877104455633}}, {"question": "\u4e0b\u5217\u6709\u5173\u6cd5\u5b66\u7684\u8868\u8ff0\uff0c\u6b63\u786e\u7684\u6709\nA. \u6cd5\u5b66\u5c31\u5176\u5c5e\u6027\u6765\u8bf4\uff0c\u65e2\u5177\u6709\u793e\u4f1a\u79d1\u5b66\u7684\u6027\u8d28\uff0c\u53c8\u5177\u6709\u4eba\u6587\u79d1\u5b66\u7684\u6027\u8d28\nB. \u4ece\u8ba4\u8bc6\u8bba\u7684\u89d2\u5ea6\uff0c\u6cd5\u5b66\u53ef\u4ee5\u5206\u4e3a\u7acb\u6cd5\u5b66\u3001\u6cd5\u5f8b\u89e3\u91ca\u5b66\u3001\u6cd5\u5f8b\u793e\u4f1a\u5b66\u7b49\nC. \u7531\u4e8e\u5404\u56fd\u7684\u6cd5\u5f8b\u5177\u6709\u8f83\u5927\u7684\u5dee\u5f02\uff0c\u800c\u4e14\u6cd5\u5f8b\u5904\u5728\u4e0d\u65ad\u53d8\u52a8\u4e4b\u4e2d\uff0c\u56e0\u6b64\uff0c\u6cd5\u5b66\u4e0d\u53ef\u80fd\u5f62\u6210\u8f83\u4e3a\u5b8c\u6574\u7684\u77e5\u8bc6\u4f53\u7cfb\uff0c\u56e0\u800c\u7f3a\u4e4f\u79d1\u5b66\u6027\nD. \u6cd5\u5b66\u7684\u7814\u7a76\u5bf9\u8c61\u662f\u6cd5\u7684\u4ea7\u751f\u3001\u53d1\u5c55\u53ca\u5176\u89c4\u5f8b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.517223068527215, "meta-math/MetaMath-Mistral-7B": 0.6014521859162839, "itpossible/Chinese-Mistral-7B-v0.1": 0.8347794534995051, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6612030343429086, "meta-llama/Meta-Llama-3-8B": 0.8444830328981942, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u628a\u7ec4\u7ec7\u76ee\u6807\u5206\u89e3\u4e3a\u9879\u76ee\uff0c\u6309\u89c4\u5212\u7684\u9879\u76ee\u8fdb\u884c\u5206\u9636\u6bb5\u62e8\u6b3e\u7684\u9884\u7b97\u63a7\u5236\u65b9\u6cd5\u662f\nA. \u9879\u76ee\u9884\u7b97\nB. \u6295\u8d44\u9884\u7b97\nC. \u7ecf\u8425\u9884\u7b97\nD. \u96f6\u57fa\u9884\u7b97\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9201752322316326, "meta-math/MetaMath-Mistral-7B": 0.9934754761580799, "itpossible/Chinese-Mistral-7B-v0.1": 0.8998034235948925, "HuggingFaceH4/zephyr-7b-beta": 0.9989032457684246, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.790654590883283, "meta-llama/Meta-Llama-3-8B": 0.8159028121167115, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9862523971925293}}, {"question": "\u4ee5\u4e0b\u54ea\u4e24\u4e2a\u661f\u5ea7\u7684\u5929\u533a\u4e0d\u76f8\u90bb\nA. \u72ee\u5b50\u5ea7\u548c\u5c0f\u72ee\u5ea7\nB. \u5927\u72ac\u5ea7\u548c\u5c0f\u72ac\u5ea7\nC. \u53cc\u9c7c\u5ea7\u548c\u9cb8\u9c7c\u5ea7\nD. \u98de\u9a6c\u5ea7\u548c\u5c0f\u9a6c\u5ea7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2656046866868781, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9752\u5e74\u5927\u5b66\u751f\u7684\u4eba\u9645\u4ea4\u5f80\uff0c\u662f\u4ed6\u4eec\u8ba4\u8bc6\u81ea\u6211\u3001\u8ba4\u8bc6\u793e\u4f1a\u3001\u9002\u5e94\u73af\u5883\u548c\u793e\u4f1a\uff0c\u5c55\u73b0\u81ea\u6211\u3001\u5f00\u5c55\u81ea\u6211\uff0c\u5f62\u6210\u5065\u5168\u4eba\u683c\u7684\nA. \u91cd\u8981\u7279\u70b9\nB. \u6839\u672c\u7279\u70b9\nC. \u91cd\u8981\u9014\u5f84\nD. \u6839\u672c\u9014\u5f84\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f\nA. \u56db\u5ddd\u7a3b\u57ce\u7684\u9ad8\u6d77\u62d4\u5b87\u5b99\u7ebf\u89c2\u6d4b\u7ad9\uff08LHAASO\uff09\u4e3b\u8981\u63a2\u6d4b\u5b87\u5b99\u7ebf\u5f15\u53d1\u7684\u5927\u6c14\u7c07\u5c04\uff0c\u76ee\u6807\u662f\u8981\u63a2\u7d22\u9ad8\u80fd\u5b87\u5b99\u7ebf\u8d77\u6e90\u4ee5\u53ca\u76f8\u5173\u7684\u5b87\u5b99\u6f14\u5316\u3001\u9ad8\u80fd\u5929\u4f53\u6f14\u5316\u548c\u6697\u7269\u8d28\u7b49\nB. 2019\u5e744\u6708\u300a\u5929\u4f53\u7269\u7406\u5b66\u62a5\u901a\u4fe1\u300b\u4e0a\u520a\u767b\u7684\u6587\u7ae0\u79f0\uff0c\u6839\u636e\u54c8\u52c3\u7a7a\u95f4\u671b\u8fdc\u955c\u7684\u6d4b\u91cf\u6570\u636e\uff0c\u5b87\u5b99\u81a8\u80c0\u7684\u901f\u5ea6\u6bd4\u79d1\u5b66\u5bb6\u6839\u636e\u5b87\u5b99\u5927\u7206\u70b8\u540e\u8f68\u8ff9\u9884\u6d4b\u7684\u81a8\u80c0\u901f\u5ea6\u8981\u6162\nC. \u4e00\u9879\u65b0\u7684\u7814\u7a76\u8bc1\u5b9e\uff0c2019\u5e744\u670825\u65e5LIGO\u63a2\u6d4b\u5230\u7684\u5f15\u529b\u6ce2\u4e8b\u4ef6\u6781\u53ef\u80fd\u662f\u4e24\u4e2a\u4e2d\u5b50\u661f\u5408\u5e76\u7684\u7ed3\u679c\uff0c\u8be5\u7814\u7a76\u7531LIGO\u53caVirgo\u56e2\u961f\u5408\u4f5c\u5b8c\u6210\nD. 019\u5e747\u6708\uff0c\u96bc\u9e1f2\u53f7\u7b2c\u4e8c\u6b21\u6210\u529f\u964d\u843d\u5728\u201c\u9f99\u5bab\u201d\u4e0a\u91c7\u96c6\u5c0f\u884c\u661f\u5730\u4e0b\u6837\u672c\uff0c\u73b0\u5728\u5df2\u5e26\u7740\u73cd\u8d35\u7684\u6837\u672c\u8fd4\u822a\uff0c\u8ba1\u5212\u4e8e2020\u5e74\u5e74\u5e95\u8fd4\u56de\u5730\u7403\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3610488075191981, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.37030807523919834, "HuggingFaceH4/zephyr-7b-beta": 0.9798958029271893, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5220127371890727, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8637773378519286}}, {"question": "\u8003\u53e4\u5b66\u7814\u7a76\u8981\u5145\u5206\u7ed3\u5408\u6587\u732e\u8bb0\u8f7d\uff0c\u5728\u5386\u53f2\u65f6\u4ee3\u8003\u53e4\u5b66\u7684\u7814\u7a76\u4e2d\u5c24\u5176\u5982\u6b64\u3002\u4e2d\u56fd\u53e4\u4ee3\u6587\u732e__\uff0c\u81ea\u5f53\u6309\u5404\u4eba\u7684\u4e13\u4e1a\u9700\u6c42\uff0c\u62e9\u8981\u9605\u8bfb\u3002\u8981\u7d27\u7684\u662f\u5fc5\u987b\u61c2\u5f97\u6587\u732e\u53f2\u3001\u76ee\u5f55\u5b66\u7b49\uff0c\u4ee5\u4fbf\u5728\u7e41\u591a\u7684\u53e4\u7c4d\u4e2d\u5bfb\u53d6\u786e\u5207\u76f8\u5173\u7684\u8bb0\u8f7d\uff0c\u52a0\u4ee5__\u3002\u4f9d\u6b21\u586b\u5165\u5212\u6a2a\u7ebf\u90e8\u5206\u6700\u6070\u5f53\u7684\u4e00\u9879\u662f\nA. \u6052\u6cb3\u6c99\u6570\u5ba1\u6838\nB. \u6bd4\u6bd4\u7686\u662f\u8003\u5bdf\nC. \u6c57\u725b\u5145\u680b\u67e5\u8bc1\nD. \u6d69\u5982\u70df\u6d77\u8003\u6838\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6801898634041632, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4e0b\u5217\u8bd7\u8bcd\u8574\u542b\u7684\u5316\u5b66\u539f\u7406\u89e3\u91ca\u9519\u8bef\u7684\u662f\nA. \u6d2a\u7089\u7167\u7834\u591c\u6c89\u6c89\u2014\u2014\u71c3\u70e7\u91ca\u653e\u4e8c\u6c27\u5316\u78b3\nB. \u65e5\u7167\u9999\u7089\u751f\u7d2b\u70df\u2014\u2014\u591a\u73af\u82b3\u9999\u70c3\u7684\u5347\u534e\nC. \u8721\u70db\u6210\u7070\u6cea\u59cb\u5e72\u2014\u2014\u70c3\u7c7b\u7684\u4e0d\u5b8c\u5168\u71c3\u70e7\nD. \u7206\u7af9\u58f0\u4e2d\u4e00\u5c81\u9664\u2014\u2014\u7206\u70b8\u4ea7\u751f\u4e8c\u6c27\u5316\u786b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u636e\u53f6\u5fb7\u8f89\u300a\u4e66\u6797\u6e05\u8bdd\u300b\uff0c\u4e94\u4ee3\u540e\u5510\u65f6\uff0c\u5728\u5bb0\u76f8\u51af\u9053\u4e3b\u6301\u4e0b\uff0c\u5f00\u59cb\u5c06\u5112\u5bb6\u201c\u4e5d\u7ecf\u201d\u6821\u52d8\u540e\u523b\u7248\u5370\u5237\u3002\u5b8b\u521d\u56fd\u5b50\u76d1\u6709\u4e66\u7248\u56db\u5343\uff0c\u81f3\u771f\u5b97\u666f\u5fb7\u4e8c\u5e74\uff0c\u4e66\u7248\u5267\u589e\u81f3\u5341\u4e07\u3002\u6b64\u5916\u4e2d\u592e\u5d07\u6587\u9662\u3001\u53f8\u5929\u76d1\u3001\u79d8\u4e66\u76d1\u7b49\u673a\u6784\u4e5f\u90fd\u5927\u91cf\u523b\u4e66\u3002\u5b8b\u671d\u4e66\u574a\u904d\u53ca\u5168\u56fd\u5404\u5730\uff0c\u6240\u552e\u4e66\u7c4d\u5927\u591a\u7cbe\u96d5\u7ec6\u6821\u3002\u7531\u6b64\u63a8\u65ad\nA. \u96d5\u7248\u5370\u5237\u9650\u7528\u4e8e\u5b98\u65b9\u523b\u4e66\nB. \u6d3b\u5b57\u5370\u5237\u5df2\u53d6\u4ee3\u96d5\u7248\u5370\u5237\nC. \u5bb0\u76f8\u51af\u9053\u53d1\u660e\u96d5\u7248\u5370\u5237\u672f\nD. \u96d5\u7248\u5370\u5237\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7773041852853202, "meta-math/MetaMath-Mistral-7B": 0.9170014952809257, "itpossible/Chinese-Mistral-7B-v0.1": 0.7861538223968025, "HuggingFaceH4/zephyr-7b-beta": 0.9987377952276174, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9573131419977502, "meta-llama/Meta-Llama-3-8B": 0.9327526300229667, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9402124602418902}}, {"question": "\u4ee5\u4e0b\u9009\u9879\u4e2d\uff0c\u4e0e\u201c\u4e2d\u56fd\u4e00\u9999\u6e2f\u201d\u7684\u903b\u8f91\u5173\u7cfb\u76f8\u540c\u7684\u662f\nA. \u592a\u539f\u2014\u5c71\u897f\nB. \u5317\u4eac\u2014\u627f\u5fb7\nC. \u5b81\u590f\u2014\u94f6\u5ddd\nD. \u65b0\u7586\u4e00\u897f\u85cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3342400036303519, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.308893672445605}}, {"question": "\u5173\u4e8e\u5f53\u4ee3\u4e2d\u56fd\u7684\u6cd5\u5f8b\u4f53\u7cfb\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u6709\nA. \u52b3\u52a8\u6cd5\u3001\u73af\u5883\u6cd5\u3001\u6559\u80b2\u6cd5\u90fd\u5c5e\u4e8e\u793e\u4f1a\u6cd5\u90e8\u95e8\nB. \u884c\u653f\u6cd5\u5c31\u662f\u7531\u6240\u6709\u884c\u653f\u6cd5\u89c4\u6240\u7ec4\u6210\u7684\u4e00\u4e2a\u6cd5\u5f8b\u90e8\u95e8\nC. \u8bc9\u8bbc\u6cd5\u662f\u8c03\u6574\u8bc9\u8bbc\u6d3b\u52a8\u4e2d\u7684\u8bc9\u8bbc\u6cd5\u5f8b\u5173\u7cfb\u7684\u603b\u79f0\nD. \u7ecf\u6d4e\u6cd5\u662f\u8c03\u6574\u7ecf\u6d4e\u5173\u7cfb\u7684\u6cd5\u5f8b\u89c4\u8303\u7684\u603b\u79f0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3278389739895176, "itpossible/Chinese-Mistral-7B-v0.1": 0.5335280160970005, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.36815854997965053, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u79cd\u884c\u4e3a\u4f1a\u53d7\u5230200\u5143\u4ee5\u4e0a2000\u5143\u4ee5\u4e0b\u7f5a\u6b3e\uff0c\u5e76\u5904\u540a\u9500\u673a\u52a8\u8f66\u9a7e\u9a76\u8bc1\nA. \u9a7e\u8f66\u6ca1\u5e26\u9a7e\u9a76\u8bc1\nB. \u8d85\u8fc7\u89c4\u5b9a\u65f6\u901f50%\nC. \u9020\u6210\u4ea4\u901a\u4e8b\u6545\u540e\u9003\u9038\nD. \u8fdd\u53cd\u9053\u8def\u901a\u884c\u89c4\u5b9a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2010\u5e7412\u6708\uff0c\u4e2d\u592e\u7ecf\u6d4e\u5de5\u4f5c\u4f1a\u8bae\u5728\u5317\u4eac\u53ec\u5f00\u3002\u4f1a\u8bae\u6307\u51fa\uff0c2010\u5e74\u6765\uff0c\u56fd\u6c11\u7ecf\u6d4e\u8fd0\u884c\u603b\u4f53\u5e73\u8861\uff0c\u4fdd\u6301\u4e86\u8f83\u5feb\u53d1\u5c55\uff0c\u8fd9\u4e9b\u6210\u7ee9\u7684\u53d6\u5f97\u662f\u9762\u5bf9\u4e25\u5cfb\u590d\u6742\u7684\u56fd\u5185\u5916\u73af\u5883\uff0c\u4e2d\u592e\u679c\u65ad\u51b3\u7b56\uff0c\u6709\u6548\u5e94\u5bf9\u7684\u7ed3\u679c\u3002\u8fd9\u8868\u660e\nA. \u6211\u56fd\u80fd\u591f\u5b9e\u65bd\u5f3a\u6709\u529b\u7684\u5b8f\u89c2\u8c03\u63a7\nB. \u6211\u56fd\u7684\u793e\u4f1a\u4e3b\u4e49\u5e02\u573a\u7ecf\u6d4e\u4f53\u5236\u9700\u8981\u8fdb\u4e00\u6b65\u5b8c\u5584\nC. \u6211\u56fd\u7ecf\u6d4e\u53d7\u56fd\u9645\u56e0\u7d20\u7684\u5f71\u54cd\u8f83\u5c0f\nD. \u56fd\u5bb6\u653f\u7b56\u8c03\u63a7\u662f\u5e02\u573a\u5e73\u8861\u8fd0\u884c\u7684\u51b3\u5b9a\u6027\u56e0\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3735476652043149, "HuggingFaceH4/zephyr-7b-beta": 0.6064368328216266, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47511784666703544, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4655301870109041}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u4e3b\u72af\u548c\u9996\u8981\u5206\u5b50\u7684\u7406\u89e3\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u4e3b\u72af\u4e00\u5b9a\u662f\u9996\u8981\u5206\u5b50\nB. \u9996\u8981\u5206\u5b50\u53ea\u5b58\u5728\u4e8e\u5171\u540c\u72af\u7f6a\u4e2d\nC. \u4e3b\u72af\u4e0d\u4e00\u5b9a\u662f\u9996\u8981\u5206\u5b50\uff0c\u9996\u8981\u5206\u5b50\u4e0d\u4e00\u5b9a\u662f\u4e3b\u72af\nD. \u9996\u8981\u5206\u5b50\u4e00\u5b9a\u662f\u4e3b\u72af\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9563777725752303, "meta-math/MetaMath-Mistral-7B": 0.9980125032088651, "itpossible/Chinese-Mistral-7B-v0.1": 0.8142673659258245, "HuggingFaceH4/zephyr-7b-beta": 0.9999881265575715, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9882073304986468, "meta-llama/Meta-Llama-3-8B": 0.969082946684277, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9982031784101625}}, {"question": "\u5728\u63cf\u8ff0\u5851\u6599\u5927\u68da\u5185\u6c14\u4f53\u53d8\u5316\u7684\u4e0b\u5217\u65b9\u5f0f\u4e2d\uff0c\u6b63\u786e\u7684\u662f\nA. \u767d\u5929CO_{2}\u4f4e\uff0cO_{2}\u4f4e\nB. \u767d\u5929CO_{2}\u9ad8\uff0cO_{2}\u4f4e\nC. \u591c\u95f4CO_{2}\u4f4e\uff0cO_{2}\u9ad8\nD. \u591c\u95f4CO_{2}\u9ad8\uff0cO_{2}\u4f4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.509178511437664, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0e\u4e2d\u56fd\u5728\u9ec4\u5ca9\u5c9b\u95ee\u9898\u4e0a\u6709\u9886\u571f\u4e89\u8bae\u7684\u56fd\u5bb6\u662f\nA. \u6cf0\u56fd\nB. \u83f2\u5f8b\u5bbe\nC. \u9a6c\u6765\u897f\u4e9a\nD. \u8d8a\u5357\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5996159042728292, "meta-math/MetaMath-Mistral-7B": 0.7181548921038318, "itpossible/Chinese-Mistral-7B-v0.1": 0.541779116340639, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8322548075385492, "meta-llama/Meta-Llama-3-8B": 0.5818229325546384, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9932801804201892}}, {"question": "\u4e0b\u5217\u5c11\u6570\u6c11\u65cf\u4e2d\u4e3b\u8981\u751f\u6d3b\u5728\u6211\u56fd\u56db\u5927\u7267\u533a\u7684\u662f\nA. \u85cf\u65cf\u3001\u671d\u9c9c\u65cf\u3001\u9ad8\u5c71\u65cf\nB. \u54c8\u8428\u514b\u65cf\u3001\u8499\u53e4\u65cf\u3001\u85cf\u65cf\nC. \u7ef4\u543e\u5c14\u65cf\u3001\u58ee\u65cf\u3001\u4f97\u65cf\nD. \u8499\u53e4\u65cf\u3001\u571f\u5bb6\u65cf\u3001\u82d7\u65cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6450534652488301, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8741928756375771, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9900589417500607}}, {"question": "\u4e0b\u6e17\u5bb9\u91cf(\u80fd\u529b) \u66f2\u7ebf\uff0c\u662f\u6307[ ]\u3002\nA. \u964d\u96e8\u671f\u95f4\u7684\u571f\u58e4\u4e0b\u6e17\u8fc7\u7a0b\u7ebf\nB. \u5e72\u71e5\u7684\u571f\u58e4\u5728\u5145\u5206\u4f9b\u6c34\u6761\u4ef6\u4e0b\u7684\u4e0b\u6e17\u8fc7\u7a0b\u7ebf\nC. \u5145\u5206\u6e7f\u6da6\u540e\u7684\u571f\u58e4\u5728\u964d\u96e8\u671f\u95f4\u7684\u4e0b\u6e17\u8fc7\u7a0b\u7ebf\nD. \u571f\u58e4\u7684\u4e0b\u6e17\u7d2f\u79ef\u8fc7\u7a0b\u7ebf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33354054134886263, "meta-math/MetaMath-Mistral-7B": 0.47343782189047545, "itpossible/Chinese-Mistral-7B-v0.1": 0.5152277915574198, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.31031319312730204, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e4b\u6240\u4ee5\u8981\u4f7f\u80a1\u4efd\u5236\u6210\u4e3a\u516c\u6709\u5236\u7684\u4e3b\u8981\u5b9e\u73b0\u5f62\u5f0f\uff0c\u662f\u56e0\u4e3a\u80a1\u4efd\u5236\nA. \u662f\u516c\u6709\u5236\u7684\u6700\u597d\u5b9e\u73b0\u5f62\u5f0f\nB. \u6709\u5229\u4e8e\u6269\u5927\u516c\u6709\u8d44\u672c\u7684\u652f\u914d\u8303\u56f4\uff0c\u5de9\u56fa\u516c\u6709\u5236\u7684\u4e3b\u4f53\u5730\u4f4d\nC. \u5177\u6709\u660e\u663e\u7684\u79c1\u6709\u6027\u8d28\nD. \u5177\u6709\u660e\u663e\u7684\u516c\u6709\u5236\u6027\u8d28\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4693395078960189, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7777490824594336, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6749672790100275, "meta-llama/Meta-Llama-3-8B": 0.9460292523698088, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9884682320553828}}, {"question": "\u4e00\u9879\u6761\u7ea6\u6709\u610f\u4e3a\u7b2c\u4e09\u56fd\u521b\u8bbe\u4e00\u9879\u4e49\u52a1\uff0c\u82e5\u8981\u8be5\u7b2c\u4e09\u56fd\u627f\u62c5\u8be5\u9879\u4e49\u52a1\uff0c\u5fc5\u987b\u7531\nA. \u8be5\u7b2c\u4e09\u56fd\u81f3\u5c11\u8981\u9ed8\u793a\u63a5\u53d7\nB. \u8be5\u7b2c\u4e09\u56fd\u4ee5\u4e66\u9762\u660e\u793a\u63a5\u53d7\nC. \u8be5\u7b2c\u4e09\u56fd\u660e\u793a\u63a5\u53d7\nD. \u8be5\u7b2c\u4e09\u56fd\u660e\u793a\u6216\u9ed8\u793a\u63a5\u53d7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5341\u4e03\u4e16\u7eaa\u8377\u5170\u7684\u7ed8\u753b\u827a\u672f\u5c5e\u4e8e\nA. \u5e02\u6c11\u6587\u5316\nB. \u5bab\u5ef7\u6587\u5316\nC. \u5c01\u5efa\u6587\u5316\nD. \u8d35\u65cf\u6587\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4755910255341438, "meta-math/MetaMath-Mistral-7B": 0.9469001099315859, "itpossible/Chinese-Mistral-7B-v0.1": 0.5005725371118572, "HuggingFaceH4/zephyr-7b-beta": 0.6707709469687242, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6633024774516433, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9883883965080398}}, {"question": "\u5173\u4e8e\u79cd\u7fa4\u548c\u7fa4\u843d\u7684\u53d9\u8ff0\uff0c\u6b63\u786e\u7684\u662f\nA. \u4e3a\u6301\u7eed\u83b7\u5f97\u6700\u5927\u7684\u6355\u635e\u91cf\uff0c\u5e94\u4f7f\u88ab\u6355\u9c7c\u7fa4\u7684\u79cd\u7fa4\u6570\u91cf\u7ef4\u6301\u5728K/2\u6c34\u5e73\nB. \u7fa4\u843d\u6f14\u66ff\u8fc7\u7a0b\u4e2d\uff0c\u5730\u8863\u7b49\u4f4e\u7b49\u751f\u7269\u9010\u6e10\u6d88\u5931\nC. \u9884\u6d4b\u9ed1\u7ebf\u59ec\u9f20\u79cd\u7fa4\u6570\u91cf\u53d8\u5316\u8d8b\u52bf\u7684\u4e3b\u8981\u4f9d\u636e\u662f\u6027\u522b\u6bd4\u4f8b\nD. \u8349\u539f\u4e2d\uff0c\u7fa4\u843d\u53ea\u5b58\u5728\u6c34\u5e73\u7ed3\u6784\u4e0d\u5b58\u5728\u5782\u76f4\u7ed3\u6784\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.36297498834728353, "meta-math/MetaMath-Mistral-7B": 0.4939335289634524, "itpossible/Chinese-Mistral-7B-v0.1": 0.3550009601731296, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.308176776288574, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u5b8c\u5168\u7ade\u4e89\u7684\u5e02\u573a\u6761\u4ef6\u4e0b\uff0c\u51b3\u5b9a\u5de5\u8d44\u7684\u57fa\u7840\u662f\nA. \u52b3\u52a8\u529b\u4f9b\u7ed9\u5f39\u6027\nB. \u52b3\u52a8\u529b\u9700\u6c42\u5f39\u6027\nC. \u52b3\u52a8\u4ef7\u503c\nD. \u52b3\u52a8\u529b\u4ef7\u503c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u89c2\u6d4b\u573a\u5730\uff0c\u4f60\u4f1a\u770b\u5230\u6709\u7684\u7231\u597d\u8005\u50cf\u6d77\u76d7\u4e00\u6837\u7528\u773c\u7f69\u6321\u7740\u4e00\u53ea\u60ef\u7528\u773c\uff0c\u4ed6\u4eec\u662f\u5728\nA. \u953b\u70bc\u773c\u775b\u7684\u5bf9\u7126\u80fd\u529b\nB. \u5728\u5404\u5904\u8d70\u52a8\u65f6\u4fdd\u62a4\u6697\u591c\u89c6\u529b\nC. \u89c2\u6d4b\u9014\u4e2d\u8ba9\u773c\u775b\u653e\u677e\u4f11\u606f\nD. \u5728\u73a9\u89d2\u8272\u626e\u6f14\u6e38\u620f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.46780220057919447, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.685602950243673, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u5df4\u9ece\u5723\u6bcd\u9662\u300b\u7684\u5efa\u7b51\u5f62\u5f0f\u5c5e\u4e8e\nA. \u62dc\u5360\u5ead\u5f0f\u5efa\u7b51\nB. \u54e5\u7279\u5f0f\u5efa\u7b51\nC. \u7f57\u9a6c\u5f0f\u5efa\u7b51\nD. \u62c9\u4e01\u5f0f\u5efa\u7b51\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8795692273686554, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4101046003501243, "meta-llama/Meta-Llama-3-8B": 0.7380061698527718, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9869356841352324}}, {"question": "\u67d0\u79d1\u7814\u5355\u4f4d\u7684\u7ec4\u7ec7\u673a\u6784\u5206\u4e3a\u4e24\u7c7b\uff0c\u4e00\u7c7b\u662f\u4f20\u7edf\u7684\u804c\u80fd\u7ba1\u7406\u673a\u6784\uff0c\u4e00\u7c7b\u662f\u4e3a\u5b8c\u6210\u7279\u5b9a\u7814\u7a76\u4efb\u52a1\u800c\u8bbe\u7f6e\u7684\u4e13\u95e8\u4efb\u52a1\u5c0f\u7ec4\u3002\u5176\u4e2d\uff0c\u804c\u80fd\u7ba1\u7406\u673a\u6784\u4e3a\u5e38\u8bbe\u673a\u6784\uff0c\u4e13\u95e8\u4efb\u52a1\u5c0f\u7ec4\u4e3a\u975e\u5e38\u8bbe\u673a\u6784\u3002\u8fd9\u4e24\u79cd\u673a\u6784\u540c\u65f6\u5b58\u5728\u7684\u7ba1\u7406\u5f62\u6001\u662f\nA. \u804c\u80fd\u5236\nB. \u77e9\u9635\u5236\nC. \u591a\u7ef4\u5236\nD. \u76f4\u7ebf\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7687638596879403, "meta-math/MetaMath-Mistral-7B": 0.9699322835529367, "itpossible/Chinese-Mistral-7B-v0.1": 0.6941791061712363, "HuggingFaceH4/zephyr-7b-beta": 0.9761573661713339, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8613256779634564, "meta-llama/Meta-Llama-3-8B": 0.8998216845668686, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9959444619560441}}, {"question": "\u5728\u65e0\u4ea7\u9636\u7ea7\u65b0\u95fb\u53f2\u4e0a\uff0c\u7b2c\u4e00\u4e2a\u660e\u786e\u4f7f\u7528\u62a5\u520a\u5de5\u4f5c\u201c\u515a\u6027\u201d\u6982\u5ff5\uff0c\u5e76\u5bf9\u5176\u5185\u5bb9\u4f5c\u51fa\u7cfb\u7edf\u8bba\u8ff0\u7684\u662f\nA. \u6bdb\u6cfd\u4e1c\nB. \u9a6c\u514b\u601d\nC. \u6069\u683c\u65af\nD. \u5217\u5b81\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.38822592917159904, "meta-math/MetaMath-Mistral-7B": 0.6387731294104964, "itpossible/Chinese-Mistral-7B-v0.1": 0.493091943319357, "HuggingFaceH4/zephyr-7b-beta": 0.9650912468432148, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7392241647101329, "meta-llama/Meta-Llama-3-8B": 0.5280592543921383, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f53\u4ee3\u7f8e\u56fd\u653f\u6cbb\u7406\u8bba\u5bb6\u7f57\u4f2f\u7279\u00b7\u8fbe\u5c14\u6307\u51fa\uff1a\u201c\u5728\u6c11\u4e3b\u56fd\u5bb6\uff0c\u653f\u6cbb\u548c\u5b98\u50da\u7cbe\u82f1\u529b\u91cf\u56fa\u7136\u5f3a\u5927\uff0c\u8fdc\u80dc\u4e8e\u666e\u901a\u516c\u6c11\uff0c\u4f46\u4ed6\u4eec\u8fd8\u4e0d\u662f\u4e13\u5236\u541b\u4e3b\u3002\u201d\u8fd9\u8868\u660e\nA. \u653f\u6cbb\u7cbe\u82f1\u5371\u5bb3\u516c\u6c11\u57fa\u672c\u6743\u529b\nB. \u201c\u4e3b\u6743\u5728\u6c11\u201d\u7684\u89c2\u5ff5\u4e0d\u65ad\u5f3a\u5316\nC. \u6c11\u4e3b\u56fd\u5bb6\u96be\u4ee5\u907f\u514d\u541b\u4e3b\u4e13\u5236\nD. \u653f\u6cbb\u548c\u5b98\u50da\u7cbe\u82f1\u6743\u529b\u53d7\u5230\u5236\u7ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5562562863599914, "meta-math/MetaMath-Mistral-7B": 0.9304526316631015, "itpossible/Chinese-Mistral-7B-v0.1": 0.49646498696433045, "HuggingFaceH4/zephyr-7b-beta": 0.9804613813232161, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6701841305410265, "meta-llama/Meta-Llama-3-8B": 0.8936697179294143, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9548213525306943}}, {"question": "1947~1948\u5e74\uff0c\u7f8e\u56fd\u90e8\u5206\u5370\u7b2c\u5b89\u4eba\u90e8\u65cf\u9762\u4e34\u9965\u8352\uff0c\u7f8e\u56fd\u653f\u5e9c\u62d2\u7edd\u63d0\u4f9b\u6551\u6d4e\uff0c\u56e0\u4e3a\u6709\u4eba\u6307\u63a7\u4ed6\u4eec\u90e8\u65cf\u516c\u793e\u7684\u751f\u6d3b\u65b9\u5f0f\u662f\u5171\u4ea7\u4e3b\u4e49\u5f0f\u7684\u800c\u4e0d\u662f\u7f8e\u56fd\u5f0f\u7684\u3002\u8fd9\u53cd\u6620\u51fa\nA. \u6267\u653f\u8005\u529b\u56fe\u91cd\u5851\u56fd\u5bb6\u7cbe\u795e\nB. \u4e09\u6743\u5206\u7acb\u4f53\u5236\u5b58\u5728\u91cd\u5927\u7f3a\u9677\nC. \u56fd\u5bb6\u5bf9\u7ecf\u6d4e\u7684\u5e72\u9884\u52a0\u5f3a\nD. \u610f\u8bc6\u5f62\u6001\u5f71\u54cd\u653f\u5e9c\u653f\u7b56\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6599306977623481, "meta-math/MetaMath-Mistral-7B": 0.8696705655209278, "itpossible/Chinese-Mistral-7B-v0.1": 0.5653822048873248, "HuggingFaceH4/zephyr-7b-beta": 0.9995257649150553, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9573313379155812, "meta-llama/Meta-Llama-3-8B": 0.7033615635512637, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7048054615253566}}, {"question": "\u300aE\u5c0f\u8c03\u7b2c\u4e5d\u4ea4\u54cd\u66f2\uff08\u81ea\u65b0\u5927\u9646\uff09\u300b\u7684\u4f5c\u66f2\u5bb6\nA. \u5fb7\u6c83\u590f\u514b\nB. \u8d1d\u591a\u82ac\nC. \u683c\u4ec0\u6e29\nD. \u67f4\u79d1\u592b\u65af\u57fa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4596151413217764, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7446049701021908, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7815499840368519}}, {"question": "\u5173\u4e8e\u5211\u4e8b\u8d23\u4efb\u4e0e\u5211\u7f5a\u7684\u5173\u7cfb\uff0c\u4e0b\u5217\u8868\u8ff0\u9519\u8bef\u7684\u662f\nA. \u5211\u4e8b\u8d23\u4efb\u7684\u5b58\u5728\u548c\u5927\u5c0f\u76f4\u63a5\u51b3\u5b9a\u5211\u7f5a\u7684\u6709\u65e0\u548c\u8f7b\u91cd\nB. \u5211\u4e8b\u8d23\u4efb\u968f\u5b9e\u65bd\u72af\u7f6a\u800c\u4ea7\u751f\uff0c\u5211\u7f5a\u5219\u968f\u6cd5\u9662\u7684\u5b9a\u7f6a\u5224\u5211\u51b3\u5b9a\u5ba3\u544a\u751f\u6548\u800c\u51fa\u73b0\nC. \u5211\u4e8b\u8d23\u4efb\u5fc5\u987b\u901a\u8fc7\u5211\u7f5a\u6765\u5b9e\u73b0\nD. \u5211\u4e8b\u8d23\u4efb\u4f53\u73b0\u7684\u662f\u72af\u7f6a\u4eba\u5e94\u53d7\u5211\u4e8b\u60e9\u7f5a\u6027\uff0c\u5211\u7f5a\u4f53\u73b0\u7684\u662f\u72af\u7f6a\u4eba\u5b9e\u53d7\u5211\u4e8b\u60e9\u7f5a\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39368781016163396, "meta-math/MetaMath-Mistral-7B": 0.4159628043372691, "itpossible/Chinese-Mistral-7B-v0.1": 0.5451708392136039, "HuggingFaceH4/zephyr-7b-beta": 0.5896504828932734, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5635696118029061, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5748997575743954}}, {"question": "\u520a\u767b\u5e7f\u544a\u6240\u65bd\u7684\u4f20\u64ad\u5f71\u54cd\u662f\nA. \u81ea\u7531\u9009\u62e9\u7684\nB. \u4efb\u610f\u6027\u7684\nC. \u5f3a\u5236\u6027\u7684\nD. \u6709\u610f\u56fe\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.7526151274490804, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9990093502484726, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5540052805157857, "meta-llama/Meta-Llama-3-8B": 0.5052963379994225, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3500289379564213}}, {"question": "\u4f9b\u517b\u7236\u6bcd\u662f\u5c5e\u4e8e\u54ea\u79cd\u798f\u7530\nA. \u60b2\u7530\nB. \u656c\u7530\nC. \u6069\u7530\nD. \u5b5d\u7530\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u7535\u573a\u548c\u78c1\u573a\u7684\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u5728\u78c1\u573a\u4e2d\u78c1\u611f\u5e94\u5f3a\u5ea6\u4e0d\u4e3a\u96f6\u5904\u7684\u901a\u7535\u5bfc\u7ebf\u4e00\u5b9a\u53d7\u5230\u78c1\u573a\u529b\u7684\u4f5c\u7528\nB. \u7535\u573a\u4e2d\u5b58\u5728\u7535\u573a\u7ebf\uff0c\u7535\u573a\u7ebf\u4ece\u8d1f\u7535\u8377\u51fa\u53d1\uff0c\u7ec8\u6b62\u4e8e\u6b63\u7535\u8377\nC. \u5728\u7535\u573a\u4e2d\u7535\u573a\u5f3a\u5ea6\u4e0d\u4e3a\u96f6\u5904\u7684\u7535\u8377\u4e00\u5b9a\u53d7\u5230\u7535\u573a\u529b\u7684\u4f5c\u7528\nD. \u78c1\u573a\u4e2d\u5b58\u5728\u78c1\u611f\u7ebf\uff0c\u78c1\u611f\u7ebf\u4ece\u78c1\u4f53\u7684N\u6781\u51fa\u53d1\uff0c\u7ec8\u6b62\u4e8eS\u6781\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5550716681002751, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8075921654101018, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.477731327878982}}, {"question": "\u9a8c\u8bc1\u75c5\u56e0\u5047\u8bbe\u6700\u53ef\u9760\u7684\u65b9\u6cd5\u662f\nA. \u73b0\u51b5\u8c03\u67e5\nB. \u62bd\u6837\u8c03\u67e5\nC. \u961f\u5217\u7814\u7a76\nD. \u5b9e\u9a8c\u7814\u7a76\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8759422367991849, "meta-math/MetaMath-Mistral-7B": 0.9049822869138286, "itpossible/Chinese-Mistral-7B-v0.1": 0.6962816361116786, "HuggingFaceH4/zephyr-7b-beta": 0.9996392637402978, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9931209104490751, "meta-llama/Meta-Llama-3-8B": 0.9389438457203942, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9335730600991738}}, {"question": "\u8bbe$\\bigtriangleup ABC$\u7684\u5185\u89d2A\uff0cB\uff0cC\u6240\u5bf9\u8fb9\u7684\u957f\u5206\u522b\u662fa\uff0cb\uff0cc\uff0ctan(A+B)=-2sinC, c=3, $\\bigtriangleup ABC$\u7684\u5468\u957f\u53d6\u503c\u8303\u56f4\u662f\nA. $(6,3+2\\sqrt{3}]$\nB. (6,9]\nC. [6,9]\nD. $[6,3+2\\sqrt{3}]$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8ba1\u7b97\u673a\u7f51\u7edc\u6700\u65e9\u51fa\u73b0\u5728\u54ea\u4e2a\u5e74\u4ee3\nA. 20\u4e16\u7eaa80\u5e74\u4ee3\nB. 20\u4e16\u7eaa60\u5e74\u4ee3\nC. 20\u4e16\u7eaa50\u5e74\u4ee3\nD. 20\u4e16\u7eaa90\u5e74\u4ee3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3947778242532753, "meta-math/MetaMath-Mistral-7B": 0.6483555043028546, "itpossible/Chinese-Mistral-7B-v0.1": 0.420607742604153, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3833677286549997, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9a6c\u514b\u601d\u548c\u6069\u683c\u65af\u57281844\u5e74\u5408\u8457\u7684\uff08\uff09\u4e00\u6587\u4e2d\uff0c\u6307\u51fa\uff1a\u201c\u53e4\u5f80\u4eca\u6765\u6bcf\u4e2a\u6c11\u65cf\u90fd\u5728\u67d0\u4e9b\u65b9\u9762\u4f18\u8d8a\u4e8e\u5176\u4ed6\u6c11\u65cf\u201d\uff0c\u660e\u786e\u8868\u8fbe\u4e86\u6c11\u65cf\u5e73\u7b49\u7684\u601d\u60f3\u3002\nA. \u300a\u5bb6\u5ead\u3001\u79c1\u6709\u5236\u548c\u56fd\u5bb6\u7684\u8d77\u6e90\u300b\nB. \u300a\u795e\u5723\u5bb6\u65cf\uff0c\u6216\u5bf9\u6279\u5224\u7684\u6279\u5224\u6240\u4f5c\u7684\u6279\u5224\u300b\nC. \u300a\u4e16\u754c\u4eba\u6743\u5ba3\u8a00\u300b\nD. \u300a\u72ec\u7acb\u5ba3\u8a00\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.990907995209528, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u4e0d\u662f\u51fa\u81ea\u4e8e\u5bd3\u8a00\u6545\u4e8b\u7684\u6210\u8bed\u662f\nA. \u90d1\u4eba\u4e70\u5c65\nB. \u671b\u6885\u6b62\u6e34\nC. \u53f6\u516c\u597d\u9f99\nD. \u63e0\u82d7\u52a9\u957f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.40267108286059333}}, {"question": "\u4ee5\u4e0b\u4e0d\u5c5e\u4e8e\u5fc3\u7406\u7d20\u8d28\u62d3\u5c55\u65b9\u6cd5\u7684\u662f\nA. \u6e38\u620f\u6cd5\nB. \u53cd\u9988\u6cd5\nC. \u8c03\u67e5\u6cd5\nD. \u5206\u4eab\u4f53\u9a8c\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9134624369712265, "meta-math/MetaMath-Mistral-7B": 0.9673898361738398, "itpossible/Chinese-Mistral-7B-v0.1": 0.8991759446326216, "HuggingFaceH4/zephyr-7b-beta": 0.9993296083199722, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9744714348637387, "meta-llama/Meta-Llama-3-8B": 0.394184811204869, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9804946798173231}}, {"question": "\u636e\u7edf\u8ba1\uff0c1954\u5e741\u6708\u52304\u6708\uff0c\u4e2d\u56fd\u79d1\u5b66\u9662\u56fe\u4e66\u9986\u4e0a\u6d77\u5206\u9986\u4fc4\u6587\u4e66\u520a\u501f\u9605\u603b\u6570\u4e3a1953\u5e74\u540c\u671f\u76845\u500d\uff0c\u4e3a1952\u5e74\u540c\u671f\u768450\u500d\uff0c\u4e1c\u5317\u5404\u7814\u7a76\u6240\u4fc4\u6587\u4e66\u520a\u501f\u9605\u91cf\u4e5f\u5927\u5e45\u589e\u52a0\u3002\u8fd9\u8868\u660e\u5f53\u65f6\nA. \u5bf9\u82cf\u8054\u7ecf\u9a8c\u7684\u53cd\u601d\u851a\u7136\u6210\u98ce\nB. \u79d1\u5b66\u7814\u7a76\u5df2\u4e0e\u56fd\u9645\u524d\u6cbf\u63a5\u8f68\nC. \u79d1\u6559\u5174\u56fd\u6218\u7565\u5df2\u5c55\u5f00\nD. \u5de5\u4e1a\u5316\u5efa\u8bbe\u9700\u6c42\u8feb\u5207\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.41730573039315044, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6bd4\u4f8b\u539f\u5219\u662f\u884c\u653f\u6267\u6cd5\u5e94\u9075\u5faa\u7684\u91cd\u8981\u539f\u5219\u3002\u4e0b\u5217\u884c\u653f\u884c\u4e3a\u4e2d\uff0c\u7b26\u5408\u6bd4\u4f8b\u539f\u5219\u7684\u662f\nA. \u4e3a\u4fdd\u62a4\u672c\u5730\u4f01\u4e1a\u7684\u5229\u76ca\uff0c\u7981\u6b62\u672c\u5730\u8d85\u5e02\u51fa\u552e\u5916\u5730\u4f01\u4e1a\u7684\u8089\u7c7b\u5236\u54c1\nB. \u4e3a\u8fce\u63a5\u521b\u5efa\u536b\u751f\u57ce\u5e02\u8bc4\u6bd4\u68c0\u67e5\uff0c\u51b3\u5b9a\u68c0\u67e5\u671f\u95f4\u65e9\u70b9\u644a\u3001\u591c\u5bb5\u5e97\u5747\u4e0d\u5f97\u8425\u4e1a\nC. \u5728\u5bf9\u4f01\u4e1a\u8fdd\u6cd5\u884c\u4e3a\u4f5c\u51fa\u5904\u7f5a\u524d\uff0c\u4e3e\u884c\u542c\u8bc1\u4f1a\u542c\u53d6\u5176\u7533\u8fa9\nD. \u5728\u4e00\u5e74\u4e00\u5ea6\u7684\u9a6c\u62c9\u677e\u6bd4\u8d5b\u5f53\u65e5\uff0c\u5b9e\u884c\u6bd4\u8d5b\u6cbf\u7ebf\u5730\u533a\u4e34\u65f6\u4ea4\u901a\u7ba1\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7965918990085228, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7487547810775532, "meta-llama/Meta-Llama-3-8B": 0.7102142018327624, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u5c5e\u4e8e\u6b27\u5f0f\u8ddd\u79bb\u7279\u6027\u7684\u6709\nA. \u5c3a\u5ea6\u7f29\u653e\u4e0d\u53d8\u6027\nB. \u65cb\u8f6c\u4e0d\u53d8\u6027\nC. \u4e0d\u53d7\u91cf\u7eb2\u5f71\u54cd\u7684\u7279\u6027\nD. \u8003\u8651\u4e86\u6a21\u5f0f\u7684\u5206\u5e03\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5c5e\u4e8e\u978d\u5c71\u5e02\u98df\u54c1\u5b89\u5168\u59d4\u5458\u4f1a\u6210\u5458\u5355\u4f4d\u7684\u662f\nA. \u5e02\u5de5\u5546\u5c40\nB. \u4ee5\u4e0a\u5168\u90e8\u90fd\u662f\nC. \u5e02\u8d28\u76d1\u5c40\nD. \u5e02\u536b\u751f\u5c40\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.39667527544445164, "meta-math/MetaMath-Mistral-7B": 0.5408762550264521, "itpossible/Chinese-Mistral-7B-v0.1": 0.573274642706863, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8973342121003304, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c11\u65cf\u821e\u5267\u300a\u4e1d\u8def\u82b1\u96e8\u300b\u4e2d\u201c\u53cd\u5f39\u7435\u7436\u201d\u7684\u52a8\u4f5c\u8bbe\u8ba1\u6765\u6e90\u4e8e\nA. \u963f\u7ec6\u8df3\u6708\nB. \u7ef4\u543e\u5c14\u821e\u8e48\nC. \u79e7\u6b4c\nD. \u6566\u714c\u58c1\u753b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.38253224097524136, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u53e5\u4e2d\uff0c\u201c\u6613\u201d\u8868\u793a\u201c\u8f7b\u89c6\u201d\u4e49\u7684\u4e00\u53e5\u662f\nA. \u592b\u7136\u5f8c\u8db3\u4ee5\u5316\u6c11\u6613\u4fd7\uff0c\u8fd1\u8005\u8aaa\u670d\u800c\u9060\u8005\u61f7\u4e4b\u3002\nB. \u662f\u4ee5\u53e4\u4e4b\u6613\u8ca1\uff0c\u975e\u4ec1\u4e5f\uff0c\u8ca1\u591a\u4e5f\u3002\nC. \u4e16\u6613\u6642\u79fb\uff0c\u8b8a\u6cd5\u5b9c\u77e3\u3002\nD. \u9022\u4e11\u7236\u8f3f\u516c\u6613\u4f4d\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2834350284364258, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4221216124438063, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5837308368711828, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5710154969395268}}, {"question": "\u5b50\u5bab\u52a8\u8109\u5728\u8ddd\u5b50\u5bab\u9888\u5916\u4fa7\u7ea62cm \u5904\u884c\u4e8e\nA. \u8f93\u5c3f\u7ba1\u7684\u524d\u4e0a\u65b9\nB. \u8f93\u5c3f\u7ba1\u7684\u540e\u4e0b\u65b9\nC. \u8f93\u5c3f\u7ba1\u7684\u5916\u4fa7\nD. \u8f93\u5375\u7ba1\u524d\u4e0b\u65b9\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2948380239243211, "meta-math/MetaMath-Mistral-7B": 0.325455072595945, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e3a\u89c2\u5bdf\u836f\u7269$A\u3001B$\u5bf9\u67d0\u75c5\u6cbb\u6108\u7387\u7684\u5dee\u5f02\u6709\u65e0\u663e\u8457\u6027\u610f\u4e49\uff0c\u67d0\u533b\u751f\u5c06100\u4f8b\u8be5\u75c5\u60a3\u8005\u968f\u673a\u5206\u6210\u4e24\u7ec4\uff0c\u5176\u4e2d\u4e00\u7ec440\u4eba\uff0c\u670d\u7528A\u836f;\u53e6\u4e00\u7ec460\u4eba\uff0c\u670d\u7528B\u836f\u3002\u7ed3\u679c\u53d1\u73b0\uff0c\u670d\u7528$\\mathrm{A}$\u836f\u7684\u4eba\u4e2d\u670930\u4eba\u6cbb\u6108;\u670d\u7528B\u836f\u7684\u4eba\u4e2d\u670911\u4eba\u6cbb\u6108\u3002\u5e94\u9009\u7528\u7684\u7edf\u8ba1\u5b66\u65b9\u6cd5\u662f:\nA. \u56de\u5f52\u5206\u6790\nB. \u52a0\u6743$\\chi^2$\u68c0\u9a8c\nC. Kappa\u68c0\u9a8c\nD. $\\chi^2$\u68c0\u9a8c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9591846214757268, "meta-math/MetaMath-Mistral-7B": 0.989865292492515, "itpossible/Chinese-Mistral-7B-v0.1": 0.9278781860896498, "HuggingFaceH4/zephyr-7b-beta": 0.9999930072214572, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9692120006036783, "meta-llama/Meta-Llama-3-8B": 0.9050353927077915, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8114598587745018}}, {"question": "\u4e0b\u9762\u54ea\u4e00\u4e2a\u60c5\u666f\u5c5e\u4e8e\u6388\u6743\uff08Authorization\uff09\nA. \u7528\u6237\u4f7f\u7528\u52a0\u5bc6\u8f6f\u4ef6\u5bf9\u81ea\u5df1\u7f16\u5199\u7684Office\u6587\u6863\u8fdb\u884c\u52a0\u5bc6\uff0c\u4ee5\u963b\u6b62\u5176\u4ed6\u4eba\u5f97\u5230\u8fd9\u4efd\u62f7\u8d1d\u540e\u770b\u5230\u6587\u6863\u4e2d\u7684\u5185\u5bb9\nB. \u7528\u6237\u4f9d\u7167\u7cfb\u7edf\u63d0\u793a\u8f93\u5165\u7528\u6237\u540d\u548c\u53e3\u4ee4\nC. \u67d0\u4e2a\u4eba\u5c1d\u8bd5\u767b\u5f55\u5230\u4f60\u7684\u8ba1\u7b97\u673a\u4e2d\uff0c\u4f46\u662f\u53e3\u4ee4\u8f93\u5165\u7684\u4e0d\u5bf9\uff0c\u7cfb\u7edf\u63d0\u793a\u53e3\u4ee4\u9519\u8bef\uff0c\u5e76\u5c06\u8fd9\u6b21\u5931\u8d25\u7684\u767b\u5f55\u8fc7\u7a0b\u7eaa\u5f55\u5728\u7cfb\u7edf\u65e5\u5fd7\u4e2d\nD. \u7528\u6237\u5728\u7f51\u7edc\u4e0a\u5171\u4eab\u4e86\u81ea\u5df1\u7f16\u5199\u7684\u4e00\u4efdOffice\u6587\u6863\uff0c\u5e76\u8bbe\u5b9a\u54ea\u4e9b\u7528\u6237\u53ef\u4ee5\u9605\u8bfb\uff0c\u54ea\u4e9b\u7528\u6237\u53ef\u4ee5\u4fee\u6539\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5899675946343427, "meta-math/MetaMath-Mistral-7B": 0.902082839834707, "itpossible/Chinese-Mistral-7B-v0.1": 0.6507280253165881, "HuggingFaceH4/zephyr-7b-beta": 0.9957886667854294, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7222327542994589, "meta-llama/Meta-Llama-3-8B": 0.7016256644384665, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7561008234271779}}, {"question": "\u6709\u5b66\u8005\u8ba4\u4e3a\uff1a\u201c\u5c3c\u514b\u677e\u548c\u57fa\u8f9b\u683c\u8fd9\u5bf9\u5947\u7279\u7684\u642d\u6863\u6bd5\u7adf\u4f7f\u7f8e\u56fd\u5916\u4ea4\u653f\u7b56\u7529\u6389\u4e86\u5305\u88b1\uff0c\u51b2\u51fa\u4e86\u7981\u533a\uff0c\u628a\u5168\u56fd\u7684\u89c6\u7ebf\u4ece\u610f\u8bc6\u5f62\u6001\u8f6c\u5411\u4e86\u5730\u7f18\u653f\u6cbb\uff0c\u5e76\u4e14\u8c03\u6574\u4e86\u7f8e\u56fd\u653f\u7b56\u4ee5\u9002\u5e94\u56fd\u9645\u5173\u7cfb\u7ed3\u6784\u4e2d\u53d1\u751f\u7684\u6df1\u523b\u53d8\u5316\u3002\u201d\u4e0b\u5217\u5404\u9879\u4e2d\u6700\u80fd\u4f50\u8bc1\u8fd9\u4e00\u89c2\u70b9\u7684\u662f\nA. \u300a\u4e2d\u7f8e\u8054\u5408\u516c\u62a5\u300b\u53d1\u8868\nB. \u4e2d\u7f8e\u6b63\u5f0f\u5efa\u7acb\u5916\u4ea4\u5173\u7cfb\nC. \u4e2d\u56fd\u6062\u590d\u5728\u8054\u5408\u56fd\u7684\u5408\u6cd5\u5e2d\u4f4d\nD. \u4e2d\u7f8e\u7b7e\u8ba2\u671d\u9c9c\u505c\u6218\u534f\u8bae\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3332764561785216, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5089068467627693, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u80fd\u6b63\u786e\u89e3\u91ca\u4e0b\u5217\u53cd\u5e94\u539f\u7406\uff0c\u5e76\u4e14\u4e66\u5199\u6b63\u786e\u7684\u79bb\u5b50\u65b9\u7a0b\u5f0f\u662f\nA. \u5411\u8be5\u6eb6\u6db2\u4e2d\u6ef4\u5165\u5c11\u91cfFeSO4\u6eb6\u6db2\uff0c\u53cd\u5e94\u7684\u79bb\u5b50\u65b9\u7a0b\u5f0f\u4e3a2Fe2\uff0b\uff0bClO\uff0d\uff0b2H^\uff0b===Cl^\uff0d\uff0b2Fe^3\uff0b\uff0bH2O\nB. \u7528\u660e\u77fe\u4f5c\u51c0\u6c34\u5242\uff1aAl3\uff0b\uff0b3H2O===Al(OH)3\uff0b3H^\uff0b\nC. \u7528\u7a00\u786b\u9178\u53bb\u9664\u94dc\u5668\u8868\u9762\u7684Cu2(OH)2CO3\uff1aCu2(OH)2CO3\uff0b4H\uff0b===2Cu2\uff0b\uff0bCO2\uff0b3H2O\nD. \u7528\u5c0f\u82cf\u6253\u6cbb\u7597\u80c3\u9178\u8fc7\u591a\uff1aCO\uff0b2H^\uff0b===CO2\uff0bH2O\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2947319625574089, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.32304351211676685, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9039720648392019}}, {"question": "\u7ebf\u5708\u4ea7\u751f\u611f\u5e94\u7535\u52a8\u52bf\u5927\u5c0f\u6b63\u6bd4\u4e8e\u901a\u8fc7\u7ebf\u5708\u7684\nA. \u78c1\u901a\u91cf\u7684\u53d8\u5316\nB. \u7535\u6d41\u7684\u65f6\u95f4\u53d8\u5316\u91cf\nC. \u78c1\u901a\u91cf\u7684\u5927\u5c0f\nD. \u78c1\u901a\u91cf\u7684\u53d8\u5316\u7387\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4277410908864782, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4614726494090003, "meta-llama/Meta-Llama-3-8B": 0.5109596959351622, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u516c\u5171\u5173\u7cfb\u4eba\u5458\u8981\u9762\u4e34\u590d\u6742\u7684\u516c\u5173\u4e8b\u52a1\uff0c\u4e0e\u4e0d\u540c\u7c7b\u578b\u7684\u516c\u4f17\u6253\u4ea4\u9053\uff0c\u56e0\u6b64\uff0c\u5fc5\u987b\u5177\u5907\u826f\u597d\u7684\u5fc3\u7406\u7d20\u8d28\u3002\u5305\u62ec\uff1a1\u3001\u81ea\u4fe1\u7684\u5fc3\u7406\u30022\u3001\u70ed\u60c5\u5de5\u4f5c\u7684\u5fc3\u7406\u30023\u3001\u6e34\u671b\u6210\u529f\u7684\u5fc3\u7406\u30024\u3001\uff08\uff09\nA. \u5f00\u653e\u4e50\u89c2\u7684\u5fc3\u6001\nB. \u575a\u5f3a\u7684\u4fe1\u5ff5\nC. \u70ed\u5ff1\u7684\u670d\u52a1\nD. \u516c\u5173\u4e13\u4e1a\u77e5\u8bc6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7212405134299603, "meta-math/MetaMath-Mistral-7B": 0.9114187590931713, "itpossible/Chinese-Mistral-7B-v0.1": 0.589942343090752, "HuggingFaceH4/zephyr-7b-beta": 0.9726536046320103, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8207130843677933, "meta-llama/Meta-Llama-3-8B": 0.6616657960280905, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.42968584901761314}}, {"question": "\u4eba\u7684\u57fa\u7840\u4ee3\u8c22\u80fd\u91cf\u6307\u7684\u662f\nA. \u4ee5\u4e0a\u90fd\u9519\u8bef\nB. \u5904\u4e8e\u6e05\u9192\u3001\u7a7a\u8179\u3001\u5b89\u9759\u7684\u72b6\u6001\u4e0b\u7ef4\u6301\u4f53\u6e29\u7684\u6240\u9700\u6700\u4f4e\u7684\u80fd\u91cf\nC. \u5904\u4e8e\u6e05\u9192\u3001\u7a7a\u8179\u3001\u5b89\u9759\u7684\u72b6\u6001\u4e0b\u7ef4\u6301\u5fc3\u810f\u5668\u5b98\u6d3b\u52a8\u7b49\u57fa\u672c\u751f\u547d\u6d3b\u52a8\u6240\u9700\u6700\u4f4e\u7684\u80fd\u91cf\nD. A\u548cB\u4e24\u8005\u7684\u603b\u548c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4705915649876573, "meta-math/MetaMath-Mistral-7B": 0.8916288063754723, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7978765018850773, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5724400894900916, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5510\u5ba3\u5b97\u65f6\uff0c\u97e6\u5b99\u51fa\u4efb\u6c38\u5dde\u523a\u53f2\uff0c\u89c1\u5f53\u5730\u4fda\u6c11\u4e4b\u4fd7\uff1a\u4fda\u6c11\u5a5a\uff0c\u51fa\u8d22\u4f1a\u5bbe\u5ba2\uff0c\u53f7\u201c\u7834\u9152\u201d\uff0c\u663c\u591c\u96c6\uff0c\u591a\u81f3\u6570\u767e\u4eba\uff0c\u8d2b\u8005\u72b9\u6570\u5341\uff1b\u529b\u4e0d\u8db3\uff0c\u5219\u4e0d\u8fce\uff08\u5a36\uff09\uff0c\u81f3\u79c1\u5954\u8005\u3002\u97e6\u5b99\u51fa\u793a\u7ea6\u675f\uff0c\u4f7f\u7565\u5982\uff08\u6c49\uff09\u793c\uff0c\u4fd7\u9042\u6539\u3002\u53ef\u89c1\uff0c\u97e6\u5b99\nA. \u53cd\u5bf9\u4fda\u6c11\u4e3e\u529e\u5a5a\u5bb4\nB. \u63d0\u5021\u5a5a\u4fd7\u5b8c\u5168\u6c49\u5316\nC. \u53cd\u5bf9\u5a5a\u793c\u5927\u8086\u64cd\u529e\nD. \u8ba4\u53ef\u4fda\u6c11\u4e0d\u8fce\u79c1\u5954\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5820226728469899, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8651374354492078}}, {"question": "\u9047\u540e\u8f66\u53d1\u51fa\u8d85\u8f66\u4fe1\u53f7\u540e\uff0c\u53ea\u8981\u5177\u5907\u8ba9\u8d85\u6761\u4ef6\u5e94\u600e\u6837\u505a\nA. \u8fc5\u901f\u51cf\u901f\u6216\u7d27\u6025\u5236\u52a8\nB. \u9760\u9053\u8def\u53f3\u4fa7\u52a0\u901f\u884c\u9a76\nC. \u4e3b\u52a8\u51cf\u901f\u5e76\u9760\u53f3\u4fa7\u884c\u9a76\nD. \u8ba9\u51fa\u9002\u5f53\u7a7a\u95f4\u52a0\u901f\u884c\u9a76\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5549235922343089, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7532\u8def\u8fc7\u67d0\u81ea\u884c\u8f66\u4fee\u7406\u5e97\uff0c\u89c1\u6709\u4e00\u8f86\u540d\u724c\u7535\u52a8\u81ea\u884c\u8f66\uff08\u4ef7\u503c1\u4e07\u5143\uff09\u505c\u5728\u95e8\u53e3\uff0c\u6b32\u636e\u4e3a\u5df1\u6709\u3002\u7532\u89c1\u5e97\u5185\u8d27\u67b6\u4e0a\u65e0\u81ea\u884c\u8f66\u8f66\u9501\uff0c\u4fbf\u8c0e\u79f0\u8981\u8d2d\u4e70\uff0c\u50ac\u4fc3\u5e97\u4e3b\u53bb50\u7c73\u4e4b\u5916\u7684\u5e93\u623f\u62ff\u8d27\u3002\u5e97\u4e3b\u4e34\u8d70\u65f6\u5bf9\u7532\u8bf4\uff1a\u201c\u6211\u53bb\u62ff\u9501\uff0c\u4f60\u5e2e\u6211\u770b\u4e00\u4e0b\u5e97\u3002\u201d\u5728\u5e97\u4e3b\u8fdb\u5165\u4ed3\u5e93\u540e\uff0c\u7532\u4fbf\u9a91\u8d70\u7535\u52a8\u81ea\u884c\u8f66\u3002\u7532\u7684\u884c\u4e3a\u6784\u6210\nA. \u4fb5\u5360\u7f6a\nB. \u76d7\u7a83\u7f6a\nC. \u804c\u52a1\u4fb5\u5360\u7f6a\nD. \u8bc8\u9a97\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5016431259864998, "meta-math/MetaMath-Mistral-7B": 0.8921946526088663, "itpossible/Chinese-Mistral-7B-v0.1": 0.6824601284073669, "HuggingFaceH4/zephyr-7b-beta": 0.9984480833768523, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9508793575135104, "meta-llama/Meta-Llama-3-8B": 0.5579784254483179, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.775938862141311}}, {"question": "\u5c06\u5efa\u7acb\u4e0e\u53d1\u5c55\u540c\u6240\u6709\u5229\u76ca\u76f8\u5173\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u4f5c\u4e3a\u4f01\u4e1a\u8425\u9500\u7684\u5173\u952e\uff0c\u628a\u6b63\u786e\u5904\u7406\u8fd9\u4e9b\u5173\u7cfb\u4f5c\u4e3a\u4f01\u4e1a\u8425\u9500\u7684\u6838\u5fc3\uff0c\u8fd9\u6307\u7684\u662f\nA. \u4ea4\u53c9\u8425\u9500\nB. \u6574\u5408\u8425\u9500\nC. \u5173\u7cfb\u8425\u9500\nD. \u7eff\u8272\u8425\u9500\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9673421301574239, "meta-math/MetaMath-Mistral-7B": 0.9945648832633885, "itpossible/Chinese-Mistral-7B-v0.1": 0.992480775721791, "HuggingFaceH4/zephyr-7b-beta": 0.9997862024995311, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9185684318387692, "meta-llama/Meta-Llama-3-8B": 0.9849029849240861, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9995370591899922}}, {"question": "\u8bca\u65ad\u8d1d\u8d6b\u5207\u7279\u75c5\u6700\u57fa\u672c\u7684\u662f\nA. \u53cd\u590d\u53e3\u8154\u6e83\u75a1\nB. \u7ed3\u8282\u6027\u7ea2\u6591\nC. \u9488\u523a\u53cd\u5e94\nD. \u53cd\u590d\u5916\u9634\u6e83\u75a1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8744996466956003}}, {"question": "\u4e0b\u9762\u54ea\u4e00\u4e2a\u6761\u4ef6\u4e0d\u662f\u4f20\u7edf\u519c\u4e1a\u8f6c\u53d8\u4e3a\u73b0\u4ee3\u519c\u4e1a\u6240\u9700\u8981\u7684\u793e\u4f1a\u7ecf\u6d4e\u53ca\u6280\u672f\u6761\u4ef6\nA. \u5de5\u4e1a\u8981\u6709\u4e00\u5b9a\u7a0b\u5ea6\u7684\u53d1\u5c55\nB. \u6559\u80b2\u6c34\u5e73\u8981\u6709\u4e00\u5b9a\u7a0b\u5ea6\u7684\u63d0\u9ad8\nC. \u804c\u5de5\u7684\u5de5\u8d44\u8981\u6709\u5927\u5e45\u5ea6\u4e0a\u6da8\nD. \u5546\u54c1\u7ecf\u6d4e\u5fc5\u987b\u6709\u4e00\u5b9a\u7a0b\u5ea6\u7684\u53d1\u5c55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7781475931639101, "meta-math/MetaMath-Mistral-7B": 0.9811807406275678, "itpossible/Chinese-Mistral-7B-v0.1": 0.775172237388244, "HuggingFaceH4/zephyr-7b-beta": 0.9983959271972824, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9852902955396572, "meta-llama/Meta-Llama-3-8B": 0.8989788878692488, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9306698080241964}}, {"question": "\u7d20\u6709\u201c\u4e00\u83dc\u4e00\u683c\uff0c\u767e\u83dc\u767e\u5473\u201d\u4e4b\u79f0\u7684\u83dc\u80b4\u4f53\u7cfb\u662f\nA. \u9c81\u83dc\nB. \u8c6b\u83dc\nC. \u82cf\u83dc\nD. \u5ddd\u83dc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.607145348843253, "HuggingFaceH4/zephyr-7b-beta": 0.5250144426057839, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3236616576277932}}, {"question": "\u4e0b\u5217\u751f\u7269\u4e2d\uff0c\u80fd\u8fdb\u884c\u6b21\u7ea7\u751f\u4ea7\u7684\u662f\nA. \u5c0f\u9ea6\nB. \u5154\nC. \u6c34\u7a3b\nD. \u852c\u83dc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44479734009647387, "meta-math/MetaMath-Mistral-7B": 0.9113437053708281, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9997983923139849, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7710184602048554, "meta-llama/Meta-Llama-3-8B": 0.6719167855820743, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9843829431308696}}, {"question": "\u8868\u73b0\u5728\u4eba\u5bf9\u73b0\u5b9e\u7684\u6001\u5ea6\u548c\u884c\u4e3a\u65b9\u5f0f\u7684\u6bd4\u8f83\u7a33\u5b9a\u7684\u72ec\u7279\u7684\u5fc3\u7406\u7279\u5f81\u7684\u603b\u548c\u662f\nA. \u80fd\u529b\nB. \u5174\u8da3\nC. \u6c14\u8d28\nD. \u6027\u683c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9111373416496101, "meta-math/MetaMath-Mistral-7B": 0.997170861806257, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9998911869344678, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9890615230372709, "meta-llama/Meta-Llama-3-8B": 0.995097896728503, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9840682067731935}}, {"question": "\u8c03\u6574\u5e73\u7b49\u4e3b\u4f53\u4e4b\u95f4\u8d22\u4ea7\u5173\u7cfb\u548c\u4eba\u8eab\u5173\u7cfb\u7684\u90e8\u95e8\u6cd5\u662f\nA. \u5546\u6cd5\nB. \u5211\u6cd5\nC. \u6c11\u6cd5\nD. \u7ecf\u6d4e\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7733114170392283, "meta-math/MetaMath-Mistral-7B": 0.9866401043207338, "itpossible/Chinese-Mistral-7B-v0.1": 0.7557410403773823, "HuggingFaceH4/zephyr-7b-beta": 0.9919226397240964, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.786711465543891, "meta-llama/Meta-Llama-3-8B": 0.9251787029063295, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9675231919808569}}, {"question": "\u5973\u6027\u7684\u6027\u57fa\u56e0\u662f\nA. \uff38\nB. \uff38\uff39\nC. \uff39\nD. \uff38\uff38\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u7f8e\u56fd\u7684\u7535\u5f71\u9662\u7ebf\uff0c\u65e0\u8bba\u662f\u91cd\u78c5\u5927\u7247\uff0c\u8fd8\u662f\u4f4e\u6210\u672c\u5236\u4f5c\uff0c\u7968\u4ef7\u90fd\u662f\u76f8\u540c\u7684\u3002\u8fd9\u770b\u4e0a\u53bb\u5e76\u4e0d\u7b26\u5408\u201c\u9700\u6c42\u5927\u5c0f\u51b3\u5b9a\u4ef7\u683c\u9ad8\u4f4e\u201d\u7684\u7ecf\u6d4e\u5b66\u7406\u8bba\u3002\u6709\u7814\u7a76\u4eba\u5458\u6307\u51fa\uff0c\u4efb\u4f55\u4e00\u5bb6\u5f71\u9662\u540c\u671f\u4e0a\u6620\u7684\u6240\u6709\u7535\u5f71\u7968\u4ef7\u5747\u76f8\u540c\uff0c\u8fd9\u4e00\u73b0\u8c61\u5f62\u6210\u4e8e\u4e0a\u4e16\u7eaa70\u5e74\u4ee3\u3002\u4e0d\u4ec5\u7535\u5f71\u4e1a\u5982\u6b64\uff0c\u4f53\u80b2\u8d5b\u4e8b\u548c\u6f14\u51fa\u4e5f\u90fd\u9075\u5faa\u8fd9\u4e00\u89c4\u5f8b\u3002\u867d\u7136\u5728\u67d0\u4e9b\u65f6\u5019\u548c\u67d0\u4e9b\u5730\u533a\uff0c\u673a\u52a8\u5b9a\u4ef7\u80fd\u591f\u4f7f\u7535\u5f71\u516c\u53f8\u548c\u5f71\u9662\u83b7\u5f97\u66f4\u9ad8\u7684\u6536\u76ca\uff0c\u4f46\u5bf9\u4e8e\u5f71\u9662\u6765\u8bf4\uff0c\u4fdd\u6301\u4e0d\u540c\u7535\u5f71\u7684\u7968\u4ef7\u76f8\u540c\u4ecd\u7136\u5229\u5927\u4e8e\u5f0a\u3002\u8fd9\u6bb5\u6587\u5b57\u63a5\u4e0b\u6765\u6700\u6709\u53ef\u80fd\u8bb2\u8ff0\u7684\u662f\nA. \u4f53\u80b2\u8d5b\u4e8b\u548c\u6f14\u51fa\u7b49\u5176\u4ed6\u884c\u4e1a\u7968\u4ef7\u7684\u5f62\u6210\u89c4\u5f8b\nB. \u7f8e\u56fd\u7535\u5f71\u884c\u4e1a\u786e\u5b9a\u7968\u4ef7\u7684\u4e3b\u8981\u53c2\u8003\u56e0\u7d20\nC. \u4e0d\u540c\u7535\u5f71\u7968\u4ef7\u76f8\u540c\u5bf9\u5f71\u9662\u66f4\u6709\u5229\u7684\u539f\u56e0\nD. \u7535\u5f71\u5236\u4f5c\u6210\u672c\u548c\u89c2\u4f17\u9700\u6c42\u4e0e\u5f71\u7247\u5b9a\u4ef7\u7684\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7360700336227041, "meta-math/MetaMath-Mistral-7B": 0.8202679025354047, "itpossible/Chinese-Mistral-7B-v0.1": 0.5152278213320992, "HuggingFaceH4/zephyr-7b-beta": 0.9857261199599884, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8455513554211946, "meta-llama/Meta-Llama-3-8B": 0.7245996078785897, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9466573389365399}}, {"question": "\u4eba\u4eec\u5e0c\u671b\u81ea\u5df1\u80fd\u627e\u5230\u4e00\u4e2a\u7406\u60f3\u7684\u5de5\u4f5c\uff0c\u5e76\u5728\u5de5\u4f5c\u4e2d\u8fbe\u5230\u7406\u60f3\u7684\u5883\u754c\uff0c\u53d6\u5f97\u7406\u60f3\u7684\u6210\u7ee9\u3002\u8fd9\u5c5e\u4e8e\u4eba\u4eec\u5728\nA. \u751f\u6d3b\u9886\u57df\u7684\u7406\u60f3\u4fe1\u5ff5\nB. \u9053\u5fb7\u9886\u57df\u7684\u7406\u60f3\u4fe1\u5ff5\nC. \u793e\u4f1a\u9886\u57df\u7684\u7406\u60f3\u4fe1\u5ff5\nD. \u804c\u4e1a\u9886\u57df\u7684\u7406\u60f3\u4fe1\u5ff5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7540656903107701, "meta-math/MetaMath-Mistral-7B": 0.6838290041613073, "itpossible/Chinese-Mistral-7B-v0.1": 0.9103082065761272, "HuggingFaceH4/zephyr-7b-beta": 0.9821789007059626, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9556399983609236, "meta-llama/Meta-Llama-3-8B": 0.7702768705158118, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8868\u5e72\u6cd5\u3001\u6c34\u4e2d\u91cd\u6cd5\u3001\u5c01\u8721\u6cd5\u3001\u4f53\u79ef\u6cd5\u662f\u6ca5\u9752\u6df7\u5408\u6599\u5bc6\u5ea6\u8bd5\u9a8c\u76844\u79cd\u65b9\u6cd5\uff0c\u5176\u4e2d\u8868\u5e72\u6cd5\u7684\u9002\u7528\u6761\u4ef6\u662f\nA. \u9002\u7528\u4e8e\u4efb\u4f55\u6ca5\u9752\u6df7\u5408\u6599\nB. \u8bd5\u4ef6\u5438\u6c34\u7387\u5927\u4e8e2\uff05\nC. \u8bd5\u4ef6\u5438\u6c34\u7387\u5c0f\u4e8e2\uff05\nD. \u8bd5\u4ef6\u5438\u6c34\u7387\u5c0f\u4e8e0\uff0e5\uff05\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5118286528617234, "meta-math/MetaMath-Mistral-7B": 0.6093791606418323, "itpossible/Chinese-Mistral-7B-v0.1": 0.40252652413078177, "HuggingFaceH4/zephyr-7b-beta": 0.9553345206898527, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5982186136585053, "meta-llama/Meta-Llama-3-8B": 0.4777635640130837, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7930447100736623}}, {"question": "\u8bfe\u5916\u6821\u5916\u6559\u80b2\u4e0e\u8bfe\u5185\u6559\u80b2\u7684\u5171\u540c\u4e4b\u5904\u5728\u4e8e\uff0c\u5b83\u4eec\u90fd\u662f\nA. \u53d7\u6559\u5b66\u8ba1\u5212\u548c\u6559\u5b66\u5927\u7eb2\u89c4\u8303\u7684\nB. \u5e08\u751f\u5171\u540c\u53c2\u4e0e\u7684\nC. \u6709\u76ee\u7684\u3001\u6709\u8ba1\u5212\u3001\u6709\u7ec4\u7ec7\u8fdb\u884c\u7684\nD. \u5b66\u751f\u81ea\u613f\u9009\u62e9\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6455878658830148, "meta-math/MetaMath-Mistral-7B": 0.8416726438256769, "itpossible/Chinese-Mistral-7B-v0.1": 0.7466550397443407, "HuggingFaceH4/zephyr-7b-beta": 0.9953196179013176, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7615135637327458, "meta-llama/Meta-Llama-3-8B": 0.5011174548129379, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6199824454610215}}, {"question": "\u6211\u56fd\u5916\u4ea4\u653f\u7b56\u7684\u5b97\u65e8\u662f\nA. \u548c\u5e73\u5171\u5904\nB. \u7ef4\u62a4\u4e16\u754c\u548c\u5e73\u3001\u4fc3\u8fdb\u5171\u540c\u53d1\u5c55\nC. \u72ec\u7acb\u81ea\u4e3b\nD. \u53cd\u5bf9\u9738\u6743\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7662981273235706, "meta-math/MetaMath-Mistral-7B": 0.9375145261288825, "itpossible/Chinese-Mistral-7B-v0.1": 0.47343780703136834, "HuggingFaceH4/zephyr-7b-beta": 0.9980637260924824, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8938479469489534, "meta-llama/Meta-Llama-3-8B": 0.8331281835706098, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.979495144996243}}, {"question": "\u5728\u6768\u6c0f\u53cc\u7f1d\u5e72\u6d89\u5b9e\u9a8c\u4e2d\uff0c\u5982\u679c\u7f29\u77ed\u53cc\u7f1d\u95f4\u7684\u8ddd\u79bb\uff0c\u4e0b\u5217\u9648\u8ff0\u6b63\u786e\u7684\u662f\nA. \u76f8\u90bb\u660e\uff08\u6697\uff09\u7eb9\u95f4\u8ddd\u589e\u5927\nB. \u76f8\u90bb\u660e\uff08\u6697\uff09\u95f4\u8ddd\u79bb\u51cf\u5c0f\nC. \u4e0d\u80fd\u786e\u5b9a\u76f8\u90bb\u660e\uff08\u6697\uff09\u7eb9\u95f4\u8ddd\u7684\u53d8\u5316\u60c5\u51b5\nD. \u76f8\u90bb\u660e\uff08\u6697\uff09\u7eb9\u95f4\u8ddd\u4e0d\u53d8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.44623685971274624, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5358178410422609, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6420773429219765}}, {"question": "\u9a6c\u514b\u601d\u4e3b\u4e49\u7ecf\u5178\u4f5c\u5bb6\u6307\u51fa\uff0c\u201c\u5c3d\u7ba1\u6709\u8fd9\u4e9b(\u5173\u7a0e)\u4fdd\u62a4\u63aa\u65bd\uff0c\u5927\u5de5\u4e1a\u4ecd\u4f7f\u7ade\u4e89\u666e\u904d\u5316\u4e86\uff0c\u5927\u5de5\u4e1a\u521b\u9020\u4e86\u4ea4\u901a\u5de5\u5177\u2026\u2026\u628a\u6240\u6709\u7684\u8d44\u672c\u90fd\u53d8\u6210\u4e3a\u5de5\u4e1a\u8d44\u672c\uff0c\u4ece\u800c\u4f7f\u6d41\u901a\u52a0\u901f\u3001\u8d44\u672c\u96c6\u4e2d\u201d\uff1b\u201c\u5b83\u9996\u6b21\u5f00\u521b\u4e86\u4e16\u754c\u5386\u53f2\uff0c\u56e0\u4e3a\u5b83\u4f7f\u6bcf\u4e2a\u6587\u660e\u56fd\u5bb6\u4ee5\u53ca\u8fd9\u4e9b\u56fd\u5bb6\u4e2d\u7684\u6bcf\u4e00\u4e2a\u4eba\u7684\u9700\u8981\u7684\u6ee1\u8db3\u90fd\u4f9d\u8d56\u4e8e\u6574\u4e2a\u4e16\u754c\u201d\u3002\u5bf9\u6b64\u6700\u6070\u5f53\u7684\u7406\u89e3\u662f\nA. \u81ea\u7531\u7ade\u4e89\u589e\u5f3a\u4e86\u5de5\u4e1a\u8d44\u672c\u7684\u6d41\u901a\nB. \u4ea4\u901a\u5de5\u5177\u6269\u5927\u4e86\u5de5\u4e1a\u6587\u660e\u7684\u5f71\u54cd\nC. \u5de5\u4e1a\u9769\u547d\u4fc3\u8fdb\u4e86\u4e16\u754c\u5e02\u573a\u7684\u5f62\u6210\nD. \u5173\u7a0e\u4fdd\u62a4\u963b\u788d\u4e0d\u4e86\u8d44\u672c\u4e3b\u4e49\u7ade\u4e89\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7164453504562616, "meta-math/MetaMath-Mistral-7B": 0.9487366531182606, "itpossible/Chinese-Mistral-7B-v0.1": 0.6027603439651981, "HuggingFaceH4/zephyr-7b-beta": 0.9879169158512727, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9055484879136984, "meta-llama/Meta-Llama-3-8B": 0.8272616612703302, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9587676144760344}}, {"question": "___\u65f6\uff0c\u4e8c\u9879\u5206\u5e03$B(n\uff0c\\pi)$\u8fd1\u4f3c\u4e8e\u4ee5$n\\pi$\u4e3a\u53c2\u6570\u7684Poisson\u5206\u5e03\u3002\nA. $n$\u8f83\u5927\u4e14$\\pi$\u63a5\u8fd10\u62161\nB. $n$\u8f83\u5927\u4e14$\\pi$\u63a5\u8fd11\nC. $n$\u8f83\u5927\u4e14$\\pi$\u63a5\u8fd10.5\nD. $n$\u8f83\u5927\u4e14$\\pi$\u63a5\u8fd10\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\u751f\u6b96\u817a\u662f\nA. \u524d\u5217\u817a\nB. \u7cbe\u56ca\u817a\nC. \u5c3f\u9053\u7403\u817a\nD. \u777e\u4e38\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5076931079285522, "meta-math/MetaMath-Mistral-7B": 0.8090735044667879, "itpossible/Chinese-Mistral-7B-v0.1": 0.8574881924749623, "HuggingFaceH4/zephyr-7b-beta": 0.7079512173474505, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6117450762626497, "meta-llama/Meta-Llama-3-8B": 0.7743365145198149, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8542289954001312}}, {"question": "\u300a\u6e05\u53f2\u7a3f\u300b\u8bb0\u8f7d\uff1a\u201c\u4eb2\u653f\u4e4b\u65f6\uff0c\u6625\u79cb\u65b9\u5bcc\uff0c\u62b1\u5927\u6709\u4e3a\u4e4b\u5fd7\uff0c\u6b32\u5f20\u631e\u4f10\uff0c\u4ee5\u6e54(\u6d17)\u56fd\u803b\u3002\u5df2\u800c\u5e08\u5f92\u6320\u8d25\uff0c\u5272\u5730\u8f93\u5e73\uff0c\u9042\u5f15\u65b0\u8fdb\u5c0f\u81e3\uff0c\u9510\u5fd7\u66f4\u5f20\uff0c\u4e3a\u53d1\u594b\u81ea\u5f3a\u4e4b\u8ba1\u3002\u201d\u6b64\u5904\u8bc4\u4ef7\u7684\u8fd9\u4f4d\u7687\u5e1d\u662f\nA. \u5eb7\u7199\u5e1d\nB. \u5149\u7eea\u5e1d\nC. \u4e7e\u9686\u5e1d\nD. \u5ba3\u7edf\u5e1d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.47340443864112025, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.48006665261728143, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u6d1b\u9633\u7eb8\u8d35\u201d\u6bd4\u55bb\u4f5c\u54c1\u98ce\u884c\u4e00\u65f6\uff0c\u5e7f\u4e3a\u6d41\u4f20\uff0c\u8fd9\u4e2a\u6210\u8bed\u4e0e\u4ee5\u4e0b\u54ea\u90e8\u8457\u4f5c\u6709\u5173\nA. \u73ed\u56fa\u7684\u300a\u4e24\u90fd\u8d4b\u300b\nB. \u5de6\u601d\u7684\u300a\u4e09\u90fd\u8d4b\u300b\nC. \u5f20\u8861\u7684\u300a\u4e8c\u4eac\u8d4b\u300b\nD. \u53f8\u9a6c\u76f8\u5982\u7684\u300a\u957f\u516d\u8d4b\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2893743369926839, "meta-math/MetaMath-Mistral-7B": 0.3306562312783846, "itpossible/Chinese-Mistral-7B-v0.1": 0.40067967579776836, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.37480848372461606, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5938157464625802}}, {"question": "\u5baa\u6cd5\u7684\u4fee\u6539\u7a0b\u5e8f\u6bd4\u5236\u5b9a\u3001\u4fee\u6539\u666e\u901a\u6cd5\u5f8b\u66f4\u4e25\u683c\uff0c\u8981\u7531\u5168\u56fd\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u5e38\u52a1\u59d4\u5458\u4f1a\u6216\u8005\u5168\u56fd\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u5168\u4f53\u4ee3\u8868\u4e2d\u7684\nA. 2/3\u4ee5\u4e0a\u63d0\u8bae\nB. 1/2\u4ee5\u4e0a\u63d0\u8bae\nC. 1/5\u4ee5\u4e0a\u63d0\u8bae\nD. 1/3\u4ee5\u4e0a\u63d0\u8bae\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u79cd\u64cd\u4f5c\u53ef\u9000\u51faWord\u8f6f\u4ef6\nA. \u5355\u51fb\u6587\u4ef6\u83dc\u5355\uff0c\u9009\u62e9\u9000\u51fa\u9009\u9879\nB. \u5355\u51fb\u6587\u4ef6\u83dc\u5355\uff0c\u9009\u62e9\u5173\u95ed\u9009\u9879\nC. \u5355\u51fb\u6587\u4ef6\u83dc\u5355\uff0c\u9009\u62e9\u53e6\u5b58\u4e3a\u9009\u9879 \nD. \u5355\u51fb\u6587\u4ef6\u83dc\u5355\uff0c\u9009\u62e9\u53d1\u9001\u9009\u9879\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u8bb0\u6ce2\u5916\u7fc1\u300b\u4e2d\u6709\u4e24\u53e5\u633d\u8bd7\uff1a\u201c\u95e8\u751f\u6414\u767d\u9996\uff0c\u65e6\u5915\u9aa8\u6210\u7070\u201d\u3002\u5176\u4e2d\u201c\u95e8\u751f\u201d\u662f\u6307\nA. \u53f0\u9759\u519c\nB. \u4e54\u5927\u58ee\nC. \u8bb8\u5bff\u88f3\nD. \u9b4f\u5efa\u529f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.34686667406054084, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.34027867648598925, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6700\u53e4\u8001\u548c\u6700\u5177\u666e\u904d\u610f\u4e49\u7684\u5408\u4f5c\u5f62\u5f0f\u662f\nA. \u5236\u5ea6\u5316\u7684\u4f20\u7edf\u5408\u4f5c\nB. \u6307\u5bfc\u6027\u5408\u4f5c\nC. \u81ea\u53d1\u6027\u5408\u4f5c\nD. \u5951\u7ea6\u5f0f\u5408\u4f5c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.45495529608633845, "meta-math/MetaMath-Mistral-7B": 0.686943020307427, "itpossible/Chinese-Mistral-7B-v0.1": 0.38193364029483967, "HuggingFaceH4/zephyr-7b-beta": 0.8588143796443795, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5590617582087785, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.603964129072825}}, {"question": "\u5728\u4eba\u4e8b\u804c\u4f4d\u5206\u7c7b\u4e2d\uff0c\u5de5\u4f5c\u6027\u8d28\u3001\u6743\u529b\u5927\u5c0f\u3001\u8d23\u4efb\u8f7b\u91cd\u3001\u6240\u9700\u8d44\u683c\u6761\u4ef6\u3001\u5de5\u4f5c\u96be\u6613\u7a0b\u5ea6\u57fa\u672c\u76f8\u540c\u7684\u4e00\u4e2a\u804c\u4f4d\u5e8f\u5217\u79f0\u4e3a\nA. \u804c\u7b49\nB. \u804c\u7ea7\nC. \u804c\u7c7b\nD. \u804c\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6ee1\u9762\u901a\u7ea2\u7684\u75c5\u673a\u662f\nA. \u771f\u5bd2\u5047\u70ed\nB. \u90aa\u70ed\u4ea2\u76db\nC. \u865a\u9633\u4e0a\u8d8a\nD. \u9634\u865a\u706b\u65fa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28570956661341396, "meta-math/MetaMath-Mistral-7B": 0.47340442378211883, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.48765833488362453, "meta-llama/Meta-Llama-3-8B": 0.4605489813468967, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c23\u5c81\u3002\u56e0\u4e4f\u529b10\u5929\u3001\u7259\u9f88\u51fa\u8840\u4f34\u76ae\u80a4\u6de4\u65914 \u5929\u5165\u9662\uff0c\u65e2\u5f80\u4f53\u5065\u3002\u5316\u9a8c\u8840 Hb 76 g/L\uff0cWBC 25X10^9/L\uff0cPlt 29X10^9/L\uff0c\u9aa8\u9ad3\u589e\u751f\u660e\u663e\u6d3b\u8dc3\uff0c\u539f\u59cb\u7ec6\u80de\u536060%\uff0cPOX\u67d3\u8272(-)\uff0cPAS\u67d3\u8272(+)\u6210\u5757\uff0cNSE \u67d3\u8272(-)\u3002\u8be5\u60a3\u8005\u7684\u8bca\u65ad\u662f\nA. \u6025\u6027\u7ea2\u767d\u8840\u75c5\nB. \u6025\u6027\u7c92\u7ec6\u80de\u767d\u8840\u75c5\nC. \u6025\u6027\u5355\u6838\u7ec6\u80de\u767d\u8840\u75c5\nD. \u6025\u6027\u6dcb\u5df4\u7ec6\u80de\u767d\u8840\u75c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679c\u7532\u6570\u76842/3\u7b49\u4e8e\u4e59\u6570\u76843/5\uff0c\u90a3\u4e48\u7532\u6570\uff1a\u4e59\u6570\u7b49\u4e8e\nA. 10\uff1a9 \nB. 15\uff1a6\nC. 6\uff1a15\nD. 9\uff1a10 \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.38167053302342646, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7489136959839101, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3148300531811561, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u957f\u65f6\u95f4\u5367\u5e8a\u8005\u7a81\u7136\u7ad9\u8d77\u65f6\u611f\u89c9\u5934\u6655\u3001\u773c\u524d\u53d1\u9ed1\u7684\u4e3b\u8981\u539f\u56e0\u662f\nA. \u5fc3\u6cf5\u529f\u80fd\u51cf\u5f31\nB. \u5faa\u73af\u8840\u91cf\u51cf\u5c11\nC. \u964d\u538b\u53cd\u5c04\u654f\u611f\u6027\u964d\u4f4e\nD. \u9759\u8109\u56de\u6d41\u51cf\u5c11\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.47745719871974207, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8863888473443228}}, {"question": "\u5f53\u4ee3\u4e2d\u56fd\u793e\u4f1a\u53d7\u5230\u591a\u79cd\u6cd5\u5f8b\u6587\u5316\u7684\u5f71\u54cd\uff0c\u5176\u4e2d\u5f71\u54cd\u6700\u5927\u7684\u662f\nA. \u897f\u65b9\u8d44\u672c\u4e3b\u4e49\u7684\u6cd5\u5f8b\u6587\u5316\nB. \u4e2d\u56fd\u4f20\u7edf\u7684\u6cd5\u5f8b\u6587\u5316\nC. \u4e2d\u56fd\u793e\u4f1a\u4e3b\u4e49\u5efa\u8bbe\u8fc7\u7a0b\u4e2d\u5f62\u6210\u7684\u6cd5\u5f8b\u6587\u5316\nD. \u82cf\u8054\u7684\u6cd5\u5f8b\u6587\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5151454998399273, "HuggingFaceH4/zephyr-7b-beta": 0.6784101137608657, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8651630238167414}}, {"question": "\u8bbe $(X\uff0c Y)$ \u670d\u4ece\u4e8c\u7ef4\u6b63\u6001\u5206\u5e03\uff0c \u5219\u968f\u673a\u53d8\u91cf $\\xi=X+Y$ \u4e0e $\\eta=X-Y$ \u4e0d\u76f8\u5173\u7684 \u5145\u5206\u5fc5\u8981\u6761\u4ef6\u662f ( )\nA. $E\\left(X^2\\right)+[E(X)]^2=E\\left(Y^2\\right)+[E(Y)]^2$\nB. $E(X)=E(Y)$\nC. $E\\left(X^2\\right)-[E(X)]^2=E\\left(Y^2\\right)-[E(Y)]^2$\nD. $E\\left(X^2\\right)=E\\left(Y^2\\right)$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37824561029018067, "meta-math/MetaMath-Mistral-7B": 0.371068906204966, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3534716292209113, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5710154803573904}}, {"question": "\u300a\u5b66\u8bb0\u300b\u4e0a\u8bf4\uff1a\u201c\u4e0d\u9675\u8282\u800c\u65bd\u201d\uff0c\u8fd9\u53e5\u8bdd\u4f53\u73b0\u4e86\nA. \u5faa\u5e8f\u6e10\u8fdb\u7684\u6559\u5b66\u539f\u5219\nB. \u56e0\u6750\u65bd\u6559\u7684\u6559\u5b66\u539f\u5219\nC. \u542f\u53d1\u6027\u6559\u5b66\u539f\u5219\nD. \u5de9\u56fa\u6027\u6559\u5b66\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5b5f\u5b50\u8bf4\uff1a\u201c\u541b\u5b50\u6709\u4e09\u4e50\u201d\uff0c\u4e0b\u5217\u54ea\u9879\u4e0d\u5728\u5176\u201c\u4e09\u4e50\u201d\u4e4b\u5217\nA. \u4e61\u4eba\u65e0\u4e0d\u79f0\u5176\u5584\u4e5f\nB. \u5f97\u5929\u4e0b\u82f1\u624d\u800c\u6559\u80b2\u4e4b\nC. \u4ef0\u4e0d\u6127\u4e8e\u5929\uff0c\u4fef\u4e0d\u4f5c\u4e8e\u4eba\nD. \u7236\u6bcd\u4ff1\u5b58\uff0c\u5144\u5f1f\u65e0\u6545\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34686667406054084, "meta-math/MetaMath-Mistral-7B": 0.47905137183815366, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9915466824846279, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6536630745817132, "meta-llama/Meta-Llama-3-8B": 0.37578678665765913, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7ecf\u6d4e\u5168\u7403\u5316\u7684\u57fa\u7840\u662f\nA. \u6218\u540e\u79d1\u5b66\u6280\u672f\u7684\u8fc5\u731b\u53d1\u5c55\nB. \u6218\u540e\u591a\u8fb9\u8d38\u6613\u7684\u8fc5\u901f\u53d1\u5c55\nC. \u5e03\u96f7\u987f\u68ee\u6797\u4f1a\u8bae\u4f53\u7cfb\u5d29\u6e83\nD. \u6218\u540e\u91d1\u878d\u5e02\u573a\u7684\u8fc5\u901f\u53d1\u5c55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.874230870135439, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u5173\u4e8e\u5236\u52a8\u7684\u63cf\u8ff0\u4e0d\u6b63\u786e\u7684\u662f\nA. \u957f\u671f\u5236\u52a8\u53ef\u589e\u52a0\u65b0\u7684\u529f\u80fd\u969c\u788d\nB. \u957f\u671f\u5236\u52a8\u53ef\u5f15\u8d77\u5931\u7528\u7efc\u5408\u75c7\nC. \u5236\u52a8\u662f\u67d0\u4e9b\u4f24\u75c5\u5371\u91cd\u671f\u7684\u5fc5\u8981\u63aa\u65bd\nD. \u5236\u52a8\u4e0d\u4f1a\u5f15\u8d77\u5176\u4ed6\u7cfb\u7edf\u7684\u529f\u80fd\u969c\u788d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6450534584253055, "HuggingFaceH4/zephyr-7b-beta": 0.9934398481303958, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8286861799957894, "meta-llama/Meta-Llama-3-8B": 0.4789478409040162, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6428996492767176}}, {"question": "\u5361\u897f\u5c3c\u53f7\u63a2\u6d4b\u5668\u9884\u8ba1\u57282017\u5e74\u665a\u4e9b\u65f6\u5019\u7ed3\u675f\u81ea\u5df1\u571f\u661f\u7cfb\u7edf\u7684\u63a2\u6d4b\u4efb\u52a1\u3002\u6309\u7167\u8ba1\u5212\uff0c\u5b83\u5c06\u4ee5\u4f55\u79cd\u65b9\u5f0f\u7ed3\u675f\u81ea\u5df1\u7684\u63a2\u7d22\u4efb\u52a1\nA. \u76f4\u63a5\u98de\u79bb\u592a\u9633\u7cfb\nB. \u5760\u5165\u571f\u661f\nC. \u98de\u8dc3\u5929\u738b\u661f\uff0c\u7136\u540e\u98de\u79bb\u592a\u9633\u7cfb\nD. \u5760\u5165\u571f\u536b\u516d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8047399517329804, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7851148604690551, "meta-llama/Meta-Llama-3-8B": 0.43495988922323, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7814\u7a76\u8005\u5728\u8fdb\u884c\u8bd5\u9a8c\u7814\u7a76\u65f6\uff0c\u5fc5\u987b\u8003\u8651\u5230\u4e09\u7c7b\u53d8\u91cf\uff0c\u5206\u522b\u662f\u81ea\u53d8\u91cf\u3001\u56e0\u53d8\u91cf\u548c\nA. \u63a7\u5236\u53d8\u91cf\nB. \u4e0d\u53ef\u63a7\u5236\u53d8\u91cf\nC. \u73af\u5883\u53d8\u91cf\nD. \u53cd\u5e94\u53d8\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3235140024559356, "meta-math/MetaMath-Mistral-7B": 0.8502824822965996, "itpossible/Chinese-Mistral-7B-v0.1": 0.38336772865499974, "HuggingFaceH4/zephyr-7b-beta": 0.9729950484320885, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5740063293499342, "meta-llama/Meta-Llama-3-8B": 0.5941354540928966, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9424338920301636}}, {"question": "\u5728\u4f5b\u6559\u4f20\u5165\u4e2d\u539f\u4e4b\u540e\uff0c\u4fee\u7b51\u7684\u7b2c\u4e00\u5ea7\u5bfa\u5e99\u662f\nA. \u6851\u8036\u5bfa\nB. \u767d\u9a6c\u5bfa\nC. \u6f6d\u67d8\u5bfa\nD. \u5c11\u6797\u5bfa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.325455072595945, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6372737980478332, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u738b\u2f7c\u5e08\u51c6\u5907\u2f6480\u5143\u94b1\u4e3a\u5e74\u7ea7\u83b7\u5956\u8fd0\u52a8\u5458\u4e70\u5956\u54c1\uff0c\u4ed6\u5148\u82b148.6\u5143\u4e70\u4e869\u672c\u7b14\u8bb0\u672c\uff0c\u5e76\u51c6\u5907\u2f64\u4f59\u4e0b\u7684\u94b1\u4e70\u2f00\u4e9b\u94a2\u7b14\uff0c\u94a2\u7b14\u6bcf\u679d3.2\u5143\u3002\u738b\u2f7c\u5e08\u8fd8\u53ef\u4ee5\u4e70\u2f0f\u679d\u94a2\u7b14\nA. 9\nB. 10\nC. 8\nD. 11\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u25b3ABC\u4e2d\uff0ca\uff0cb\uff0cc\u5206\u522b\u662f\u2ec6A\uff0cB\uff0cC\u6240\u5bf9\u7684\u8fb9\uff0c\u4e14a\uff1d6\uff0cb\uff1d8\uff0cA\uff1d30\u00b0\uff0c\u5219\u6ee1\u2f9c\u6761\u4ef6\u7684\u4e09\u2ec6\u5f62\u6709\nA. \u65e0\u6570\u4e2a\nB. 1\u4e2a\nC. 0\u4e2a\nD. 2\u4e2a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u79fb\u6c11\u6d41\u884c\u75c5\u5b66\u7684\u7814\u7a76\u76ee\u7684\u662f\u63a2\u7d22\nA. \u80bf\u7624\u548c\u6162\u6027\u75c5\u7684\u9057\u4f20\u56e0\u7d20\nB. \u536b\u751f\u6c34\u5e73\u5bf9\u75be\u75c5\u7684\u5f71\u54cd\nC. \u75be\u75c5\u4e0e\u73af\u5883\u7684\u5173\u7cfb\nD. \u9057\u4f20\u548c\u73af\u5883\u7684\u5173\u7cfb\u5b70\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u51dd\u8840\u8fc7\u7a0b\u4e2d\u51dd\u8840\u9176\u539f\u4e0e\u51dd\u8840\u56e0\u5b50\u7ed3\u5408\u540e\uff0c\u8f6c\u53d8\u4e3a\u6709\u6d3b\u6027\u7684\u51dd\u8840\u9176\uff0c\u800c\u51dd\u8840\u9176\u7684\u4ea7\u751f\u53c8\u80fd\u52a0\u901f\u51dd\u8840\u9176\u539f\u4e0e\u51dd\u8840\u56e0\u5b50\u7684\u7ed3\u5408\uff0c\u4e0b\u5217\u54ea\u9879\u8c03\u8282\u8fc7\u7a0b\u7684\u673a\u5236\u4e0e\u6b64\u6700\u4e3a\u76f8\u4f3c\nA. \u4e34\u8fd1\u6392\u5375\u65f6\uff0c\u96cc\u6027\u6fc0\u7d20\u6d53\u5ea6\u5347\u9ad8\uff0c\u4fc3\u8fdb\u4fc3\u6027\u817a\u6fc0\u7d20\u5206\u6ccc\nB. \u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u6355\u98df\u8005\u6570\u91cf\u589e\u957f\uff0c\u4f7f\u88ab\u6355\u98df\u8005\u6570\u91cf\u51cf\u5c11\nC. \u5bd2\u51b7\u65f6\uff0c\u7532\u72b6\u817a\u6fc0\u7d20\u6d53\u5ea6\u5347\u9ad8\uff0c\u6291\u5236\u4fc3\u7532\u72b6\u817a\u6fc0\u7d20\u5206\u6ccc\nD. \u8fdb\u9910\u540e\uff0c\u80f0\u5c9b\u7d20\u5206\u6ccc\u589e\u591a\uff0c\u4f7f\u8840\u7cd6\u6d53\u5ea6\u4e0b\u964d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3128363857141096, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u4f53\u5185\u751f\u7269\u6d3b\u6027\u6700\u5f3a\u7684\u96c4\u6fc0\u7d20\u662f\nA. \u53cc\u6c22\u777e\u916e\nB. \u96c4\u70ef\u4e8c\u916e\nC. \u777e\u916e\nD. \u96c4\u916e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6143787615120356, "itpossible/Chinese-Mistral-7B-v0.1": 0.39188406424799155, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3277078385444777, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0d\u9508\u94a2\u5236\u54c1\u4e0e\u6211\u4eec\u7684\u65e5\u5e38\u751f\u6d3b\u5bc6\u5207\u76f8\u5173\uff0c\u4e0d\u9508\u94a2\u7684\u4e3b\u8981\u7ec4\u6210\u5143\u7d20\u662f\nA. \u94c1\u3001\u94ec\u3001\u954d\nB. \u94dc\u3001\u94c1\u3001\u94ec\nC. \u94c1\u3001\u78b3\nD. \u94dc\u3001\u950c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7644889551958606, "meta-math/MetaMath-Mistral-7B": 0.7847591356669986, "itpossible/Chinese-Mistral-7B-v0.1": 0.821582187445139, "HuggingFaceH4/zephyr-7b-beta": 0.9523433664328198, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.9722446047625978, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.996470848733343}}, {"question": "\u6668\u96fe\u4e2d\u8f83\u9ad8\u7684\u542b\u6709\u54ea\u79cd\u5143\u7d20\u4e0d\u5229\u4e8e\u4eba\u4f53\u5065\u5eb7\nA. \u94dd\nB. \u6c2f\nC. \u94c5\nD. \u6c2e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6047390173301035, "meta-math/MetaMath-Mistral-7B": 0.46443206200322357, "itpossible/Chinese-Mistral-7B-v0.1": 0.5889976934894816, "HuggingFaceH4/zephyr-7b-beta": 0.9939402969953522, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8638175409942268, "meta-llama/Meta-Llama-3-8B": 0.533743622810805, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9300362035289427}}, {"question": "\u4e0d\u5c5e\u4e8e\u9ab6\u4e1b\u7684\u795e\u7ecf\u662f\nA. \u81c0\u4e0b\u795e\u7ecf\nB. \u80a1\u795e\u7ecf\nC. \u5750\u9aa8\u795e\u7ecf\nD. \u81c0\u4e0a\u795e\u7ecf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28650612283664895, "meta-math/MetaMath-Mistral-7B": 0.29258944112561125, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3148300531811561, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5c0f\u738b\u8bf4\uff1a\u5982\u679c\u660e\u5929\u4e0d\u4e0b\u5927\u96e8\uff0c\u6211\u4e00\u5b9a\u53bb\u770b\u8db3\u7403\u6bd4\u8d5b \u4ee5\u4e0b\u54ea\u9879\u4e3a\u771f\uff0c\u53ef\u4ee5\u8bc1\u660e\u5c0f\u738b\u6ca1\u6709\u8bf4\u771f\u8bdd?a\u5929\u6ca1\u4e0b\u5927\u96e8\uff0c\u5c0f\u738b\u6ca1\u53bb\u770b\u8db3\u7403\u8d5b\uff1bb\u5929\u4e0b\u5927\u96e8\uff0c\u5c0f\u738b\u53bb\u770b\u4e86\u8db3\u7403\u8d5b\uff1bc\u5929\u4e0b\u5927\u96e8\uff0c\u5c0f\u738b\u6ca1\u53bb\u770b\u8db3\u7403\u8d5b\nA. ab\nB. a\nC. b\nD. c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u4eec\u5728\u5b9e\u8df5\u4e2d\u5f62\u6210\u7684\u5bf9\u4e8e\u4eba\u751f\u76ee\u7684\u548c\u610f\u4e49\u7684\u6839\u672c\u770b\u6cd5\u548c\u6001\u5ea6\u7684\u662f\nA. \u4eba\u751f\u89c2\nB. \u81ea\u7136\u89c2\nC. \u5386\u53f2\u89c2\nD. \u79d1\u5b66\u89c2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9438891691827241, "meta-math/MetaMath-Mistral-7B": 0.9932324886768344, "itpossible/Chinese-Mistral-7B-v0.1": 0.9859117306250309, "HuggingFaceH4/zephyr-7b-beta": 0.9998057747857777, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9965555863568422, "meta-llama/Meta-Llama-3-8B": 0.9717737581327486, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9953746729696961}}, {"question": "\u67d0\u4e00\u6c9f\u901a\u4e3b\u4f53\u548c\u4e24\u4e2a\u4ee5\u4e0a\u7684\u4e0a\u7ea7\u7ec4\u7ec7\u8fdb\u884c\u6c9f\u901a\uff0c\u540c\u65f6\u53c8\u548c\u4e0b\u7ea7\u7ec4\u7ec7\u4fdd\u6301\u94fe\u5f0f\u6c9f\u901a\u7684\u6c9f\u901a\u65b9\u5f0f\uff0c\u88ab\u5f62\u8c61\u7684\u79f0\u4e3a\nA. Y\u5f0f\u6c9f\u901a\nB. \u8f6e\u5f0f\u6c9f\u901a\nC. \u73af\u5f0f\u6c9f\u901a\nD. \u5168\u901a\u9053\u6c9f\u901a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "21\u4e16\u7eaa\u94f6\u884c\u4e1a\u51fa\u73b0\u4e86\u7f51\u4e0a\u94f6\u884c\u3001\u7535\u5b50\u94f6\u884c\u3001\u6570\u5b57\u94f6\u884c\uff0c\u8fd9\u4e9b\u94f6\u884c\u7684\u7a81\u51fa\u7279\u70b9\u662f\u2014\u2014\u4efb\u4f55\u4eba\u3001\u5728\u4efb\u4f55\u65f6\u5019\u3001\u4efb\u4f55\u5730\u70b9\u3001\u91c7\u53d6\u4efb\u4f55\u65b9\u5f0f\uff0c\u90fd\u80fd\u83b7\u5f97\u6240\u9700\u8981\u7684\u4efb\u4f55\u91d1\u878d\u4ea7\u54c1\u548c\u670d\u52a1\u3002\u5b83\u4eec\u4f7f\u4e3a\u4e2a\u4eba\u5ba2\u6237\u63d0\u4f9b\u65b9\u4fbf\u3001\u5feb\u6377\u3001\u9ad8\u6548\u3001\u5b89\u5168\u7684\u4e1a\u52a1\u53d8\u4e3a\u53ef\u80fd\u3002\u5bf9\u6750\u6599\u4e2d\u63d0\u5230\u7684\u94f6\u884c\u7684\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f\nA. \u8fd9\u4e9b\u94f6\u884c\u53ef\u4ee5\u81ea\u884c\u51b3\u5b9a\u6240\u7ecf\u8425\u7684\u4e1a\u52a1\uff0c\u4e0d\u53d7\u4e2d\u592e\u94f6\u884c\u9886\u5bfc\nB. \u5b83\u4eec\u7684\u51fa\u73b0\u662f\u94f6\u884c\u4e1a\u53d1\u5c55\u7684\u5fc5\u7136\u7ed3\u679c\nC. \u8fd9\u4e9b\u94f6\u884c\u4ecd\u7136\u4ee5\u7ecf\u8425\u4eba\u6c11\u5e01\u7684\u4fe1\u8d37\u4e3a\u4e3b\u8981\u4e1a\u52a1\nD. \u8fd9\u4e9b\u94f6\u884c\u4e0d\u80fd\u81ea\u884c\u51b3\u5b9a\u5b58\u8d37\u6b3e\u5229\u7387\u6c34\u5e73\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5676326363218376, "meta-math/MetaMath-Mistral-7B": 0.812007569377346, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8764266413007081, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8477927685221144, "meta-llama/Meta-Llama-3-8B": 0.5276865681397754, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u201c\u4e94\u56db\u201d\u6587\u5b66\u9769\u547d\u4e2d\uff0c\u63d0\u51fa\u201c\u56fd\u8bed\u7684\u6587\u5b66\uff0c\u6587\u5b66\u7684\u56fd\u8bed\u201d\u7684\u4eba\u7269\u662f\nA. \u9648\u72ec\u79c0\nB. \u80e1\u9002\nC. \u5218\u534a\u519c\nD. \u94b1\u7384\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4918490412639172, "meta-math/MetaMath-Mistral-7B": 0.8653767533860655, "itpossible/Chinese-Mistral-7B-v0.1": 0.35686329776860143, "HuggingFaceH4/zephyr-7b-beta": 0.8963908220378921, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4507224118589883, "meta-llama/Meta-Llama-3-8B": 0.7845480137282593, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8fc7\u7a0b\u80fd\u53cc\u5411\u8fdb\u884c\u7684\u662f\nA. \u80fd\u91cf\u5728\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u6d41\u52a8\nB. \u690d\u7269\u751f\u957f\u7d20\u7684\u6781\u6027\u8fd0\u8f93\nC. HIV\u75c5\u6bd2\u7684\u9057\u4f20\u4fe1\u606f\u5728DNA\u548cRNA\u4e4b\u95f4\u7684\u6d41\u52a8\nD. \u53cd\u5c04\u6d3b\u52a8\u4e2d\uff0c\u5174\u594b\u5728\u795e\u7ecf\u7ea4\u7ef4\u4e0a\u7684\u4f20\u5bfc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.35686329776860143, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f53\u5185\u8f6c\u8fd0\u4e00\u78b3\u5355\u4f4d\u7684\u8f7d\u4f53\u662f\nA. \u751f\u7269\u7d20\nB. SAM\nC. \u56db\u6c22\u53f6\u9178\nD. \u53f6\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5521983915090277, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.28661281177772774, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u79cd\u884c\u4e3a\u4e4b\u6240\u4ee5\u88ab\u89c6\u4e3a\u504f\u5dee\u884c\u4e3a\uff0c\u662f\u56e0\u4e3a\u793e\u4f1a\u7684\u6743\u529b\u96c6\u56e2\u7ed9\u8fd9\u79cd\u884c\u4e3a\u8d34\u4e0a\u4e86\u504f\u5dee\u7684\u6807\u7b7e\uff0c\u504f\u5dee\u884c\u4e3a\u53ea\u662f\u4e00\u79cd\u88ab\u793e\u4f1a\u5b9a\u4e49\u4e3a\u504f\u5dee\u7684\u884c\u4e3a\u3002\u8fd9\u662f\u89e3\u91ca\u504f\u5dee\u884c\u4e3a\u7684\nA. \u6807\u7b7e\u7406\u8bba\nB. \u793a\u8303\u7406\u8bba\nC. \u6587\u5316\u4f20\u9012\u7406\u8bba\nD. \u529f\u80fd\u7406\u8bba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.858179445852016, "meta-math/MetaMath-Mistral-7B": 0.9453231103610917, "itpossible/Chinese-Mistral-7B-v0.1": 0.7718681363830373, "HuggingFaceH4/zephyr-7b-beta": 0.7891887602988542, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8865703201972461, "meta-llama/Meta-Llama-3-8B": 0.8575665965751161, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9785177323484234}}, {"question": "\u5bf9\u4e8e\u4e00\u4e2a\u8f83\u957f\u7cfb\u5217\u7684\u6750\u6599\uff0c\u4e2d\u95f4\u90e8\u5206\u8bb0\u5fc6\u7684\u6548\u679c\u5dee\uff0c\u4e24\u7aef\u6548\u679c\u597d\uff0c\u8fd9\u662f\u7531\u4e8e\u4e2d\u95f4\u90e8\u5206\u53d7\u5230\u54ea\u79cd\u6291\u5236\u7684\u5e72\u6270\nA. \u5012\u6444\nB. \u5355\u4e00\nC. \u524d\u6444\nD. \u53cc\u91cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u987e\u708e\u6b66\u300a\u65e5\u77e5\u5f55\u300b\u4e2d\u5199\u9053\uff1a\u201c\u5fc3\u4e0d\u5f85\u4f20\u4e5f\uff0c\u6d41\u884c\u5929\u5730\u95f4\uff0c\u8d2f\u5f7b\u53e4\u4eca\u800c\u4e0d\u540c\u8005\u7406\u4e5f\uff0c\u7406\u5177\u4e8e\u543e\u5fc3\u800c\u9a8c\u4e8e\u7269\u3002\u5fc3\u8005\uff0c\u6240\u4ee5\u7edf\u5b97\u6b64\u7406\u800c\u522b\u5176\u662f\u975e\uff0c\u5fc3\u5b66\u4e8c\u5b57\uff0c\u300a\u516d\u7ecf\u300b\u3001\u5b54\u5b5f\u6240\u4e0d\u9053\u3002\u201d\u8fd9\u4e00\u89c2\u70b9\u8868\u660e\u4ed6\nA. \u4ee5\u62ef\u6551\u65f6\u4ee3\u4e3a\u5df1\u4efb\nB. \u5bf9\u9646\u738b\u5fc3\u5b66\u6301\u6279\u5224\u7684\u6001\u5ea6\nC. \u4e3b\u5f20\u56de\u5f52\u5b54\u5b5f\u5112\u5b66\nD. \u80af\u5b9a\u5fc3\u5b66\u7684\u4fee\u8eab\u517b\u6027\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4804574945937191, "meta-math/MetaMath-Mistral-7B": 0.8929821572879225, "itpossible/Chinese-Mistral-7B-v0.1": 0.3976839276686677, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f01\u4e1a\u5efa\u9020\u56fa\u5b9a\u8d44\u4ea7\u8fbe\u5230\u9884\u5b9a\u53ef\u4f7f\u7528\u72b6\u6001\u524d\u6240\u53d1\u751f\u7684\u652f\u51fa\u6784\u6210\u56fa\u5b9a\u8d44\u4ea7\u7684\nA. \u91cd\u7f6e\u4ef7\u503c\nB. \u539f\u59cb\u4ef7\u503c\nC. \u6298\u4f59\u4ef7\u503c\nD. \u51c0\u503c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.48091734978098305, "meta-math/MetaMath-Mistral-7B": 0.6547273119542076, "itpossible/Chinese-Mistral-7B-v0.1": 0.5262332654733343, "HuggingFaceH4/zephyr-7b-beta": 0.9980098089028561, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8513289354881399, "meta-llama/Meta-Llama-3-8B": 0.7588135949842414, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.38335319777388044}}, {"question": "\u7537\u6027\uff0c65 \u5c81\u3002\u53f3\u8179\u80a1\u6c9f\u5305\u5757 3 \u5e74\uff0c\u5367\u4f4d\u53ef\u6d88\u5931\uff0c12 \u5c0f\u65f6\u524d\u7a81\u7136\u4e0d\u80fd\u8fd8\u7eb3\uff0c\u5e76\u51fa\u73b0\u53f3\u4e0b\u8179\u75db\u3002\u67e5\u4f53\uff1aT38\u2103\uff0cP100 \u6b21/\u5206\uff0c\u53f3\u4fa7\u8179\u80a1\u6c9f 4\u00d73cm \u80bf\u5757\uff0c\u89e6\u75db\u660e\u663e\uff0c\u53f3\u4e0b\u8179\u6709\u538b\u75db\u53ca\u808c\u7d27\u5f20\u3002\u6b63\u786e\u7684\u6cbb\u7597\u63aa\u65bd\u662f\nA. \u6025\u884c\u759d\u4fee\u8865\u672f\nB. \u5207\u9664\u574f\u6b7b\u80a0\u7ba1\u3001\u9ad8\u4f4d\u7ed3\u624e\u759d\u56ca\nC. \u6ce8\u5c04\u6b62\u75db\u5242\u540e\u624b\u6cd5\u590d\u4f4d\nD. \u5207\u9664\u574f\u6b7b\u80a0\u7ba1\u540e\u759d\u4fee\u8865\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3341942123326676, "meta-math/MetaMath-Mistral-7B": 0.6558704388781162, "itpossible/Chinese-Mistral-7B-v0.1": 0.4152631491355974, "HuggingFaceH4/zephyr-7b-beta": 0.9301003631647863, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7587786298374306, "meta-llama/Meta-Llama-3-8B": 0.34115916906196886, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.758214664993723}}, {"question": "\u5728\u7535\u529b\u7f51\u4e2d\uff0c\u5f53\u7535\u611f\u5143\u4ef6\u4e0e\u7535\u5bb9\u5143\u4ef6\u53d1\u751f\u4e32\u8054\u611f\u6297\u7b49\u4e8e\u5bb9\u6297\u65f6\uff0c\u5c31\u4f1a\u53d1\uff08\uff09\u8c10\u632f\u73b0\u8c61\u3002\nA. \u94c1\u78c1\nB. \u78c1\u573a\nC. \u7535\u538b\nD. \u7535\u6d41\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5512351589956723, "meta-math/MetaMath-Mistral-7B": 0.6213812467901185, "itpossible/Chinese-Mistral-7B-v0.1": 0.5280592543921383, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5241546450465493, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.581568221094561}}, {"question": "\u75c5\u539f\u4f53\u80fd\u591f\u5f15\u8d77\u4e34\u5e8a\u75be\u75c5\u7684\u80fd\u529b\u88ab\u79f0\u4e3a\nA. \u81f4\u75c5\u529b\nB. \u4fb5\u88ad\u529b\nC. \u4f20\u67d3\u6027\nD. \u6bd2\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5843215089077143, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6869430171972232, "meta-llama/Meta-Llama-3-8B": 0.6411275022985651, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8208208097661542}}, {"question": "SMA\u9762\u5c42\u538b\u5b9e\u5ea6\u662f\u6307\nA. \u73b0\u573a\u5b9e\u9645\u6e7f\u5bc6\u5ea6\u4e0e\u5ba4\u5185\u51fb\u5b9e\u8bd5\u9a8c\u6700\u5927\u6e7f\u5bc6\u5ea6\u4e4b\u6bd4\nB. \u73b0\u573a\u5b9e\u9645\u5e72\u5bc6\u5ea6\u4e0e\u5ba4\u5185\u51fb\u5b9e\u8bd5\u9a8c\u6700\u5927\u5e72\u5bc6\u5ea6\u4e4b\u6bd4\nC. \u73b0\u573a\u5b9e\u9645\u5bc6\u5ea6\u4e0e\u5ba4\u5185\u6807\u51c6\u5bc6\u5ea6\u4e4b\u6bd4\nD. \u73b0\u573a\u5b9e\u9645\u5e72\u5bc6\u5ea6\u4e0e\u5ba4\u5185\u6807\u51c6\u5bc6\u5ea6\u4e4b\u6bd4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34388766643271823, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6568296999583718, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.42858632103350996}}, {"question": "\u4e2d\u56fd\u3001\u82cf\u8054\u7ed3\u675f30\u5e74\u7684\u8bba\u6218\u3001\u9694\u79bb\u3001\u5bf9\u6297\uff0c\u51b3\u5b9a\u5b9e\u73b0\u5173\u7cfb\u6b63\u5e38\u5316\uff0c\u4f46\u4e24\u56fd\u6700\u9ad8\u9886\u5bfc\u4eba\u89c1\u9762\uff0c\u7a76\u7adf\u65bd\u884c\u4ec0\u4e48\u793c\u8282\uff0c\u5f15\u8d77\u4e16\u4eba\u5173\u6ce8\u3002\u539f\u6765\uff0c\u793e\u4f1a\u4e3b\u4e49\u56fd\u5bb6\u4e4b\u95f4\u9886\u5bfc\u4eba\u89c1\u9762\uff0c\u603b\u4f1a\u4e92\u76f8\u62e5\u62b1\u3001\u751a\u81f3\u4eb2\u543b\u5bf9\u65b9\u7684\u8138\u988a\u3002\u7ed3\u679c\u73b0\u5728\u4e2d\u82cf\u4e24\u56fd\u9886\u5bfc\u4eba\u7684\u89c1\u9762\u793c\u8282\u91c7\u7528\u4e86\u56fd\u9645\u4e0a\u901a\u884c\u7684\u201c\u63e1\u624b\u201d\u3002\u8fd9\u8868\u660e\nA. \u4e16\u754c\u591a\u6781\u5316\u8d8b\u52bf\u5f00\u59cb\u51fa\u73b0\nB. \u4e2d\u56fd\u5949\u884c\u4e0d\u7ed3\u76df\u7684\u5916\u4ea4\u653f\u7b56\nC. \u7f8e\u82cf\u4e89\u9738\u5bfc\u81f4\u82cf\u8054\u5373\u5c06\u89e3\u4f53\nD. \u4e2d\u82cf\u5173\u7cfb\u8fd8\u5f85\u8fdb\u4e00\u6b65\u6539\u5584\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7952253247330686, "meta-math/MetaMath-Mistral-7B": 0.848911670760236, "itpossible/Chinese-Mistral-7B-v0.1": 0.37672193122020287, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8771729973653304, "meta-llama/Meta-Llama-3-8B": 0.5461997617017545, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5317\u4eac\u5e02\u5386\u53f2\u60a0\u4e45\uff0c\u5176\u5efa\u5236\u5728\u5404\u671d\u5404\u4ee3\u4e2d\u66fe\u6709\u4e0d\u540c\u540d\u79f0\uff0c\u4ee5\u4e0b\u9009\u9879\u4e2d\uff0c\u4e0d\u662f\u5176\u5386\u53f2\u540d\u79f0\u7684\u662f\nA. \u71d5\u4eac\nB. \u5927\u90fd\nC. \u6c74\u6881\nD. \u84df\u57ce\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.7522567617903574, "itpossible/Chinese-Mistral-7B-v0.1": 0.7559610215899282, "HuggingFaceH4/zephyr-7b-beta": 0.9789860497244022, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7822509999721988, "meta-llama/Meta-Llama-3-8B": 0.5085732723474835, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5072759837222186}}, {"question": "\u5355\u7eaf\u955c\u4e0b\u8840\u5c3f\u7684 IgA \u80be\u75c5\u7684\u4e3b\u8981\u6cbb\u7597\u65b9\u6cd5\u662f\nA. \u63a7\u5236\u611f\u67d3\nB. \u7cd6\u76ae\u8d28\u6fc0\u7d20\nC. \u5bf9\u75c7\u6cbb\u7597\nD. ACEI/ARB\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6177599398112316, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe $\\lim _{x \\rightarrow 0} \\frac{(1+x)(1+2 x)(1+3 x)+a}{x}=6$\uff0c \u5219 $a$ \u7684\u503c\u4e3a ().\nA. -1\nB. 2\nC. 3\nD. 1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7b80\u8c10\u632f\u52a8\u7684\u80fd\u91cf\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u662f\nA. \u7b80\u8c10\u632f\u52a8\u7684\u52a8\u80fd\u5b88\u6052\nB. \u7b80\u8c10\u632f\u52a8\u89d2\u52a8\u91cf\u5b88\u6052\nC. \u7b80\u8c10\u632f\u52a8\u7684\u673a\u68b0\u80fd\u5b88\u6052\nD. \u7b80\u8c10\u632f\u52a8\u7684\u52bf\u80fd\u5b88\u6052\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6221494233759275, "HuggingFaceH4/zephyr-7b-beta": 0.9613255085461491, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.550102245011365, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5821971784472353}}, {"question": "\u9a7e\u9a76\u673a\u52a8\u8f66\u5728\u8fdb\u51fa\u975e\u673a\u52a8\u8f66\u9053\u65f6\uff0c\u6700\u9ad8\u901f\u5ea6\u4e0d\u80fd\u8d85\u8fc7\u591a\u5c11\nA. 60\u516c\u91cc/\u5c0f\u65f6\nB. 30\u516c\u91cc/\u5c0f\u65f6\nC. 50\u516c\u91cc/\u5c0f\u65f6\nD. 40\u516c\u91cc/\u5c0f\u65f6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3578936445062351, "meta-math/MetaMath-Mistral-7B": 0.8213019619932181, "itpossible/Chinese-Mistral-7B-v0.1": 0.47011281842775526, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.535729619338768, "meta-llama/Meta-Llama-3-8B": 0.4577567506435654, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7751302422182833}}, {"question": "\u5173\u4e8e\u6559\u80b2\u7684\u76f8\u5bf9\u72ec\u7acb\u6027\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u662f()\nA. \u6559\u80b2\u53ef\u4ee5\u4fc3\u8fdb\u793e\u4f1a\u6d41\u52a8\nB. \u6559\u80b2\u5177\u6709\u81ea\u8eab\u7684\u7279\u70b9\u3001\u89c4\u5f8b\nC. \u6559\u80b2\u53d1\u5c55\u53d7\u793e\u4f1a\u7ecf\u6d4e\u53d1\u5c55\u6c34\u5e73\u5236\u7ea6\uff0c\u4f46\u4e0d\u80fd\u7b80\u5355\u5730\u7528\u7ecf\u6d4e\u89c4\u5f8b\u66ff\u4ee3\u6559\u80b2\u89c4\u5f8b\nD. \u6559\u80b2\u7684\u53d1\u5c55\u5177\u6709\u5386\u53f2\u7ee7\u627f\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4874271334514432, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.70055525438136}}, {"question": "\u6839\u636e\u76d1\u7763\u4e3b\u4f53\u548c\u76d1\u7763\u6743\u7684\u6027\u8d28\u4e0d\u540c\uff0c\u53ef\u5c06\u6cd5\u5f8b\u76d1\u7763\u5212\u5206\u4e3a\nA. \u4e8b\u524d\u76d1\u7763\u548c\u4e8b\u540e\u76d1\u7763\nB. \u5bf9\u4eba\u7684\u76d1\u7763\u548c\u5bf9\u4e8b\u7684\u76d1\u7763\nC. \u56fd\u5bb6\u76d1\u7763\u548c\u975e\u56fd\u5bb6\u76d1\u7763\nD. \u5185\u90e8\u76d1\u7763\u548c\u5916\u90e8\u76d1\u7763\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.40006943344474327, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5057620240887127}}, {"question": "\u98df\u54c1\u8f90\u7167\u7528\u7684\u8f90\u7167\u6e90\u4e3b\u8981\u91c7\u7528\nA. X\u5c04\u7ebf\nB. \u4ee5\u4e0a\u5168\u90e8\nC. \u94c0235\nD. \u94b460\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.47482074772687166, "itpossible/Chinese-Mistral-7B-v0.1": 0.4343032346299149, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.36611589614957524, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u7269\u4f53\u505a\u66f2\u7ebf\u8fd0\u52a8\uff0c\u4e0b\u5217\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\uff1a\nA. \u7269\u4f53\u505a\u66f2\u7ebf\u8fd0\u52a8\u65f6\u6240\u53d7\u7684\u5408\u5916\u529b\u4e00\u5b9a\u662f\u53d8\u529b\nB. \u7269\u4f53\u6240\u53d7\u7684\u5408\u5916\u529b\u4e0d\u4e3a\u96f6\u65f6\u4e00\u5b9a\u505a\u66f2\u7ebf\u8fd0\u52a8\nC. \u7269\u4f53\u5728\u6052\u529b\u7684\u4f5c\u7528\u4e0b\u4e0d\u80fd\u505a\u66f2\u7ebf\u8fd0\u52a8\nD. \u7269\u4f53\u505a\u66f2\u7ebf\u8fd0\u52a8\u65f6\u6240\u53d7\u7684\u5408\u5916\u529b\u4e00\u5b9a\u4e0d\u4e3a\u96f6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.34668920612958254, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5c5e\u4e8e\u9003\u907f\u6027\u5fc3\u7406\u9632\u5fa1\u673a\u5236\u7684\u662f\nA. \u8865\u507f\nB. \u8f6c\u79fb\nC. \u538b\u6291\nD. \u5347\u534e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u9664\u54ea\u9879\u5916\uff0c\u5747\u4e3a\u98ce\u5bd2\u611f\u5192\u4e0e\u98ce\u70ed\u611f\u5192\u7684\u4e3b\u8981\u9274\u522b\u4f9d\u636e\nA. \u6d41\u6d95\u7684\u6e05\u4e0e\u6d4a\nB. \u5934\u75db\u8eab\u75bc\u4e0e\u5426\nC. \u6076\u5bd2\u53d1\u70ed\u7684\u5b70\u8f7b\u5b70\u91cd\nD. \u6e34\u4e0e\u4e0d\u6e34\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3542800494789494, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u7ba1\u7406\u804c\u80fd\u4e2d\uff0c\u5177\u6709\u4e3b\u4f53\u5e7f\u6cdb\u6027\u7279\u70b9\u7684\u662f\nA. \u534f\u8c03\nB. \u6fc0\u52b1\nC. \u7ec4\u7ec7\nD. \u8ba1\u5212\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.344785902992216, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8944928409117712}}, {"question": "\u683c\u6597\u5bf9\u6297\u6027\u9879\u7fa4\u7ade\u6280\u80fd\u529b\u7684\u51b3\u5b9a\u56e0\u7d20\u4e2d\uff0c\u8d77\u4e3b\u5bfc\u4f5c\u7528\uff0c\u5e76\u5bf9\u63d0\u9ad8\u8fd0\u52a8\u6210\u7ee9\u8d77\u51b3\u5b9a\u6027\u4f5c\u7528\u7684\u56e0\u7d20\u662f\nA. \u5fc3\u7406\nB. \u4f53\u80fd\nC. \u667a\u80fd\nD. \u4f53\u80fd\u3001\u6280\u80fd\u4e0e\u6218\u672f\u80fd\u529b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7931291180828496, "meta-math/MetaMath-Mistral-7B": 0.9238575167656586, "itpossible/Chinese-Mistral-7B-v0.1": 0.7990974946927178, "HuggingFaceH4/zephyr-7b-beta": 0.9819017771804912, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9182770741040388, "meta-llama/Meta-Llama-3-8B": 0.6371021352516333, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9243840940503217}}, {"question": "\u5df2\u77e5\u5177\u6709\u4e24\u4e2a\u7ec8\u6b62\u539f\u56e0\u7684\u591a\u51cf\u56e0\u6a21\u578b\uff0c\u7ec8\u6b62\u529b\u5206\u522b\u4e3a:$\\mu_{x+t}^{(1)}=\\frac{t}{100}\uff0c\\mu_{x+t}^{(2)}=\\frac{1}{100}(t \\geqslant 0)$\uff0c\u7ed9\u5b9a\u72b6\u6001\u5728$t$\u65f6\u523b\u7ec8\u6b62\uff0c\u5219$J$\u7684\u6761\u4ef6\u5206\u5e03\u5f8b\u6b63\u786e\u7684\u4e3a ( )\u3002(1)$h(j \\mid t)=\\left\\{\\begin{array}{ll}\\mu_{x+t}^{(1)} / \\mu_{x+t}^\\tau=\\frac{t}{t+1}\uff0c& (j=1) \\\\ \\mu_{x+t}^{(2)} / \\mu_{x+t}^\\tau=\\frac{1}{t+1}\uff0c& (j=2)\\end{array} ;(2) h(j \\mid t)=\\frac{t}{t+1}\uff0cj=2 ;\\right.$(3)$h(j \\mid t)=\\frac{1}{t+1}\uff0cj=1$\u3002\nA. (1) (2)\nB. (1)\nC. (1) (3)\nD. (2)\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u80e1\u67d0\u592b\u5987\u56e0\u6709\u4e8b\u5916\u51fa\uff0c\u4fbf\u96c7\u4e00\u4fdd\u59c6\u4e34\u65f6\u5728\u5bb6\u7167\u770b\u5b69\u5b50\uff0c\u4e8b\u540e\u4ed8\u7ed9\u4fdd\u59c6500\u5143\u94b1\u3002\u5173\u4e8e\u80e1\u67d0\u592b\u5987\u4e0e\u4fdd\u59c6\u4e4b\u95f4\u6cd5\u5f8b\u5173\u7cfb\u7684\u5ba2\u4f53\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u662f\nA. \u7167\u770b\u5b69\u5b50\u7684\u52b3\u52a1\u548c500\u5143\u62a5\u916c\nB. \u5b69\u5b50\u7684\u5b89\u5168\u3001\u5065\u5eb7\nC. \u5b69\u5b50\nD. \u4fdd\u59c6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5710229383388895, "meta-math/MetaMath-Mistral-7B": 0.9077105213915648, "itpossible/Chinese-Mistral-7B-v0.1": 0.5175540000984752, "HuggingFaceH4/zephyr-7b-beta": 0.7311720825251835, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9669148640754756, "meta-llama/Meta-Llama-3-8B": 0.5501667453606544, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7843428786096532}}, {"question": "\u4e00\u4e2a\u4e8c\u8fdb\u5236\u6e90X\u53d1\u51fa\u7b26\u53f7\u96c6\u4e3a{-1,1}\uff0c\u7ecf\u8fc7\u79bb\u6563\u65e0\u8bb0\u5fc6\u4fe1\u9053\u4f20\u8f93\uff0c\u7531\u4e8e\u4fe1\u9053\u4e2d\u566a\u97f3\u7684\u5b58\u5728\uff0c\u63a5\u6536\u7aefY\u6536\u5230\u7b26\u53f7\u96c6\u4e3a{-1,1,0}\u3002\u5df2\u77e5P(x=-1)=1/4\uff0cP(x=1)=3/4\uff0cP(y=-1|x=-1)=4/5\uff0cP(y=0|x=-1)=1/5\uff0cP(y=1|x=1)=3/4\uff0cP(y=0|x=1)=1/4\uff0c\u6c42\u6761\u4ef6\u71b5H(Y|X)\nA. 0.5372\nB. 0.2375\nC. 0.5273\nD. 0.3275\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28496556819453067, "meta-math/MetaMath-Mistral-7B": 0.43932249108141946, "itpossible/Chinese-Mistral-7B-v0.1": 0.29972404264597124, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u8981\u575a\u6301\u516c\u6709\u5236\u7684\u4e3b\u4f53\u5730\u4f4d\uff0c\u5c31\u8981\u505a\u5230\nA. \u516c\u6709\u5236\u7ecf\u6d4e\u63a7\u5236\u56fd\u6c11\u7ecf\u6d4e\u547d\u8109\nB. \u56fd\u6709\u7ecf\u6d4e\u63a7\u5236\u56fd\u6c11\u7ecf\u6d4e\u547d\u8109\nC. \u516c\u6709\u8d44\u4ea7\u5728\u5404\u9886\u57df\u4e2d\u5360\u4e3b\u4f53\nD. \u4f18\u5148\u53d1\u5c55\u5927\u578b\u56fd\u6709\u4f01\u4e1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.45287319179460506, "HuggingFaceH4/zephyr-7b-beta": 0.9235142570259652, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f57\u9a6c\u5e1d\u56fd\u5bf9\u57fa\u7763\u6559\u5f92\u7684\u7b2c\u4e00\u6b21\u5927\u89c4\u6a21\u7684\u8feb\u5bb3\u53d1\u751f\u5728\u8c01\u5f53\u653f\u65f6\u671f\u3002\nA. \u54c8\u5fb7\u826f\nB. \u56fe\u62c9\u771f\nC. \u5c4b\u5927\u7ef4\nD. \u5c3c\u7984\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3984585186882418, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8343113440018528, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f71\u54cd\u795e\u7ecf\u7ea4\u7ef4\u52a8\u4f5c\u7535\u4f4d\u5e45\u5ea6\u7684\u56e0\u7d20\u662f\nA. \u9608\u7535\u4f4d\u6c34\u5e73\nB. \u523a\u6fc0\u7684\u65f6\u95f4\nC. \u523a\u6fc0\u7684\u5f3a\u5ea6\nD. \u7ec6\u80de\u5916\u6db2 Na+\u6d53\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65b9\u7f57\u5170\u3001\u7ae0\u79cb\u67f3\u8fd9\u4e24\u4e2a\u4eba\u7269\u5f62\u8c61\u51fa\u73b0\u4e8e\u8305\u76fe\u7684\u54ea\u4e00\u90e8\u5c0f\u8bf4\nA. \u300a\u8679\u300b\nB. \u300a\u91ce\u8537\u8587\u300b\nC. \u300a\u8680\u300b\nD. \u300a\u4e09\u4eba\u884c\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31508031524410823, "meta-math/MetaMath-Mistral-7B": 0.3460058095032025, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u6743\u95ee\u9898\u5168\u9762\u8fdb\u5165\u56fd\u9645\u6cd5\u9886\u57df\u662f\u5728\nA. \u7b2c\u4e8c\u6b21\u4e16\u754c\u5927\u6218\u540e\nB. 1948\u5e74\u300a\u4e16\u754c\u4eba\u6743\u5ba3\u8a00\u300b\u901a\u8fc7\u4e4b\u540e\nC. \u7b2c\u4e00\u6b21\u4e16\u754c\u5927\u6218\u540e\nD. 1966\u5e74\u4e24\u4e2a\u56fd\u9645\u4eba\u6743\u516c\u7ea6\u901a\u8fc7\u4e4b\u540e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u897f\u65b9\u4e16\u754c\u5386\u53f2\u6700\u4e45\u3001\u5f71\u54cd\u6700\u5927\u7684\u516c\u5171\u5e7f\u64ad\u7535\u89c6\u673a\u6784\u662f\nA. \u82f1\u56fd\u5e7f\u64ad\u7535\u53f0\nB. \u5fb7\u56fd\u7535\u89c6\u4e8c\u53f0\nC. \u6cd5\u56fd\u5e7f\u64ad\u7535\u53f0\nD. \u5fb7\u56fd\u516c\u5171\u5e7f\u64ad\u8054\u76df\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.40747868679049903, "meta-math/MetaMath-Mistral-7B": 0.7034605133944821, "itpossible/Chinese-Mistral-7B-v0.1": 0.4443287245585862, "HuggingFaceH4/zephyr-7b-beta": 0.7249858132504471, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.49909317650827145, "meta-llama/Meta-Llama-3-8B": 0.6035369487933283, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9746576543639143}}, {"question": "\u4fe1\u606f\u7cfb\u7edf\u5b89\u5168\u6d4b\u8bc4\u65b9\u6cd5\u4e2d\u7684\u6a21\u7cca\u6d4b\u8bd5\u662f\u4e00\u79cd\u9ed1\u76d2\u6d4b\u8bd5\u6280\u672f\uff0c\u5b83\u5c06\u5927\u91cf\u7684\u7578\u5f62\u6570\u636e\u8f93\u5165\u5230\u76ee\u6807\u7a0b\u5e8f\u4e2d\uff0c\u901a\u8fc7\u76d1\u6d4b\u7a0b\u5e8f\u7684\u5f02\u5e38\u6765\u53d1\u73b0\u88ab\u6d4b\u7a0b\u5e8f\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u5173\u4e8e\u6a21\u7cca\u6d4b\u8bd5\uff0c\u4ee5\u4e0b\u8bf4\u6cd5\u9519\u8bef\u7684\u662f\nA. \u6a21\u7cca\u6d4b\u8bd5\u4e0d\u9700\u8981\u7a0b\u5e8f\u7684\u6e90\u4ee3\u7801\u5c31\u53ef\u4ee5\u53d1\u73b0\u95ee\u9898\nB. \u6a21\u7cca\u6d4b\u8bd5\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u52a8\u6001\u6f0f\u6d1e\u6316\u6398\u6280\u672f\uff0c\u4e0d\u5b58\u5728\u8bef\u62a5\uff0c\u4e5f\u4e0d\u9700\u8981\u4eba\u5de5\u8fdb\u884c\u5927\u91cf\u7684\u9006\u5411\u5206\u6790\u5de5\u4f5c\nC. \u4e0e\u767d\u76d2\u6d4b\u8bd5\u76f8\u6bd4\uff0c\u5177\u6709\u66f4\u597d\u7684\u9002\u7528\u6027\nD. \u6a21\u7cca\u6d4b\u8bd5\u53d7\u9650\u4e8e\u88ab\u6d4b\u7cfb\u7edf\u7684\u5185\u90e8\u5b9e\u73b0\u7ec6\u8282\u548c\u590d\u6742\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7389\u7c73\u4e2d\u7684\u5c3c\u514b\u9178\u4e0d\u6613\u88ab\u4eba\u4f53\u5438\u6536\u5229\u7528\u662f\u56e0\u4e3a\u5176\u5316\u5b66\u7ed3\u6784\u4e3b\u8981\u4e3a\u4f55\u578b\uff1f\nA. \u6e38\u79bb\u578b\nB. \u534a\u7ed3\u5408\u578b\nC. \u7ed3\u5408\u578b\nD. \u534a\u6e38\u79bb\u578b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3642793811541735, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4457208936603918, "HuggingFaceH4/zephyr-7b-beta": 0.43458176575008284, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3689108554330874, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5df2\u77e5A=1+1/2+1/3+1/4+...+1/11+1/12\uff0c\u5219A\u7684\u6574\u6570\u90e8\u5206\u662f\nA. 3\nB. 1\nC. 2\nD. 4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.3808577081319581, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.325455072595945, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u2f64C\u8bed\u2f94\u7f16\u5199\u7684\u7a0b\u5e8f\u9700\u8981\u2f64\uff08 \uff09\u7a0b\u5e8f\u7ffb\u8bd1\u540e\u8ba1\u7b97\u673a\u624d\u80fd\u8bc6\u522b\nA. \u7f16\u8bd1\nB. \u89e3\u91ca\nC. \u6c47\u7f16\nD. \u8fde\u63a5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8044894486327886, "meta-math/MetaMath-Mistral-7B": 0.8575436643607959, "itpossible/Chinese-Mistral-7B-v0.1": 0.8185861150006766, "HuggingFaceH4/zephyr-7b-beta": 0.9995379716409538, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9310764787626515, "meta-llama/Meta-Llama-3-8B": 0.537478241348515, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6506859348917259}}, {"question": "\u4e0a\u9053\u8def\u884c\u9a76\u7684\u673a\u52a8\u8f66\u6709\u54ea\u79cd\u60c5\u5f62\u4ea4\u901a\u8b66\u5bdf\u53ef\u4f9d\u6cd5\u6263\u7559\u8f66\u8f86\nA. \u672a\u643a\u5e26\u673a\u52a8\u8f66\u767b\u8bb0\u8bc1\u4e66\nB. \u672a\u653e\u7f6e\u57ce\u5e02\u73af\u4fdd\u6807\u5fd7\nC. \u672a\u643a\u5e26\u4fdd\u9669\u5408\u540c\nD. \u672a\u653e\u7f6e\u4fdd\u9669\u6807\u5fd7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u7269\u8d28\u5faa\u73af\u7684\u8303\u56f4\u4e0d\u540c\uff0c\u751f\u7269\u5730\u7403\u5316\u5b66\u5faa\u73af\u53ef\u5206\u4e3a\nA. \u6c14\u76f8\u578b\u548c\u6c89\u79ef\u578b\u5faa\u73af\nB. \u6c14\u76f8\u578b\u5faa\u73af\u548c\u5730\u8d28\u5927\u5faa\u73af\nC. \u751f\u7269\u5c0f\u5faa\u73af\u548c\u5730\u8d28\u5927\u5faa\u73af\nD. \u751f\u7269\u5c0f\u5faa\u73af\u548c\u6c14\u76f8\u578b\u5faa\u73af\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3092879478814681, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5840663984447833}}, {"question": "\u300a\u7b49\u5f85\u6208\u591a\u300b\u662f()\u620f\u5267\u7684\u4ee3\u8868\u4f5c\u54c1\u3002\nA. \u5b58\u5728\u4e3b\u4e49\nB. \u8352\u8bde\u6d3e\nC. \u8c61\u5f81\u4e3b\u4e49\nD. \u8868\u73b0\u4e3b\u4e49\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3632122848838467, "itpossible/Chinese-Mistral-7B-v0.1": 0.5792585299413737, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u65e0\u8bba\u4ece\u4e8b\u4ec0\u4e48\u804c\u4e1a\u7684\u4eba\uff0c\u5728\u804c\u4e1a\u6d3b\u52a8\u4e2d\u90fd\u5e94\u8be5\u8868\u91cc\u5982\u4e00\u3001\u606a\u5b88\u8bfa\u8a00\u3001\u8bb2\u6c42\u4fe1\u8a89\u3001\u9075\u5b88\u804c\u4e1a\u7eaa\u5f8b\u3002\u8fd9\u662f\u804c\u4e1a\u9053\u5fb7\u4e2d\nA. \u529e\u4e8b\u516c\u9053\u7684\u8981\u6c42\nB. \u670d\u52a1\u7fa4\u4f17\u7684\u8981\u6c42\nC. \u8bda\u5b9e\u5b88\u4fe1\u7684\u8981\u6c42\nD. \u7231\u5c97\u656c\u4e1a\u7684\u8981\u6c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8942281895895713, "meta-math/MetaMath-Mistral-7B": 0.958349711073563, "itpossible/Chinese-Mistral-7B-v0.1": 0.9758389350060426, "HuggingFaceH4/zephyr-7b-beta": 0.7303405986874371, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.967993811248383, "meta-llama/Meta-Llama-3-8B": 0.4890223324161174, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9643675610446368}}, {"question": "\u5728\u4e2d\u56fd\u53e4\u5178\u795e\u8bdd\u6545\u4e8b\u300a\u897f\u6e38\u8bb0\u300b\u4e2d\uff0c\u5b59\u609f\u7a7a\u624b\u6301\u4e00\u6839\u5982\u610f\u91d1\u7b8d\u68d2\uff0c\u91cd\u4e00\u4e07\u4e09\u5343\u4e94\u767e\u65a4\u3002\u5e73\u65f6\u5b59\u609f\u7a7a\u4f1a\u628a\u91d1\u7b8d\u68d2\u53d8\u6210\u201c\u7ee3\u82b1\u9488\u201d\u5927\u5c0f\u653e\u5728\u8033\u6735\u91cc\u3002\u4e0e\u8fd9\u6839\u201c\u7ee3\u82b1\u9488\u201d\u7684\u5bc6\u5ea6\u6700\u4e3a\u63a5\u8fd1\u7684\u5929\u4f53\u662f\nA. \u53c2\u5bbf\u56db\nB. \u5929\u72fc\u661f\u4f34\u661f\nC. \u7ec7\u5973\u661f\nD. \u87f9\u72b6\u661f\u4e91\u4e2d\u5fc3\u661f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.36150852560561064, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.35347162922091135, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u53e4\u4ee3\u96c5\u5178\u57ce\u90a6\uff0c\u966a\u5ba1\u6cd5\u5ead\u51e0\u4e4e\u53ef\u4ee5\u5ba1\u67e5\u5f53\u65f6\u653f\u6cbb\u751f\u6d3b\u4e2d\u7684\u6240\u6709\u95ee\u9898\uff0c\u751a\u81f3\u5305\u62ec\u516c\u6c11\u5927\u4f1a\u548c\u8bae\u4e8b\u4f1a\u901a\u8fc7\u7684\u6cd5\u4ee4\uff0c\u5e76\u8fdb\u884c\u6700\u7ec8\u5224\u51b3\u3002\u8fd9\u8bf4\u660e\nA. \u5224\u51b3\u4f53\u73b0\u6743\u529b\u6765\u6e90\nB. \u6cd5\u5f8b\u670d\u4ece\u6c11\u4f17\u610f\u613f\nC. \u6cd5\u5f8b\u9762\u524d\u4eba\u4eba\u5e73\u7b49\nD. \u5168\u4f53\u516c\u6c11\u53c2\u4e0e\u653f\u6cbb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u5b9e\u884c\u5dee\u522b\u5b9a\u4ef7\u65f6\uff0c\u4e3a\u4f7f\u5229\u6da6\u6700\u5927\uff0c\u5e94\u5f53\u4f7f\nA. \u6bcf\u4e2a\u5e02\u573a\u7684\u8fb9\u9645\u6210\u672c\u76f8\u7b49\nB. \u6bcf\u4e2a\u5e02\u573a\u7684\u8fb9\u9645\u6536\u5165\u90fd\u7b49\u4e8e\u4ea7\u54c1\u7684\u8fb9\u9645\u6210\u672c\nC. \u4f01\u4e1a\u7684\u8fb9\u9645\u6536\u5165\u7b49\u4e8e\u6bcf\u4e2a\u5e02\u573a\u7684\u8fb9\u9645\u6210\u672c\nD. \u5404\u4e2a\u5e02\u573a\u7684\u8fb9\u9645\u6536\u5165\u4e4b\u548c\u7b49\u4e8e\u4ea7\u54c1\u7684\u8fb9\u9645\u6210\u672c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.687588824686232, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4345819400480277, "meta-llama/Meta-Llama-3-8B": 0.3802392203343767, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4668385739412232}}, {"question": "\u4e0b\u5217\u7269\u8d28\u7684\u7528\u9014\u4e2d\u5229\u7528\u4e86\u5176\u8fd8\u539f\u6027\u7684\u662f\nA. SO2\u6f02\u767d\u7ec7\u7269\nB. \u7528\u8461\u8404\u7cd6\u5236\u955c\u6216\u4fdd\u6e29\u74f6\u80c6\nC. \u7528Na2S\u9664\u53bb\u5e9f\u6c34\u4e2d\u7684Hg2\uff0b\nD. Na2O2\u4f5c\u4f9b\u6c27\u5242\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u6c34\u5219\u8f7d\u821f\uff0c\u6c34\u5219\u8986\u821f\u201d\u662f\u8c01\u7684\u540d\u8a00\nA. \u5b54\u5b50\nB. \u8001\u5b50\nC. \u8340\u5b50\nD. \u5b5f\u5b50\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28570956661341396, "meta-math/MetaMath-Mistral-7B": 0.3436615088034303, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3088936322941584, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.593815718796808}}, {"question": "\u571f\u58e4\u7a33\u5b9a\u4e0b\u6e17\u9636\u6bb5\uff0c\u964d\u6c34\u8865\u7ed9\u5730\u4e0b\u5f84\u6d41\u7684\u6c34\u5206\u4e3b\u8981\u662f[ ]\u3002\nA. \u5438\u7740\u6c34\nB. \u8584\u819c\u6c34\nC. \u91cd\u529b\u6c34\nD. \u6bdb\u7ba1\u6c34\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4022693320309888, "meta-math/MetaMath-Mistral-7B": 0.4566948812278862, "itpossible/Chinese-Mistral-7B-v0.1": 0.30702389681474196, "HuggingFaceH4/zephyr-7b-beta": 0.6800382030844129, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6965837342151495, "meta-llama/Meta-Llama-3-8B": 0.28966338381871215, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7395913347592731}}, {"question": "1990\u5e745\u67085\u65e5\u5f00\u59cb\uff0c\u6c11\u4e3b\u5fb7\u56fd\u3001\u8054\u90a6\u5fb7\u56fd\u4ee5\u53ca\u82cf\u7f8e\u82f1\u6cd5\u56db\u56fd\u7684\u4ee3\u8868\u5728\u83ab\u65af\u79d1\u7b7e\u7f72\u4e86\u300a\u5173\u4e8e\u6700\u7ec8\u89e3\u51b3\u5fb7\u56fd\u95ee\u9898\u7684\u6761\u7ea6\u300b\uff0c\u6761\u7ea6\u89c4\u5b9a\uff1a\u56db\u4e2a\u5360\u9886\u56fd\u653e\u5f03\u5360\u9886\u5fb7\u56fd\u7684\u6743\u5229\u548c\u8d23\u4efb\uff0c\u89e3\u6563\u4e0e\u6b64\u89c4\u5b9a\u76f8\u5173\u7684\u4e00\u5207\u673a\u6784\u3002\u8fd9\u4e00\u73b0\u8c61\nA. \u6539\u53d8\u4e86\u6b27\u6d32\u653f\u6cbb\u7248\u56fe\nB. \u662f\u7f8e\u82cf\u4e89\u9738\u7ed3\u675f\u7684\u8868\u73b0\nC. \u8bf4\u660e\u5fb7\u56fd\u518d\u6b21\u6b63\u5f0f\u5b9e\u73b0\u7edf\u4e00\nD. \u6807\u5fd7\u7740\u4e1c\u6b27\u5267\u53d8\u7684\u5f00\u59cb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u626c\u5dde\u516b\u602a\u662f\u6307\u5728\u54ea\u4e2a\u671d\u4ee3\u671d\u6d3b\u8dc3\u5728\u626c\u5dde\u76848\u4f4d\u98ce\u683c\u76f8\u8fd1\u7684\u9769\u65b0\u6d3e\u753b\u5bb6\u3002\nA. \u6e05\nB. \u5143\nC. \u5b8b\nD. \u660e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3204796408282626, "meta-math/MetaMath-Mistral-7B": 0.5156969594147073, "itpossible/Chinese-Mistral-7B-v0.1": 0.42439568819260687, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6748164781787358, "meta-llama/Meta-Llama-3-8B": 0.6119269768656436, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.678178791260787}}, {"question": "\u67d0\u5bb6\u7535\u4f01\u4e1a\u751f\u4ea73\u79cd\u4e0d\u540c\u578b\u53f7\u7684\u7535\u51b0\u7bb1\u30016\u79cd\u4e0d\u540c\u578b\u53f7\u7684\u6d17\u8863\u673a\u30014\u79cd\u4e0d\u540c\u578b\u53f7\u7684C\u7a7a\u8c03\u30015\u79cd\u4e0d\u540c\u578b\u53f7\u7684\u7535\u89c6\u673a\u3002\u8be5\u4f01\u4e1a\u4ea7\u54c1\u7ec4\u5408\u7684\u5bbd\u5ea6\u662f\nA. 1\nB. 4\nC. 18\nD. 3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f20\u7edf\u56fd\u9645\u6cd5\u8ba4\u4e3a\u6218\u4e89\u5f00\u59cb\u7684\u5f62\u5f0f\u5e94\u662f\nA. \u4e24\u519b\u5bf9\u5cd9\nB. \u5ba3\u5e03\u65ad\u7edd\u5916\u4ea4\u5173\u7cfb\nC. \u5ba3\u6218\nD. \u4ea4\u706b(\u5f00\u6218)\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.46110489853800163, "meta-math/MetaMath-Mistral-7B": 0.5497592250009111, "itpossible/Chinese-Mistral-7B-v0.1": 0.563569611802906, "HuggingFaceH4/zephyr-7b-beta": 0.5620308053278659, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5678412873968466, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u508d\u665a\uff0c\u7532\u9a7e\u9a76\u62d6\u62c9\u673a\u5728\u4e61\u6751\u516c\u8def\u4e0a\u884c\u9a76\u3002\u4e59\u62db\u624b\u642d\u8f66\uff0c\u7532\u8ba9\u5176\u4e0a\u8f66\uff0c\u5e76\u544a\u77e5\u8f66\u4e0a\u6709\u4e00\u53e3\u7a7a\u68fa\u6750\u3002\u4e0d\u4e45\u4e0b\u8d77\u5927\u96e8\uff0c\u4e59\u8eb2\u8fdb\u68fa\u6750\u907f\u96e8\uff0c\u8fc7\u4e86\u4e00\u4f1a\u513f\u7761\u7740\u4e86\uff0c\u540e\u6765\u53c8\u6709\u4e19\u8bf7\u6c42\u642d\u8f66\uff0c\u7532\u4e5f\u8ba9\u5176\u4e0a\u4e86\u8f66\uff0c\u4e59\u9192\u540e\u624b\u6258\u68fa\u6750\uff0c\u900f\u9732\u51fa\u5934\u6765\u900f\u6c14\uff0c\u4e19\u5413\u5f97\u5927\u558a\u201c\u6709\u9b3c\u201d\uff0c\u8df3\u4e0b\u8f66\u81f4\u5de6\u817f\u9aa8\u6298\u3002\u7532\u8ba9\u4e59\u642d\u8f66\u7684\u884c\u4e3a\u5c5e\u4e8e\nA. \u4e8b\u5b9e\u884c\u4e3a\nB. \u5408\u540c\u884c\u4e3a\nC. \u60c5\u8c0a\u884c\u4e3a\nD. \u65e0\u56e0\u7ba1\u7406\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6700\u5bb9\u6613\u53d1\u751f\u5d4c\u987f\u7684\u8179\u5916\u759d\u662f\nA. \u8179\u80a1\u6c9f\u76f4\u759d\nB. \u80a1\u759d\nC. \u8179\u80a1\u6c9f\u659c\u759d\nD. \u8110\u759d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3615085256056106, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6210\u5bf9\u7684\u8111\u9885\u9aa8\u6709\nA. \u8776\u9aa8\nB. \u6795\u9aa8\nC. \u9876\u9aa8\nD. \u98a7\u9aa8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u4e2a\u7ed3\u679c\u7684\u6570\u503c\u6982\u7387\u7b49\u4e8e\u76f8\u5bf9\u90a3\u4e2a\u7ed3\u679c\u53ef\u83b7\u5f97\u7684\u7ed3\u679c\u6570()\u53ef\u80fd\u7ed3\u679c\u7684\u603b\u6570\u3002\nA. \u4e58\nB. \u9664\nC. \u51cf\nD. \u52a0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3638582843838116, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3322664660792864, "meta-llama/Meta-Llama-3-8B": 0.5477079952750986, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.616635817933711}}, {"question": "\u8336\u7684\u6545\u4e61\u5728\nA. \u4e2d\u56fd\nB. \u5370\u5ea6\nC. \u5df4\u897f\nD. \u65e5\u672c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "()\u76f4\u63a5\u5f71\u54cd\u7740\u7ec4\u7ec7\u5176\u4ed6\u5404\u65b9\u9762\u7684\u5173\u7cfb\uff0c\u5982\u5458\u5de5\u5bb6\u5c5e\u5173\u7cfb\u3001\u672c\u5730\u987e\u5ba2\u5173\u7cfb\u7b49\u3002\nA. \u6d88\u8d39\u8005\u5173\u7cfb\nB. \u5a92\u4ecb\u5173\u7cfb\nC. \u793e\u533a\u5173\nD. \u653f\u5e9c\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5100678294082243, "itpossible/Chinese-Mistral-7B-v0.1": 0.503954059753991, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f57\u9a6c\u6559\u4f1a\u5bf9\u4e8e\u795e\u5723\u7f57\u9a6c\u5e1d\u56fd\u7684\u80dc\u5229\u7684\u91cd\u8981\u539f\u56e0\u662f\nA. \u5e73\u6c11\u62e5\u62a4\nB. \u6559\u7687\u638c\u6743\nC. \u6cd5\u56fd\u56fd\u738b\u4eec\u5bf9\u4e8e\u6559\u4f1a\u7684\u652f\u6301\nD. \u56fd\u738b\u4fe1\u6559\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u6279\u5224\u5927\u6c49\u65cf\u4e3b\u4e49\u300b\u4e00\u6587\u7684\u4f5c\u8005\u662f\nA. \u5218\u5c11\u5947\nB. \u674e\u7ef4\u6c49\nC. \u5468\u6069\u6765\nD. \u6bdb\u6cfd\u4e1c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5165796946301179, "meta-math/MetaMath-Mistral-7B": 0.6167399212609026, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5390780548599158, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.45787070887097386, "meta-llama/Meta-Llama-3-8B": 0.5777518074461865, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u5c06\u519b\u65cf\u300b\u4e2d\u201c\u4e09\u89d2\u8138\u201d\u4e0e\u201c\u5c0f\u7626\u4e2b\u5934\u201d\u7684\u5173\u7cfb\u662f\nA. \u604b\u4eba\u5173\u7cfb\nB. \u5144\u59b9\u5173\u7cfb\nC. \u592b\u59bb\u5173\u7cfb\nD. \u7236\u5973\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bbe $y = y(x)$ \u662f\u65b9\u7a0b $y^{\\prime \\prime}+2 y^{\\prime}+y=e^{3 x}$ \u6ee1\u8db3 $y(0)=y^{\\prime}(0)=0$ \u7684\u89e3\uff0c \u5219 $x \\rightarrow 0$ \u65f6\uff0c \u4e0e $y(x)$ \u7b49\u4ef7\u7684\u662f.\nA. $\\ln \\cos x$\nB. $e^{\\tan x}-e^{\\sin x}$\nC. $x \\cos x-\\sin x$\nD. $\\int_0^x \\frac{\\sin t^2}{t} d t$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.40733945211455386, "HuggingFaceH4/zephyr-7b-beta": 0.6182932976447534, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4fe1\u8d37\u8d44\u91d1\u7684\u57fa\u672c\u7279\u5f81\u662f\nA. \u6548\u76ca\u6027\nB. \u56fa\u5b9a\u6027\nC. \u5f3a\u5236\u6027\nD. \u6709\u507f\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5321077558398241, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9975274354535659, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5965969992929653, "meta-llama/Meta-Llama-3-8B": 0.5364028384567927, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4903262494581686}}, {"question": "\u674e\u67d0\u548c\u738b\u67d0\u662f\u540c\u684c\uff0c\u674e\u67d0\u559c\u6b22\u5728\u5988\u5988\u9762\u524d\u201c\u6492\u5a07\u201d\uff0c\u800c\u738b\u67d0\u6bd4\u8f83\u201c\u56fa\u6267\u201d\uff0c\u8fd9\u4e00\u73b0\u8c61\u8bf4\u660e\u4eba\u683c\u5177\u6709\nA. \u7a33\u5b9a\u6027\nB. \u7efc\u5408\u6027\nC. \u590d\u6742\u6027\nD. \u72ec\u7279\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3287145937103622, "HuggingFaceH4/zephyr-7b-beta": 0.49815855481824717, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4228468872210798, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7530\u6c49\u201c\u4e94\u56db\u201d\u65f6\u671f\u620f\u5267\u521b\u4f5c\u7684\u91cd\u8981\u827a\u672f\u7279\u8272\u662f\nA. \u73b0\u4ee3\u4e3b\u4e49\u4e0e\u8c61\u5f81\u4e3b\u4e49\u7194\u4e3a\u4e00\u4f53\nB. \u6d6a\u6f2b\u4e3b\u4e49\u4e0e\u8c61\u5f81\u4e3b\u4e49\u7194\u4e3a\u4e00\u4f53\nC. \u73b0\u5b9e\u4e3b\u4e49\u4e0e\u540e\u73b0\u4ee3\u4e3b\u4e49\u7194\u4e3a\u4e00\u4f53\nD. \u73b0\u5b9e\u4e3b\u4e49\u4e0e\u6d6a\u6f2b\u4e3b\u4e49\u7194\u4e3a\u4e00\u4f53\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.45038870366142036, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u54ea\u9879\u8bf4\u6cd5\u662f\u9519\u8bef\u7684\nA. \u8f6e\u72b6\u6a21\u578b\u4e2d\u7684\u751f\u7269\uff0c\u7269\u7406\u5316\u5b66\u548c\u793e\u4f1a\u73af\u5883\u4e09\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u6bd4\u91cd\u56e0\u75c5\u79cd\u7684\u53d8\u5316\u800c\u5f02\nB. \u75c5\u56e0\u7f51\u7528\u4e8e\u75be\u75c5\u591a\u56e0\u7d20\u7814\u7a76\u800c\u4e0d\u5f3a\u8c03\u75be\u75c5\u5355\u4e00\u81f4\u75c5\u56e0\u5b50\u7684\u4f5c\u7528\nC. \u8f6e\u72b6\u6a21\u578b\u7528\u4e8e\u75be\u75c5\u7684\u591a\u56e0\u7d20\u7814\u7a76\u800c\u4e0d\u5f3a\u8c03\u75be\u75c5\u5355\u4e00\u81f4\u75c5\u56e0\u5b50\u7684\u4f5c\u7528\nD. \u8f6e\u72b6\u6a21\u578b\u7684\u751f\u7269\uff0c\u7269\u7406\u5316\u5b66\u548c\u793e\u4f1a\u73af\u5883\u4e09\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u6bd4\u91cd\u968f\u5bbf\u4e3b\u7684\u9057\u4f20\u56e0\u7d20\u7684\u4e0d\u540c\u800c\u5f02\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4439022731906857}}, {"question": "\u4e0b\u5217\u54ea\u4e9b\u65b9\u6cd5\u4e0d\u53ef\u4ee5\u7528\u6765\u5bf9\u9ad8\u7ef4\u6570\u636e\u8fdb\u884c\u964d\u7ef4\nA. LASSO\nB. Bagging\nC. \u4e3b\u6210\u5206\u5206\u6790\u6cd5\nD. \u805a\u7c7b\u5206\u6790\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.7447037908167273, "itpossible/Chinese-Mistral-7B-v0.1": 0.6327065929962138, "HuggingFaceH4/zephyr-7b-beta": 0.7117912124466376, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5911064991090391, "meta-llama/Meta-Llama-3-8B": 0.6981525908860498, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9425663686776483}}, {"question": "\u6bcd\u4eb2\u6768\u67d0\u5916\u51fa\u6253\u5de5\uff0c\u5c0615\u5c81\u7684\u513f\u5b50\u5c0f\u5f3a\u7559\u4e0b\u957f\u671f\u5355\u72ec\u5c45\u4f4f\u3002\u6768\u67d0\u7684\u505a\u6cd5\nA. \u5408\u6cd5\uff0c\u53ef\u4ee5\u6539\u5584\u5c0f\u5f3a\u7684\u7269\u8d28\u751f\u6d3b\u6761\u4ef6\nB. \u4e0d\u5408\u6cd5\uff0c\u4e0d\u5f97\u8ba9\u4e0d\u6ee116\u5468\u5c81\u8005\u8131\u79bb\u76d1\u62a4\u5355\u72ec\u5c45\u4f4f\nC. \u5408\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5c0f\u5f3a\u7684\u72ec\u7acb\u751f\u6d3b\u80fd\u529b\nD. \u4e0d\u5408\u6cd5\uff0c\u4e0d\u5f97\u8ba9\u4e0d\u6ee118\u5468\u5c81\u8005\u8131\u79bb\u76d1\u62a4\u5355\u72ec\u5c45\u4f4f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6229771444461023, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5221472948878818, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5306219201780618}}, {"question": "\u6c49\u5b57\u56fd\u6807\u7801\uff08GB2312-80\uff09\u89c4\u5b9a\u7684\u6c49\u5b57\u7f16\u7801\uff0c\u6bcf\u4e2a\u6c49\u5b57\u2f64\nA. \u56db\u4e2a\u5b57\u8282\u8868\u793a\nB. \u4e09\u4e2a\u5b57\u8282\u8868\u793a\nC. \u2f06\u4e2a\u5b57\u8282\u8868\u793a\nD. \u2f00\u4e2a\u5b57\u8282\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7811350224627049, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5443943211945877, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u91d1\u8272\u8461\u8404\u7403\u83cc\u6240\u5f15\u8d77\u7684\u98df\u54c1\u5b89\u5168\u95ee\u9898\u5728\u7f8e\u56fd\u6240\u5728\u7684\u6bd4\u4f8b\u662f\nA. 0.3\nB. 0.33\nC. 0.4\nD. 0.2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728Excel\u4e2d\uff0c\u6570\u636e\u7b5b\u9009\u662f\u5e7f\u6cdb\u4f7f\u7528\u7684\u7edf\u8ba1\u5de5\u5177\u3002\u4ee5\u4e0b\u6709\u5173\u5176\u529f\u80fd\u7684\u8868\u8ff0\uff0c\u6b63\u786e\u7684\u9009\u9879\u662f\nA. \u5c06\u4e0d\u6ee1\u8db3\u6761\u4ef6\u7684\u8bb0\u5f55\u663e\u793a\uff0c\u800c\u9690\u85cf\u6ee1\u8db3\u6761\u4ef6\u7684\u6570\u636e\nB. \u5c06\u6ee1\u8db3\u6761\u4ef6\u7684\u8bb0\u5f55\u663e\u793a\uff0c\u800c\u9690\u85cf\u4e0d\u6ee1\u8db3\u6761\u4ef6\u7684\u6570\u636e\nC. \u5c06\u6ee1\u8db3\u6761\u4ef6\u7684\u8bb0\u5f55\u663e\u793a\uff0c\u800c\u5220\u9664\u4e0d\u6ee1\u8db3\u6761\u4ef6\u7684\u6570\u636e\nD. \u5c06\u4e0d\u6ee1\u8db3\u6761\u4ef6\u7684\u8bb0\u5f55\u663e\u793a\uff0c\u800c\u5220\u9664\u6ee1\u8db3\u6761\u4ef6\u7684\u6570\u636e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7027575723209931, "meta-math/MetaMath-Mistral-7B": 0.9958381932284838, "itpossible/Chinese-Mistral-7B-v0.1": 0.44617913165673817, "HuggingFaceH4/zephyr-7b-beta": 0.9999766733476277, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7060190189546227, "meta-llama/Meta-Llama-3-8B": 0.7325352783092064, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.904951536080795}}, {"question": "\u6c5f\u6cfd\u6c11\u8bf4\uff1a\u201c\u5fd8\u8bb0\u8fdc\u5927\u7406\u60f3\u800c\u53ea\u987e\u773c\u524d\uff0c\u5c31\u4f1a\u5931\u53bb\u524d\u8fdb\u65b9\u5411\uff0c\u79bb\u5f00\u73b0\u5b9e\u5de5\u4f5c\u800c\u7a7a\u8c08\u8fdc\u5927\u7406\u60f3\uff0c\u5c31\u4f1a\u8131\u79bb\u5b9e\u9645\u3002\u201d\u6c5f\u6cfd\u6c11\u6240\u8bf4\u7684\u201c\u8fdc\u5927\u7406\u60f3\u201d\u662f\u6307\nA. \u4e2a\u4eba\u5bf9\u7f8e\u597d\u751f\u6d3b\u7684\u5411\u5f80\u4e0e\u8ffd\u6c42 \nB. \u5171\u4ea7\u4e3b\u4e49\u8fdc\u5927\u7406\u60f3\nC. \u4e2a\u4eba\u5bf9\u5c06\u6765\u804c\u4e1a\u7684\u5411\u5f80\u4e0e\u8ffd\u6c42\nD. \u5efa\u8bbe\u4e2d\u56fd\u7279\u8272\u793e\u4f1a\u4e3b\u4e49\u5171\u540c\u7406\u60f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5434013918432056, "itpossible/Chinese-Mistral-7B-v0.1": 0.8608820223825966, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.587195504399312, "meta-llama/Meta-Llama-3-8B": 0.7005705562897605, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u72af\u7f6a\u7684\u57fa\u672c\u7279\u5f81\u4e2d\uff0c\uff08\uff09\u4f53\u73b0\u4e86\u7f6a\u5211\u6cd5\u5b9a\u539f\u5219\u7684\u8981\u6c42\u548c\u5211\u6cd5\u7684\u9650\u5236\u4e0e\u4fdd\u969c\u529f\u80fd\u3002\nA. \u5e94\u53d7\u5211\u7f5a\u5904\u7f5a\u6027\nB. \u4e3b\u89c2\u6076\u6027\nC. \u5211\u4e8b\u8fdd\u6cd5\u6027\nD. \u4e25\u91cd\u7684\u793e\u4f1a\u5371\u5bb3\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35686367501298644, "meta-math/MetaMath-Mistral-7B": 0.5127221638772507, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7670074829612646, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5158824242932266, "meta-llama/Meta-Llama-3-8B": 0.35686329776860143, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u5916\u4ea4\u653f\u7b56\u7684\u51c6\u786e\u8868\u8ff0\u662f\nA. \u548c\u5e73\u5916\u4ea4\u653f\u7b56\nB. \u72ec\u7acb\u81ea\u4e3b\u7684\u548c\u5e73\u5916\u4ea4\u653f\u7b56\nC. \u72ec\u7acb\u81ea\u4e3b\u7684\u5916\u4ea4\u653f\u7b56\nD. \u7ef4\u62a4\u4e16\u754c\u548c\u5e73\u3001\u4fc3\u8fdb\u5171\u540c\u53d1\u5c55\u7684\u5916\u4ea4\u653f\u7b56\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.747166174481864, "itpossible/Chinese-Mistral-7B-v0.1": 0.49303230127139963, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u4e13\u4e1a\u5efa\u8bbe\u5b9e\u8df5\u4e2d\uff0c\u4e0d\u5c5e\u4e8e\u786c\u4ef6\u8bbe\u65bd\u7684\u662f\nA. \u4e13\u4e1a\u6587\u5316\nB. \u5b9e\u8bad\u57fa\u5730\nC. \u4eea\u5668\u8bbe\u65bd\nD. \u6559\u5b66\u573a\u5730\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.30269030268167313, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.48280112859829744, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5321077706795383, "meta-llama/Meta-Llama-3-8B": 0.9443083902111942, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9848911106883439}}, {"question": "\u5e72\u6270\u7d20\u6297\u75c5\u6bd2\u611f\u67d3\u7684\u673a\u5236\nA. \u5f71\u50cf\u75c5\u6bd2\u88c5\u914d\nB. \u76f4\u63a5\u5e72\u6270\u75c5\u6bd2 mRNA \u7684\u8f6c\u5f55\nC. \u8bf1\u5bfc\u7ec6\u80de\u4ea7\u751f\u6297\u75c5\u6bd2\u86cb\u767d\nD. \u963b\u6b62\u75c5\u6bd2\u8fdb\u5165\u6613\u611f\u67d3\u7ec6\u80de\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9283852772957292, "meta-math/MetaMath-Mistral-7B": 0.9947560112338645, "itpossible/Chinese-Mistral-7B-v0.1": 0.7190973115371546, "HuggingFaceH4/zephyr-7b-beta": 0.9997713436923149, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6137416942511659, "meta-llama/Meta-Llama-3-8B": 0.37480846975763354, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9487702293841697}}, {"question": "\u5229\u4e50\u5305\u88c5\u7eb8\u5bb9\u5668\u5bf9\u5305\u88c5\u6750\u6599\u7684\u6740\u83cc\u91c7\u7528\nA. \u7d2b\u5916\u7ebf\nB. \u8fc7\u6c27\u5316\u6c22\nC. \u8d85\u9ad8\u6e29\u77ac\u65f6\nD. 121\u00b0C\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u51fd\u6570\u4e2d\uff0c\u4e3a\u5076\u51fd\u6570\u7684\u662f\nA. $y=log_{3}{x}$\nB. $y=3x^{2}-1$\nC. $y=x^{3}-3$\nD. $y=3^{x}$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28570956661341396, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.31049698480066085, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.38253224097524136, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.532152111264687}}, {"question": "\u73ed\u4e3b\u4efb\u8bf7\u5c0f\u660e\u540c\u5b66\u5e2e\u52a9\u7edf\u8ba1\u8003\u8bd5\u6210\u7ee9\uff0c\u5c0f\u660e\u53ef\u4ee5\u9009\u7528\u7684\u8f6f\u4ef6\u662f\nA. KV3000\nB. Photoshop\nC. \u753b\u56fe\nD. Excel\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9795183187580824, "meta-math/MetaMath-Mistral-7B": 0.9983301548171256, "itpossible/Chinese-Mistral-7B-v0.1": 0.983522207868195, "HuggingFaceH4/zephyr-7b-beta": 0.9999860970316747, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9966306342796353, "meta-llama/Meta-Llama-3-8B": 0.9603772693191838, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9978217968985661}}, {"question": "\u8bb8\u614e\u7684\u201c\u672c\u65e0\u5176\u5b57\uff0c\u4f9d\u58f0\u6258\u4e8b\u201d\u662f\u6307\nA. \u5f62\u58f0\nB. \u8f6c\u6ce8\nC. \u4f1a\u610f\nD. \u5047\u501f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6669628579701383}}, {"question": "\u5f53\u5229\u76ca\u76f8\u5173\u8005\u7684\u5229\u76ca\u4e92\u76f8\u53d1\u751f\u51b2\u7a81\u65f6\uff0c\u4e00\u822c\u6765\u8bf4\u5904\u4e8e\u4f18\u5148\u5730\u4f4d \u7684\u662f\nA. \u4f01\u4e1a\u5229\u76ca\nB. \u4e2a\u4eba\u5229\u76ca\nC. \u56fd\u5bb6\u5229\u76ca\nD. \u793e\u4f1a\u5229\u76ca\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7138662433455928, "meta-math/MetaMath-Mistral-7B": 0.5301274070881712, "itpossible/Chinese-Mistral-7B-v0.1": 0.6564656319040928, "HuggingFaceH4/zephyr-7b-beta": 0.9979275510184461, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8201403775170126, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5982\u679ca\uff1ab=c\uff1ad\uff0c\u90a3\u4e48\u4e0d\u6210\u2f74\u7684\u7b49\u5f0f\u6709\nA. a\uff1ad=c\uff1ab\nB. b\uff1aa=d\uff1ac\nC. a: c=b: d \nD. ad=bc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u51b3\u5b9a\u6d88\u8d39\u8005\u7684\u52b3\u52a8\u4f9b\u7ed9\u884c\u4e3a\u7684\u662f\nA. \u52b3\u52a8\u548c\u95f2\u6687\u7684\u9009\u62e9\u66ff\u4ee3\nB. \u52b3\u52a8\u548c\u4f4e\u6863\u5546\u54c1\u7684\u9009\u62e9\u66ff\u4ee3\nC. \u52b3\u52a8\u548c\u6d88\u8d39\u7684\u9009\u62e9\u66ff\u4ee3\nD. \u52b3\u52a8\u548c\u6b63\u5e38\u5546\u54c1\u7684\u9009\u62e9\u66ff\u4ee3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3956423662301385, "HuggingFaceH4/zephyr-7b-beta": 0.6495220501802229, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5460116600796732, "meta-llama/Meta-Llama-3-8B": 0.6137569310661348, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6535510230879374}}, {"question": "\u5728\u90d1\u548c\u4e0b\u897f\u6d0b\u7684\u516d\u767e\u591a\u5e74\u540e\uff0c\u4e2d\u56fd\u6d77\u519b\u7f16\u961f\u4e8e2008\u5e7412\u670825\u65e5\u4ece\u4e09\u4e9a\u8d77\u822a\u5230\u7d22\u9a6c\u91cc\u6d77\u57df\u62a4\u822a\u3002\u5bf9\u8fd9\u4e00\u5386\u53f2\u6027\u4e8b\u4ef6\u8bc4\u8ff0\u6b63\u786e\u7684\u6709\uff1aa\u4eca\u65e5\u7684\u4e2d\u56fd\u518d\u6b21\u62e5\u6709\u4e86\u4e16\u754c\u6700\u5f3a\u7684\u6d77\u519b\u8230\u961f\uff1bb\u8fd9\u6b21\u8fdc\u822a\u53cd\u6620\u4e86\u516d\u767e\u5e74\u6765\u4e2d\u56fd\u7531\u519c\u4e1a\u6587\u660e\u5411\u5de5\u4e1a\u6587\u660e\u9010\u6b65\u8f6c\u53d8\u540e\u7684\u5fc5\u7136\u8d8b\u52bf\uff1bc\u53e4\u4eca\u4e2d\u56fd\u4e24\u6b21\u8fdc\u822a\u7684\u4e3b\u8981\u76ee\u7684\u662f\u4e0d\u540c\u7684\uff1bd\u7ef4\u62a4\u4e16\u754c\u822a\u8fd0\u5b89\u5168\uff0c\u6253\u51fb\u6d77\u76d7\uff0c\u663e\u793a\u4e2d\u56fd\u662f\u8d1f\u8d23\u4efb\u7684\u5927\u56fd\nA. acd\nB. cd\nC. abcd\nD. bcd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6bdb\u6cfd\u4e1c\u5728\u5199\u7ed9\u4ed6\u7684\u8001\u5e08\u5f90\u7279\u7acb\u7684\u4fe1\u4e2d\u8bf4\uff1a\u201c\u4f60\u662f\u6211\u4e8c\u5341\u5e74\u524d\u7684\u5148\u751f\uff0c\u4f60\u73b0\u5728\u4ecd\u7136\u662f\u6211\u7684\u5148\u751f\uff0c\u4f60\u5c06\u6765\u5fc5\u5b9a\u8fd8\u662f\u6211\u5148\u751f\u3002\u201d\u8fd9\u8bf4\u660e\u6559\u5e08\u5bf9\u5b66\u751f\u7684\u5f71\u54cd\u5177\u6709\nA. \u5c42\u6b21\u6027\nB. \u81ea\u89c9\u6027\nC. \u6df1\u8fdc\u6027\nD. \u6807\u51c6\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6994172906460036, "meta-math/MetaMath-Mistral-7B": 0.8760584963776636, "itpossible/Chinese-Mistral-7B-v0.1": 0.8826909742674653, "HuggingFaceH4/zephyr-7b-beta": 0.9652609678032738, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9128367591021364, "meta-llama/Meta-Llama-3-8B": 0.39855633424175924, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.993970546757275}}, {"question": "\u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u5baa\u6cd5\u300b\u89c4\u5b9a\uff1a\u4fdd\u536b\u7956\u56fd\u3001\u62b5\u6297\u4fb5\u7565\u662f\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u6bcf\u4e00\u4e2a\u516c\u6c11\u7684\u795e\u5723\u804c\u8d23\u3002\u4f9d\u7167\u6cd5\u5f8b\uff08\uff09\u662f\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u516c\u6c11\u7684\u5149\u8363\u4e49\u52a1\u3002\nA. \u79ef\u6781\u652f\u63f4\u90e8\u961f\nB. \u670d\u5175\u5f79\u548c\u53c2\u52a0\u6c11\u5175\u7ec4\u7ec7\nC. \u4ece\u4e8b\u5de5\u519c\u4e1a\u751f\u4ea7\nD. \u53c2\u52a0\u56fd\u9632\u5efa\u8bbe\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.44960652891062297, "meta-math/MetaMath-Mistral-7B": 0.7961833495798049, "itpossible/Chinese-Mistral-7B-v0.1": 0.5035120248189403, "HuggingFaceH4/zephyr-7b-beta": 0.9157113738817567, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4892974856057249, "meta-llama/Meta-Llama-3-8B": 0.7039910150739233, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f9d\u7167\u6cd5\u56fd\u6cd5\uff0c\u5408\u540c\u8fdd\u7ea6\u8d23\u4efb\u7684\u5f52\u8d23\u539f\u5219\u662f\nA. \u65e0\u8fc7\u5931\u8d23\u4efb\u539f\u5219\nB. \u7ed3\u679c\u8d23\u4efb\u539f\u5219\nC. \u8fc7\u5931\u8d23\u4efb\u539f\u5219\nD. \u516c\u5e73\u8d23\u4efb\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5304785717033514, "meta-math/MetaMath-Mistral-7B": 0.5172608851746797, "itpossible/Chinese-Mistral-7B-v0.1": 0.35686329776860143, "HuggingFaceH4/zephyr-7b-beta": 0.6341779752093782, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5730552181801112, "meta-llama/Meta-Llama-3-8B": 0.55726046262847, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7443389659846631}}, {"question": "\u8981\u6bd4\u8f83\u7684\u4e24\u7ec4\u6570\u503c\u578b\u8d44\u6599\u5448\u660e\u663e\u504f\u6001\u5206\u5e03\uff0c $n_1\uff0c n_2$ \u5747\u5c0f\u4e8e 30 \uff0c\u4e14\u7ecf\u7edf\u8ba1\u68c0\u9a8c $\\sigma_1^2 \\neq \\sigma_2^2$\uff0c\u6b64\u65f6\u5b9c\u91c7\u7528\u54ea\u79cd\u68c0\u9a8c\u65b9\u6cd5?\nA. \u79e9\u548c\u68c0\u9a8c\nB. $t$ \u68c0\u9a8c\nC. $t^{\\prime}$ \u68c0\u9a8c\nD. $u$ \u68c0\u9a8c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5778375529872066, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6223761486268764, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3460058095032025}}, {"question": "\u81ea\u6211\u4e0e\u4ed6\u4eba\u3001\u4e2a\u4eba\u5bf9\u4e2a\u4eba\u7684\u4f20\u64ad\u6d3b\u52a8\uff0c\u5c5e\u4e8e\nA. \u7ec4\u7ec7\u4f20\u64ad\nB. \u4eba\u9645\u4f20\u64ad\nC. \u81ea\u6211\u4f20\u64ad\nD. \u5927\u4f17\u4f20\u64ad\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6562847211093612, "itpossible/Chinese-Mistral-7B-v0.1": 0.8406376925963773, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8828683530868691, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6627698572785145}}, {"question": "\u4fc3\u8fdb\u793e\u533a\u5c45\u6c11\u7684\u793e\u4f1a\u53c2\u4e0e\uff0c\u57f9\u517b\u793e\u533a\u5c45\u6c11\u7684\u4e92\u52a9\u5408\u4f5c\u7cbe\u795e\uff0c\u63d0\u9ad8\u793e\u533a\u5c45\u6c11\u81ea\u529b\u66f4\u751f\u548c\u5171\u540c\u5408\u4f5c\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\uff0c\u8fd9\u5c5e\u4e8e\u793e\u533a\u53d1\u5c55\u7684\nA. \u4efb\u52a1\u76ee\u6807\nB. \u57fa\u672c\u76ee\u6807\nC. \u8fc7\u7a0b\u76ee\u6807\nD. \u6839\u672c\u76ee\u6807\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5951879318738643, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9a6c\u65af\u6d1b\u7684\u9700\u6c42\u5c42\u6b21\u7406\u8bba\u4e2d\uff0c\u6700\u4f4e\u5c42\u6b21\u7684\u9700\u6c42\u662f\nA. \u81ea\u5c0a\u9700\u6c42\nB. \u793e\u4f1a\u9700\u6c42\nC. \u5b89\u5168\u9700\u6c42\nD. \u751f\u7406\u9700\u6c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9646583245693643, "meta-math/MetaMath-Mistral-7B": 0.9888857451940812, "itpossible/Chinese-Mistral-7B-v0.1": 0.9624229355812246, "HuggingFaceH4/zephyr-7b-beta": 0.9999884499287424, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9775621772555084, "meta-llama/Meta-Llama-3-8B": 0.9890615227047689, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9992036578249173}}, {"question": "\u6211\u56fd\u7b2c\u4e00\u4e2a\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u7a7a\u95f4\u5b9e\u9a8c\u5ba4\u662f\nA. \u5929\u821f\u4e00\u53f7\nB. \u5929\u5bab\u4e8c\u53f7\nC. \u5929\u5bab\u4e00\u53f7\nD. \u795e\u821f\u5341\u4e00\u53f7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5276429754611573, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5790345013138003}}, {"question": "\u5728\u6b63\u65b9\u4f53ABCD-A1B2C1D1\u4e2d\uff0cBB1\u4e0e\u5e73\u9762ACD1\u6240\u6210\u89d2\u7684\u4f59\u5f26\u503c\u4e3a\nA. \\sqrt{3}/2\nB. 2/3\nC. \\sqrt{2}/3\nD. \\sqrt{6}/3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.28937433699268383, "HuggingFaceH4/zephyr-7b-beta": 0.5327079261390885, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.29068935354339714, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5bf9\u4e00\u5f20\u5904\u7406\u5b8c\u6210\u7684\u56fe\u7247\uff0c\u5206\u522b\u7528BMP\uff0824\u4f4d\u4f4d\u56fe\uff09\u548cJPG\u4e24\u79cd\u683c\u5f0f\u4fdd\u5b58\u540c\u4e00\u5f20\u56fe\u7247\u65f6\uff0c\u751f\u6210\u7684\u4e24\u4e2a\u6587\u4ef6\u6240\u5360\u7528\u7684\u78c1\u76d8\u7a7a\u95f4\nA. \u4e0d\u80fd\u786e\u5b9a\nB. \u4e24\u8005\u4e00\u6837\u5927\nC. JPG\u683c\u5f0f\u7684\u5927\nD. BMP\u683c\u5f0f\u7684\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5080054561843346, "meta-math/MetaMath-Mistral-7B": 0.47038960370145844, "itpossible/Chinese-Mistral-7B-v0.1": 0.3710689062049661, "HuggingFaceH4/zephyr-7b-beta": 0.8968755436570286, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.49516342280620107, "meta-llama/Meta-Llama-3-8B": 0.45478471919427815, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8546159834523058}}, {"question": "\u9881\u53d1\u201c\u5357\u68ee\u62a4\u7167\u201d\u7684\u76ee\u7684\u662f\nA. \u4f7f\u96be\u6c11\u5177\u6709\u65c5\u884c\u8bc1\u4ef6\nB. \u4f7f\u96be\u6c11\u6539\u53d8\u56fd\u7c4d\nC. \u4f7f\u65e0\u56fd\u7c4d\u4eba\u53d6\u5f97\u56fd\u7c4d\nD. \u652f\u6301\u5404\u56fd\u96be\u6c11\u4e8b\u52a1\u90e8\u95e8\u7684\u5de5\u4f5c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.29892260683555066, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3789723923714366, "HuggingFaceH4/zephyr-7b-beta": 0.9991594585225415, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8022145877352509, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8304853532465942}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u4e0d\u5c5e\u4e8e\u5b66\u6821\u4ea7\u751f\u7684\u5fc5\u5907\u6761\u4ef6\u7684\u662f()\u3002\nA. \u7269\u8d28\u57fa\u7840\nB. \u8111\u529b\u52b3\u52a8\u4e0e\u4f53\u529b\u52b3\u52a8\u76f8\u5206\u79bb\nC. \u53d1\u660e\u5370\u5237\u672f\nD. \u7ecf\u9a8c\u7684\u79ef\u7d2f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3614425597318518, "meta-math/MetaMath-Mistral-7B": 0.49670463221146527, "itpossible/Chinese-Mistral-7B-v0.1": 0.31283638571410965, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5441962514559905, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.618779931016896}}, {"question": "\u4ee5\u4e0b\u5173\u4e8eNAT\u7684\u8bf4\u6cd5\u4e2d\uff0c\u9519\u8bef\u7684\u662f\nA. NAT\u5141\u8bb8\u4e00\u4e2a\u673a\u6784\u4e13\u7528Intranet\u4e2d\u7684\u4e3b\u673a\u900f\u660e\u5730\u8fde\u63a5\u5230\u516c\u5171\u57df\u4e2d\u7684\u4e3b\u673a\uff0c\u65e0\u9700\u6bcf\u53f0\u5185\u90e8\u4e3b\u673a\u90fd\u62e5\u6709\u6ce8\u518c\u7684\uff08\u5df2\u7ecf\u8d8a\u6765\u8d8a\u7f3a\u4e4f\u7684\uff09\u5168\u5c40\u4e92\u8054\u7f51\u5730\u5740\nB. \u52a8\u6001NAT\u53c8\u53eb\u7f51\u7edc\u5730\u5740\u7aef\u53e3\u8f6c\u6362NAPT\nC. \u9759\u6001NAT\u662f\u8bbe\u7f6e\u8d77\u6765\u6700\u7b80\u5355\u548c\u6700\u5bb9\u6613\u5b9e\u73b0\u7684\u4e00\u79cd\u5730\u5740\u8f6c\u6362\u65b9\u5f0f\uff0c\u5185\u90e8\u7f51\u7edc\u4e2d\u7684\u6bcf\u4e2a\u4e3b\u673a\u90fd\u88ab\u6c38\u4e45\u6620\u5c04\u6210\u5916\u90e8\u7f51\u7edc\u4e2d\u7684\u67d0\u4e2a\u5408\u6cd5\u5730\u5740\nD. \u52a8\u6001NAT\u4e3b\u8981\u5e94\u7528\u4e8e\u62e8\u53f7\u548c\u9891\u7e41\u7684\u8fdc\u7a0b\u8fde\u63a5\uff0c\u5f53\u8fdc\u7a0b\u7528\u6237\u8fde\u63a5\u4e0a\u4e4b\u540e\uff0c\u52a8\u6001NAT\u5c31\u4f1a\u5206\u914d\u7ed9\u7528\u6237\u4e00\u4e2aIP\u5730\u5740\uff0c\u5f53\u7528\u6237\u65ad\u5f00\u65f6\uff0c\u8fd9\u4e2aIP\u5730\u5740\u5c31\u4f1a\u88ab\u91ca\u653e\u800c\u7559\u5f85\u4ee5\u540e\u4f7f\u7528\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\u65f6\uff0c\u9644\u4ef6\u7684\u5927\u5c0f\u662f\u6709\u9650\u5236\u7684\u3002\u5047\u8bbe\u67d0\u7f51\u7ad9\u89c4\u5b9a\u9644\u4ef6\u7684\u6700\u5927\u503c\u4e3a5M\uff0c\u73b0\u6709\u4e00\u4f4d\u540c\u5b66\u9700\u53d1\u9001\u603b\u5bb9\u91cf\u4e3a20024KB\u7684\u591a\u4e2a\u6587\u4ef6\u3002\u5219\u4ed6\u5728\u8be5\u7f51\u7ad9\u4e2d\u81f3\u5c11\u53d1\u9001\u7684\u6b21\u6570\u662f\nA. 3\nB. 2\nC. 1\nD. 4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "SM3\u5bc6\u7801\u6742\u51d1\u7b97\u6cd5\u7684\u6d88\u606f\u5206\u7ec4\u957f\u5ea6\u4e3a\uff08 \uff09\u6bd4\u7279\u3002\nA. 128\nB. 1024\nC. 64\nD. 512\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7266007698954582, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u65e0\u6536\u6da9\u655b\u75ae\u529f\u6548\u7684\u836f\u7269\u662f\nA. \u4e4c\u8d3c\u9aa8\nB. \u4e73\u9999\nC. \u5b69\u513f\u8336\nD. \u7089\u7518\u77f3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.32486943359923176, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4f5b\u6559\u4e3a\u56fd\u6559\u7684\u56fd\u5bb6\u662f\nA. \u9a6c\u6765\u897f\u4e9a\nB. \u65b0\u52a0\u5761\nC. \u65e5\u672c\nD. \u6cf0\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6774678199367639, "HuggingFaceH4/zephyr-7b-beta": 0.9922037593112328, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.40252652413078177, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9113437791847547}}, {"question": "\u968f\u7740\u56fd\u529b\u7684\u589e\u5f3a\u548c\u6539\u9769\u5f00\u653e\u7684\u8fdb\u4e00\u6b65\u6df1\u5165\uff0c\u4e2d\u56fd\u4e0e\u4e16\u754c\u8d70\u5f97\u66f4\u8fd1\u3002\u6cd5\u56fd\u524d\u603b\u7edf\u5e0c\u62c9\u514b\u8bf4\uff1a\u201c\u4ece\u591a\u6781\u5316\u89d2\u5ea6\u6765\u770b\uff0c\u4eca\u5929\u7684\u4e16\u754c\u683c\u5c40\u5df2\u7ecf\u53d1\u751f\u4e86\u6539\u53d8\u3002\u65e0\u8bba\u662f\u91d1\u878d\u5371\u673a\u3001\u6c14\u5019\u53d8\u5316\u8fd8\u662f\u80fd\u6e90\u8d44\u6e90\u7b49\u5168\u7403\u6027\u95ee\u9898\uff0c\u6ca1\u6709\u4e2d\u56fd\u53c2\u4e0e\u90fd\u65e0\u4ece\u8c08\u8d77\u3002\u201d\u4e3a\u4e86\u6709\u6548\u5e94\u5bf9\u4e16\u754c\u683c\u5c40\u7684\u6539\u53d8\uff0c\u6211\u56fd\u5e94\u5f53a\u575a\u5b9a\u4e0d\u79fb\u5730\u575a\u6301\u5bf9\u5916\u5f00\u653e\u653f\u7b56\uff0c\u79ef\u6781\u53c2\u4e0e\u56fd\u9645\u5206\u5de5\u4e0e\u5408\u4f5c b\u575a\u6301\u72ec\u7acb\u81ea\u4e3b\u3001\u81ea\u529b\u66f4\u751f\u548c\u5e73\u7b49\u4e92\u5229\u539f\u5219\uff0c\u79ef\u6781\u53d1\u5c55\u4e0e\u4e16\u754c\u5404\u56fd\u7684\u53cb\u597d\u5173\u7cfb c\u52a0\u5f3a\u56fd\u9645\u4ea4\u6d41\u4e0e\u5408\u4f5c\uff0c\u4fc3\u8fdb\u516c\u6b63\u5408\u7406\u7684\u56fd\u9645\u65b0\u79e9\u5e8f\u7684\u5f62\u6210 d\u8c03\u6574\u6211\u56fd\u5916\u8d38\u7ed3\u6784\uff0c\u4ee5\u201c\u8d70\u51fa\u53bb\u201d\u4e3a\u4e3b\uff0c\u4ee5\u201c\u5f15\u8fdb\u6765\u201d\u4e3a\u8f85\nA. cd\nB. abc\nC. ab\nD. acd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5129709717356818, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.325455072595945, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f5c\u4e3a\u4e2d\u56fd\u5171\u4ea7\u515a\u548c\u793e\u4f1a\u4e3b\u4e49\u4e8b\u4e1a\u6307\u5bfc\u601d\u60f3\u7684\u9a6c\u514b\u601d\u4e3b\u4e49\u662f\u6307\nA. \u5173\u4e8e\u65e0\u4ea7\u9636\u7ea7\u6597\u4e89\u7684\u6027\u8d28\u3001\u76ee\u7684\u548c\u89e3\u653e\u6761\u4ef6\u7684\u5b66\u8bf4\nB. \u5217\u5b81\u521b\u7acb\u7684\u57fa\u672c\u7406\u8bba\u3001\u57fa\u672c\u89c2\u70b9\u548c\u57fa\u672c\u65b9\u6cd5\u6784\u6210\u7684\u79d1\u5b66\u4f53\u7cfb\nC. \u65e0\u4ea7\u9636\u7ea7\u4e89\u53d6\u81ea\u8eab\u89e3\u653e\u548c\u6574\u4e2a\u4eba\u7c7b\u89e3\u653e\u7684\u5b66\u8bf4\u4f53\u7cfb\nD. \u4e0d\u4ec5\u6307\u9a6c\u514b\u601d\u6069\u683c\u65af\u521b\u7acb\u7684\u57fa\u672c\u7406\u8bba\u3001\u57fa\u672c\u89c2\u70b9\u548c\u5b66\u8bf4\u7684\u4f53\u7cfb\uff0c\u4e5f\u5305\u62ec\u7ee7\u627f\u8005\u5bf9\u5b83\u7684\u53d1\u5c55\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6101527209672628, "meta-math/MetaMath-Mistral-7B": 0.8607492105167351, "itpossible/Chinese-Mistral-7B-v0.1": 0.522018781870025, "HuggingFaceH4/zephyr-7b-beta": 0.9606751822738523, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7767058911768149, "meta-llama/Meta-Llama-3-8B": 0.9233215699797028, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9847251703114056}}, {"question": "\u64cd\u4f5c\u8f6c\u6362\u5f00\u5173\u7684\u89c4\u8303\u7528\u8bed\u662f\nA. \u53d6\u4e0b\u3001\u88c5\u4e0a\nB. \u6295\u5165\u3001\u9000\u51fa\nC. \u5207\u81f3\nD. \u62c9\u5f00\u3001\u5408\u4e0a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7537\u6027\uff0c18\u5c81\u3002\u8fd1\u4e00\u5e74\u53cd\u590d\u8170\u90e8\u75bc\u75db\uff0c\u6d3b\u52a8\u53d7\u9650\u3002\u8fd1\u4e24\u6708\u6765\u53c8\u51fa\u73b0\u53cc\u9acb\u75bc\u75db\uff0c\u8f7b\u5ea6\u5c48\u66f2\u7578\u5f62\uff0c\u9700\u62c4\u62d0\u884c\u8d70\u3002X\u7ebf\u68c0\u67e5\u663e\u793a\uff1a\u53cc\u4fa7\u9ab6\u9ac2\u5173\u8282\u9762\u6a21\u7cca\uff0c\u53cc\u4fa7\u80a1\u9aa8\u5934\u8868\u9762\u6bdb\u7cd9\uff0c\u9acb\u5173\u8282\u95f4\u9699\u53d8\u7a84\u3002\u4ee5\u4e0b\u68c0\u67e5\u4e2d\uff0c\u5bf9\u8be5\u60a3\u8005\u660e\u786e\u8bca\u65ad\u5e2e\u52a9\u6700\u5927\u7684\u662f\nA. ESR\u3001RF\u3001\u9ab6\u9ac2\u5173\u8282CT\nB. ESR\u3001RF\u3001\u9ab6\u9ac2\u5173\u8282MRI\nC. ESR\u3001HLA-B27\u3001\u9ab6\u9ac2\u5173\u8282CT\nD. ESR\u3001HLA-B27\u3001\u9ab6\u9ac2\u5173\u8282MRI\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4258861893623922, "HuggingFaceH4/zephyr-7b-beta": 0.9957012575193871, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.428770836363952, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e00\u822c\u4f5c\u7269\u9700\u6c34\u9ad8\u5cf0\u671f\u5728\nA. \u4e2d\u671f\nB. \u64ad\u79cd\u51fa\u82d7\u671f\nC. \u540e\u671f\nD. \u524d\u671f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6264764290770114, "itpossible/Chinese-Mistral-7B-v0.1": 0.29660173325630934, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5596398665013496, "meta-llama/Meta-Llama-3-8B": 0.39666589335380015, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.33762049688160123}}, {"question": "\u526f\u4ea4\u611f\u795e\u7ecf\u5174\u594b\u5f15\u8d77\u7684\u553e\u6db2\u7279\u70b9\u4e3a\nA. \u5c11\u91cf\u3001\u7a00\u8584\nB. \u5927\u91cf\u3001\u9ecf\u7a20\nC. \u5927\u91cf\u3001\u7a00\u8584\nD. \u5c11\u91cf\u3001\u9ecf\u7a20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.46219615528099145, "meta-math/MetaMath-Mistral-7B": 0.5397401132359269, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9945359334471723, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7513176414371249, "meta-llama/Meta-Llama-3-8B": 0.34040633907578, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0a\u56e0\u7279\u7f51\u6d4f\u89c8\u4fe1\u606f\uff0c\u5e38\u7528\u7684\u6d4f\u89c8\u5668\u662f\nA. Internet Explorer\nB. WPS2000\nC. Word2000\nD. KV300\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9864117372037285, "meta-math/MetaMath-Mistral-7B": 0.9993419062747698, "itpossible/Chinese-Mistral-7B-v0.1": 0.9868242580879174, "HuggingFaceH4/zephyr-7b-beta": 0.9996832061559447, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.895067627276364, "meta-llama/Meta-Llama-3-8B": 0.9862271321445069, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9982931268430624}}, {"question": "\u516c\u5171\u5173\u7cfb\u5b66\u662f\u6307\uff08\uff09\u516c\u5171\u5173\u7cfb\u5de5\u4f5c\u7ecf\u9a8c\uff0c\uff08\uff09\u516c\u5171\u5173\u7cfb\u5de5\u4f5c\uff0c\uff08\uff09\u7ec4\u7ec7\u4e3a\u8fbe\u5230\u516c\u5171\u5173\u7cfb\u76ee\u7684\u800c\u7cfb\u7edf\u5730\u8fdb\u884c\u516c\u5171\u5173\u7cfb\u6d3b\u52a8\u7684\u4e00\u95e8\u7efc\u5408\u6027\u3001\u5e94\u7528\u6027\u5b66\u79d1\u3002\nA. \u6307\u5bfc\u3001\u603b\u7ed3\u3001\u7814\u7a76\nB. \u603b\u7ed3\u3001\u6307\u5bfc\u3001\u7814\u7a76\nC. \u603b\u7ed3\u3001\u7814\u7a76\u3001\u6307\u5bfc\nD. \u7814\u7a76\u3001\u6307\u5bfc\u3001\u603b\u7ed3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8ba4\u77e5\u98ce\u683c\u5dee\u5f02\u6027\u5177\u4f53\u8868\u73b0\u4e3a\uff1a\nA. \u4ee5\u4e0a\u90fd\u662f\nB. \u573a\u4f9d\u5b58\u6027\u4e0e\u573a\u72ec\u7acb\u6027\nC. \u590d\u5408\u578b\u4e0e\u53d1\u6563\u578b\nD. \u51b2\u52a8\u578b\u4e0e\u6c89\u601d\u578b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3403222644268806, "meta-math/MetaMath-Mistral-7B": 0.7589989011620891, "itpossible/Chinese-Mistral-7B-v0.1": 0.3142819961202564, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6710930353480283, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5185\u2ec6\u7684\u5ea6\u6570\u4e3a\u6574\u6570\u7684\u6b63 \u8fb9\u5f62\u7684\u4e2a\u6570\u662f\nA. 24\nB. 22\nC. 18\nD. 20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.35823796793174434, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "2014\u5e74\u662f\u7b2c\u4e8c\u6b21\u4e16\u754c\u5927\u6218\u7206\u53d175\u5468\u5e74\uff0c3\u670828\u65e5\uff0c\u56fd\u5bb6\u4e3b\u5e2d\u4e60\u8fd1\u5e73\u5728\u5fb7\u56fd\u79d1\u5c14\u4f2f\u57fa\u91d1\u4f1a\u53d1\u8868\u6f14\u8bb2\uff0c\u8c08\u5230\u4e8c\u6218\u65f6\u5f15\u7528\u5230\u7684\u540d\u8a00\u662f\nA. \u5386\u53f2\u7684\u9053\u8def\u4e0d\u662f\u6d85\u74e6\u5927\u8857\u4e0a\u7684\u4eba\u884c\u9053\uff0c\u5b83\u5b8c\u5168\u662f\u5728\u7530\u91ce\u4e2d\u524d\u8fdb\u7684\nB. \u80dc\u5229\u4e0d\u4f1a\u5411\u6211\u8d70\u6765\uff0c\u6211\u5fc5\u987b\u81ea\u5df1\u8d70\u5411\u80dc\u5229\nC. \u4e16\u754c\u4e0a\u6700\u5bbd\u9614\u7684\u662f\u6d77\u6d0b\uff0c\u6bd4\u6d77\u6d0b\u66f4\u5bbd\u9614\u7684\u662f\u5929\u7a7a\uff0c\u6bd4\u5929\u7a7a\u66f4\u5bbd\u9614\u7684\u662f\u4eba\u7684\u80f8\u6000\nD. \u8c01\u5fd8\u8bb0\u5386\u53f2\uff0c\u8c01\u5c31\u4f1a\u5728\u7075\u9b42\u4e0a\u751f\u75c5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3834538534301589, "meta-math/MetaMath-Mistral-7B": 0.6387009546046626, "itpossible/Chinese-Mistral-7B-v0.1": 0.5788762316964701, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u67d0\u6821\u73ed\u9645\u7bee\u7403\u8054\u8d5b\u4e2d\uff0c\u6bcf\u573a\u2f50\u8d5b\u90fd\u8981\u5206\u51fa\u80dc\u8d1f\uff0c\u6bcf\u961f\u80dc\u2f00\u573a\u5f973\u5206\uff0c\u8d1f\u2f00\u573a\u5f971\u5206\uff0c\u5982\u679c\u67d0\u73ed\u8981\u5728\u7b2c\u2f00\u8f6e\u768428\u573a\u2f50\u8d5b\u4e2d\u2f84\u5c11\u5f9743\u5206\uff0c\u90a3\u4e48\u8fd9\u4e2a\u73ed\u2f84\u5c11\u8981\u80dc\u591a\u5c11\u573a\nA. 6\nB. 5\nC. 7\nD. 8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53cc\u4fa7\u9ac2\u5d74\u6700\u9ad8\u70b9\u7684\u8fde\u7ebf\u5e73\u5bf9\nA. \u7b2c4\u8170\u690e\u68d8\u7a81\nB. \u7b2c5\u8170\u690e\u68d8\u7a81\nC. \u7b2c3\u8170\u690e\u68d8\u7a81\nD. \u7b2c1\u9ab6\u690e\u68d8\u7a81\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6613\u67d0\u6301\u67aa\u62a2\u52ab\u5f00\u8f66\u7684\u7532\uff0c\u5f3a\u884c\u62a2\u8d70\u624b\u673a\u540e\u4e0b\u8f66\u9003\u8dd1\u3002\u7532\u7acb\u5373\u5f00\u8f66\u8ffd\u8d76\uff0c\u5728\u6613\u67d0\u5f80\u524d\u8dd1\u4e86100\u7c73\u5904\u5c06\u5176\u817f\u649e\u9aa8\u6298\u5e76\u593a\u56de\u624b\u673a\u3002\u5173\u4e8e\u7532\u7684\u884c\u4e3a\u7684\u6027\u8d28\uff0c\u4e0b\u5217\u9009\u9879\u6b63\u786e\u7684\u662f\nA. \u88ab\u5bb3\u4eba\u627f\u8bfa\nB. \u7d27\u6025\u907f\u9669\nC. \u6b63\u5f53\u9632\u536b\nD. \u81ea\u6551\u884c\u4e3a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4700120073440385, "meta-math/MetaMath-Mistral-7B": 0.9621050278885028, "itpossible/Chinese-Mistral-7B-v0.1": 0.5683674717811994, "HuggingFaceH4/zephyr-7b-beta": 0.996944705081393, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9165542605672027, "meta-llama/Meta-Llama-3-8B": 0.6032831066611845, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9894937650246249}}, {"question": "\u6839\u636e\u4e0b\u5217\u54ea\u4e00\u9879\u6765\u9009\u62e9\u8bfe\u7a0b\u7c7b\u578b\u662f\u804c\u4e1a\u5b66\u6821\u4e13\u4e1a\u6cbb\u7406\u7684\u91cd\u8981\u65b9\u9762\nA. \u4e13\u4e1a\u7279\u70b9\nB. \u6559\u80b2\u76ee\u6807\nC. \u793e\u4f1a\u9700\u6c42\nD. \u5b66\u751f\u6c34\u5e73\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9009\u62e9\u65ad\u8def\u5668\u906e\u65ad\u5bb9\u91cf\u5e94\u6839\u636e\u5b89\u88c5\uff08\uff09\u6765\u51b3\u5b9a\u3002\nA. \u6700\u5927\u77ed\u8def\u7535\u6d41\nB. \u6700\u5927\u8d1f\u8377\nC. \u53d8\u538b\u5668\u7684\u5bb9\u91cf\nD. \u6700\u5927\u7535\u538b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7022899664434523, "meta-math/MetaMath-Mistral-7B": 0.9250686196903729, "itpossible/Chinese-Mistral-7B-v0.1": 0.8215122607143112, "HuggingFaceH4/zephyr-7b-beta": 0.9984903645704438, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8930902618159473, "meta-llama/Meta-Llama-3-8B": 0.6547762282226853, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6398181538300687}}, {"question": "1971\u5e749\u670822\u65e5\uff0c\u7f8e\u56fd\u548c\u65e5\u672c\u7b49\u56fd\u5bb6\u63d0\u51fa\u201c\u5173\u4e8e\u4ee3\u8868\u6743\u7684\u51b3\u8bae\u8349\u6848\u201d\u65f6\u6307\u51fa\uff1a\u201c\uff08\u4e00\uff09\u5179\u786e\u8ba4\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u7684\u4ee3\u8868\u6743\uff0c\u5e76\u4e14\u5efa\u8bae\u5b83\u5f97\u5230\u5b89\u5168\u7406\u4e8b\u4f1a\u5e38\u4efb\u7406\u4e8b\u56fd\u4e4b\u4e00\u7684\u5e2d\u4f4d\uff1b\uff08\u4e8c\uff09\u786e\u8ba4\u201c\u4e2d\u534e\u6c11\u56fd\u201d\u7ee7\u7eed\u62e5\u6709\u4ee3\u8868\u6743\uff1b\uff08\u4e09\uff09\u5efa\u8bae\u8054\u5408\u56fd\uff5e\u5207\u7ec4\u7ec7\u548c\u4e13\u95e8\u673a\u6784\u5728\u51b3\u5b9a\u4e2d\u56fd\u4ee3 \u8868\u6743\u65f6\u8003\u8651\u672c\u51b3\u8bae\u7684\u6761\u6b3e\u3002\u201d\u8fd9\u4e00\u201c\u51b3\u8bae\u8349\u6848\u201d\u4e3b\u8981\u53cd\u6620\u4e86\u7f8e\u65e5\nA. \u53cd\u5bf9\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u53d6\u4ee3\u53f0\u6e7e\u5728\u8054\u5408\u56fd\u7684\u5e2d\u4f4d\nB. \u53cd\u5bf9\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u548c\u53f0\u6e7e\u540c\u65f6\u53c2\u52a0\u8054\u5408\u56fd\nC. \u5c0a\u91cd\u8054\u5408\u56fd\u56fd\u9645\u7ec4\u7ec7\u7684\u6743\u529b\u548c\u5730\u4f4d\nD. \u4e3b\u5f20\u6062\u590d\u65b0\u4e2d\u56fd\u5728\u8054\u5408\u56fd\u7684\u5408\u6cd5\u5e2d\u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5297915344724723, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8054\u5408\u56fd\u6559\u79d1\u6587\u7ec4\u7ec7\u628a\u54ea\u4e00\u5929\u5b9a\u4e3a\u54f2\u5b66\u65e5\nA. 10\u6708\u7684\u7b2c\u4e09\u4e2a\u661f\u671f\u56db\nB. 11\u6708\u7684\u7b2c\u4e09\u4e2a\u661f\u671f\u65e5\nC. 11\u6708\u7684\u7b2c\u4e09\u4e2a\u661f\u671f\u56db\nD. 10\u6708\u7684\u7b2c\u4e09\u4e2a\u661f\u671f\u65e5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4436867416134287, "meta-math/MetaMath-Mistral-7B": 0.4644320550434436, "itpossible/Chinese-Mistral-7B-v0.1": 0.35096048266444385, "HuggingFaceH4/zephyr-7b-beta": 0.8349890328703394, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5893012300888714, "meta-llama/Meta-Llama-3-8B": 0.4488746704506832, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u73bb\u3001\u6cfc\u3001\u6478\u3001\u4f5b\u201d\u5e94\u8be5\u62fc\u5199\u6210\nA. buo\u3001puo\u3001muo\u3001fuo\nB. be\u3001pe\u3001me\u3001fe\nC. bo\u3001po\u3001mo\u3001fo\nD. bu\u3001pu\u3001mu\u3001fu\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.41353800851238687, "meta-math/MetaMath-Mistral-7B": 0.7664818245682399, "itpossible/Chinese-Mistral-7B-v0.1": 0.38603625887814164, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.43682726830261503, "meta-llama/Meta-Llama-3-8B": 0.5977709785301617, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4641850345470232}}, {"question": "\u201c\u8840\u4e4b\u5e9c\u201d\u662f\u6307\nA. \u809d\nB. \u8109\nC. \u51b2\u8109\nD. \u5fc3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5747\u8861\u662f\u6307\u4f9b\u7ed9\u548c\u9700\u6c42\u8fbe\u5230\u4e86\uff08\uff09\u72b6\u6001\u3002\nA. \u4e0d\u5e73\u8861\nB. \u5e73\u8861\nC. \u6700\u5c0f\u91cf\nD. \u6700\u5927\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9505860971248122, "meta-math/MetaMath-Mistral-7B": 0.998926276256029, "itpossible/Chinese-Mistral-7B-v0.1": 0.9206597107627601, "HuggingFaceH4/zephyr-7b-beta": 0.9999106584195395, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9665199063826548, "meta-llama/Meta-Llama-3-8B": 0.9683853458402683, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9965297242271376}}, {"question": "\u88ab\u89c6\u4e3a\u514b\u670d\u201c\u5e02\u573a\u5931\u7075\u201d\u6700\u6709\u6548\u673a\u5236\u7684\u793e\u4f1a\u7ec4\u7ec7\u662f\nA. \u7b2c\u4e09\u90e8\u95e8\nB. \u80a1\u4efd\u5236\u516c\u53f8\nC. \u516c\u5171\u90e8\u95e8\nD. \u653f\u5e9c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6559\u80b2\u8fc7\u7a0b\u4e2d\u4eba\u4e0e\u4eba\u4e4b\u95f4\u6700\u57fa\u672c\u3001\u6700\u91cd\u8981\u7684\u4eba\u9645\u5173\u7cfb\u662f\nA. \u4ee5\u4e0a\u90fd\u6b63\u786e\nB. \u540c\u4f34\u5173\u7cfb\nC. \u5e08\u751f\u5173\u7cfb\nD. \u5f02\u6027\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8313426240483479, "HuggingFaceH4/zephyr-7b-beta": 0.9964556317737406, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6717095721226317, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5606442555299598}}, {"question": "\u706b\u661f\u57282018\u5e74\u5927\u51b2\u65f6\u4f4d\u4e8e\u54ea\u4e2a\u661f\u5ea7\u5929\u533a\u5185\nA. \u72ee\u5b50\u5ea7\nB. \u6469\u7faf\u5ea7\nC. \u5929\u79e4\u5ea7\nD. \u5de8\u87f9\u5ea7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u2f54\u5e73\u684c\u2faf\u4e0a\u2f00\u91cd200\u2f5c\u7684\u7269\u4f53\uff0c\u4e0e\u684c\u2faf\u95f4\u7684\u6ed1\u52a8\u6469\u64e6\u7cfb\u6570\u4e3a0.2\uff0c\u5f53\u4f9d\u6b21\u2f6415N\uff0c30N\uff0c80N\u7684\u2f54\u5e73\u62c9\u2f12\u62c9\u6b64\u7269\u4f53\u65f6\uff0c\u7269\u4f53\u53d7\u5230\u7684\u6469\u64e6\u2f12\u4f9d\u6b21\u4e3a\uff1a\uff08\u8bbe\u6700\u2f24\u9759\u6469\u64e6\u2f12\u7b49\u4e8e\u6ed1\u52a8\u6469\u64e6 \uff09\nA. 0N\uff0c 15N\uff0c15N\uff1b\nB. 15N\uff0c40N\uff0c40N\nC. 15N\uff0c30N\uff0c40N\uff1b\nD. 0N\uff0c 20N\uff0c40N\uff1b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35946737878573115, "meta-math/MetaMath-Mistral-7B": 0.6109588368402581, "itpossible/Chinese-Mistral-7B-v0.1": 0.33397544183964467, "HuggingFaceH4/zephyr-7b-beta": 0.554270259315621, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5014154751172246, "meta-llama/Meta-Llama-3-8B": 0.3546612443924434, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5374780324619164}}, {"question": "\u5728\u75c5\u56e0\u5b66\u4e0a\uff0c\u8f6e\u72b6\u6a21\u578b\u4e0e\u4e09\u89d2\u6a21\u578b\u76f8\u6bd4\uff0c\u5176\u4e3b\u8981\u7684\u4e0d\u540c\u70b9\u662f\nA. \u66f4\u5f3a\u8c03\u5f71\u54cd\u75be\u75c5\u53d1\u751f\u7684\u4e09\u8981\u7d20\nB. \u66f4\u5f3a\u8c03\u7279\u5f02\u6027\u75c5\u539f\u5b66\u8bf4\nC. \u6709\u5dee\u522b\u5730\u770b\u5f85\u75be\u75c5\u7684\u4e09\u8981\u7d20\nD. \u5f3a\u8c03\u73af\u5883\u53ca\u73af\u5883\u4e0e\u673a\u4f53\u7684\u5bc6\u5207\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2909633459929373, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7470050651895969, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3454901351726418, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u53e4\u4ee3\u9879\u7fbd\u201c\u7834\u91dc\u6c89\u821f\u201d\u6218\u80dc\u79e6\u519b\u662f\u5728\u54ea\u6b21\u6218\u5f79\nA. \u6606\u9633\u4e4b\u6218\nB. \u7267\u91ce\u4e4b\u6218\nC. \u5b98\u6e21\u4e4b\u6218\nD. \u5de8\u9e7f\u4e4b\u6218\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3919135642359265, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u2f00\u4f4d\u4e2d\u5b66\u2f63\u5bf9\u5730\u7403\u7684\u5438\u5f15\u2f12\u7ea6\u4e3a\nA. 0N\nB. 100N\nC. 500N\nD. \u65e0\u7a77\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8bb8\u614e\u6240\u4e3e\u201c\u8003\u3001\u8001\u201d\u4e24\u5b57\uff0c\u7528\u6765\u8bf4\u660e\u7684\u201c\u516d\u4e66\u201d\u4e2d\u7684\u5b9a\u4e49\u662f\nA. \u4ee5\u4e8b\u7232\u540d\uff0c\u53d6\u8b6c\u76f8\u6210\nB. \u672c\u7121\u5176\u5b57\uff0c\u4f9d\u8072\u8a17\u4e8b\nC. \u756b\u6210\u5176\u7269\uff0c\u968f\u9ad4\u8a70\u8a58\nD. \u5efa\u985e\u4e00\u9996\uff0c\u540c\u610f\u76f8\u53d7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2821833983601388, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5efa\u7b51\u5c5e\u4e8e\nA. \u9020\u578b\nB. \u88c5\u9970\nC. \u5b9e\u7528\u827a\u672f\nD. \u8868\u73b0\u827a\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5530369205600102, "meta-math/MetaMath-Mistral-7B": 0.6251333465099599, "itpossible/Chinese-Mistral-7B-v0.1": 0.6811217772610656, "HuggingFaceH4/zephyr-7b-beta": 0.9996670320431515, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8379314075318663, "meta-llama/Meta-Llama-3-8B": 0.8164577106704087, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9420370271871632}}, {"question": "\u4e0b\u5217\u54ea\u4e00\u9879\u4e0d\u5c5e\u4e8e\u7530\u95f4\u7684\u8bd7\u96c6\nA. \u300a\u5927\u5830\u6cb3\u300b\nB. \u300a\u4e2d\u56fd\u7267\u6b4c\u300b\nC. \u300a\u672a\u660e\u96c6\u300b\nD. \u300a\u4e2d\u56fd\u519c\u6751\u7684\u6545\u4e8b\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u900f\u660e\u8840\u6813\u7684\u4e3b\u8981\u6210\u5206\u662f\nA. \u7ea4\u7ef4\u86cb\u767d\nB. \u7ea2\u7ec6\u80de\nC. \u767d\u7ec6\u80de\nD. \u8840\u5c0f\u677f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3013812289234467, "meta-math/MetaMath-Mistral-7B": 0.9003701901655806, "itpossible/Chinese-Mistral-7B-v0.1": 0.4481817085741831, "HuggingFaceH4/zephyr-7b-beta": 0.934685470898547, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3300364758848943, "meta-llama/Meta-Llama-3-8B": 0.4025265366722339, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9673877568278215}}, {"question": "\u5f20\u7ade\u8bf4\uff1a\u53ea\u6709\u6b63\u5f0f\u4ee3\u8868\u624d\u53ef\u4ee5\u53d1\u8a00\u3002 \u5218\u5f3a\u8bf4\uff1a\u4e0d\u5bf9\u5427!\u674e\u8d35\u4e5f\u662f\u6b63\u5f0f\u4ee3\u8868\u4f46\u4ed6\u5e76\u53d1\u8a00 \u5218\u5f3a\u7684\u56de\u7b54\u662f\u628a\u5f20\u7ade\u7684\u8bdd\u9519\u8bef\u7684\u7406\u89e3\u4e3a\u4ee5\u4e0b\u54ea\u9879?\nA. \u674e\u8d35\u8981\u53d1\u8a00\nB. \u6240\u6709\u53d1\u8a00\u7684\u4eba\u90fd\u662f\u6b63\u5f0f\u4ee3\u8868\nC. \u6ca1\u6709\u6b63\u5f0f\u4ee3\u8868\u53d1\u8a00\nD. \u6240\u6709\u6b63\u5f0f\u4ee3\u8868\u90fd\u53d1\u8a00\u4e86\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4fe1\u606f\u7684\u4f20\u64ad\u6c9f\u901a\u5728\u516c\u5171\u5173\u7cfb\u4e2d\u5177\u6709\u4e00\u79cd\u72ec\u7279\u7684\u4f5c\u7528\uff0c\uff08\uff09\u5229\u7528\u4f20\u64ad\u6c9f\u901a\u624b\u6bb5\u53bb\u5efa\u7acb\u548c\u7ef4\u6301\u4e0e\u516c\u4f17\u4e4b\u95f4\u7684\u4ea4\u6d41\uff0c\u53bb\u4e86\u89e3\u548c\u5f71\u54cd\u516c\u4f17\u7684\u610f\u89c1\u3001\u6001\u5ea6\u548c\u884c\u4e3a\uff0c\u8fd9\u662f\u516c\u5171\u5173\u7cfb\u5de5\u4f5c\u4e0e\u5176\u4ed6\u7ba1\u7406\u5de5\u4f5c\u7684\u663e\u8457\u533a\u522b\u3002\nA. \u4f01\u4e1a\nB. \u516c\u5173\nC. \u793e\u4f1a\u7ec4\u7ec7\nD. \u7ec4\u7ec7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u7684\u4eba\u6ca1\u6709\u201c\u4e13\u4e1a\u6027\u201d\u89c2\u5ff5\uff0c\u6b23\u8d4f\u5168\u624d\u3001\u901a\u624d\uff0c\u4e0d\u91cd\u89c6\u4e43\u81f3\u9119\u89c6\u4e13\u4e1a\u4eba\u624d\uff0c\u4ee5\u4e3a\u53ea\u8981\u609f\u201c\u9053\u201d\uff0c\u5c31\u53ef\u4ee5__\uff0c\u4ec0\u4e48\u95ee\u9898\u90fd\u53ef\u4ee5\u8fce\u5203\u800c\u89e3\uff0c\u4ec0\u4e48\u9886\u57df\u90fd\u53ef\u4ee5\u53bb\u5750\u800c\u8bba\u9053\u3002\u8fd9\u79cd\u89c2\u5ff5\u5728\u90e8\u5206\u5f53\u4ee3\u4eba\u6587\u5b66\u8005\u4e2d\u4ecd\u7136\u5b58\u5728\uff0c\u79d1\u7814\u5de5\u4f5c\u8005\u5728\u4ed6\u4eec\u773c\u4e2d\u53ea\u662f\u4e9b\u5173\u6ce8\u7ec6\u679d\u672a\u8282\u7684\u6280\u672f\u5458\uff0c\u4e0d\u5982\u4ed6\u4eec\u638c\u63e1\u5148\u8fdb\u7684\u54f2\u5b66\u601d\u60f3\u540e\uff0c\u53ef\u4ee5\u7ad9\u5f97\u9ad8\u770b\u5f97\u8fdc\uff0c\u4e43\u81f3\u4ee5\u79d1\u5b66\u5bfc\u5e08\u81ea\u5c45\uff0c\u53ef\u4ee5\u4e3a\u79d1\u5b66\u7684\u53d1\u5c55__\u3002\u4f9d\u6b21\u586b\u5165\u5212\u6a2a\u7ebf\u90e8\u5206\u6700\u6070\u5f53\u7684\u4e00\u9879\u662f\nA. \u4e3e\u4e00\u53cd\u4e09\u51fa\u8c0b\u5212\u7b56\nB. \u878d\u4f1a\u8d2f\u901a\u6307\u70b9\u8ff7\u6d25\nC. \u89c1\u5fae\u77e5\u8457\u5efa\u8a00\u732e\u7b56\nD. \u89e6\u7c7b\u65c1\u901a\u8fd0\u7b79\u5e37\u5e44\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4213546647510929, "itpossible/Chinese-Mistral-7B-v0.1": 0.2801290860251075, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.40553129844907915, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5951009299701138}}, {"question": "\u5f62\u58f0\u5b57\u4e49\u7b26\u7684\u8868\u4e49\u7279\u70b9\u662f\nA. \u5927\u591a\u6570\u4e49\u7b26\u90fd\u80fd\u8868\u793a\u8bcd\u7684\u5177\u4f53\u610f\u4e49\u3002\nB. \u5927\u591a\u6570\u4e49\u7b26\u90fd\u4e0d\u80fd\u8868\u793a\u8bcd\u4e49\u7684\u6240\u5c5e\u8303\u56f4\u3002\nC. \u5927\u591a\u6570\u4e49\u7b26\u90fd\u80fd\u8868\u793a\u8bcd\u4e49\u6240\u5c5e\u7684\u8303\u56f4\u3002\nD. \u5927\u591a\u6570\u4e49\u7b26\u90fd\u6ca1\u6709\u8868\u4e49\u4f5c\u7528\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5774258021728516, "meta-math/MetaMath-Mistral-7B": 0.9518367651629002, "itpossible/Chinese-Mistral-7B-v0.1": 0.46128371107342453, "HuggingFaceH4/zephyr-7b-beta": 0.981540794114561, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8773308507992363, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6709\u5173\u201c\u96c6\u89e3\u7c7b\u201d\u7684\u8bf4\u6cd5\uff0c\u8868\u8ff0\u6b63\u786e\u7684\u4e00\u53e5\u662f\nA. \u201c\u96c6\u89e3\u7c7b\u201d\u7684\u6700\u5927\u7279\u70b9\u662f\u7ef4\u62a4\u524d\u4eba\u6ce8\u91ca\u7684\u89c2\u70b9\u3002\nB. \u201c\u96c6\u89e3\u7c7b\u201d\u662f\u6307\u6c47\u96c6\u5404\u5bb6\u6ce8\u89e3\uff0c\u518d\u52a0\u4e0a\u81ea\u5df1\u6ce8\u89e3\u7684\u4e00\u79cd\u53e4\u6ce8\u7c7b\u578b\u3002\nC. \u201c\u96c6\u89e3\u7c7b\u201d\u7684\u4ee3\u8868\u4f5c\u662f\u664b\u4ee3\u675c\u9884\u7684\u300a\u6625\u79cb\u7ecf\u4f20\u96c6\u89e3\u300b\u3002\nD. \u201c\u96c6\u89e3\u7c7b\u201d\u7684\u91cd\u70b9\u662f\u5bf9\u524d\u4eba\u6ce8\u91ca\u7684\u8865\u5145\u548c\u8ba2\u6b63\u3002\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4629595989704193, "meta-math/MetaMath-Mistral-7B": 0.9866581037341576, "itpossible/Chinese-Mistral-7B-v0.1": 0.44827810524264994, "HuggingFaceH4/zephyr-7b-beta": 0.9989436110405141, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4304908423132082, "meta-llama/Meta-Llama-3-8B": 0.6135467555547893, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9733205992636216}}, {"question": "\u5728\u57fa\u7840\u8bad\u7ec3\u9636\u6bb5\uff0c\u8bad\u7ec3\u8d1f\u8377\u589e\u52a0\u7684\u65b9\u5f0f\u662f\nA. \u8df3\u8dc3\u5f0f\nB. \u952f\u9f7f\u5f0f\nC. \u5faa\u5e8f\u6e10\u8fdb\nD. \u6ce2\u6d6a\u5f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6366680654312518, "meta-math/MetaMath-Mistral-7B": 0.8477927723678117, "itpossible/Chinese-Mistral-7B-v0.1": 0.5358178558669542, "HuggingFaceH4/zephyr-7b-beta": 0.9610538685982466, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8000276402794246, "meta-llama/Meta-Llama-3-8B": 0.5990210269638426, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.905721465689404}}, {"question": "\u5728\u5e02\u573a\u7ecf\u6d4e\u4e2d\u4ec5\u9760\u5951\u7ea6\u4f26\u7406\u7684\u7ea6\u675f\uff0c\u5951\u7ea6\u662f\u5f88\u96be\u987a\u5229\u5b8c\u6210\u7684\u3002\u5e02\u573a\u7ecf\u6d4e\u53d1\u5c55\u7684\u5fc5\u7136\u662f\u5c06\u5951\u7ea6\u5173\u7cfb\nA. \u5e02\u573a\u5316\nB. \u8bda\u4fe1\u5316\nC. \u6cd5\u5f8b\u5316\nD. \u4f26\u7406\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5de5\u4e1a\u9769\u547d\u6539\u53d8\u4e86\u82f1\u56fd\u7684\u793e\u4f1a\u7ed3\u6784\uff0c\u7ecf\u6d4e\u7684\u6301\u7eed\u589e\u957f\u9020\u6210\u4e2d\u7b49\u9636\u7ea7\u4e0e\u5de5\u4eba\u9636\u7ea7\u529b\u91cf\u7684\u58ee\u5927\uff0c\u8d35\u65cf\u3001\u5927\u5730\u4e3b\u7684\u5730\u4f4d\u4e0e\u7ecf\u6d4e\u5b9e\u529b\u90fd\u5927\u4e3a\u4e0b\u964d\u4e86\uff0c\u4f46\u4ed6\u4eec\u4ecd\u628a\u6301\u653f\u6cbb\u6743\u529b\uff0c\u9020\u6210\u6743\u529b\u5206\u914d\u4e0e\u793e\u4f1a\u529b\u91cf\u5bf9\u6bd4\u9ad8\u5ea6\u8131\u8282\u7684\u72b6\u6001\u3002\u201d\u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u201c\u72b6\u6001\u51b6\uff0c\u82f1\u56fd\u5728\u653f\u6cbb\u4e0a\nA. \u9881\u5e03\u300a\u6743\u5229\u6cd5\u6848\u300b\uff0c\u786e\u7acb\u4e86\u8bae\u4f1a\u4e3b\u6743\nB. \u6539\u9769\u8bae\u4f1a\u5236\u5ea6\uff0c\u4e2d\u7b49\u9636\u7ea7\u66f4\u591a\u53c2\u653f\nC. \u5efa\u7acb\u5185\u9601\u673a\u6784\uff0c\u9650\u5236\u56fd\u738b\u7684\u884c\u653f\u6743\nD. \u53d1\u52a8\u5149\u8363\u9769\u547d\uff0c\u4e0e\u65b0\u8d35\u65cf\u9636\u5c42\u59a5\u534f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6309\u7167\u8d26\u6237\u7684\u7528\u9014\u548c\u7ed3\u6784\u5206\u7c7b\uff0c\u201c\u7d2f\u8ba1\u6298\u65e7\u201d\u8d26\u6237\u5c5e\u4e8e\nA. \u8ba1\u4ef7\u5bf9\u6bd4\u8d26\u6237\nB. \u96c6\u5408\u5206\u914d\u8d26\u6237\nC. \u76d8\u5b58\u8d26\u6237\nD. \u8c03\u6574\u8d26\u6237\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3348234986789106, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "20\u4e16\u7eaa50\u5e74\u4ee3\u4e2d\u671f\uff0c\u7f8e\u56fd\u4e00\u4f4d\u8457\u540d\u9ed1\u4eba\u7235\u58eb\u4e50\u6f14\u5531\u5bb6\uff0c\u5728\u7f8e\u56fd\u65b0\u95fb\u7f72\u7684\u8d44\u52a9\u548c\u5b89\u6392\u4e0b\uff0c\u591a\u6b21\u8d74\u7f8e\u6d32\u7b49\u5730\u5de1\u6f14\uff0c\u8d62\u5f97\u4e86\u5927\u91cf\u6b4c\u8ff7\uff0c\u5f88\u591a\u4eba\u901a\u8fc7\u5979\u7684\u6f14\u5531\u77e5\u9053\u4e86\u7f8e\u56fd\u3002\u7f8e\u56fd\u653f\u5e9c\u673a\u6784\u652f\u6301\u8be5\u6f14\u5531\u5bb6\u6d77\u5916\u5de1\u6f14\u7684\u4e3b\u8981\u76ee\u7684\u662f\nA. \u4e89\u53d6\u56fd\u5185\u9ed1\u4eba\u9009\u6c11\u652f\u6301\nB. \u5c55\u793a\u7f8e\u56fd\u7684\u7ecf\u6d4e\u5b9e\u529b\nC. \u62b5\u5236\u4e0d\u7ed3\u76df\u8fd0\u52a8\u7684\u53d1\u5c55\nD. \u4e0e\u82cf\u8054\u4e89\u593a\u4e2d\u95f4\u5730\u5e26\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4934439960219093, "meta-math/MetaMath-Mistral-7B": 0.4258861893623922, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9423047051290002, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u827a\u672f\u7684\u9996\u8981\u529f\u80fd\u662f\nA. \u542f\u8499\nB. \u5ba1\u7f8e\nC. \u8ba4\u8bc6\nD. \u611f\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.38258859286778335, "itpossible/Chinese-Mistral-7B-v0.1": 0.587841911644165, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6633024707958219, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9979135548740485}}, {"question": "\u4e0b\u5217\u6709\u5173\u6cd5\u7cfb\u4e0e\u6cd5\u5f8b\u4f53\u7cfb\u542b\u4e49\u7684\u8868\u8ff0\uff0c\u54ea\u9879\u662f\u4e0d\u6b63\u786e\u7684\uff1f\nA. \u6cd5\u5f8b\u4f53\u7cfb\u662f\u7531\u4e00\u4e2a\u56fd\u5bb6\u7684\u5baa\u6cd5\u3001\u884c\u653f\u6cd5\u3001\u6c11\u6cd5\u3001\u7ecf\u6d4e\u6cd5\u3001\u5211\u6cd5\u3001\u8bc9\u8bbc\u6cd5\u7b49\u6784\u6210\u7684\u5185\u90e8\u548c\u8c10\u4e00\u81f4\u3001\u6709\u673a\u8054\u7cfb\u7684\u6574\u4f53\nB. \u6cd5\u7cfb\u662f\u5177\u6709\u540c\u4e00\u5386\u53f2\u4f20\u7edf\u7684\u56fd\u5bb6\u548c\u5730\u533a\u7684\u6cd5\u7684\u603b\u79f0\nC. \u6cd5\u5f8b\u4f53\u7cfb\u662f\u4e00\u56fd\u4e4b\u5185\u7684\u6cd5\u6784\u6210\u7684\u4f53\u7cfb\uff0c\u4e0d\u5305\u62ec\u5176\u4ed6\u56fd\u5bb6\u7684\u6cd5\u6216\u5b8c\u6574\u610f\u4e49\u7684\u56fd\u9645\u6cd5\nD. \u6cd5\u7cfb\u662f\u6839\u636e\u82f1\u56fd\u666e\u901a\u6cd5\uff08\u5224\u4f8b\u6cd5\uff09\u548c\u6b27\u6d32\u5927\u9646\u6cd5\u5178\u6cd5\u7684\u5386\u53f2\u4f20\u7edf\u800c\u5bf9\u6cd5\u6240\u4f5c\u7684\u5206\u7c7b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.989854863191342, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6962816444217953}}, {"question": "\u75ab\u6e90\u5730\u8303\u56f4\u7684\u5927\u5c0f\u53d6\u51b3\u4e8e\nA. \u4f20\u67d3\u6e90\u6d3b\u52a8\u8303\u56f4\uff0c\u4f20\u64ad\u9014\u5f84\u7684\u7279\u70b9\u548c\u5468\u56f4\u4eba\u7fa4\u7684\u514d\u75ab\u72b6\u6001\nB. \u4f20\u67d3\u6e90\u7684\u6d3b\u52a8\u8303\u56f4\uff0c\u6392\u51fa\u75c5\u539f\u4f53\u7684\u6570\u91cf\u53ca\u6bd2\u529b\nC. \u4f20\u67d3\u6e90\u7684\u6570\u91cf\uff0c\u6d3b\u52a8\u8303\u56f4\u53ca\u5468\u56f4\u4eba\u53e3\u6570\u591a\u5c11\nD. \u4f20\u67d3\u6e90\u7684\u6d3b\u52a8\u8303\u56f4\uff0c\u6570\u91cf\u548c\u5468\u56f4\u4eba\u7fa4\u7684\u514d\u75ab\u72b6\u6001\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5377671686655124, "meta-math/MetaMath-Mistral-7B": 0.5482071756600734, "itpossible/Chinese-Mistral-7B-v0.1": 0.3817240769874595, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7160087261855475, "meta-llama/Meta-Llama-3-8B": 0.5372303909522381, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9747224546269284}}, {"question": "\u6211\u56fd\u6b63\u5f0f\u9881\u5e03\u7684\u7b2c\u4e00\u4e2a\u73b0\u4ee3\u5b66\u5236\u662f\nA. \u7678\u4e11\u5b66\u5236\nB. \u7678\u536f\u5b66\u5236\nC. \u58ec\u620c\u5b66\u5236\nD. \u58ec\u5bc5\u5b66\u5236\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3542800456958427, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u8180\u80f1\u8bf4\u6cd5\u6b63\u786e\u7684\u662f\nA. \u8180\u80f1\u5c16\u5904\u4e3a\u5c3f\u9053\u5185\u53e3\nB. \u5c5e\u4e8e\u8179\u819c\u5185\u4f4d\u5668\u5b98\nC. \u8180\u80f1\u9888\u7684\u540e\u65b9\u6709\u524d\u5217\u817a\nD. \u7a7a\u865a\u65f6\u4f4d\u4e8e\u5c0f\u9aa8\u76c6\u8154\u5185\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9a6c\u514b\u601d\u628a\u9ed1\u683c\u5c14\u7684\u8fa9\u8bc1\u6cd5\u79f0\u4e3a\nA. \u7cbe\u9ad3\nB. \u57fa\u672c\u5185\u6838\nC. \u5408\u7406\u5185\u6838\nD. \u6838\u5fc3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u4eba\u5ba3\u6cc4\u88ab\u538b\u6291\u4e86\u7684\u6b32\u671b\uff0c\u8fd9\u5728\u4fe1\u606f\u7684\u5206\u7c7b\u4e0a\u88ab\u79f0\u4e3a\u4f55\u79cd\u4fe1\u606f?\nA. \u590d\u539f\u6027\u4fe1\u606f\nB. \u77e5\u8bc6\u6027\u4fe1\u606f\nC. \u7ef4\u6301\u6027\u4fe1\u606f\nD. \u6559\u80b2\u6027\u4fe1\u606f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3233250233605477, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4304106889248509, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u949d\u89d2\u4e09\u89d2\u5f62\u7684\u5185\u89d2\u548c()\u9510\u89d2\u4e09\u89d2\u5f62\u7684\u5185\u89d2\u548c\nA. \u5c0f\u4e8e\nB. \u65e0\u6cd5\u786e\u5b9a\nC. \u5927\u4e8e\nD. \u7b49\u4e8e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u8bcd\u8bed\u4e2d\u6709\u9519\u522b\u5b57\u7684\u2f00\u9879\u662f\nA. \u5639\u4eae \u2ed4\u69db \u83e9\u63d0\u6811 \u9053\u542c\u9014\u8bf4 \nB. \u5ac9\u5992 \u2f46\u8f9c \u9713\u8679\u706f \u9e26\u96c0\u2f46\u58f0\nC. \u627f\u8bfa \u6851\u6893 \u95ed\u2ed4\u7fb9 \u6291\u626c\u987f\u632b\nD. \u5d99\u5ccb \u60d8\u7136 \u7ecf\u4f1f\u7ebf \u767b\u5cf0\u9020\u6781\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u6211\u56fd\u793e\u4f1a\u4e3b\u4e49\u521d\u7ea7\u9636\u7ea7\uff0c\u793e\u4f1a\u4e3b\u4e49\u96c6\u4f53\u4e3b\u4e49\u9053\u5fb7\u539f\u5219\u5177\u6709\u591a\u5c42\u6b21\u7684\u9053\u5fb7\u8981\u6c42\u3002\u5176\u4e2d\u6700\u57fa\u672c\u7684\u9053\u5fb7\u8981\u6c42\u662f\nA. \u635f\u516c\u80a5\u79c1\u3001\u635f\u4eba\u5229\u5df1\nB. \u5148\u516c\u540e\u79c1\u3001\u5148\u4eba\u540e\u5df1\nC. \u65e0\u79c1\u5949\u732e\u3001\u4e00\u5fc3\u4e3a\u516c\nD. \u516c\u79c1\u517c\u987e\u3001\u4e0d\u635f\u516c\u80a5\u79c1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5283089520997177, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.993951342090032, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5349679934739762, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u57fa\u4e8e\u7edf\u8ba1\u7684\u5206\u8bcd\u65b9\u6cd5\u4e3a\nA. \u6b63\u5411\u91cf\u6700\u5927\u5339\u914d\u6cd5\nB. \u6761\u4ef6\u968f\u673a\u573a\nC. \u6700\u5c11\u5207\u5206\nD. \u9006\u5411\u91cf\u6700\u5927\u5339\u914d\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5600823228651131}}, {"question": "\u4e0b\u5217\u54ea\u9879\u6d89\u53ca\u56fd\u5bb6\u5b89\u5168\u7684\u4e8b\u9879\u4e0d\u9700\u8981\u5ba1\u67e5\u76d1\u7ba1?\nA. \u53ef\u80fd\u5f71\u54cd\u56fd\u5bb6\u5b89\u5168\u7684\u7f51\u7edc\u4fe1\u606f\u6280\u672f\u4ea7\u54c1\u548c\u670d\u52a1\u7684\u9879\u76ee\nB. \u53ef\u80fd\u5f71\u54cd\u56fd\u5bb6\u5b89\u5168\u7684\u5916\u5546\u6295\u8d44\u7684\u9879\u76ee\nC. \u5bf9\u5f71\u54cd\u56fd\u5bb6\u5b89\u5168\u7684\u7279\u5b9a\u7269\u9879\u548c\u5173\u952e\u6280\u672f\u7684\u9879\u76ee\nD. \u53ef\u80fd\u5f71\u54cd\u4e0d\u7279\u5b9a\u7269\u9879\u548c\u5173\u952e\u6280\u672f\u7684\u9879\u76ee\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.515077361244583, "meta-math/MetaMath-Mistral-7B": 0.853589358107486, "itpossible/Chinese-Mistral-7B-v0.1": 0.48438633060354563, "HuggingFaceH4/zephyr-7b-beta": 0.9839745995827822, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8509611048539398, "meta-llama/Meta-Llama-3-8B": 0.7333071371104574, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6980666006577726}}, {"question": "\u68c0\u5bdf\u9662\u5e72\u8b66\u674e\u67d0\u5728\u8d1f\u8d23\u4fa6\u67e5\u67d0\u56fd\u6709\u516c\u53f8\u5973\u7ecf\u7406\u738b\u67d0\u8d2a\u6c61\u6848\u65f6\uff0c\u5bf9\u738b\u67d0\u8d2a\u6c61\u516c\u6b3e15\u4e07\u5143\u7684\u7f6a\u884c\u4e0d\u8ffd\u67e5\u3001\u4e0d\u53d6\u8bc1\uff0c\u5e76\u5c06\u7f6a\u8bc1\u6750\u6599\u9500\u6bc1\uff0c\u4ee5\u5e2e\u52a9\u738b\u67d0\u9003\u907f\u5904\u7f5a\u3002\u738b\u67d0\u5728\u674e\u67d0\u7684\u5e2e\u52a9\u4e0b\u83b7\u91ca\u540e\uff0c\u674e\u67d0\u4ee5\u6b64\u4e3a\u8981\u631f\uff0c\u5c06\u738b\u67d0\u591a\u6b21\u5978\u6c61\u3002\u674e\u67d0\u7684\u884c\u4e3a\u9664\u6784\u6210\u5f3a\u5978\u7f6a\u5916\uff0c\u8fd8\u6784\u6210\nA. \u73a9\u5ffd\u804c\u5b88\u7f6a\nB. \u5f87\u79c1\u6789\u6cd5\u7f6a\nC. \u5305\u5e87\u7f6a\nD. \u6ee5\u7528\u804c\u6743\u7f6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3888795602338667, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.48626403789062894, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4997353272938423}}, {"question": "\u5341\u4e00\u5c4a\u4e09\u4e2d\u5168\u4f1a\u4ee5\u6765\uff0c\u6211\u515a\u5236\u5b9a\u7684\u4e00\u7cfb\u5217\u6b63\u786e\u7684\u8def\u7ebf\u3001\u65b9\u9488\u3001\u653f\u7b56\u4fc3\u8fdb\u4e86\u6211\u56fd\u7ecf\u6d4e\u7684\u8fc5\u731b\u53d1\u5c55\uff0c\u8fd9\u8bf4\u660e\nA. \u793e\u4f1a\u4e3b\u4e49\u793e\u4f1a\u7684\u53d1\u5c55\u4e0d\u53d7\u7ecf\u6d4e\u57fa\u7840\u51b3\u5b9a\u4e0a\u5c42\u5efa\u7b51\u89c4\u5f8b\u7684\u5236\u7ea6\nB. \u4e0a\u5c42\u5efa\u7b51\u7684\u53d1\u5c55\u51b3\u5b9a\u7ecf\u6d4e\u57fa\u7840\u7684\u53d1\u5c55\u65b9\u5411 \nC. \u4e0a\u5c42\u5efa\u7b51\u5bf9\u7ecf\u6d4e\u57fa\u7840\u5177\u6709\u79ef\u6781\u7684\u80fd\u52a8\u4f5c\u7528\nD. \u7ecf\u6d4e\u57fa\u7840\u53d1\u5c55\u7684\u9053\u8def\u662f\u7531\u4e0a\u5c42\u5efa\u7b51\u51b3\u5b9a\u7684 \n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7499784239679264, "meta-math/MetaMath-Mistral-7B": 0.9704605355226912, "itpossible/Chinese-Mistral-7B-v0.1": 0.7726834685242019, "HuggingFaceH4/zephyr-7b-beta": 0.9942673903913917, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6264291679068347, "meta-llama/Meta-Llama-3-8B": 0.6527360896178257, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7641913011397555}}, {"question": "\u4e0e\u5730\u7403\u4e0a\u2f53\u538b\u5e26\u548c\u2edb\u5e26\u5b63\u8282\u79fb\u52a8\u7684\u539f\u56e0\u2f46\u5173\u7684\u662f\nA. \u2ee9\u2f9a\u4ea4\u2ec6\nB. \u592a\u9633\u76f4\u5c04\u70b9\u7684\u79fb\u52a8\nC. \u5730\u2faf\u6469\u64e6\u2f12\nD. \u5730\u7403\u516c\u8f6c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6266245919840825, "HuggingFaceH4/zephyr-7b-beta": 0.7224068067827415, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4861793253544494, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u5e02\u573a\u7ecf\u6d4e\u7684\u57fa\u672c\u5c5e\u6027\u4e0e\u8981\u6c42\u4e2d\uff0c\u6784\u6210\u5e73\u7b49\u4ea4\u6362\u7684\u5148\u51b3\u6761\u4ef6\u7684\u662f\nA. \u8ffd\u6c42\u5229\u76ca\u6700\u5927\u5316\nB. \u4ea7\u6743\u5173\u7cfb\u72ec\u7acb\u5316\nC. \u751f\u4ea7\u8981\u7d20\u5546\u54c1\u5316\nD. \u4f01\u4e1a\u884c\u4e3a\u5951\u7ea6\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6549376112390788, "meta-math/MetaMath-Mistral-7B": 0.9114187434536584, "itpossible/Chinese-Mistral-7B-v0.1": 0.4925437097888873, "HuggingFaceH4/zephyr-7b-beta": 0.999420083533522, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6906256778733589, "meta-llama/Meta-Llama-3-8B": 0.3919626083521393, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7535\u8bdd\u4ea4\u6362\u7cfb\u7edf\u91c7\u2f64\u7684\u662f\nA. \u5206\u7ec4\u4ea4\u6362\nB. \u7ebf\u8def\u4ea4\u6362\nC. \u62a5\u2f42\u4ea4\u6362\nD. \u4fe1\u53f7\u4ea4\u6362\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5935962017195566, "meta-math/MetaMath-Mistral-7B": 0.7282443525860569, "itpossible/Chinese-Mistral-7B-v0.1": 0.8888997257920915, "HuggingFaceH4/zephyr-7b-beta": 0.9816772473548285, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8170540730162249, "meta-llama/Meta-Llama-3-8B": 0.9352197907466009, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9979434294408667}}, {"question": "\u4e0b\u5217\u5143\u7d20\u4e2d\u5c5e\u4e8e\u96c6\u5408{x|x=2k k\u662f\u2f83\u7136\u6570}\u7684\u662f\nA. -2\nB. 3\nC. 10\nD. 7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.29863342676099575, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u2f42\u6bb5\u5185\u5bb9\uff0c\u5bf9\u4e0b\u2faf\u4e24\u4e2a\u75c5\u53e5\u7684\u4fee\u6539\u90fd\u6b63\u786e\u7684\u2f00\u9879\u662f\uff1a\u5728\u5b66\u6821\u5f00\u5c55\u620f\u5267\u5b66\u4e60\u3001\u620f\u5267\u5b9e\u8df5\u7684\u8fc7\u7a0b\u4e2d\uff0ci.\u5f88\u591a\u5bb6\u2ed3\u52aa\u2f12\u4e3a\u5b69\u2f26\u4e89\u53d6\u4e0a\u53f0\u6f14\u51fa\u3002\u4ed6\u4eec\u5e0c\u671b\u5b69\u2f26\u53d8\u5f97 \u66f4\u52a0\u5f00\u6717\u3001\u66f4\u52a0\u2f83\u4fe1\uff0cii.\u8bed\u2f94\u8868\u8fbe\u80fd\u2f12\u548c\u8868\u6f14\u2f54\u5e73\u5f97\u5230\u6539\u5584\u3002\u770b\u6765\uff0c\u5bb6\u2ed3\u4eec\u90fd\u2fae\u5e38\u6ce8\u91cd\u5bf9\u5b69\u2f26\u7efc\u5408\u7d20\u8d28 \u7684\u57f9\u517b\u3002\nA. i.\u53e5\u5e94\u5728\u201c\u5bb6\u2ed3\u201d\u4e4b\u524d\u52a0\u4e0a\u201c\u5b66\u2f63\u201d ii.\u53e5\u5e94\u5c06\u201c\u6539\u5584\u201d\u6539\u4e3a\u201c\u6539\u53d8\u201d\nB. i.\u53e5\u5e94\u5728\u201c\u6f14\u51fa\u201d\u4e4b\u540e\u52a0\u4e0a\u201c\u7684\u673a\u4f1a\u201d ii.\u53e5\u5e94\u5c06\u201c\u6539\u5584\u201d\u6539\u4e3a\u201c\u6539\u53d8\u201d \nC. i.\u53e5\u5e94\u5728\u201c\u6f14\u51fa\u201d\u4e4b\u540e\u52a0\u4e0a\u201c\u7684\u673a\u4f1a\u201d ii.\u53e5\u5e94\u5c06\u201c\u6539\u5584\u201d\u6539\u4e3a\u201c\u63d0\u2fbc\u201d \nD. i.\u53e5\u5e94\u5728\u201c\u5bb6\u2ed3\u201d\u4e4b\u524d\u52a0\u4e0a\u201c\u5b66\u2f63\u201d ii.\u53e5\u5e94\u5c06\u201c\u6539\u5584\u201d\u6539\u4e3a\u201c\u63d0\u2fbc\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3383963899443145, "meta-math/MetaMath-Mistral-7B": 0.3956423662301385, "itpossible/Chinese-Mistral-7B-v0.1": 0.34801883283797147, "HuggingFaceH4/zephyr-7b-beta": 0.6062821932174619, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4350764598732043, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6524355391299177}}, {"question": "\u5fc3\u7406\u7814\u7a76\u8868\u660e\uff0c\u7537\u5973\u5728\u5fc3\u7406\u4e0a\u6709\u5f88\u5927\u5dee\u522b\u3002\u7537\u4eba\u91cd\u7269\u8d28\uff0c\u5973\u4eba\u91cd\u611f\u60c5\uff1b\u7537\u4eba\u957f\u4e8e\u63a8\u7406\uff0c\u5973\u4eba\u505a\u4e8b\u51ed\u76f4\u89c9\u3002\u4e24\u4eba\u671d\u5915\u76f8\u5904\uff0c\u5fc5\u7136\u4f1a\u4ea7\u751f\nA. \u884c\u4e3a\u51b2\u7a81\nB. \u60c5\u7231\u51b2\u7a81\nC. \u5fc3\u7406\u51b2\u7a81\nD. \u6027\u683c\u51b2\u7a81\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4671452781346152, "meta-math/MetaMath-Mistral-7B": 0.3984585186882418, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6043962909424726, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5632763796179502, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9c81\u8fc5\u300a\u91ce\u8349\u300b\u4e2d\u7684\u6563\u6587\u6700\u521d\u9646\u7eed\u53d1\u8868\u5728\u54ea\u4e2a\u520a\u7269\u4e0a\nA. \u300a\u83bd\u539f\u300b\nB. \u300a\u6668\u62a5\u526f\u520a\u300b\nC. \u300a\u8bed\u4e1d\u300b\nD. \u300a\u65b0\u9752\u5e74\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2766930999728216, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5243222918601541, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u5168\u56fd\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u548c\u5730\u65b9\u5404\u7ea7\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u9009\u4e3e\u6cd5\u300b\u7b2c53\u6761\u89c4\u5b9a\uff1a\u7701\u3001\u81ea\u6cbb\u533a\u3001\u76f4\u8f96\u5e02\u7684\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u53ca\u5176\u5e38\u52a1\u59d4\u5458\u4f1a\u6839\u636e\u672c\u6cd5\u53ef\u4ee5\u5236\u5b9a\u9009\u4e3e\u5b9e\u65bd\u7ec6\u5219\uff0c\u62a5\u5168\u56fd\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u5e38\u52a1\u59d4\u5458\u4f1a\u5907\u6848\u3002\u8fd9\u4e00\u89c4\u5219\u5c5e\u4e8e\nA. \u975e\u786e\u5b9a\u6027\u89c4\u5219\nB. \u59d4\u4efb\u6027\u89c4\u5219\nC. \u51c6\u7528\u6027\u89c4\u5219\nD. \u6388\u6743\u6027\u89c4\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5620893951039037, "HuggingFaceH4/zephyr-7b-beta": 0.990624817357722, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5326190845563334, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7976018544783884}}, {"question": "\u6563\u6253\u6bd4\u8d5b\u6bcf\u573a\u6bd4\u8d5b\u91c7\u7528\u4e09\u5c40\u4e24\u80dc\u5236\uff0c\u6bcf\u5c40\u51c0\u62533\u5206\u949f\uff0c\u5c40\u95f4\u4f11\u606f\nA. 1\u5206\u949f\nB. 1\u520630\u79d2\nC. 2\u5206\u949f\nD. 30\u79d2\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3744481484445332, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7269\u6743\u7684\u53d6\u5f97\u3001\u8f6c\u79fb\u3001\u53d8\u66f4\u548c\u6d88\u706d\u7684\u6761\u4ef6\uff0c\u4e00\u822c\u5e94\u9002\u7528\nA. \u7269\u4e4b\u6240\u5728\u5730\u6cd5\nB. \u6240\u6709\u4eba\u7684\u672c\u56fd\u6cd5\nC. \u6cd5\u9662\u5730\u6cd5\nD. \u6240\u6709\u4eba\u7684\u4f4f\u6240\u5730\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9500990669594194, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5418889370632913, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u9009\u9879\u4e2d\u4e0e\u201c\u4ea1\u7f8a\u8865\u7262\u201d\u610f\u601d\u6700\u63a5\u8fd1\u7684\u662f\nA. \u5931\u4e4b\u4e1c\u9685\uff0c\u6536\u4e4b\u6851\u6986\nB. \u5f80\u8005\u4e0d\u53ef\u8c0f\uff0c\u6765\u8005\u72b9\u53ef\u8ffd\nC. \u4eba\u65e0\u8fdc\u8651\uff0c\u5fc5\u6709\u8fd1\u5fe7\nD. \u7978\u516e\uff0c\u798f\u4e4b\u6240\u501a\uff0c\u798f\u516e\uff0c\u795d\u4e4b\u6240\u4f0f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u96c6\u4f53\u4e3b\u4e49\u4e2d\u7684\u201c\u96c6\u4f53\u201d\u5e94\u7406\u89e3\u4e3a\nA. \u7ecf\u6d4e\u6240\u6709\u5236\u610f\u4e49\u4e0a\u7684\nB. \u67d0\u4e2a\u5c0f\u56e2\u4f53\u6216\u67d0\u4e2a\u5355\u4f4d\nC. \u4ee5\u65e0\u4ea7\u9636\u7ea7\u4e3a\u6838\u5fc3\u7684\u5229\u76ca\u96c6\u56e2\nD. \u6574\u4e2a\u793e\u4f1a\u6574\u4e2a\u56fd\u5bb6\u5728\u5185\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\uff08\uff09\u4e0d\u662f\u7cbe\u786e\u4f5c\u6218\u7684\u4e3b\u8981\u7279\u70b9\u3002\nA. \u4f7f\u6218\u573a\u751f\u5b58\u95ee\u9898\u53d8\u5f97\u81f3\u5173\u91cd\u8981\nB. \u4fe1\u606f\u4e0e\u706b\u529b\u9ad8\u5ea6\u878d\u5408\nC. \u4fc3\u4f7f\u4f5c\u6218\u8282\u594f\u660e\u663e\u52a0\u5feb\nD. \u5927\u89c4\u6a21\u6467\u6bc1\u654c\u519b\u76ee\u6807\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5209093345431858, "meta-math/MetaMath-Mistral-7B": 0.6402258488384964, "itpossible/Chinese-Mistral-7B-v0.1": 0.5463490554079229, "HuggingFaceH4/zephyr-7b-beta": 0.5085273820639793, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7804888156519002, "meta-llama/Meta-Llama-3-8B": 0.6237889931643359, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u4e8b\u5206\u7c7b\u4e3b\u8981\u6709\u804c\u4f4d\u5206\u7c7b\u548c\u54c1\u4f4d\u5206\u7c7b\u4e24\u79cd\uff0c\u8fd1\u4ee3\u4eba\u4e8b\u54c1\u4f4d\u5206\u7c7b\u59cb\u521b\u4e8e\nA. \u65e5\u672c\nB. \u6cd5\u56fd\nC. \u7f8e\u56fd\nD. \u82f1\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5927\u529b\u53d1\u5c55\u8282\u80fd\u670d\u52a1\u4ea7\u4e1a\u548c\u73af\u4fdd\u4ea7\u4e1a\uff0c\u5f00\u53d1\u98ce\u80fd\u3001\u592a\u9633\u80fd\u7b49\u6e05\u6d01\u3001\u53ef\u518d\u751f\u80fd\u6e90\uff0c\u5173\u952e\u5728\u4e8e\u56fd\u5bb6\u7ed9\u4e88\u653f\u7b56\u652f\u6301\uff0c\u4f7f\u4f01\u4e1a\u4ea7\u54c1\u5728\u4ef7\u683c\u4e0a\u5904\u4e8e\u4f18\u52bf\u3002\u8fd9\u4e3b\u8981\u8bf4\u660e\uff0c\u4ef7\u683c\u53d8\u52a8\nA. \u5f71\u54cd\u5e02\u573a\u7684\u4f9b\u6c42\u5173\u7cfb\nB. \u6709\u52a9\u4e8e\u751f\u4ea7\u9002\u9500\u5bf9\u8def\u3001\u9ad8\u8d28\u91cf\u4ea7\u54c1\nC. \u8c03\u6574\u4ea7\u54c1\u7ed3\u6784\u548c\u751f\u4ea7\u89c4\u6a21\nD. \u63d0\u9ad8\u52b3\u52a8\u751f\u4ea7\u7387\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u751f\u76ee\u7684\u662f\u4eba\u751f\u5b9e\u8df5\u6d3b\u52a8\u7684\u603b\u76ee\u6807\u3002\u5f53\u4ee3\u5927\u5b66\u751f\u5e94\u5f53\u786e\u7acb\u7684\u79d1\u5b66\u3001\u9ad8\u5c1a\u7684\u4eba\u751f\u76ee\u7684\u662f\nA. \u4e3a\u4e2a\u4eba\u548c\u5168\u5bb6\u6c42\u6e29\u9971\nB. \u4e3a\u4eba\u6c11\u670d\u52a1\nC. \u4e3a\u4e2a\u4eba\u6c42\u6743\u5229\u3001\u6c42\u4eab\u4e50\nD. \u4e3b\u89c2\u4e3a\u81ea\u5df1\uff0c\u5ba2\u89c2\u4e3a\u4ed6\u4eba\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8971991609002063, "meta-math/MetaMath-Mistral-7B": 0.9935713832214186, "itpossible/Chinese-Mistral-7B-v0.1": 0.9331719759372987, "HuggingFaceH4/zephyr-7b-beta": 0.9975657743716294, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8556678519562967, "meta-llama/Meta-Llama-3-8B": 0.9065482422546387, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8069739919065541}}, {"question": "\u5730\u7406\u8001\u5e08\u6559\u5b66\u751f\u8bb0\u5fc6\u201c\u4e5e\u529b\u9a6c\u624e\u7f57\u5c71\u201d\u65f6\uff0c\u4e3a\u65b9\u4fbf\u5b66\u751f\u8bb0\u5fc6\uff0c\u5c06\u4e4b\u620f\u79f0\u4e3a\u201c\u9a91\u7740\u9a6c\u6253\u7740\u9523\u201d\u3002\u8fd9\u79cd\u5b66\u4e60\u7b56\u7565\u5c5e\u4e8e\nA. \u7cbe\u7ec6\u52a0\u5de5\u7b56\u7565\nB. \u5143\u8ba4\u77e5\u7b56\u7565\nC. \u7ec4\u7ec7\u7b56\u7565\nD. \u590d\u8ff0\u7b56\u7565\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9888\u690e\u6700\u4e3b\u8981\u7279\u70b9\u662f\nA. \u6a2a\u7a81\u6709\u808b\u51f9\nB. \u68d8\u7a81\u957f\uff0c\u659c\u5411\u540e\u4e0b\nC. \u6a2a\u7a81\u6709\u5b54\nD. \u690e\u4f53\u8f83\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u7269\u8d28\u4e2d\uff0c\u4e0d\u5c5e\u4e8e\u9ad8\u80fd\u5316\u5408\u7269\u7684\u662f\nA. \u4e59\u9170 CoA\nB. \u4e8c\u78f7\u9178\u80de\u82f7\nC. \u78f7\u9178\u808c\u9178\nD. 2\uff0c3-\u4e8c\u78f7\u9178\u7518\u6cb9\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3452066156620081, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.7339892209110732, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9676\u884c\u77e5\u66fe\u8bf4:\u201c\u6211\u4eec\u6df1\u4fe1\u6559\u80b2\u662f\u56fd\u5bb6\u4e07\u5e74\u6839\u672c\u5927\u8ba1\u3002\u201d\u8fd9\u662f\u56e0\u4e3a\u6559\u80b2\u5177\u6709()\uff0c\u53ea\u8981\u4eba\u7c7b\u793e\u4f1a\u5b58\u5728\uff0c\u5c31\u5b58\u5728\u6559\u80b2\uff0c\u800c\u6559\u80b2\u5728\u4efb\u4f55\u4e00\u4e2a\u65f6\u4ee3\u3001\u4efb\u4f55\u4e00\u4e2a\u793e\u4f1a\u90fd\u6c38\u8fdc\u5177\u6709\u4e0d\u53ef\u66ff\u4ee3\u7684\u4f5c\u7528\u3002\nA. \u5386\u53f2\u6027\nB. \u6c38\u6052\u6027\nC. \u957f\u671f\u6027\nD. \u76f8\u5bf9\u72ec\u7acb\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7795430221934665, "meta-math/MetaMath-Mistral-7B": 0.961049141044361, "itpossible/Chinese-Mistral-7B-v0.1": 0.8211731385461768, "HuggingFaceH4/zephyr-7b-beta": 0.9833761645003948, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8924431745450226, "meta-llama/Meta-Llama-3-8B": 0.8830035955989602, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9461651543580023}}, {"question": "\u4f9d\u82f1\u7f8e\u6cd5\uff0c\u7531\u81f3\u5c11\u4e00\u540d\u666e\u901a\u5408\u4f19\u4eba\uff0c\u81f3\u5c11\u4e00\u540d\u6709\u9650\u5408\u4f19\u4eba\u7ec4\u6210\u7684\u4f01\u4e1a\u79f0\u4e3a\nA. \u7279\u6b8a\u5408\u4f19\nB. \u6709\u9650\u5408\u4f19\nC. \u6c11\u4e8b\u5408\u4f19\nD. \u666e\u901a\u5408\u4f19\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34993229125498293, "meta-math/MetaMath-Mistral-7B": 0.5557113057851092, "itpossible/Chinese-Mistral-7B-v0.1": 0.47995611658487325, "HuggingFaceH4/zephyr-7b-beta": 0.9023076496061931, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3411591735664304, "meta-llama/Meta-Llama-3-8B": 0.42588620393615373, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5fc3\u7406\u5b66\u662f\u7814\u7a76\u4eba\u7684\uff08\uff09\u7684\u79d1\u5b66\u3002\nA. \u5fc3\u7406\u8fc7\u7a0b\nB. \u5fc3\u7406\u73b0\u8c61\nC. \u4e2a\u6027\u5fc3\u7406\nD. \u8ba4\u8bc6\u8fc7\u7a0b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5707146789904386, "meta-math/MetaMath-Mistral-7B": 0.6688886787685878, "itpossible/Chinese-Mistral-7B-v0.1": 0.5694886365608682, "HuggingFaceH4/zephyr-7b-beta": 0.9502887646923851, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7439008530022475, "meta-llama/Meta-Llama-3-8B": 0.6425200055771021, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9856947269855324}}, {"question": "\u78c1\u76d8\u5b58\u50a8\u5668\u7684\u7b49\u5f85\u65f6\u95f4\u662f\u6307\nA. \u78c1\u76d8\u65cb\u8f6c\u2f00\u5468\u6240\u9700\u7684\u65f6\u95f4\nB. \u78c1\u76d8\u65cb\u8f6c\u534a\u5468\u6240\u9700\u7684\u65f6\u95f4\nC. \u78c1\u76d8\u65cb\u8f6c1/3\u5468\u6240\u9700\u7684\u65f6\u95f4\nD. \u78c1\u76d8\u65cb\u8f6c2/3\u5468\u6240\u9700\u7684\u65f6\u95f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.43495988922323, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.579309074806017}}, {"question": "\u7b2c\u4e8c\u6b21\u4e16\u754c\u5927\u6218\u4e2d\uff0c\u7f8e\u56fd\u8d22\u653f\u90e8\u957f\u6469\u6839\u7d22\u66fe\u5411\u53c2\u8bae\u9662\u6307\u51fa\uff0c\u7f8e\u56fd\u8981\u5efa\u7acb\u4e00\u79cd\u4e16\u754c\u4f53\u7cfb\uff0c\u4ee5\u4fbf\u8ba9\u201c\u4f01\u4e1a\u5bb6\u4eec\u53ef\u4ee5\u6309\u7167\u5546\u4e1a\u539f\u5219\u8fdb\u884c\u56fd\u9645\u8d38\u6613\u548c\u56fd\u9645\u6295\u8d44\u201d\u3002\u6218\u4e89\u7ed3\u675f\u540e\uff0c\u8fd9\u4e00\u5efa\u8bae\u4f53\u73b0\u4e3a\nA. \u201c\u51b7\u6218\u201d\u79e9\u5e8f\u7684\u5efa\u7acb\nB. \u300a\u5173\u8d38\u603b\u534f\u5b9a\u300b\u7b7e\u7f72\nC. \u5317\u7f8e\u81ea\u7531\u8d38\u6613\u533a\u5efa\u7acb\nD. \u4e16\u754c\u8d38\u6613\u7ec4\u7ec7\u5efa\u7acb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7119025175010218, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6211\u56fd\u5728\u6279\u51c6\u52a0\u5165\u6d77\u7259\u300a\u57df\u5916\u9001\u8fbe\u516c\u7ea6\u300b\u7684\u51b3\u5b9a\u4e2d\uff0c\u6307\u5b9a\u6709\u6743\u63a5\u53d7\u5916\u56fd\u901a\u8fc7\u9886\u4e8b\u9014\u5f84\u8f6c\u9012\u7684\u6587\u4e66\u7684\u4e2d\u592e\u673a\u5173\u662f\nA. \u5916\u4ea4\u90e8\nB. \u516c\u5b89\u90e8\nC. \u6700\u9ad8\u4eba\u6c11\u6cd5\u9662\nD. \u53f8\u6cd5\u90e8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "Enter\u952e\u7684\u4e2d\u6587\u540d\u79f0\u662f\nA. \u56de\u8f66\u952e\nB. \u63a7\u5236\u952e\nC. \u5927\u5199\u5b57\u6bcd\u9501\u5b9a\u952e\nD. \u4e0a\u6863\u952e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.976724844006219, "meta-math/MetaMath-Mistral-7B": 0.997614661921888, "itpossible/Chinese-Mistral-7B-v0.1": 0.9355278087144558, "HuggingFaceH4/zephyr-7b-beta": 0.9977470403949857, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9919464230927011, "meta-llama/Meta-Llama-3-8B": 0.9023642311478891, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9952648326886138}}, {"question": "\u4ee5\u4e0b\u4e0d\u5c5e\u4e8e\u56fd\u9645\u653f\u6cbb\u683c\u5c40\u7684\u662f\nA. \u7ef4\u4e5f\u7eb3\u4f53\u7cfb\nB. \u5a01\u65af\u7279\u4f10\u5229\u4e9a\u4f53\u7cfb\nC. \u51e1\u5c14\u8d5b\u534e\u76db\u987f\u4f53\u7cfb\nD. \u4e1c\u4e9a\u5c01\u8d21\u4f53\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6507280524106016, "meta-math/MetaMath-Mistral-7B": 0.7614473911399835, "itpossible/Chinese-Mistral-7B-v0.1": 0.5771122464302983, "HuggingFaceH4/zephyr-7b-beta": 0.9948327173082988, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.42693270069524886, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9270328610939784}}, {"question": "\u6c11\u65cf\u95ee\u9898\u4ea7\u751f\u7684\u57fa\u672c\u539f\u56e0\nA. \u6c11\u65cf\u4ea4\u5f80\nB. \u6c11\u65cf\u5dee\u5f02\nC. \u6c11\u65cf\u77db\u76fe\nD. \u6c11\u65cf\u7684\u793e\u4f1a\u539f\u56e0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6536529431343858, "itpossible/Chinese-Mistral-7B-v0.1": 0.5177440647729845, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c11\u65cf\u6559\u80b2\u653f\u7b56\u3001\u6cd5\u5236\u662f\u6c11\u65cf\u6559\u80b2\u53d1\u5c55\u7684\nA. \u5fc5\u7136\u8981\u6c42\u548c\u5fc5\u7136\u7ed3\u679c\nB. \u57fa\u672c\u524d\u63d0\nC. \u5fc5\u7136\u7ed3\u679c\nD. \u5fc5\u7136\u8981\u6c42\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.32368788362516976, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8179755696904735}}, {"question": "\u7531\u4e8e\u751f\u4ea7\u6280\u672f\u6216\u793e\u4f1a\u4f53\u5236\u65b9\u9762\u7684\u53d8\u9769\u800c\u5f15\u8d77\u7684\u89c4\u6a21\u8f83\u5927\u7684\u793e\u4f1a\u6d41\u52a8\uff0c\u88ab\u79f0\u4e3a\nA. \u81ea\u7531\u6d41\u52a8\nB. \u5782\u76f4\u6d41\u52a8\nC. \u7ed3\u6784\u6d41\u52a8\nD. \u7ade\u4e89\u6d41\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.47408280994552626, "meta-math/MetaMath-Mistral-7B": 0.7414013575262903, "itpossible/Chinese-Mistral-7B-v0.1": 0.4302838214483543, "HuggingFaceH4/zephyr-7b-beta": 0.9363911571566507, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5873519879633196, "meta-llama/Meta-Llama-3-8B": 0.5602456663271304, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9373710195206788}}, {"question": "\u201c\u5730\u7403\u6751\u201d\u7684\u9884\u8a00\u662f\u7531\u4e0b\u9762\u54ea\u4f4d\u5b66\u8005\u63d0\u51fa\u6765\u7684\uff1f\nA. \u6885\u7f57\u7ef4\u8328\nB. \u9ea6\u514b\u9c81\u6c49\nC. \u4f0a\u5c3c\u65af\nD. \u65bd\u62c9\u59c6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4619154283166679, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4176007408374005, "meta-llama/Meta-Llama-3-8B": 0.6561505118446185, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6281558611143396}}, {"question": "\u4e0b\u5217\u5173\u8282\u68c0\u67e5\u7ed3\u679c\u4e0e\u75be\u75c5\u7684\u5173\u7cfb\uff0c\u9519\u8bef\u7684\u662f\nA. \u9ea6\u6c0f\u8bd5\u9a8c\uff08+\uff09\uff1a\u534a\u6708\u677f\u635f\u4f24\nB. \u6d6e\u9acc\u8bd5\u9a8c\uff08+\uff09\uff1a\u819d\u5173\u8282\u79ef\u6db2\nC. \u524d\u62bd\u5c49\u8bd5\u9a8c\uff08+\uff09\uff1a\u540e\u4ea4\u53c9\u97e7\u5e26\u65ad\u88c2\nD. \u7814\u78e8\u8bd5\u9a8c\uff08+\uff09\uff1a\u534a\u6708\u677f\u635f\u4f24\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.325455072595945, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.511796853490026}}, {"question": "\u4e0b\u5217\u516c\u5f0f\u4e2d\uff0c\u65e2\u9002\u7528\u4e8e\u70b9\u7535\u8377\u4ea7\u751f\u7684\u9759\u7535\u573a\uff0c\u4e5f\u9002\u7528\u4e8e\u5300\u5f3a\u7535\u573a\u7684\u6709\uff1a\uff081\uff09\u573a\u5f3aE=F/q\uff082\uff09\u573a\u5f3aE=U/d\uff083\uff09\u573a\u5f3a$E=kQ/r^{2}$\uff084\uff09\u7535\u573a\u529b\u505a\u529fW=Uq\nA. \uff081\uff09\uff083\uff09\nB. \uff082\uff09\uff084\uff09\nC. \uff081\uff09\uff084\uff09\nD. \uff082\uff09\uff083\uff09\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3157387487413704, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5743337862509161, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.39546991145364585, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "BS7799\u6807\u51c6\u662f\u82f1\u56fd\u6807\u51c6\u534f\u4f1a\u5236\u5b9a\u7684\u4fe1\u606f\u5b89\u5168\u7ba1\u7406\u4f53\u7cfb\u6807\u51c6\uff0c\u5b83\u5305\u62ec\u4e24\u4e2a\u90e8\u5206\uff1a\u300a\u4fe1\u606f\u5b89\u5168\u7ba1\u7406\u5b9e\u65bd\u6307\u5357\u300b\u548c\u300a\u4fe1\u606f\u5b89\u5168\u7ba1\u7406\u4f53\u7cfb\u89c4\u8303\u548c\u5e94\u7528\u6307\u5357\u300b\u3002\u4f9d\u636e\u8be5\u6807\u51c6\u53ef\u4ee5\u7ec4\u7ec7\u5efa\u7acb\u3001\u5b9e\u65bd\u4e0e\u4fdd\u6301\u4fe1\u606f\u5b89\u5168\u7ba1\u7406\u4f53\u7cfb\uff0c\u4f46\u4e0d\u80fd\u5b9e\u73b0\nA. \u5f3a\u5316\u5458\u5de5\u7684\u4fe1\u606f\u5b89\u5168\u610f\u8bc6\uff0c\u89c4\u8303\u7ec4\u7ec7\u4fe1\u606f\u5b89\u5168\u884c\u4e3a\nB. \u5bf9\u7ec4\u7ec7\u5185\u5173\u952e\u4fe1\u606f\u8d44\u4ea7\u7684\u5b89\u5168\u6001\u52bf\u8fdb\u884c\u52a8\u6001\u76d1\u6d4b\nC. \u4fc3\u4f7f\u7ba1\u7406\u5c42\u575a\u6301\u8d2f\u5f7b\u4fe1\u606f\u5b89\u5168\u4fdd\u969c\u4f53\u7cfb\nD. \u901a\u8fc7\u4f53\u7cfb\u8ba4\u8bc1\u5c31\u8868\u660e\u4f53\u7cfb\u7b26\u5408\u6807\u51c6\uff0c\u8bc1\u660e\u7ec4\u7ec7\u6709\u80fd\u529b\u4fdd\u969c\u91cd\u8981\u4fe1\u606f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6c11\u6cd5\u6cd5\u7cfb\u7684\u53d1\u5c55\u662f\u4ee5\nA. \u7f57\u9a6c\u6cd5\u4e3a\u57fa\u7840\nB. \u5224\u4f8b\u6cd5\u4e3a\u57fa\u7840\nC. \u666e\u901a\u6cd5\u4e3a\u57fa\u7840\nD. \u8861\u5e73\u6cd5\u4e3a\u57fa\u7840\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7182768357818875, "HuggingFaceH4/zephyr-7b-beta": 0.69068275569364, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.665561198809431, "meta-llama/Meta-Llama-3-8B": 0.7782784371433874, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9531582110859453}}, {"question": "\u4e3b\u53d8\u51b7\u5907\u7528\u6307\nA. \u662f\u6307\u53d8\u538b\u5668\u4e00\u4fa7\u65ad\u8def\u5668\u5747\u62c9\u5f00\nB. \u662f\u6307\u53d8\u538b\u5668\u4e00\u4fa7\u5f00\u5173\u53ca\u5200\u95f8\u5747\u62c9\u5f00\nC. \u662f\u6307\u53d8\u538b\u5668\u5404\u4fa7\u5f00\u5173\u53ca\u5200\u95f8\u5747\u62c9\u5f00\nD. \u662f\u6307\u53d8\u538b\u5668\u5404\u4fa7\u65ad\u8def\u5668\u5747\u62c9\u5f00\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4066251921577214, "meta-math/MetaMath-Mistral-7B": 0.5683674439666834, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.5359909305602018, "meta-llama/Meta-Llama-3-8B": 0.47776356401308373, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5149178195362778}}, {"question": "\u6cd5\u5bf9\u751f\u4ea7\u529b\u7684\u4f5c\u7528\u4e00\u822c\u8981\u901a\u8fc7\nA. \u79d1\u5b66\u6280\u672f\u7684\u4e2d\u4ecb\nB. \u751f\u4ea7\u5173\u7cfb\u7684\u4e2d\u4ecb\nC. \u793e\u4f1a\u5173\u7cfb\u7684\u4e2d\u4ecb\nD. \u7ecf\u6d4e\u57fa\u7840\u7684\u4e2d\u4ecb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.4516954533836804, "itpossible/Chinese-Mistral-7B-v0.1": 0.41172531682861635, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.38822592917159904, "meta-llama/Meta-Llama-3-8B": 0.3993599025515596, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u4e0b\u65e0\u529f\u8865\u507f\u529e\u6cd5\u54ea\u4e00\u79cd\u662f\u4e0d\u80fd\u91c7\u7528\u7684\nA. \u53d8\u7535\u7ad9\u6bcd\u7ebf\u4e0a\u88c5\u8bbe\u7535\u5bb9\u5668\nB. \u53d1\u7535\u673a\u7aef\u88c5\u8bbe\u7535\u5bb9\u5668\nC. \u914d\u7535\u7ebf\u8def\u4e0a\u88c5\u8bbe\u5e76\u8054\u7535\u5bb9\u5668\nD. \u7528\u6237\u5904\u88c5\u8bbe\u5e76\u8054\u7535\u5bb9\u5668\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5567495501650369, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.32479161563275305, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6cd5\u5f8b\u5173\u7cfb\u7684\u610f\u5fd7\u6027\u7684\u8868\u8ff0\uff0c\u4e0d\u6b63\u786e\u7684\u6709\nA. \u56fd\u5bb6\u610f\u5fd7\u5bf9\u4e8e\u6cd5\u5f8b\u5173\u7cfb\u7684\u4ea7\u751f\u548c\u5b9e\u73b0\u8d77\u7740\u4e3b\u5bfc\u4f5c\u7528\uff1b\u6cd5\u5f8b\u5173\u7cfb\u53c2\u52a0\u8005\u7684\u610f\u5fd7\u5bf9\u4e8e\u6cd5\u5f8b\u89c4\u8303\u4e2d\u6240\u4f53\u73b0\u7684\u56fd\u5bb6\u610f\u5fd7\u7684\u5b9e\u73b0\u53c8\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\nB. \u6cd5\u5f8b\u5173\u7cfb\u867d\u7136\u5177\u6709\u610f\u5fd7\u6027\uff0c\u4f46\u4efb\u4f55\u6cd5\u5f8b\u5173\u7cfb\u90fd\u6839\u6e90\u4e8e\u4e00\u5b9a\u7684\u7ecf\u6d4e\u5173\u7cfb\uff0c\u53cd\u6620\u4e00\u5b9a\u7684\u7ecf\u6d4e\u5173\u7cfb\u7684\u8981\u6c42\nC. \u4efb\u4f55\u6cd5\u5f8b\u5173\u7cfb\u7684\u5efa\u7acb\u90fd\u65e2\u8981\u4f53\u73b0\u56fd\u5bb6\u610f\u5fd7\uff0c\u4e5f\u8981\u4f53\u73b0\u6cd5\u5f8b\u5173\u7cfb\u53c2\u52a0\u8005\u7684\u610f\u5fd7\nD. \u6cd5\u5f8b\u89c4\u8303\u5177\u6709\u56fd\u5bb6\u610f\u5fd7\u6027\uff0c\u6cd5\u5f8b\u5173\u7cfb\u6839\u636e\u6cd5\u5f8b\u89c4\u8303\u800c\u5efa\u7acb\uff0c\u56e0\u6b64\u4e5f\u5c31\u662f\u6839\u636e\u56fd\u5bb6\u610f\u5fd7\u800c\u5efa\u7acb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53ef\u4ee5\u5c06\u4e24\u4e2a\u6b63\u5e38\u8fde\u9501\u7fa4\u6539\u7ec4\u4e3a\u4e24\u4e2a\u65b0\u7684\u8fde\u9501\u7fa4\u7684\u7ed3\u6784\u53d8\u5f02\u662f\nA. \u91cd\u590d\nB. \u6613\u4f4d\nC. \u7f3a\u5931\nD. \u5012\u4f4d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4430448139737191, "meta-math/MetaMath-Mistral-7B": 0.9012559183592642, "itpossible/Chinese-Mistral-7B-v0.1": 0.4839879942290105, "HuggingFaceH4/zephyr-7b-beta": 0.9377440919172013, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7113775552346839, "meta-llama/Meta-Llama-3-8B": 0.5636743584840147, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9616169507585018}}, {"question": "\u5b66\u4e60\u52a8\u673a\u7f3a\u4e4f\u7684\u4e3b\u8981\u8868\u73b0\u4e0d\u5305\u62ec\u4ee5\u4e0b\nA. \u6ce8\u610f\u529b\u4e0b\u964d\nB. \u7126\u8651\u8fc7\u4f4e\nC. \u5c3d\u529b\u9003\u907f\u5b66\u4e60\nD. \u5bb9\u6613\u81ea\u8d23\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3774280162150933, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.353703093539192, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4ee5\u5f17\u6d1b\u4f0a\u5fb7\u5fc3\u7406\u5206\u6790\u7406\u8bba\u4e3a\u4f9d\u636e\u7684\u4eba\u683c\u6d4b\u9a8c\nA. \u7f57\u590f\u514b\u58a8\u6e0d\u6d4b\u9a8c\nB. \u7231\u5fb7\u534e\u4e2a\u4eba\u5174\u8da3\u91cf\u8868\u6d4b\u9a8c\nC. \u9752\u5e74\u6027\u683c\u95ee\u5377\u6d4b\u9a8c\nD. \u660e\u5c3c\u901f\u5ea6\u4eba\u683c\u6d4b\u9a8c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3702580020579643, "meta-math/MetaMath-Mistral-7B": 0.5190768443635878, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7937704461344908, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6bb7\u5546\u7532\u9aa8\u535c\u8f9e\uff1a\u201c\u7678\u536f\u535c\uff0c\u4eca\u65e5\u96e8\u3002\u5176\u81ea\u897f\u6765\u96e8?\u5176\u81ea\u4e1c\u6765\u96e8?\u5176\u81ea\u5317\u6765\u96e8?\u5176\u81ea\u5357\u6765\u96e8?\u201d\u5bf9\u6b64\u7406\u89e3\u4e0d\u6b63\u786e\u7684\u662f\nA. \u535c\u8f9e\u53cd\u6620\u51fa\u53e4\u4ee3\u751f\u4ea7\u529b\u4f4e\u4e0b\nB. \u535c\u8f9e\u4e2d\u6587\u5b57\u4e0e\u73b0\u4ee3\u6c49\u5b57\u975e\u5e38\u63a5\u8fd1\nC. \u8bf4\u660e\u53e4\u4ee3\u4eba\u4eec\u5bf9\u81ea\u7136\u7684\u4f9d\u8d56\u6027\u5f3a\nD. \u535c\u8f9e\u53ef\u88ab\u89c6\u4e3a\u539f\u59cb\u5f62\u6001\u7684\u519c\u4e8b\u8bd7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.25780995786150784, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6474756970361458, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.819087900867317, "meta-llama/Meta-Llama-3-8B": 0.3534716292209113, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u793e\u4f1a\u4e3b\u4e49\u5b9e\u73b0\u7531\u7a7a\u60f3\u5230\u79d1\u5b66\u53d1\u5c55\u7684\u6807\u5fd7\u662f\nA. \u7a7a\u60f3\u793e\u4f1a\u4e3b\u4e49\u7406\u60f3\u7684\u7834\u706d\nB. \u201c\u5171\u4ea7\u4e3b\u4e49\u8005\u540c\u76df\u201d\u7684\u5efa\u7acb\nC. \u300a \u5171\u4ea7\u515a\u5ba3\u8a00 \u300b \u7684\u53d1\u8868\nD. \u65e0\u4ea7\u9636\u7ea7\u9769\u547d\u7684\u80dc\u5229\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.37449772085713634, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3938024279923704, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.902473592481629}}, {"question": "11\u67081\u65e5\u4e2d\u56fd\u5171\u4ea7\u515a\u7b2c\u5341\u516b\u6b21\u5168\u56fd\u4ee3\u8868\u5927\u4f1a\u65b0\u95fb\u4e2d\u5fc3\u7f51\u7ad9\u6b63\u5f0f\u5f00\u901a\u3002\u516c\u6c11\u53ef\u4ee5\u901a\u8fc7\u7f51\u7edc\u8fbe\u81ea\u5df1\u7684\u610f\u613f\uff0c\u79ef\u6781\u53c2\u4e0e\u5230\u5341\u516b\u5927\u7684\u6d3b\u52a8\u4e2d\uff0c\u4e2d\u5fc3\u7f51\u7ad9\u5c06\u4e3a\u7f51\u6c11\u63d0\u4f9b\u5404\u79cd\u670d\u52a1\u3002\u7f51\u7ad9\u7684\u5f00\u901aa\u65b9\u4fbf\u4e86\u516c\u6c11\u76f4\u63a5\u7ba1\u7406\u56fd\u5bb6\u4e8b\u52a1 b\u52a0\u5f3a\u4e86\u515a\u540c\u4eba\u6c11\u7fa4\u4f17\u7684\u8054\u7cfbc\u62d3\u5bbd\u4e86\u516c\u6c11\u53c2\u4e0e\u653f\u6cbb\u751f\u6d3b\u7684\u6e20\u9053 d\u6269\u5927\u4e86\u516c\u6c11\u7684\u653f\u6cbb\u6743\u5229\nA. ab\nB. cd\nC. bc\nD. ad\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3248694388439252, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\uff08\uff09%=\u53d1\u82bd\u521d\u671f\u6b63\u5e38\u53d1\u82bd\u7c92\u6570/\u4f9b\u68c0\u603b\u7c92\u6570\u00d7100\nA. \u53d1\u82bd\u52bf\nB. \u53d1\u82bd\u7387\nC. \u79cd\u5b50\u51c0\u5ea6\nD. \u54c1\u79cd\u7eaf\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u808c\u8089\u5728\u6536\u7f29\u4ea7\u751f\u529b\u7684\u540c\u65f6\u88ab\u62c9\u957f\u7684\u6536\u7f29\u79f0\u4e3a\nA. \u7b49\u52a8\u6536\u7f29\nB. \u7b49\u957f\u6536\u7f29\nC. \u79bb\u5fc3\u6536\u7f29\nD. \u5411\u5fc3\u6536\u7f29\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u300a\u5a5a\u59fb\u6cd5\u300b\u7684\u6709\u5173\u89c4\u5b9a\uff0c\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u6784\u6210\u6cd5\u9662\u5224\u51b3\u79bb\u5a5a\u7684\u6cd5\u5b9a\u4e8b\u7531\u4e0d\u5305\u62ec\nA. \u592b\u59bb\u611f\u60c5\u4e0d\u548c\u800c\u5206\u5c45\u7684\nB. \u6709\u8d4c\u535a\u3001\u5438\u6bd2\u7b49\u6076\u4e60\u5c61\u6559\u4e0d\u6539\u7684\nC. \u91cd\u5a5a\u7684\nD. \u5b9e\u65bd\u5bb6\u5ead\u66b4\u529b\u7684\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.44653092158389224, "itpossible/Chinese-Mistral-7B-v0.1": 0.3017444864199176, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u4e0d\u662f\u6267\u884c\u793e\u4f1a\u516c\u5171\u4e8b\u52a1\u4f5c\u7528\u7684\u662f\nA. \u6709\u5173\u6280\u672f\u89c4\u8303\u7684\u6cd5\u5f8b\nB. \u6709\u5173\u751f\u4ea7\u529b\u548c\u79d1\u5b66\u6280\u672f\u7684\u6cd5\u5f8b\nC. \u6709\u5173\u4e00\u822c\u6587\u5316\u4e8b\u52a1\u7684\u6cd5\u5f8b\nD. \u6709\u5173\u72af\u7f6a\u548c\u5211\u7f5a\u7684\u6cd5\u5f8b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2986334267609957, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3926712568084176}}, {"question": "\u4e0b\u5217\u5173\u4e8e\u6559\u80b2\u7684\u793e\u4f1a\u5c5e\u6027\uff0c\u8868\u8ff0\u6b63\u786e\u7684\u662f()\nA. \u6559\u80b2\u53d1\u5c55\u4f1a\u53d7\u5230\u653f\u6cbb\u7ecf\u6d4e\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u6559\u80b2\u4e0e\u653f\u6cbb\u7ecf\u6d4e\u540c\u6b65\u53d1\u5c55\nB. \u6559\u80b2\u662f\u57f9\u517b\u548c\u53d1\u5c55\u4eba\u7684\u6d3b\u52a8\uff0c\u6240\u4ee5\u5728\u4e00\u5b9a\u610f\u4e49\u4e0a\u5177\u6709\u751f\u4ea7\u6027\nC. \u201c\u5341\u5e74\u6811\u6728\uff0c\u767e\u5e74\u6811\u4eba\u201d\u8868\u660e\u6559\u80b2\u5177\u6709\u5386\u53f2\u6027\nD. \u6559\u80b2\u65e2\u53d7\u5f53\u65f6\u751f\u4ea7\u529b\u7684\u5236\u7ea6\uff0c\u540c\u65f6\u4e5f\u53d7\u751f\u4ea7\u5173\u7cfb\u7684\u5236\u7ea6\uff0c\u4f53\u73b0\u4e86\u6559\u80b2\u5177\u6709\u7ee7\u627f\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f9d1980\u5e74\u300a\u8054\u5408\u56fd\u56fd\u9645\u8d27\u7269\u9500\u552e\u5408\u540c\u516c\u7ea6\u300b\u7684\u89c4\u5b9a\uff0c\u5b83\u6240\u8c03\u6574\u7684\u201c\u56fd\u9645\u201d\u8d27\u7269\u9500\u552e\u5408\u540c\u7684\u5224\u5b9a\u6807\u51c6\u662f\nA. \u5f53\u4e8b\u4eba\u8d27\u7269\u6216\u8d27\u6b3e\u5f97\u5728\u4e0d\u540c\u56fd\u5bb6\u4ea4\u4ed8\nB. \u5f53\u4e8b\u4eba\u5728\u4e0d\u540c\u56fd\u5bb6\u6709\u4f4f\u6240\nC. \u5f53\u4e8b\u4eba\u5177\u6709\u4e0d\u540c\u56fd\u7c4d\nD. \u5f53\u4e8b\u4eba\u7684\u8425\u4e1a\u6240\u5728\u4e0d\u540c\u56fd\u5bb6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4efb\u4f55\u4eba\u90fd\u8981\u9762\u5bf9\u5e76\u5904\u7406\u4eba\u4e0e\u81ea\u7136\u7684\u5173\u7cfb\u95ee\u9898\u3002\u4e0b\u5217\u9009\u9879\u4e2d\uff0c\u771f\u6b63\u4f53\u73b0\u4e86\u4eba\u4e0e\u81ea\u7136\u5173\u7cfb\u7684\u57fa\u7840\u662f\nA. \u4eba\u6d88\u6781\u5730\u4f9d\u8d56\u81ea\u7136\u751f\u6d3b\nB. \u4eba\u72ec\u7acb\u4e8e\u81ea\u7136\u800c\u5b58\u5728\nC. \u4eba\u9ad8\u4e8e\u81ea\u7136\nD. \u4eba\u6839\u636e\u81ea\u5df1\u7684\u9700\u8981\u5229\u7528\u548c\u6539\u9020\u81ea\u7136\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9763802987992102, "meta-math/MetaMath-Mistral-7B": 0.9974928017418888, "itpossible/Chinese-Mistral-7B-v0.1": 0.9901304428191356, "HuggingFaceH4/zephyr-7b-beta": 0.9996029624955008, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9923173498973314, "meta-llama/Meta-Llama-3-8B": 0.9528483975187683, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9878738910588971}}, {"question": "\u5173\u4e8e\u53ef\u4fe1\u533a\u95f4\u548c\u5047\u8bbe\u68c0\u9a8c\uff0c\u4e0d\u6b63\u786e\u7684\u662f\nA. \u4ee5\u4e0a\u7ed3\u8bba\u5747\u4e0d\u5bf9\nB. \u53ef\u4fe1\u533a\u95f4\u53ef\u56de\u7b54\u5047\u8bbe\u68c0\u9a8c\u7684\u95ee\u9898\nC. \u53ef\u4fe1\u533a\u95f4\u7528\u4e8e\u8bf4\u660e\u91cf\u7684\u5927\u5c0f\uff0c\u5047\u8bbe\u68c0\u9a8c\u7528\u4e8e\u63a8\u65ad\u8d28\u7684\u4e0d\u540c\nD. \u53ef\u4fe1\u533a\u95f4\u6bd4\u5047\u8bbe\u68c0\u9a8c\u53ef\u63d0\u4f9b\u66f4\u591a\u7684\u4fe1\u606f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.6615534004881104, "itpossible/Chinese-Mistral-7B-v0.1": 0.4489842344514252, "HuggingFaceH4/zephyr-7b-beta": 0.8794497778247697, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4242631597241338, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.31828968051251794}}, {"question": "\u5546\u54c1\u5185\u5728\u7684\u4f7f\u7528\u4ef7\u503c\u4e0e\u4ef7\u503c\u7684\u77db\u76fe\uff0c\u5176\u5b8c\u5907\u7684\u5916\u5728\u8868\u73b0\u662f\nA. \u8d44\u672c\u4e0e\u96c7\u4f63\u52b3\u52a8\u4e4b\u95f4\u7684\u5bf9\u7acb\nB. \u5546\u54c1\u4e0e\u8d27\u5e01\u4e4b\u95f4\u7684\u5bf9\u7acb\nC. \u79c1\u4eba\u52b3\u52a8\u4e0e\u793e\u4f1a\u52b3\u52a8\u4e4b\u95f4\u7684\u5bf9\u7acb \nD. \u5546\u54c1\u4e0e\u5546\u54c1\u4e4b\u95f4\u7684\u5bf9\u7acb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4097932749405994, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5549235873065769}}, {"question": "1NADH+H+\u7ecfNADH\u6c27\u5316\u547c\u5438\u94fe\u4f20\u9012\uff0c\u6700\u540e\u4ea4\u7ed90.5O2\u751f\u6210\u6c34\uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\u751f\u6210\u51e0\u5206\u5b50ATP\nA. 4\nB. 3\nC. 2.5\nD. 1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.49574336161932187}}, {"question": "\u5c5e\u4e8e\u81c2\u540e\u7fa4\u808c\u7684\u662f\nA. \u80b1\u4e09\u5934\u808c\nB. \u80b1\u4e8c\u5934\u808c\nC. \u5599\u80b1\u808c\nD. \u4e09\u89d2\u808c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.34478590299221606, "meta-llama/Meta-Llama-3-8B": 0.2828645123477969, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.43773635315522375}}, {"question": "\u5c0f\u4e1c\u6bcf\u6b21\u9501\u95e8\u79bb\u5bb6\u540e\uff0c\u660e\u77e5\u5df2\u9501\u8fc7\u95e8\uff0c\u4f46\u603b\u662f\u7591\u5fc3\u95e8\u6ca1\u6709\u9501\u4e0a\uff0c\u975e\u8981\u8fd4\u56de\u68c0\u67e5\u624d\u5b89\u5fc3\u3002\u4ed6\u7684\u8fd9\u79cd\u8868\u73b0\u5c5e\u4e8e\nA. \u5f3a\u8feb\u6050\u60e7\nB. \u5f3a\u8feb\u884c\u4e3a\nC. \u5f3a\u8feb\u7126\u8651\nD. \u5f3a\u8feb\u89c2\u5ff5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.5899676090530538, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.4668385445259093, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8938882546067071}}, {"question": "\u5173\u4e8e\u6cd5\u6cbb\u4e0e\u5fb7\u6cbb\u7684\u8054\u7cfb\u4e0e\u533a\u522b\uff0c\u4e0b\u5217\u8bf4\u6cd5\u4e0d\u6b63\u786e\u7684\u6709\nA. \u73b0\u4ee3\u793e\u4f1a\u7684\u6cd5\u5f8b\u4e0e\u9053\u5fb7\u4ecd\u7136\u6709\u7740\u4e0d\u53ef\u5206\u5272\u7684\u8054\u7cfb\uff0c\u6cbb\u7406\u56fd\u5bb6\u65e2\u8981\u4f9d\u9760\u6cd5\u5f8b\uff0c\u4e5f\u8981\u4f9d\u9760\u9053\u5fb7\nB. \u6cd5\u6cbb\u5f3a\u8c03\u5c06\u793e\u4f1a\u5173\u7cfb\u7eb3\u5165\u6cd5\u5f8b\u7684\u8f68\u9053\uff0c\u7528\u5e26\u6709\u6743\u5a01\u6027\u3001\u5f3a\u5236\u6027\u7684\u6cd5\u5f8b\u89c4\u8303\u6216\u4e25\u5211\u5cfb\u6cd5\u6cbb\u7406\u793e\u4f1a\uff1b\u5fb7\u6cbb\u7684\u4e2d\u5fc3\u542b\u4e49\u662f\u6307\u5e94\u5f53\u901a\u8fc7\u63d0\u9ad8\u7edf\u6cbb\u8005\u7684\u9053\u5fb7\u6c34\u5e73\u6765\u6cbb\u7406\u56fd\u5bb6\nC. \u6cd5\u6cbb\u4e0e\u4eba\u6cbb\u76f8\u5bf9\u7acb\uff0c\u5fb7\u6cbb\u4e0d\u5fc5\u7136\u548c\u4eba\u6cbb\u76f8\u51b2\u7a81\nD. \u5728\u5f53\u4ee3\u4e2d\u56fd\uff0c\u6cd5\u6cbb\u4e0e\u5fb7\u6cbb\u6709\u7740\u76ee\u6807\u7684\u4e00\u81f4\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6402258268700489, "HuggingFaceH4/zephyr-7b-beta": 0.9984081832396543, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.650269419697385, "meta-llama/Meta-Llama-3-8B": 0.5892061768360743, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u4e09\u56fd\u6f14\u4e49\u300b\u4e2d\u6709\u5173\u7fbd\u201c\u5355\u5200\u8d74\u4f1a\u201d\u7684\u6545\u4e8b\uff0c\u5386\u53f2\u4e0a\u4e5f\u786e\u6709\u201c\u5355\u5200\u8d74\u4f1a\u201d\u4e00\u4e8b\uff0c\u8d74\u4f1a\u7684\u662f\nA. \u5f20\u98de\nB. \u8d75\u4e91\nC. \u9c81\u8083\nD. \u5468\u745c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3926712521609814, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5929\u5143\u6027\u6784\u9020\u7684\u4e24\u6027\u751f\u7406\u5b66\u5dee\u5f02\u6027\u6784\u67b6\u51b3\u5b9a\u4e86\u7537\u5973\u8eab\u5fc3\u53d1\u5c55\u5b58\u5728\u7740\u5ba2\u89c2\u7684\nA. \u5dee\u5f02\u6027\nB. \u540c\u5f02\u6027\nC. \u540c\u4e00\u6027\nD. \u7edf\u4e00\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9246830472060036, "meta-math/MetaMath-Mistral-7B": 0.993130523509572, "itpossible/Chinese-Mistral-7B-v0.1": 0.7869431304440002, "HuggingFaceH4/zephyr-7b-beta": 0.9961540559130883, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9692968816190802, "meta-llama/Meta-Llama-3-8B": 0.9245674981615231, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9913555808978564}}, {"question": "\u5e72\u54b3\u5c11\u75f0\uff0c\u6216\u75f0\u6db2\u80f6\u7c98\u96be\u54af\uff0c\u591a\u56e0\u611f\u53d7\u54ea\u79cd\u75c5\u90aa?\nA. \u5bd2\u90aa\nB. \u71e5\u90aa\nC. \u98ce\u90aa\nD. \u6691\u90aa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6072891619497223, "meta-math/MetaMath-Mistral-7B": 0.903641508177973, "itpossible/Chinese-Mistral-7B-v0.1": 0.8096155547732278, "HuggingFaceH4/zephyr-7b-beta": 0.9245185911569056, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6695349142389789, "meta-llama/Meta-Llama-3-8B": 0.6804111551132461, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6709\u5b8c\u5168\u7ade\u4e89\u5916\u90e8\u5e02\u573a\u7684\u4e2d\u95f4\u4ea7\u54c1\u7684\u8f6c\u79fb\u4ef7\u683c\u5e94\u5f53\nA. \u7b49\u4e8e\u8fb9\u9645\u6210\u672c\nB. \u7b49\u4e8e\u5e73\u5747\u6210\u672c\nC. \u7b49\u4e8e\u5e02\u573a\u4ef7\u683c\nD. \u7b49\u4e8e\u5e73\u5747\u6210\u672c\u52a0\u76ee\u6807\u5229\u6da6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u897f\u6e56\u7684\u767d\u5824\u7684\u5f97\u540d\u4e0e\u54ea\u4f4d\u8bd7\u4eba\u6709\u5173\nA. \u767d\u5c45\u6613\nB. \u674e\u5546\u9690\nC. \u675c\u752b\nD. \u674e\u767d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u675c\u9c81\u95e8\u5728\u6566\u4fc3\u56fd\u4f1a\u5c3d\u65e9\u901a\u8fc7\u9a6c\u6b47\u5c14\u8ba1\u5212\u7684\u7279\u522b\u54a8\u6587\u4e2d\u8bf4\uff1a\u201c\u8981\u4f7f\u7ecf\u6d4e\u590d\u5174\u83b7\u5f97\u6210\u529f\uff0c\u5fc5\u987b\u91c7\u53d6\u67d0\u79cd\u5bf9\u4ed8\u5185\u90e8\u548c\u5916\u90e8\u4fb5\u7565\u7684\u4fdd\u536b\u6027\u63aa\u65bd\u3002\u201d\u6b64\u540e\u91c7\u53d6\u7684\u201c\u4fdd\u536b\u6027\u63aa\u65bd\u201d\u662f\u5efa\u7acb\nA. \u5317\u5927\u897f\u6d0b\u516c\u7ea6\u7ec4\u7ec7\nB. \u534e\u6c99\u6761\u7ea6\u7ec4\u7ec7\nC. \u6b27\u6d32\u5171\u540c\u4f53\nD. \u8054\u5408\u56fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.6019793020405412, "meta-math/MetaMath-Mistral-7B": 0.6776241950362899, "itpossible/Chinese-Mistral-7B-v0.1": 0.6561504782250622, "HuggingFaceH4/zephyr-7b-beta": 0.9237003105369651, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9346854688963734, "meta-llama/Meta-Llama-3-8B": 0.9288160545773131, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9880025996368299}}, {"question": "\u4eba\u7c7b\u5bf9\u6027\u6b32\u4ea7\u751f\u4f9d\u8d56\u7684\u4e3b\u8981\u56e0\u7d20\u4e0d\u5305\u62ec\nA. \u73af\u5883\u56e0\u7d20\nB. \u9057\u4f20\u56e0\u7d20\nC. \u5fc3\u7406\u56e0\u7d20\nD. \u751f\u7406\u56e0\u7d20\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7406\u60f3\u7535\u538b\u6e90\u548c\u7406\u60f3\u7535\u6d41\u6e90\u95f4\nA. \u6709\u6761\u4ef6\u4e0b\u7684\u7b49\u6548\u5173\u7cfb\nB. \u6709\u7b49\u6548\u53d8\u6362\u5173\u7cfb\nC. \u4e0d\u80fd\u786e\u5b9a\u6709\u6ca1\u6709\u7b49\u6548\u5173\u7cfb\nD. \u6ca1\u6709\u7b49\u6548\u53d8\u6362\u5173\u7cfb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "20\u4e16\u7eaa30\u5e74\u4ee3\u4e2d\u671f\uff0c\u300a\u65b0\u4e2d\u534e\u300b\u8f7d\u6587\uff1a\u201c\u73b0\u5728\u4f60\u968f\u4fbf\u62c9\u4f4f\u4e00\u4e2a\u7a0d\u7a0d\u7559\u5fc3\u4e2d\u56fd\u7ecf\u6d4e\u95ee\u9898\u7684\u4eba\uff0c\u95ee\u4ed6\u4e2d\u56fd\u7ecf\u6d4e\u6027\u8d28\u5982\u4f55\uff0c\u4ed6\u5c31\u6beb\u4e0d\u72b9\u8c6b\u5730\u7b54\u590d\u4f60\uff1a\u4e2d\u56fd\u7ecf\u6d4e\u662f\u534a\u6b96\u6c11\u5730\u6027\u534a\u5c01\u5efa\u6027\u7ecf\u6d4e\u3002\u201d\u8fd9\u53ef\u4ee5\u7528\u6765\u8bf4\u660e\u5f53\u65f6\nA. \u7ecf\u6d4e\u7406\u8bba\u95ee\u9898\u5f15\u8d77\u6c11\u4f17\u7684\u666e\u904d\u5173\u6ce8\nB. \u77e5\u8bc6\u754c\u5bf9\u4e2d\u56fd\u793e\u4f1a\u6027\u8d28\u7684\u8ba4\u8bc6\u76f8\u540c\nC. \u9a6c\u514b\u601d\u4e3b\u4e49\u601d\u60f3\u65b9\u6cd5\u5f97\u5230\u4f20\u64ad\nD. \u5b98\u50da\u8d44\u672c\u4e3b\u4e49\u5728\u4e2d\u56fd\u8fc5\u901f\u81a8\u80c0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.35898421896516036, "meta-math/MetaMath-Mistral-7B": 0.3824574552031223, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.8298908195101717, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.6565702804998903, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8924846026707615}}, {"question": "\u4fe1\u606f\u5b89\u5168\u98ce\u9669\u7f3a\u53e3\u662f\u6307\nA. \u8ba1\u7b97\u673a\u7f51\u7edc\u8fd0\u884c\uff0c\u7ef4\u62a4\u7684\u6f0f\u6d1e\nB. IT\u7684\u53d1\u5c55\u4e0e\u5b89\u5168\u6295\u5165\uff0c\u5b89\u5168\u610f\u8bc6\u548c\u5b89\u5168\u624b\u6bb5\u7684\u4e0d\u5e73\u8861\nC. \u4fe1\u606f\u5316\u4e2d\uff0c\u4fe1\u606f\u4e0d\u8db3\u4ea7\u751f\u7684\u6f0f\u6d1e\nD. \u8ba1\u7b97\u4e2d\u5fc3\u7684\u706b\u707e\u9690\u60a3\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.5285251892613986, "itpossible/Chinese-Mistral-7B-v0.1": 0.6653878982314144, "HuggingFaceH4/zephyr-7b-beta": 0.7000695450747585, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7444201919383618, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.977769019282313}}, {"question": "\u751f\u7269\u6218\u5242\u7684\u65bd\u653e\u65b9\u5f0f\u662f\nA. \u98de\u673a\u6295\u70b8\u5f39\u3001\u98de\u673a\u91ca\u653e\u7ec6\u83cc\u6218\u5242\u6c14\u6eb6\u80f6\u3001\u654c\u7279\u6295\u653e\u7ec6\u83cc\nB. \u706b\u70ae\u53d1\u5c04\u3001\u7a7a\u4e2d\u6295\u63b7\u3001\u98de\u673a\u6295\u70b8\u5f39\nC. \u706b\u70ae\u53d1\u5c04\u3001\u7a7a\u4e2d\u6295\u63b7\u3001\u98de\u673a\u6295\u70b8\u5f39\u3001\u98de\u673a\u91ca\u653e\u7ec6\u83cc\u6218\u5242\u6c14\u6eb6\u80f6\u3001\u654c\u7279\u6295\u653e\u7ec6\u83cc\nD. \u98df\u7269\u6295\u653e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5383481967048824, "meta-math/MetaMath-Mistral-7B": 0.4863146217654378, "itpossible/Chinese-Mistral-7B-v0.1": 0.554505314047859, "HuggingFaceH4/zephyr-7b-beta": 0.8366905925728241, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7650549865527577, "meta-llama/Meta-Llama-3-8B": 0.5572604543557039, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7596841105624493}}, {"question": "\u8001\u738b\u5f53\u4e865\u5e74\u7684\u603b\u7ecf\u7406\uff0c\u5de5\u4f5c\u5bcc\u6709\u6210\u6548\u3002\u4ed6\u7684\u57fa\u672c\u7ecf\u9a8c\u662f\uff1a\u4f5c\u4e3a\u9ad8\u7ea7\u7ba1\u7406\u4eba\u5458\u5fc5\u987b\u628a\u65e5\u5e38\u4e8b\u52a1\u5904\u7406\u6743\u6388\u7ed9\u4e0b\u7ea7\uff0c\u81ea\u5df1\u53ea\u4fdd\u7559\u91cd\u8981\u4e8b\u9879\u7684\u51b3\u7b56\u6743\u4e0e\u76d1\u7763\u6743\u3002\u5176\u7406\u8bba\u4f9d\u636e\u5c31\u662f\u79d1\u5b66\u7ba1\u7406\u7406\u8bba\u5021\u5bfc\u7684\nA. \u8df3\u677f\u539f\u5219\nB. \u4f8b\u5916\u539f\u5219\nC. \u7edf\u4e00\u6307\u6325\u539f\u5219\nD. \u6807\u51c6\u5316\u65b9\u6cd5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8308602117193503, "meta-math/MetaMath-Mistral-7B": 0.9766630146841263, "itpossible/Chinese-Mistral-7B-v0.1": 0.33012265646127237, "HuggingFaceH4/zephyr-7b-beta": 0.999776517508858, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9511505587627097, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0d\u4f1a\u964d\u4f4e\u98df\u7ba1\u4e0b\u62ec\u7ea6\u808c\u538b\u529b\u7684\u836f\u7269\u662f\nA. \u785d\u9178\u7518\u6cb9\nB. \u83ab\u6c99\u5fc5\u5229\nC. \u963f\u6258\u54c1\nD. \u785d\u82ef\u5730\u5e73\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6559\u80b2\u79d1\u5b66\u5236\u5ea6\u5c5e\u4e8e\nA. \u6d3e\u751f\u5236\u5ea6\nB. \u672c\u6e90\u5236\u5ea6\nC. \u4ece\u5c5e\u5236\u5ea6\nD. \u81ea\u53d1\u5236\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.35370309353919205, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u710a\u63a5\u65b9\u6cd5\u4e2d\uff0c\u710a\u63a5\u98de\u6e85\u6700\u5927\u7684\u710a\u63a5\u65b9\u6cd5\u662f\nA. \u624b\u5de5\u7535\u5f27\u710a\nB. CO2\u6c14\u4f53\u4fdd\u62a4\u710a\nC. \u57cb\u5f27\u81ea\u52a8\u710a\nD. \u6c14\u710a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4761104304272197}}, {"question": "DNA \u53d7\u70ed\u53d8\u6027\u65f6\u51fa\u73b0\u7684\u73b0\u8c61\u662f\nA. \u6700\u5927\u5149\u5438\u6536\u5cf0\u6ce2\u957f\u53d1\u751f\u8f6c\u79fb\nB. \u591a\u805a\u6838\u82f7\u9178\u94fe\u53d8\u4e3a\u5355\u6838\u82f7\u9178\nC. \u6eb6\u6db2\u9ecf\u5ea6\u589e\u52a0\nD. 260nm \u6ce2\u957f\u5904\u7684\u5438\u5149\u5ea6\u589e\u9ad8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.33110093121618506, "meta-math/MetaMath-Mistral-7B": 0.5867953289430351, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.969271215773195, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6137512117933633, "meta-llama/Meta-Llama-3-8B": 0.39418482013173545, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "20\u4e16\u7eaa\u521d\uff0c\u82f1\u56fd\u9996\u76f8\u963f\u65af\u594e\u65af\u8bf4\uff1a\u201c\u6211\u4eec\u73b0\u5728\u6709\u4e00\u4e2a\u7262\u56fa\u786e\u7acb\u4e86\u4e24\u767e\u5e74\u7684\u4f20\u7edf\uff0c\u5373\u5f52\u6839\u5230\u5e95\uff0c\u738b\u4f4d\u7684\u5360\u6709\u8005\u63a5\u53d7\u5176\u5927\u81e3\u7684\u5efa\u8bae\u5e76\u636e\u6b64\u884c\u4e8b\u3002\u201d\u8fd9\u4e00\u4f20\u7edf\u7684\u786e\u7acb\uff0c\u4f7f\u4e00\u4e2a\u4ee5\u5c0f\u519c\u4e1a\u548c\u624b\u5de5\u4e1a\u751f\u4ea7\u4e3a\u4e3b\u7684\u56fd\u5bb6\u53d8\u6210\u4e86\u4e00\u4e2a\u5178\u578b\u7684\u8d44\u672c\u4e3b\u4e49\u56fd\u5bb6\uff0c\u6210\u4e3a\u6b27\u6d32\u5404\u56fd\u6548\u4eff\u7684\u5bf9\u8c61\u3002\u5404\u56fd\u6548\u4eff\u7684\u7406\u7531\u662f\nA. \u82f1\u56fd\u201c\u5149\u8363\u9769\u547d\u201d\u5ba3\u544a\u4e86\u6b27\u6d32\u65b0\u793e\u4f1a\u653f\u6cbb\u5236\u5ea6\u7684\u8bde\u751f\nB. \u6b96\u6c11\u4e3b\u4e49\u6df1\u523b\u5f71\u54cd\u4e86\u82f1\u56fd\u201c\u4e16\u754c\u5de5\u5382\u201d\u7684\u5730\u4f4d\nC. \u82f1\u56fd\u7ecf\u6d4e\u4e0a\u7684\u6210\u5c31\u5f97\u76ca\u4e8e\u5176\u5236\u5ea6\u8bbe\u8ba1\nD. \u82f1\u56fd\u542f\u8499\u601d\u60f3\u5960\u5b9a\u4e86\u8d44\u4ea7\u9636\u7ea7\u6c11\u4e3b\u4e3b\u4e49\u653f\u6cbb\u7684\u7406\u8bba\u57fa\u7840\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8891606939221471, "meta-math/MetaMath-Mistral-7B": 0.9724119875730198, "itpossible/Chinese-Mistral-7B-v0.1": 0.5408762846295913, "HuggingFaceH4/zephyr-7b-beta": 0.9960386145349127, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9460231129356467, "meta-llama/Meta-Llama-3-8B": 0.8011531450147988, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.985314234833051}}, {"question": "\u8db3\u91cf\u4e0b\u5217\u7269\u8d28\u4e0e\u76f8\u540c\u8d28\u91cf\u7684\u94dd\u53cd\u5e94\uff0c\u653e\u51fa\u6c22\u6c14\u4e14\u6d88\u8017\u6eb6\u8d28\u7269\u8d28\u7684\u91cf\u6700\u5c11\u7684\u662f\nA. \u7a00\u786b\u9178\nB. \u7a00\u785d\u9178\nC. \u6c22\u6c27\u5316\u94a0\u6eb6\u6db2\nD. \u76d0\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34932640219984107, "meta-math/MetaMath-Mistral-7B": 0.3967757573269966, "itpossible/Chinese-Mistral-7B-v0.1": 0.33065623127838456, "HuggingFaceH4/zephyr-7b-beta": 0.551739502638, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6692518243164326}}, {"question": "\u6731\u71b9\u8bf4\uff1a\u201c\u5fb7\u8005\uff0c\u5f97\u4e5f\uff0c\u884c\u9053\u800c\u6709\u5f97\u4e8e\u5fc3\u8005\u4e5f\u3002\u201d\u8fd9\u53e5\u8bdd\u8bf4\u660e\nA. \u4e2a\u4eba\u54c1\u5fb7\u7684\u5f62\u6210\u79bb\u4e0d\u5f00\u5b9e\u8df5\nB. \u9053\u5fb7\u662f\u4e00\u79cd\u5ba2\u89c2\u884c\u4e3a\nC. \u9053\u5fb7\u662f\u4e00\u79cd\u4e3b\u89c2\u60f3\u8c61\nD. \u4e2a\u4eba\u54c1\u5fb7\u7684\u517b\u6210\u4e0d\u4f9d\u8d56\u793e\u4f1a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5173\u4e8e\u5e73\u629b\u7269\u4f53\u7684\u8fd0\u52a8\uff0c\u4ee5\u4e0b\u8bf4\u6cd5\u4e2d\u6b63\u786e\u7684\u9009\u9879\u662f\nA. \u7269\u4f53\u843d\u5730\u65f6\u7684\u6c34\u5e73\u4f4d\u79fb\u4e0e\u521d\u901f\u5ea6\u65e0\u5173\nB. \u7269\u4f53\u843d\u5730\u65f6\u7684\u6c34\u5e73\u4f4d\u79fb\u4e0e\u629b\u51fa\u70b9\u7684\u9ad8\u5ea6\u65e0\u5173\nC. \u521d\u901f\u5ea6\u8d8a\u5927\uff0c\u7269\u4f53\u5728\u7a7a\u4e2d\u8fd0\u52a8\u7684\u65f6\u95f4\u8d8a\u957f\nD. \u53ef\u4ee5\u770b\u6210\u6c34\u5e73\u65b9\u5411\u7684\u5300\u901f\u8fd0\u52a8\u548c\u7ad6\u76f4\u65b9\u5411\u7684\u81ea\u7531\u843d\u4f53\u8fd0\u52a8\u7684\u5408\u8fd0\u52a8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5056008131535089, "meta-math/MetaMath-Mistral-7B": 0.6747140251553249, "itpossible/Chinese-Mistral-7B-v0.1": 0.5409270745343874, "HuggingFaceH4/zephyr-7b-beta": 0.7270375585918765, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7881654632545947, "meta-llama/Meta-Llama-3-8B": 0.8656877532605187, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.974178490044609}}, {"question": "\u987a\u53cd\u5b50\nA. \u662f\u6307\u4e00\u4e2a\u5f00\u653e\u9605\u8bfb\u6846\u7684\u9057\u4f20\u529f\u80fd\u5355\u4f4d\nB. \u5728\u771f\u6838\u751f\u7269\u4e2d\uff0c\u5b83\u4e00\u822c\u7b49\u540c\u4e8e\u57fa\u56e0\nC. \u6307\u4e00\u4e2a\u57fa\u56e0\u7684\u5168\u90e8\u5e8f\u5217\nD. \u5728\u539f\u6838\u751f\u7269\u4e2d\uff0c\u5b83\u4e0d\u7b49\u540c\u4e8e\u57fa\u56e0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.31049698480066085, "meta-math/MetaMath-Mistral-7B": 0.6039641534833575, "itpossible/Chinese-Mistral-7B-v0.1": 0.4869348553536824, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5408\u7406\u5b89\u6392\u3001\u4f7f\u7528\u548c\u8c03\u914d\u4eba\u5458\u7684\u57fa\u672c\u4f9d\u636e\u662f\nA. \u4eba\u5458\u57f9\u8bad\nB. \u4eba\u4e8b\u76d1\u7763\nC. \u4eba\u5458\u8003\u8bc4\nD. \u4eba\u4e8b\u9009\u62d4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4f20\u7edf\u540d\u83dc\u201c\u5e72\u70e7\u5ca9\u9ca4\u201d\u662f\u5178\u578b\u7684\nA. \u7ca4\u83dc\nB. \u9c81\u83dc\nC. \u5ddd\u83dc\nD. \u6dee\u626c\u83dc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u300a\u56fd\u5bb6\u5b89\u5168\u6cd5\u300b\u7684\u89c4\u5b9a\uff0c()\u6839\u636e\u5baa\u6cd5\u548c\u6cd5\u5f8b\uff0c\u5236\u5b9a\u6d89\u53ca\u56fd\u5bb6\u5b89\u5168\u7684\u884c\u653f\u6cd5\u89c4\uff0c\u89c4\u5b9a\u6709\u5173\u884c\u653f\u63aa\u65bd\uff0c\u53d1\u5e03\u6709\u5173\u51b3\u5b9a\u548c\u547d\u4ee4\u3002\nA. \u56fd\u52a1\u9662\nB. \u56fd\u5bb6\u5b89\u5168\u90e8\nC. \u5168\u56fd\u4eba\u5927\nD. \u515a\u4e2d\u592e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.419343443983974, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3672766963760478, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.8925598001181388, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u9ec4\u97e7\u5e26\u8fde\u4e8e\u4e24\u4e2a\u76f8\u90bb\u7684\nA. \u690e\u5f13\u4e4b\u95f4\nB. \u690e\u5f13\u677f\u4e4b\u95f4\nC. \u68d8\u7a81\u4e4b\u95f4\nD. \u690e\u5f13\u6839\u4e4b\u95f4\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.39677577159305577, "itpossible/Chinese-Mistral-7B-v0.1": 0.29863342676099575, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.45787070887097386}}, {"question": "\u60a3\u8005\uff0c\u7537\uff0c50 \u5c81\uff0c2 \u4e2a\u6708\u524d\u996e\u9152\u540e\u5267\u70c8\u8179\u75db\uff0c\u4f4f\u9662 30 \u5929\u540e\u597d\u8f6c\uff0c\u8fd1\u65e5\u4e0a\u8179\u7a0d\u5de6\u53d1\u73b0\u4e00\u5305\u5757\uff0c\u6709\u8f7b\u5ea6\u538b\u75db\uff0c\u6b64\u60a3\u8005\u5e94\u9ad8\u5ea6\u6000\u7591\u4e3a\nA. \u80f0\u817a\u5047\u6027\u56ca\u80bf\nB. \u80f0\u817a\u708e\u6027\u5305\u5757\nC. \u80f0\u817a\u6076\u6027\u80bf\u7624\nD. \u813e\u810f\u80bf\u7624\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7898\u7f3a\u4e4f\u4f1a\u5bfc\u81f4\u513f\u7ae5\u3001\u9752\u5c11\u5e74\nA. \u5fc3\u7406\u75be\u75c5\nB. \u7532\u4ea2\nC. \u65e0\u529b\nD. \u751f\u957f\u53d1\u80b2\u548c\u667a\u529b\u53d7\u5f71\u54cd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9309628987385524, "meta-math/MetaMath-Mistral-7B": 0.9453566901503226, "itpossible/Chinese-Mistral-7B-v0.1": 0.958466947255355, "HuggingFaceH4/zephyr-7b-beta": 0.9999293282100576, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9540814441905878, "meta-llama/Meta-Llama-3-8B": 0.9590912391165695, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9728711361771909}}, {"question": "\u5df2\u77e5\u53d7\u8840\u8005\u8840\u578b\u4e3aB\u578b\uff0c\u5728\u4ea4\u53c8\u914d\u8840\u8bd5\u9a8c\u4e2d\u4e3b\u4fa7\u4e0d\u51dd\u96c6\uff0c\u6b21\u4fa7\u51dd\u96c6\uff0c\u4f9b\u8840\u8005\u8840\u578b\u53ef\u80fd\u662f\nA. B\u578b\nB. AB\u578b\nC. A\u578b\nD. O\u578b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u540e\u5de5\u4e1a\u793e\u4f1a\u8bba\u201d\u63d0\u51fa\uff0c\u5904\u4e8e\u793e\u4f1a\u4e2d\u5fc3\u5730\u4f4d\u7684\u662f\nA. \u79d1\u5b66\u6280\u672f\nB. \u4fe1\u606f\u6280\u672f\nC. \u4e13\u4e1a\u4e0e\u6280\u672f\u4eba\u5458\nD. \u7406\u8bba\u77e5\u8bc6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f57\u68ee\u5854\u5c14\u6548\u5e94\u8bf4\u660e\uff0c\u80fd\u5bf9\u5b66\u751f\u4ea7\u751f\u5de8\u5927\u5f71\u54cd\u7684\u662f\nA. \u6559\u5e08\u7684\u4eba\u683c\u7279\u70b9\nB. \u6559\u5e08\u7684\u6559\u5b66\u6c34\u5e73\nC. \u6559\u5e08\u7684\u5a01\u4fe1\nD. \u6559\u5e08\u5bf9\u5b66\u751f\u7684\u671f\u671b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.9630372034484609, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.47484964566879106, "meta-llama/Meta-Llama-3-8B": 0.9354130426936964, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8df3\u8dc3\u4e2d\u7684\u9ad8\u5ea6\u6bd4\u8d5b\uff0c\u8fd0\u52a8\u5458\u5728\u6bcf\u4e2a\u9ad8\u5ea6\u8fde\u7eed\u51e0\u6b21\u8bd5\u8df3\u5931\u8d25\uff0c\u5373\u88ab\u53d6\u6d88\u6bd4\u8d5b\u8d44\u683c\u3002\nA. \u56db\u6b21\nB. \u516d\u6b21\nC. \u4e24\u6b21\nD. \u4e09\u6b21\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u8840\u6db2\u4e2d\u80f0\u5c9b\u7d20\u6837\u751f\u957f\u56e0\u5b50-1\u6c34\u5e73\u660e\u663e\u5347\u9ad8\uff0c\u5e38\u63d0\u793a\u7684\u75be\u75c5\u662f\nA. \u7cd6\u5c3f\u75c5\nB. \u4f8f\u5112\u75c7\nC. \u5446\u5c0f\u75c7\nD. \u80a2\u7aef\u80a5\u5927\u75c7\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.6570540697349027, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u4e00\u65b9\u5bf9\u7acb\u573a\u7684\u62a8\u51fb\u5fc5\u987b\u4e0e\u53e6\u4e00\u65b9\u5374\u662f\u5df2\u63d0\u51fa\u7684\u7acb\u573a\u6709\u5173\u201d\u3002\u8fd9\u662f\u6279\u5224\u6027\u8ba8\u8bba\u89c4\u5219\u4e2d\u7684\nA. \u4e3e\u8bc1\u89c4\u5219\nB. \u81ea\u7531\u89c4\u5219\nC. \u7acb\u573a\u89c4\u5219\nD. \u8d23\u4efb\u89c4\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8830035955989604, "meta-math/MetaMath-Mistral-7B": 0.9696316725733207, "itpossible/Chinese-Mistral-7B-v0.1": 0.7479268644222677, "HuggingFaceH4/zephyr-7b-beta": 0.9988100154545432, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9337757269610836, "meta-llama/Meta-Llama-3-8B": 0.4443287098421572, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9091289941513401}}, {"question": "\u51fa\u81ea\u300a\u53cc\u6845\u8239\u300b\u4e00\u8bd7\u7684\u4e00\u7ec4\u610f\u8c61\u662f\nA. \u8239\u3001\u5cb8\u3001\u98ce\u66b4\u3001\u6708\nB. \u8239\u3001\u5cb8\u3001\u706f\u3001\u6708\nC. \u8239\u3001\u5cb8\u3001\u98ce\u66b4\u3001\u706f\nD. \u8239\u3001\u5cb8\u3001\u96fe\u3001\u6d6a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e2d\u56fd\u5bf9\u5916\u5173\u7cfb\u7684\u6839\u672c\u539f\u5219\u548c\u6838\u5fc3\u662f\nA. \u8054\u5408\u7ed3\u76df\nB. \u72ec\u7acb\u81ea\u4e3b\nC. \u7ef4\u62a4\u548c\u5e73\uff0c\u4fc3\u8fdb\u53d1\u5c55\nD. \u5168\u65b9\u4f4d\u5f00\u653e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7956108088098537, "meta-math/MetaMath-Mistral-7B": 0.9677696012621435, "itpossible/Chinese-Mistral-7B-v0.1": 0.8266431031232362, "HuggingFaceH4/zephyr-7b-beta": 0.992244592633672, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9008298781477693, "meta-llama/Meta-Llama-3-8B": 0.8212327315956056, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5316\u7b80$\\sqrt{3}-\\sqrt{3}(1-\\sqrt{3})$\u7684\u7ed3\u679c\u662f\nA. 3\nB. -3\nC. $\\sqrt{3}$\nD. $-\\sqrt{3}$\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e3b\u5b58\u50a8\u5668\u548cCPU\u4e4b\u95f4\u589e\u52a0\u2fbc\u901f\u7f13\u51b2\u5b58\u50a8\u5668\u7684\u2f6c\u7684\u662f\nA. \u65e2\u6269\u2f24\u4e3b\u5b58\u5bb9\u91cf\u2f1c\u6269\u2f24CPU\u901a\u2f64\u5bc4\u5b58\u5668\u6570\u91cf\nB. \u6269\u2f24CPU\u4e2d\u901a\u2f64\u5bc4\u5b58\u5668\u7684\u6570\u91cf\nC. \u89e3\u51b3CPU\u548c\u4e3b\u5b58\u4e4b\u95f4\u7684\u901f\u5ea6\u5339\u914d\u95ee\u9898\nD. \u6269\u2f24\u4e3b\u5b58\u50a8\u5668\u7684\u5bb9\u91cf\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9668256481588791, "meta-math/MetaMath-Mistral-7B": 0.998408624103171, "itpossible/Chinese-Mistral-7B-v0.1": 0.9594597624184615, "HuggingFaceH4/zephyr-7b-beta": 0.999843875848825, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.997668749100726, "meta-llama/Meta-Llama-3-8B": 0.9771663025482827, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9990925136204322}}, {"question": "\u4e0b\u5217\u6b66\u5668\u4e2d\u90a3\u79cd\u66fe\u88ab\u65af\u5927\u6797\u8a89\u4e3a\u201c\u6218\u4e89\u4e4b\u795e\u201d\uff1f\nA. \u4f5c\u6218\u98de\u673a\nB. \u5766\u514b\nC. \u5730\u5730\u5bfc\u5f39\nD. \u706b\u70ae\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u57fa\u7ebf\u7684\u957f\u5ea6\u4e00\u822c()\nA. \u6108\u77ed\u6108\u597d\nB. \u89c6\u6cb3\u5bbd B \u800c\u5b9a\uff0c\u4e00\u822c\u5e94\u4e3a 0.6B\nC. \u957f\u77ed\u5bf9\u6d4b\u91cf\u6ca1\u6709\u5f71\u54cd\nD. \u6108\u957f\u6108\u597d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7054592190225913, "meta-math/MetaMath-Mistral-7B": 0.8327197402775983, "itpossible/Chinese-Mistral-7B-v0.1": 0.7044108938774374, "HuggingFaceH4/zephyr-7b-beta": 0.9985973196796176, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.8730329914750249, "meta-llama/Meta-Llama-3-8B": 0.8304072092850933, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9277926177583923}}, {"question": "\u6d32\u9645\u5bfc\u5f39\u7684\u5c04\u7a0b\u4e00\u822c\u5728\u591a\u5c11\u516c\u91cc\u4ee5\u4e0a\nA. \u4e00\u767e\nB. \u5341\u4e07\nC. \u4e00\u4e07\nD. \u4e00\u5343\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.4924566254739903, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5768462454534348}}, {"question": "\u989c\u8272\u5bf9\u83dc\u80b4\u7684\u4f5c\u7528\u4e3b\u8981\u6709\u589e\u8fdb\u98df\u6b32\u548c\nA. \u638c\u63e1\u706b\u5019\nB. \u89c6\u89c9\u6b23\u8d4f\nC. \u63d0\u9ad8\u53e3\u611f\nD. \u8fa8\u522b\u751f\u719f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.8583109176555176, "meta-math/MetaMath-Mistral-7B": 0.9761719454383595, "itpossible/Chinese-Mistral-7B-v0.1": 0.6216065160032058, "HuggingFaceH4/zephyr-7b-beta": 0.9796560833995093, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7877854249191468, "meta-llama/Meta-Llama-3-8B": 0.7868727622155435, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7303993325086084}}, {"question": "\u4e0b\u5217\u8bcd\u7684\u5b57\u5f62\u9519\u8bef\u7684\u4e00\u7ec4\u662f\nA. \u6865\u6881\nB. \u8d44\u6001\nC. \u6f02\u6d41\nD. \u4fae\u8fb1\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.353703093539192}}, {"question": "\u8bb0\u8ff0\u4eba\u4eec\u53ea\u6709\u4f9d\u636e\u201c\u9686\u3001\u8d64\u5df4\u3001\u57f9\u6839\u201d\u4e09\u56e0\u7d20\uff0c\u9009\u62e9\u5408\u9002\u7684\u996e\u98df\uff0c\u8bb2\u6c42\u996e\u98df\u7684\u5c5e\u6027\uff0c\u4e25\u683c\u517b\u751f\uff0c\u624d\u80fd\u83b7\u5f97\u5065\u5eb7\uff0c\u4e89\u53d6\u957f\u5bff\u7684\u85cf\u6587\u672c\u662f\nA. \u300a\u672c\u8349\u7eb2\u76ee\u300b\nB. \u300a\u996e\u98df\u987b\u77e5\u300b\nC. \u300a\u996e\u81b3\u6b63\u8981\u300b\nD. \u300a\u4f5b\u5b66\u517b\u751f\u7ecf\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.6762997417580463, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5e7f\u544a\u5b9a\u4f4d\u7684\u76ee\u7684\u5728\u4e8e\nA. \u786e\u5b9a\u5e7f\u544a\u5546\u54c1\u7684\u4f4d\u7f6e\nB. \u660e\u786e\u5e7f\u544a\u7684\u7279\u70b9\nC. \u7a81\u51fa\u5e7f\u544a\u5546\u54c1\u7684\u4e2a\u6027\nD. \u63d0\u9ad8\u5e7f\u544a\u9488\u5bf9\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.36717002281588484, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3845315259135733, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e13\u95e8\u7814\u7a76\u7ba1\u7406\u6d3b\u52a8\u53ca\u5176\u57fa\u672c\u89c4\u5f8b\u548c\u4e00\u822c\u65b9\u6cd5\u7684\u79d1\u5b66\u662f\nA. \u7ba1\u7406\nB. \u73b0\u4ee3\u7ba1\u7406\u5b66\nC. \u7ba1\u7406\u601d\u60f3\nD. \u7ba1\u7406\u5b66\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9230710092788332, "meta-math/MetaMath-Mistral-7B": 0.9890548380148297, "itpossible/Chinese-Mistral-7B-v0.1": 0.8899638245848446, "HuggingFaceH4/zephyr-7b-beta": 0.9998175653335721, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7839560003740853, "meta-llama/Meta-Llama-3-8B": 0.8576279111102868, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u9879\u76ee\u4e2d\uff0c\u7528\u4e8e\u6838\u7b97\u4e8b\u4e1a\u5355\u4f4d\u4ece\u5916\u5355\u4f4d\u501f\u6b3e\u7684\u8d26\u6237\u662f\nA. \u201c\u9884\u6536\u8d26\u6b3e\u201d\u4e0e\u201c\u76c8\u4f59\u516c\u79ef\u201d\nB. \u201c\u5e94\u4ed8\u8d26\u6b3e\u201d\nC. \u201c\u5176\u4ed6\u5e94\u4ed8\u6b3e\u201d\nD. \u201c\u501f\u5165\u6b3e\u201d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.49091631759122706, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7707562012064245, "HuggingFaceH4/zephyr-7b-beta": 0.9459149096506105, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6453540444620791, "meta-llama/Meta-Llama-3-8B": 0.7144263486083567, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.5781299940933122}}, {"question": "\u5f53\u4ee3\u56fd\u9645\u793e\u4f1a\u4e2d\u5f71\u54cd\u6700\u5927\u3001\u4f20\u64ad\u6700\u5e7f\u3001\u6700\u5177\u751f\u547d\u529b\u7684\u4e00\u79cd\u6559\u80b2\u601d\u6f6e\u662f()\nA. \u6559\u80b2\u56fd\u9645\u5316\nB. \u6559\u80b2\u5236\u5ea6\u5316\nC. \u6559\u80b2\u7ec8\u8eab\u5316\nD. \u6559\u80b2\u6c11\u4e3b\u5316\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7368704070501707}}, {"question": "\u884c\u8f66\u4e2d\u9047\u5217\u961f\u6a2a\u8fc7\u9053\u8def\u7684\u5b66\u751f\u65f6\uff0c\u5e94\u600e\u6837\u505a\nA. \u964d\u4f4e\u8f66\u901f\u3001\u7f13\u6162\u901a\u8fc7\nB. \u63d0\u524d\u52a0\u901f\u62a2\u884c\nC. \u8fde\u7eed\u9e23\u5587\u53ed\u50ac\u4fc3\nD. \u505c\u8f66\u8ba9\u884c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.8737319138582773, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5f71\u54cd\u4e00\u78b3\u5355\u4f4d\u4ee3\u8c22\u7684\u7ef4\u751f\u7d20\u662f\nA. \u7ef4\u751f\u7d20B6\u548c\u56db\u6c22\u53f6\u9178\nB. \u7ef4\u751f\u7d20B6\u548c\u6cdb\u9178\nC. \u7ef4\u751f\u7d20B12 \u548c\u56db\u6c22\u53f6\u9178\nD. \u53f6\u9178\u548c\u6cdb\u9178\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.42863709286533597, "meta-math/MetaMath-Mistral-7B": 0.5229506575449361, "itpossible/Chinese-Mistral-7B-v0.1": 0.4655094774097466, "HuggingFaceH4/zephyr-7b-beta": 0.8847471365377423, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6826148440918425, "meta-llama/Meta-Llama-3-8B": 0.4846426226016112, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u56fd\u5916\u6700\u65e9\u7684\u6559\u80b2\u5b66\u8457\u4f5c\u662f\nA. \u300a\u8bba\u96c4\u8fa9\u5bb6\u300b\nB. \u300a\u653f\u6cbb\u5b66\u539f\u7406\u300b\nC. \u300a\u7406\u60f3\u56fd\u300b\nD. \u300a\u8bba\u6f14\u8bf4\u5bb6\u7684\u6559\u80b2\u300b\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3755886305530716, "meta-math/MetaMath-Mistral-7B": 0.39558002231356837, "itpossible/Chinese-Mistral-7B-v0.1": 0.38812073416459414, "HuggingFaceH4/zephyr-7b-beta": 0.9999084946697292, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3583275739447147, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "1942\u5e74\uff0c\u4e2d\u5171\u4e2d\u592e\u89c4\u5b9a\uff1a\u201c\u4e00\u5207\u5c1a\u672a\u5b9e\u884c\u51cf\u79df\u7684\u5730\u533a\uff0c\u5176\u79df\u989d\u7167\u6297\u6218\u524d\u79df\u989d\u51cf\u4f4e\u767e\u5206\u4e4b\u4e8c\u5341\u4e94\u201d\u201c\u5728\u6e38\u51fb\u533a\u53ca\u654c\u5360\u70b9\u7ebf\u9644\u8fd1\uff0c\u53ef\u6bd4\u4e8c\u4e94\u51cf\u79df\u8fd8\u5c11\u4e00\u70b9\uff0c\u53ea\u51cf\u4e8c\u6210\u3001\u4e00\u6210\u4e94\u6216\u4e00\u6210\u3002\u201d\u8fd9\u4e00\u89c4\u5b9a\u65e8\u5728\nA. \u524a\u5f31\u56fd\u6c11\u515a\u5728\u654c\u540e\u7684\u52bf\u529b\nB. \u6253\u51fb\u5c01\u5efa\u5730\u4e3b\u7ecf\u6d4e\nC. \u7ea0\u6b63\u738b\u660e\u201c\u5de6\u201d\u503e\u8def\u7ebf\nD. \u52a0\u5f3a\u5404\u9636\u5c42\u7684\u6297\u65e5\u5927\u8054\u53f0\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5fe7\u8005\u89c1\u4e4b\u5219\u5fe7\uff0c\u559c\u8005\u89c1\u77e5\u5219\u559c\uff0c\"\u8fd9\u53e5\u8bdd\u4e3b\u8981\u4f53\u73b0\u60c5\u7eea\u5177\u6709\nA. \u611f\u67d3\u529f\u80fd\nB. \u52a8\u673a\u529f\u80fd\nC. \u7ec4\u7ec7\u529f\u80fd\nD. \u4fe1\u53f7\u529f\u80fd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.47776397412058574, "meta-math/MetaMath-Mistral-7B": 0.6493938158015177, "itpossible/Chinese-Mistral-7B-v0.1": 0.6093791524447273, "HuggingFaceH4/zephyr-7b-beta": 0.6578763741776635, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9139900889850343, "meta-llama/Meta-Llama-3-8B": 0.5482601385401137, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.990256598239006}}, {"question": "\u5728\u5357\u4eac\u56fd\u6c11\u653f\u5e9c\u7684\u63a8\u52a8\u4e0b\uff0c\u5168\u56fd\u5404\u5730\u56fd\u8d27\u754c\u4e00\u81f4\u54cd\u5e94\uff0c\u56fd\u8d27\u8fd0\u52a8\u66fe\u4e00\u5ea6\u9887\u5177\u58f0\u52bf\u3002\u7279\u522b\u5728\u4e0a\u6d77\uff0c\u4f5c\u4e3a\u7ecf\u6d4e\u5927\u57e0\uff0c\u5168\u56fd\u7ecf\u6d4e\u91cd\u9547\uff0c\u63d0\u5021\u56fd\u8d27\u8fd0\u52a8\u8f70\u8f70\u70c8\u70c8\uff0c\u6c11\u4f17\u8e0a\u8dc3\u53c2\u52a0\uff0c\u52301930\u5e7410\u67087\u65e5\uff0c\u5df2\u8fdb\u884c\u4e86\u7b2c\u4e09\u6b21\u56fd\u8d27\u8fd0\u52a8\u5927\u4f1a\uff0c\u6210\u6548\u663e\u8457\u3002\u8fd9\u4e00\u505a\u6cd5\u6709\u5229\u4e8e\nA. \u52a0\u5feb\u5de5\u4e1a\u4f53\u7cfb\u7684\u5f62\u6210\nB. \u963b\u65ad\u897f\u65b9\u7684\u7ecf\u6d4e\u4fb5\u7565\nC. \u4fc3\u8fdb\u6c11\u65cf\u5de5\u4e1a\u7684\u53d1\u5c55\nD. \u8c03\u6574\u5931\u8861\u7684\u56fd\u6c11\u7ecf\u6d4e\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.5106361772911493, "meta-math/MetaMath-Mistral-7B": 0.5421903548499456, "itpossible/Chinese-Mistral-7B-v0.1": 0.6923076609241593, "HuggingFaceH4/zephyr-7b-beta": 0.9719611927082583, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.8542731821733099}}, {"question": "\u6559\u80b2\u76ee\u7684\u7684\u5236\u5b9a\u53d7\u5230\u8bf8\u591a\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u5176\u4e2d\u51b3\u5b9a\u6559\u80b2\u76ee\u7684\u7684\u6027\u8d28\u3001\u65b9\u5411\u548c\u5185\u6db5\u7684\u56e0\u7d20\u662f\nA. \u6587\u5316\u4f20\u7edf\u548c\u6559\u80b2\u4f20\u7edf\nB. \u53d7\u6559\u80b2\u8005\u7684\u8eab\u5fc3\u53d1\u5c55\u7279\u70b9\nC. \u54f2\u5b66\u601d\u60f3\u548c\u6559\u80b2\u601d\u60f3\nD. \u751f\u4ea7\u529b\u6c34\u5e73\u548c\u653f\u6cbb\u7ecf\u6d4e\u5236\u5ea6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u629b\u7269\u7ebfy^2=10x\u7684\u7126\u70b9\u5230\u51c6\u7ebf\u7684\u8ddd\u79bb\u662f\nA. 15/2\nB. 10\nC. 5/2\nD. 5\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5708046485496913, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6613\u4f7f\u4eba\u51fa\u73b0\u5404\u79cd\u8840\u8bc1\u7684\u662f\nA. \u706b\u90aa\nB. \u6691\u90aa\nC. \u71e5\u90aa\nD. \u98ce\u90aa\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.3919626225575906, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.3626062801770276, "meta-llama/Meta-Llama-3-8B": 0.2980747244459592, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.4369798752501992}}, {"question": "\u4ea7\u4e1a\u8d44\u672c\u5212\u5206\u4e3a\u8d27\u5e01\u8d44\u672c\u3001\u751f\u4ea7\u8d44\u672c\u3001\u5546\u54c1\u8d44\u672c\u7684\u4f9d\u636e\u662f\u8d44\u672c\u5404\u4e2a\u90e8\u5206\nA. \u5728\u4ef7\u503c\u589e\u6b96\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u4e0d\u540c\nB. \u4ef7\u503c\u5468\u8f6c\u65b9\u5f0f\u4e0d\u540c\nC. \u5b58\u5728\u7684\u7269\u8d28\u5f62\u6001\u4e0d\u540c\nD. \u5728\u5faa\u73af\u4e2d\u7684\u804c\u80fd\u4e0d\u540c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.39916230455840296, "HuggingFaceH4/zephyr-7b-beta": 0.6681209088889764, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4eba\u9020\u5730\u7403\u536b\u661f\u4ee5\u5730\u5fc3\u4e3a\u5706\u5fc3\uff0c\u505a\u5300\u901f\u5706\u5468\u8fd0\u52a8\uff0c\u4ee5\u4e0b\u8bf4\u6cd5\u6b63\u786e\u7684\u9009\u9879\u662f\nA. \u534a\u5f84\u8d8a\u5927\uff0c\u901f\u5ea6\u8d8a\u5c0f\uff0c\u5468\u671f\u8d8a\u5c0f\nB. \u6240\u6709\u536b\u661f\u89d2\u901f\u5ea6\u90fd\u76f8\u540c\uff0c\u4e0e\u534a\u5f84\u65e0\u5173\nC. \u6240\u6709\u536b\u661f\u7684\u901f\u5ea6\u5747\u662f\u76f8\u540c\u7684\uff0c\u4e0e\u534a\u5f84\u65e0\u5173\nD. \u534a\u5f84\u8d8a\u5927\uff0c\u901f\u5ea6\u8d8a\u5c0f\uff0c\u5468\u671f\u8d8a\u5927\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f51\u7edc\u653b\u51fb\u7684\u79cd\u7c7b\u6709\nA. \u9ed1\u5ba2\u653b\u51fb\uff0c\u75c5\u6bd2\u653b\u51fb\nB. \u7269\u7406\u653b\u51fb\uff0c\u9ed1\u5ba2\u653b\u51fb\uff0c\u75c5\u6bd2\u653b\u51fb\nC. \u786c\u4ef6\u653b\u51fb\uff0c\u8f6f\u4ef6\u653b\u51fb\nD. \u7269\u7406\u653b\u51fb\uff0c\u8bed\u6cd5\u653b\u51fb\uff0c\u8bed\u4e49\u653b\u51fb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.701625657276978, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6559\u5b66\u8fc7\u7a0b\u662f\u4e00\u79cd\u7279\u6b8a\u7684\u8ba4\u8bc6\u8fc7\u7a0b\uff0c\u5176\u533a\u522b\u4e8e\u4eba\u7c7b\u4e00\u822c\u8ba4\u8bc6\u7684\u7279\u70b9\u662f\nA. \u63a2\u7d22\u6027\u3001\u95f4\u63a5\u6027\u548c\u5f15\u5bfc\u6027\nB. \u4e3b\u52a8\u6027\u3001\u9605\u8bfb\u6027\u548c\u5f15\u5bfc\u6027\nC. \u95f4\u63a5\u6027\u3001\u5f15\u5bfc\u6027\u548c\u590d\u6742\u6027\nD. \u95f4\u63a5\u6027\u3001\u5f15\u5bfc\u6027\u548c\u7b80\u6d01\u6027\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5468\u67d0\u9a7e\u9a76\u4e00\u8f86\u8f7b\u578b\u53a2\u5f0f\u8d27\u8f66\uff08\u642d\u8f7d22\u4eba\uff09\u884c\u9a76\u5bf9\u4e19\u5bdf\u516c\u8def79\u516c\u91cc\u52a0150\u7c73\u5904\u65f6\uff0c\u5760\u5165\u9053\u8def\u4e00\u4fa7\u5c71\u5d16\uff0c\u9020\u621012\u4eba\u6b7b\u4ea1\u300110\u4eba\u53d7\u4f24\u3002\u5468\u67d0\u7684\u4e3b\u8981\u8fdd\u6cd5\u884c\u4e3a\u662f\u4ec0\u4e48\nA. \u8d85\u901f\u884c\u9a76\nB. \u9a7e\u9a76\u903e\u671f\u672a\u68c0\u9a8c\u7684\u673a\u52a8\u8f66\nC. \u8d27\u8fd0\u673a\u52a8\u8f66\u8f7d\u5ba2\nD. \u75b2\u52b3\u9a7e\u9a76\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.7393069891363211, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.590772348314523, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.7724649781836236}}, {"question": "\u4ee5\u4e0b\u54ea\u4e2a\u4e0d\u662f\u519c\u4e1a\u5bf9\u56fd\u6c11\u7ecf\u6d4e\u7684\u4f5c\u7528\nA. \u4e89\u53d6\u5916\u6c47\nB. \u79ef\u7d2f\u751f\u4ea7\u8981\u7d20\nC. \u63d0\u4f9b\u98df\u54c1\u548c\u5de5\u4e1a\u539f\u6599\nD. \u8f93\u9001\u9ad8\u79d1\u6280\u4eba\u624d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.9131708469922719, "meta-math/MetaMath-Mistral-7B": 0.9886051446552563, "itpossible/Chinese-Mistral-7B-v0.1": 0.9525753232981973, "HuggingFaceH4/zephyr-7b-beta": 0.9999414971638473, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.997621906229122, "meta-llama/Meta-Llama-3-8B": 0.9338381355050726, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u5b9e\u7269\u5f62\u6001\u7684\u8d44\u672c\u4e2d\uff0c\u540c\u65f6\u5c5e\u4e8e\u751f\u4ea7\u8d44\u672c\u3001\u4e0d\u53d8\u8d44\u672c\u548c\u56fa\u5b9a\u8d44\u672c\u7684\u662f\nA. \u673a\u5668\u8bbe\u5907\nB. \u539f\u6599\u548c\u71c3\u6599\nC. \u8f85\u52a9\u6750\u6599\nD. \u5546\u4e1a\u8bbe\u65bd\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.46259144076326525, "meta-math/MetaMath-Mistral-7B": 0.5159701843829047, "itpossible/Chinese-Mistral-7B-v0.1": 0.44241938329302644, "HuggingFaceH4/zephyr-7b-beta": 0.9995141350295492, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.7380765707815686, "meta-llama/Meta-Llama-3-8B": 0.9083925281895031, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.6272430878576191}}, {"question": "\u73b0\u4ee3\u516c\u5171\u5173\u7cfb\u5728\u4e2d\u56fd\u7ecf\u5386\u4e86\u56db\u4e2a\u53d1\u5c55\u65f6\u671f\uff0c\u8bf7\u95ee\u201c\u5f15\u8fdb\u915d\u917f\u671f\u201d\u662f\u4ece\u4ec0\u4e48\u65f6\u5019\u5230\u4ec0\u4e48\u65f6\u5019\uff1f\nA. 20\u4e16\u7eaa90\u5e74\u4ee3\u4e2d\u671f\u81f3\u4eca\nB. 20\u4e16\u7eaa90\u5e74\u4ee3\u521d\u81f390\u5e74\u4e2d\nC. 20\u4e16\u7eaa80\u5e74\u4ee3\u4e2d\u81f380\u5e74\u4ee3\u672b\nD. 20\u4e16\u7eaa70\u5e74\u4ee3\u672b80\u5e74\u4ee3\u521d\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9234694558336339, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u53ef\u80fd\u53d1\u751f\u7eb5\u8188\u6251\u52a8\u7684\u80f8\u90e8\u635f\u4f24\u662f\nA. \u5f00\u653e\u6027\u6c14\u80f8\nB. \u95ed\u5408\u6027\u6c14\u80f8\nC. \u5f20\u529b\u6027\u6c14\u80f8\nD. \u8fdb\u884c\u6027\u8840\u80f8\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.34458732107001483, "meta-math/MetaMath-Mistral-7B": 0.4895760593273492, "itpossible/Chinese-Mistral-7B-v0.1": 0.31828968051251794, "HuggingFaceH4/zephyr-7b-beta": 0.5045404767153542, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.4578706792802335, "meta-llama/Meta-Llama-3-8B": 0.29660173325630934, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7535\u529b\u7cfb\u7edf\u74e6\u89e3\u662f\u6307\nA. \u4e24\u4e2a\u4ee5\u4e0a\u6c34\u7535\u5382\u57ae\u575d\nB. \u7531\u4e8e\u5404\u79cd\u539f\u56e0\u5f15\u8d77\u7684\u7535\u529b\u7cfb\u7edf\u975e\u6b63\u5e38\u89e3\u5217\u6210\u51e0\u4e2a\u72ec\u7acb\u7cfb\u7edf\nC. \u7cfb\u7edf\u7535\u538b\u5d29\u6e83\nD. \u7cfb\u7edf\u4e3b\u529b\u7535\u5382\u5168\u90e8\u5931\u53bb\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.7773284927149262, "meta-math/MetaMath-Mistral-7B": 0.953689573359323, "itpossible/Chinese-Mistral-7B-v0.1": 0.8794899194850639, "HuggingFaceH4/zephyr-7b-beta": 0.99769997075397, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.9753223836115663, "meta-llama/Meta-Llama-3-8B": 0.9191740480784265, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9550484799178256}}, {"question": "\u5728\u827a\u672f\u7684\u5b9a\u4e49\u95ee\u9898\u4e0a\uff0c\u63d0\u51fa\u201c\u96c6\u4f53\u65e0\u610f\u8bc6\u8bf4\u201d\u7684\u7f8e\u5b66\u5bb6\u662f\nA. \u6d1b\u514b\nB. \u8363\u683c\nC. \u5f17\u6d1b\u4f0a\u5fb7\nD. \u82cf\u73ca\u00b7\u6717\u683c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.49869532315275966, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u51fd\u6570 $f(x\uff0c y)=\\left\\{\\begin{array}{lc}0\uff0c & x y=0\uff0c \\\\ x \\sin \\frac{1}{x}+y \\sin \\frac{1}{x}\uff0c & x y \\neq 0\uff0c\\end{array}\\right.$ \u5219\u6781\u9650 $\\lim _{\\substack{x \\rightarrow 0 \\\\ y \\rightarrow 0}} f(x\uff0c y)()$.\nA. \u7b49\u4e8e 1\nB. \u7b49\u4e8e 0\nC. \u7b49\u4e8e 2\nD. \u4e0d\u5b58\u5728\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.3789723868131119, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u5143\u82b3\uff0c\u4f60\u600e\u4e48\u770b\uff1f\u201d\u7535\u89c6\u5267\u300a\u795e\u63a2\u72c4\u4ec1\u6770\u300b\u4e2d\u4e3b\u4eba\u516c\u7684\u8fd9\u53e5\u8ffd\u95ee\u8fd1\u6765\u8d70\u7ea2\u7f51\u7edc\uff0c\u6210\u4e3a\u516c\u4f17\u8868\u8fbe\u8bc9\u6c42\u548c\u8d28\u7591\u7684\u7ecf\u5178\u53e5\u5f0f\u3002\u8fd1\u5e74\u6765\uff0c\u968f\u7740\u516c\u4f17\u53c2\u4e0e\u610f\u8bc6\u3001\u8868\u8fbe\u610f\u8bc6\u3001\u76d1\u7763\u610f\u8bc6\u7684\u589e\u5f3a\uff0c\u5bf9\u5404\u7ea7\u7ba1\u7406\u90e8\u95e8\u6765\u8bf4\uff0c\u7c7b\u4f3c\u7684\u63d0\u95ee\u5e76\u4e0d\u964c\u751f\u3002\u5bf9\u6b64\uff0c\u653f\u5e9c\u5e94\u6301\u7684\u6001\u5ea6\u662f\uff1aa\u5c0a\u91cd\u516c\u6c11\u77e5\u60c5\u6743\uff0c\u771f\u8bda\u56de\u5e94\u8d28\u7591\uff1bb\u4fdd\u6301\u6c89\u9ed8\uff0c\u8ba9\u4e8b\u5b9e\u8bf4\u8bdd\uff1bc\u8fce\u5408\u8d28\u7591\u58f0\u97f3\uff0c\u8c03\u6574\u653f\u5e9c\u51b3\u7b56\uff1bd\u81ea\u89c9\u63a5\u53d7\u76d1\u7763\uff0c\u53ca\u65f6\u6539\u8fdb\u5de5\u4f5c\nA. ab\nB. ad\nC. bd\nD. bc\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0.3430088241712856, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u201c\u8001\u543e\u8001\u4ee5\u53ca\u4eba\u4e4b\u8001\u201d\u662f\u4e2d\u534e\u6c11\u65cf\u7684\u4f20\u7edf\u7f8e\u5fb7\uff0c\u8fd9\u53e5\u8bdd\u662f\u8c01\u6700\u65e9\u63d0\u51fa\u7684\nA. \u5b54\u5b50\nB. \u5b5f\u5b50\nC. \u58a8\u5b50\nD. \u97e9\u975e\u5b50\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.3202204083030029, "meta-math/MetaMath-Mistral-7B": 0.3845315259135733, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6162\u6027\u80be\u8870\u7aed\u7ee7\u53d1\u7532\u72b6\u65c1\u817a\u529f\u80fd\u4ea2\u8fdb\u65f6\u5e94\u7ed9\u4e88\nA. \u7cd6\u76ae\u8d28\u6fc0\u7d20\nB. \u53f8\u7ef4\u62c9\u59c6\nC. rHuEPO\nD. \u8840\u7ba1\u7d27\u5f20\u7d20\u8f6c\u6362\u9176\u6291\u5236\u5242\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.35686329776860143, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f8e\u56fd\u6cd5\u5b66\u5bb6\u67ef\u91cc\u6559\u6388\u57281963\u5e74\u51fa\u7248\u7684\u300a\u51b2\u7a81\u6cd5\u8bba\u6587\u96c6\u300b\uff0c\u6781\u529b\u53cd\u5bf9\u901a\u8fc7\u51b2\u7a81\u89c4\u8303\u6765\u9009\u62e9\u6cd5\u5f8b\uff0c\u8ba4\u4e3a\u5e94\u629b\u5f03\u4f20\u7edf\u7684\u51b2\u7a81\u6cd5\u89c4\u5219\uff0c\u800c\u4ee3\u4e4b\u4ee5\u201c\u5229\u76ca\u5206\u6790\u201d\u7684\u65b9\u6cd5\uff0c\u6b64\u5b66\u8bf4\u88ab\u79f0\u4e3a\nA. \u653f\u5e9c\u5229\u76ca\u5206\u6790\u8bf4\nB. \u6700\u5bc6\u5207\u8054\u7cfb\u539f\u5219\nC. \u672c\u5730\u6cd5\u8bf4\nD. \u610f\u601d\u81ea\u6cbb\u539f\u5219\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.7517488741679917, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.533743622810805}}, {"question": "\u673a\u52a8\u8f66\u5728\u6ca1\u6709\u9650\u901f\u6807\u5fd7\u3001\u6807\u7ebf\u7684\u9053\u8def\u60c5\u51b5\u4e0b\uff0c\u5982\u679c\u540c\u65b9\u5411\u53ea\u6709\u4e00\u6761\u673a\u52a8\u8f66\u9053\u7684\u57ce\u5e02\u9053\u8def\u89c4\u5b9a\u6700\u9ad8\u65f6\u901f\u4e3a\nA. 40\u516c\u91cc/\u5c0f\u65f6\nB. 30\u516c\u91cc/\u5c0f\u65f6\nC. 50\u516c\u91cc/\u5c0f\u65f6\nD. 70\u516c\u91cc/\u5c0f\u65f6\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u5728\u975e\u5b89\u5168\u7684\u901a\u4fe1\u73af\u5883\u4e2d\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u6d88\u606f\u6765\u6e90\u7684\u53ef\u9760\u6027\uff0c\u901a\u5e38\u91c7\u7528\u7684\u5b89\u5168\u9632\u62a4\u6280\u672f \u662f\nA. \u6d88\u606f\u8ba4\u8bc1\u6280\u672f\nB. \u4fe1\u606f\u9690\u85cf\u6280\u672f\nC. \u6570\u636e\u52a0\u5bc6\u6280\u672f\nD. \u6570\u5b57\u6c34\u5370\u6280\u672f\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.725543531816229, "meta-math/MetaMath-Mistral-7B": 0.873478555715605, "itpossible/Chinese-Mistral-7B-v0.1": 0.916641542141403, "HuggingFaceH4/zephyr-7b-beta": 0.9956917932193147, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6283076738510794, "meta-llama/Meta-Llama-3-8B": 0.8597904988819867, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9256547179721086}}, {"question": "\u71d5\u9ea6\u7ec6\u80de\u764c\u662f\u6307\nA. \u80ba\u7c7b\u764c\nB. \u80ba\u817a\u764c\nC. \u80ba\u9cde\u764c\nD. \u80ba\u5c0f\u7ec6\u80de\u764c\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.2942201655178624, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.9626045403188482, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.5873401501765404, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.9953047353759817}}, {"question": "\u4e0b\u5217\u54ea\u4f4d\u4e0d\u662f\u6e05\u672b\u88ab\u6740\u5bb3\u7684\u201c\u620a\u620c\u516d\u541b\u5b50\u201d\u4e4b\u4e00\nA. \u6797\u65ed\nB. \u5218\u5149\u7b2c\nC. \u5eb7\u6709\u4e3a\nD. \u6768\u9510\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0.28570956661341396, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0.3994863357617844}}, {"question": "\u201c\u9877\u523b\u95f4\u5343\u79cb\u4e8b\u4e1a\uff0c\u65b9\u5bf8\u5730\u4e07\u91cc\u6c5f\u5c71\uff1b\u4e09\u4e94\u6b65\u884c\u904d\u5929\u4e0b\uff0c\u516d\u4e03\u4eba\u767e\u4e07\u96c4\u5175\u201d\u63cf\u5199\u7684\u662f\nA. \u5b98\u573a\nB. \u4e0b\u68cb\nC. \u620f\u53f0\nD. \u6218\u573a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u6839\u636e\u5baa\u6cd5\u548c\u6cd5\u5f8b\u89c4\u5b9a\uff0c\u5bf9\u88ab\u5265\u593a\u653f\u6cbb\u6743\u5229\u7684\u516c\u6c11\u6240\u4eab\u6709\u7684\u6743\u5229\u548c\u81ea\u7531\u7684\u8868\u8ff0\u9519\u8bef\u7684\u662f\nA. \u53ef\u4ee5\u4eab\u6709\u5b97\u6559\u4fe1\u4ef0\u7684\u81ea\u7531\nB. \u53ef\u4ee5\u4eab\u6709\u51fa\u7248\u8457\u4f5c\u7684\u81ea\u7531\nC. \u65e2\u4e0d\u4eab\u6709\u9009\u4e3e\u6743\u4e5f\u4e0d\u4eab\u6709\u88ab\u9009\u4e3e\u6743\nD. \u53ef\u4ee5\u4eab\u6709\u79d1\u7814\u548c\u827a\u672f\u521b\u4f5c\u7684\u81ea\u7531?\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0b\u5217\u6cbb\u7597\u98ce\u6e7f\u75c5\u7684\u975e\u7b1b\u4f53\u6297\u708e\u836f\u7269\u4e2d\uff0c\u80c3\u80a0\u9053\u4e0d\u826f\u53cd\u5e94\u6700\u5c0f\u7684\u662f\nA. \u708e\u75db\u559c\u5eb7\nB. \u53cc\u6c2f\u82ac\u9178\nC. \u8418\u666e\u751f\nD. \u585e\u6765\u6614\u5e03\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.4392175618787783, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.2906893535433972, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u4e0d\u5c5e\u4e8e\u201c\u9634\u76db\u5219\u5185\u5bd2\u201c\u673a\u7406\u7684\u662f\nA. \u5bd2\u6c14\u79ef\u4e8e\u80f8\u4e2d\nB. \u8840\u51dd\u6ce3\nC. \u536b\u6c14\u4e0d\u5f97\u6cc4\u8d8a\nD. \u8109\u4e0d\u901a\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0.2801288226217134, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u300a\u56fd\u5bb6\u4e2d\u957f\u671f\u6559\u80b2\u6539\u9769\u53d1\u5c55\u89c4\u5212\u7eb2\u8981\uff082010-2020\uff09\u5e74\u300b\u63d0\u51fa\uff0c\u8981\u6559\u80b2\u6446\u5728\u4f18\u5148\u53d1\u5c55\u7684\u6218\u7565\u5730\u4f4d\u3002\u5bf9\u4e8e\u6559\u80b2\u4f18\u5148\u53d1\u5c55\u6218\u7565\u7684\u7406\u89e3\uff0c\u4ee5\u4e0b\u9009\u9879\u4e2d\u4e0d\u6070\u5f53\u7684\u662f\nA. \u8d22\u653f\u8d44\u91d1\u4f18\u5148\u4fdd\u969c\u6559\u80b2\u6295\u5165\nB. \u7ecf\u6d4e\u793e\u4f1a\u53d1\u5c55\u89c4\u5212\u4f18\u5148\u5b89\u6392\u6559\u80b2\u53d1\u5c55\nC. \u793e\u4f1a\u8d44\u6e90\u4f18\u5148\u5411\u6559\u80b2\u9886\u57df\u503e\u659c\nD. \u516c\u5171\u8d44\u6e90\u4f18\u5148\u6ee1\u8db3\u6559\u80b2\u548c\u4eba\u529b\u8d44\u6e90\u5f00\u53d1\u9700\u8981\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0, "cognitivecomputations/dolphin-2.6-mistral-7b": 0.6188756664262173, "meta-llama/Meta-Llama-3-8B": 0, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}, {"question": "\u7f8e\u519b\u7814\u5236\u7684\u201c\u9ec4\u8702\u201d\u53cd\u5766\u514b\u5bfc\u5f39\uff0c\u5728\u53d1\u5c04\u540e\uff0c\u4e0d\u5177\u6709\u7684\u529f\u80fd\u662f\nA. \u80fd\u81ea\u52a8\u722c\u9ad8\nB. \u80fd\u81ea\u52a8\u9690\u85cf\u81ea\u5df1\nC. \u80fd\u81ea\u52a8\u88ad\u51fb\u76ee\u7684\u7684\u8981\u5bb3\u90e8\u4f4d\nD. \u80fd\u81ea\u52a8\u641c\u7d22\n\u7b54\u6848\uff1a", "scores": {"mistralai/Mistral-7B-v0.1": 0, "meta-math/MetaMath-Mistral-7B": 0, "itpossible/Chinese-Mistral-7B-v0.1": 0, "HuggingFaceH4/zephyr-7b-beta": 0.5835213057009617, "cognitivecomputations/dolphin-2.6-mistral-7b": 0, "meta-llama/Meta-Llama-3-8B": 0.29068935354339714, "cognitivecomputations/dolphin-2.9-llama3-8b": 0}}]