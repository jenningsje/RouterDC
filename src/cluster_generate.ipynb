{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/chensh/miniconda3/envs/vllm/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load sample list\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from openTSNE import TSNE\n",
    "import torch\n",
    "\n",
    "from train_router_mdeberta import RouterDataset, RouterModule\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from transformers import T5EncoderModel, T5Tokenizer, AutoTokenizer, DebertaV2Model\n",
    "\n",
    "dataset_paths = [\"../datasets/split2_model7/mmlu_train.json\",\"../datasets/split2_model7/gsm8k-train.json\", \"../datasets/split2_model7/cmmlu_train.json\", \"../datasets/split2_model7/arc_challenge_train.json\", \"../datasets/split2_model7/humaneval_train.json\",]\n",
    "\n",
    "data_types = [ \"multi_attempt\", \"probability\", \"probability\", \"probability\", \"multi_attempt\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/mdeberta-v3-base\", truncation_side='left', padding=True)\n",
    "encoder_model = DebertaV2Model.from_pretrained(\"microsoft/mdeberta-v3-base\").to(\"cuda\")\n",
    "\n",
    "\n",
    "number_per_dataset = 2000\n",
    "\n",
    "router_datasets = [RouterDataset(data_path, data_type=data_types[i], dataset_id=i, size=number_per_dataset) for i, data_path in enumerate(dataset_paths)]\n",
    "for router_dataset in router_datasets:\n",
    "    router_dataset.register_tokenizer(tokenizer)\n",
    "router_dataset = ConcatDataset(router_datasets)\n",
    "router_dataloader = DataLoader(router_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "router_model = RouterModule(encoder_model, hidden_state_dim=768, node_size=len(router_datasets[0].router_node), similarity_function=\"cos\").to(\"cpu\")\n",
    "router_model.to('cuda')\n",
    "\n",
    "\n",
    "# get predicted label \n",
    "all_hidden_states = []\n",
    "dataset_set_ids = []\n",
    "cluster_ids = []\n",
    "predicts = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(router_dataloader):\n",
    "        input, _, dataset_id, cluster_id = batch\n",
    "        input.to(\"cuda\")\n",
    "        predict, hidden_states = router_model(**input)\n",
    "        dataset_set_ids.append(dataset_id)\n",
    "        cluster_ids.append(cluster_id)\n",
    "        predicts.append(predict)\n",
    "        all_hidden_states.append(hidden_states)\n",
    "\n",
    "\n",
    "all_hidden_states = torch.concat(all_hidden_states)\n",
    "predicts = torch.concat(predicts)\n",
    "cluster_ids = torch.concat(cluster_ids).numpy() \n",
    "_, max_index = torch.max(predicts, dim=1)\n",
    "dataset_set_ids = torch.concat(dataset_set_ids).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MulticoreTSNE import MulticoreTSNE as M_TSNE\n",
    "from openTSNE import TSNE\n",
    "np_hidden_states = all_hidden_states.cpu().numpy()\n",
    "tsne_result = M_TSNE(n_components=5, n_jobs=12).fit_transform(np_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import numpy as np\n",
    "import random as random\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "n_clusters_list = [8]\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "for n_clusters in n_clusters_list:\n",
    "    x = tsne_result\n",
    "    kmeans = KMeans(n_clusters=n_clusters, max_iter=1000)\n",
    "\n",
    "    kmeans.fit(x)\n",
    "\n",
    "    kmeans_labels = kmeans.labels_.tolist()\n",
    "    # kmeans_labels = np.array(kmeans_labels)\n",
    "\n",
    "    labels_split = [kmeans_labels[i*number_per_dataset: (i+1)*number_per_dataset] for i in range(len(dataset_paths))]\n",
    "    base_output_path = f\"./datasets/split2_model7_cluster\"\n",
    "    os.makedirs(base_output_path, exist_ok=True)\n",
    "\n",
    "\n",
    "    for i, data_path in enumerate(dataset_paths) :\n",
    "        cluster_ids = labels_split[i]\n",
    "        \n",
    "        with open(data_path, 'r') as f:\n",
    "            if data_path.endswith('.json'):\n",
    "                sample_list = json.load(f)\n",
    "        new_sample_list = []\n",
    "        for j, sample in enumerate(sample_list):\n",
    "            if j >= 2000:\n",
    "                break\n",
    "            new_sample = sample \n",
    "            new_sample['cluster_id'] = cluster_ids[j]\n",
    "            new_sample_list.append(new_sample)\n",
    "        with open(os.path.join(base_output_path, data_path.split('/')[-1]), \"w\" ) as f:\n",
    "            json.dump(new_sample_list ,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
