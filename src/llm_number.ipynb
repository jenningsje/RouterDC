{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "model_list = [  \n",
    "                ['mistralai','Mistral-7B-v0.1'],\n",
    "                ['meta-math','MetaMath-Mistral-7B'],\n",
    "                [\"HuggingFaceH4\",\"zephyr-7b-beta\"],\n",
    "                ['itpossible','Chinese-Mistral-7B-v0.1'],\n",
    "                [\"cognitivecomputations\",\"dolphin-2.6-mistral-7b\"],\n",
    "                [\"meta-llama\",\"Meta-Llama-3-8B\"],\n",
    "                [\"cognitivecomputations\",\"dolphin-2.9-llama3-8b\"],\n",
    "                ]\n",
    "model_list = [item[0] + '/' + item[1] for item in model_list]\n",
    "\n",
    "\n",
    "source_output_path = \"/data/home/chensh/projects/LLM_router/datasets/split2_model7\"\n",
    "\n",
    "for llm_number in range(3,4):\n",
    "    target_output_path = f\"/data/home/chensh/projects/LLM_router/datasets/llm_numbers/model_{llm_number}_2\"\n",
    "    os.makedirs(target_output_path)\n",
    "    source_dataset_list = os.listdir(source_output_path)\n",
    "    for source_dataset in source_dataset_list:\n",
    "        with open(os.path.join(source_output_path, source_dataset),'r') as f: \n",
    "            source_data = json.load(f)\n",
    "        target_data = []\n",
    "        for item in source_data:\n",
    "            target_item = item\n",
    "            target_score = {}\n",
    "            for key, value in item['scores'].items():\n",
    "                if key in model_list[:llm_number]:\n",
    "                    target_score[key] = value\n",
    "            target_item['scores'] = target_score\n",
    "            target_data.append(target_item)\n",
    "\n",
    "        with open(os.path.join(target_output_path, source_dataset),'w') as f: \n",
    "            json.dump(target_data, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/chensh/miniconda3/envs/vllm/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "/data/home/chensh/miniconda3/envs/vllm/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "from openTSNE import TSNE\n",
    "import torch\n",
    "\n",
    "from train_router_mdeberta_v2 import RouterDataset, RouterModule\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from transformers import T5EncoderModel, T5Tokenizer, AutoTokenizer, DebertaV2Model\n",
    "\n",
    "dataset_paths = [\"../datasets/split2_model7/gsm8k-train.json\",\"../datasets/split2_model7/mmlu_train.json\",\"../datasets/split2_model7/humaneval_train.json\",\"../datasets/split2_model7/arc_challenge_train.json\",\"../datasets/split2_model7/cmmlu_train.json\"]\n",
    "data_types = [\"multi_attempt\", \"multi_attempt\", \"probability\", \"probability\", \"probability\"]\n",
    "\n",
    "# dataset_paths = [\"../datasets/split2_model7/gsm8k-train.json\"]\n",
    "# data_types = [\"multi_attempt\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/home/chensh/data/huggingface_model/microsoft/mdeberta-v3-base\", truncation_side='left', padding=True)\n",
    "encoder_model = DebertaV2Model.from_pretrained(\"/data/home/chensh/data/huggingface_model/microsoft/mdeberta-v3-base\").to(\"cuda\")\n",
    "\n",
    "router_datasets = [RouterDataset(data_path, data_type=data_types[i], dataset_id=i, size=2000) for i, data_path in enumerate(dataset_paths)]\n",
    "for router_dataset in router_datasets:\n",
    "    router_dataset.register_tokenizer(tokenizer)\n",
    "router_dataset = ConcatDataset(router_datasets)\n",
    "router_dataloader = DataLoader(router_dataset, batch_size=64)\n",
    "\n",
    "router_model = RouterModule(encoder_model, hidden_state_dim=768, node_size=len(router_datasets[0].router_node), similarity_function=\"cos\").to(\"cpu\")\n",
    "\n",
    "# state_dict = torch.load(\"/data/home/chensh/projects/LLM_router/logs/router_debug/split2_model5/old/sample_loss_weight_1_cos_top_k_1_last_k_3_learning_rate_0.00005_step_500_t_1/model.pth\")\n",
    "# router_model.load_state_dict(state_dict)\n",
    "router_model.to('cuda')\n",
    "\n",
    "all_hidden_states = []\n",
    "ids = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(router_dataloader):\n",
    "        input, _, id = batch\n",
    "        input.to(\"cuda\")\n",
    "        # hidden_states =  router_model.backbone(**input)['last_hidden_state'][:,0,:]\n",
    "        hidden_states =  torch.max(router_model.backbone(**input)['last_hidden_state'], dim=1) \n",
    "        ids.append(id)\n",
    "        all_hidden_states.append(hidden_states[0])\n",
    "len(all_hidden_states)\n",
    "all_hidden_states = torch.concat(all_hidden_states)\n",
    "\n",
    "from MulticoreTSNE import MulticoreTSNE as M_TSNE\n",
    "from openTSNE import TSNE\n",
    "np_hidden_states = all_hidden_states.cpu().numpy()\n",
    "\n",
    "tsne_result2 = M_TSNE(n_components=5, n_jobs=12).fit_transform(np_hidden_states)\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import numpy as np\n",
    "import random as random\n",
    "\n",
    "n_clusters_list = [5]\n",
    "\n",
    "seed = 41\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "for n_clusters in n_clusters_list:\n",
    "    x = tsne_result2\n",
    "    kmeans = KMeans(n_clusters=n_clusters, max_iter=1000)\n",
    "\n",
    "    # 对样本数据进行聚类\n",
    "    kmeans.fit(x)\n",
    "\n",
    "    # 获取聚类结果\n",
    "    labels = kmeans.labels_.tolist()\n",
    "\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    labels_split = [labels[i*2000: (i+1)*2000] for i in range(len(dataset_paths))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for llm_number in range(3,4):\n",
    "    target_output_path = f\"/data/home/chensh/projects/LLM_router/datasets/llm_numbers/model_{llm_number}_2_cluster\"\n",
    "    os.makedirs(target_output_path, exist_ok=True)\n",
    "    base_path = f\"/data/home/chensh/projects/LLM_router/datasets/llm_numbers/model_{llm_number}_2\"\n",
    "    datasets = [\"gsm8k-train.json\",\"mmlu_train.json\",\"humaneval_train.json\",\"arc_challenge_train.json\",\"cmmlu_train.json\"]\n",
    "    \n",
    "\n",
    "    for i, data_path in enumerate(datasets) :\n",
    "        base_data_path = os.path.join(base_path, data_path)\n",
    "        cluster_ids = labels_split[i]\n",
    "        \n",
    "        with open(base_data_path, 'r') as f:\n",
    "            if data_path.endswith('.json'):\n",
    "                sample_list = json.load(f)\n",
    "        new_sample_list = []\n",
    "        for j, sample in enumerate(sample_list):\n",
    "            if j >= 2000:\n",
    "                break\n",
    "            new_sample = sample \n",
    "            new_sample['cluster_id'] = cluster_ids[j]\n",
    "            new_sample_list.append(new_sample)\n",
    "        with open(os.path.join(target_output_path, data_path), \"w\" ) as f:\n",
    "            json.dump(new_sample_list ,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
