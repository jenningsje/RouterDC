{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_list = [  \n",
    "                ['mistralai','Mistral-7B-v0.1'],\n",
    "                ['meta-math','MetaMath-Mistral-7B'],\n",
    "                ['itpossible','Chinese-Mistral-7B-v0.1'],\n",
    "                [\"HuggingFaceH4\",\"zephyr-7b-beta\"],\n",
    "                [\"cognitivecomputations\",\"dolphin-2.6-mistral-7b\"],\n",
    "                [\"meta-llama\",\"Meta-Llama-3-8B\"],\n",
    "                [\"cognitivecomputations\",\"dolphin-2.9-llama3-8b\"],\n",
    "                [\"meta-llama\",\"Llama-2-13b-hf\"],\n",
    "                ]\n",
    "\n",
    "output_file_path = \"/data/home/chensh/projects/LLM_router/logs/voting_log/split_model7/voting_cache\"\n",
    "os.makedirs(output_file_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2667\n",
      "63.304058865416565\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# for mmlu (multiple choice problem)\n",
    "\n",
    "task_list = ['abstract_algebra', 'anatomy', 'astronomy', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics', 'high_school_psychology', 'high_school_statistics', 'high_school_us_history', 'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law', 'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing', 'medical_genetics', 'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition', 'philosophy', 'prehistory', 'professional_accounting', 'professional_law', 'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies', 'sociology', 'us_foreign_policy', 'virology', 'world_religions']\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    count = 0\n",
    "    choices_head = ['A. ', 'B. ', 'C. ', 'D. ']\n",
    "    for task in task_list:\n",
    "        model_pre = model_info[0]\n",
    "        model = model_info[1]\n",
    "        with open(f\"/data/home/chensh/projects/LLM_router/output/mmlu_5shot/{model}/pretrained____data__home__chensh__data__huggingface_model__{model_pre}__{model}_mmlu_{task}.jsonl\", 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        for i in range(len(json_file)):\n",
    "            resps = np.array(json_file[i]['resps'])\n",
    "            log_probability = resps[:,0,0]\n",
    "            probability = np.exp(log_probability)\n",
    "            prediction = np.argmax(probability)\n",
    "            answer = json_file[i]['doc']['answer']\n",
    "\n",
    "            if model_index == 0:\n",
    "                output_data.append(\n",
    "                    {\"question\":json_file[i]['doc'][\"question\"] + \n",
    "                                \"\".join([\"\\n\" + choices_head[j] + choice for j, choice in enumerate(json_file[i]['doc']['choices'])]), \"predictions\": {model_pre+\"/\" + model: prediction}, \"answer\": answer}\n",
    "                )\n",
    "            else:\n",
    "                output_data[count][\"predictions\"][model_pre+\"/\" + model] = prediction\n",
    "                count += 1\n",
    "\n",
    "train_split_index = random.sample(range(len(output_data)), len(output_data))\n",
    "output_data = [output_data[index] for index in train_split_index]\n",
    "train_split = output_data[:int(0.7* len(output_data))]\n",
    "test_split = output_data[int(0.7* len(output_data)):]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "correct = 0\n",
    "data_len = len(test_split)\n",
    "for item_dict in test_split:\n",
    "    predictions = item_dict['predictions']\n",
    "    predictions_list = list(predictions.values())\n",
    "    prediction_counter = Counter(predictions_list)\n",
    "    final_predict, _ = prediction_counter.most_common()[0]\n",
    "    if final_predict == int(item_dict['answer']):\n",
    "        correct += 1\n",
    "\n",
    "print(correct)\n",
    "print(correct / data_len * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8951\n",
      "63.74448084318474\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1650\n",
      "47.482014388489205\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(52)\n",
    "\n",
    "# modified_model_names = [\"Meta-Llama-3-8B\",\"Llama3-8B-Chinese-Chat\",\"dolphin-2.9-llama3-8b\"]\n",
    "# for modified_model_name in modified_model_names:\n",
    "#     path = os.path.join(\"/data/home/chensh/projects/LLM_router/output/cmmlu\", modified_model_name)\n",
    "#     files = os.listdir(path)\n",
    "#     for file in files:\n",
    "#         if file.startswith(\"pretrained\"):\n",
    "#             os.rename(os.path.join(path, file), os.path.join(path, file.replace(\",max_model_len__4096\", \"\")))    \n",
    "\n",
    "# for cmmlu (multiple choice problem)\n",
    "\n",
    "task_list = ['agronomy', 'anatomy', 'ancient_chinese', 'arts', 'astronomy', 'business_ethics', 'chinese_civil_service_exam', 'chinese_driving_rule', 'chinese_food_culture', 'chinese_foreign_policy', 'chinese_history', 'chinese_literature', 'chinese_teacher_qualification', 'clinical_knowledge', 'college_actuarial_science', 'college_education', 'college_engineering_hydrology', 'college_law', 'college_mathematics', 'college_medical_statistics', 'college_medicine', 'computer_science', 'computer_security', 'conceptual_physics', 'construction_project_management', 'economics', 'education', 'electrical_engineering', 'elementary_chinese', 'elementary_commonsense', 'elementary_information_and_technology', 'elementary_mathematics', 'ethnology', 'food_science', 'genetics', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_geography', 'high_school_mathematics', 'high_school_physics', 'high_school_politics', 'human_sexuality', 'international_law', 'journalism', 'jurisprudence', 'legal_and_moral_basis', 'logical', 'machine_learning', 'management', 'marketing', 'marxist_theory', 'modern_chinese', 'nutrition', 'philosophy', 'professional_accounting', 'professional_law', 'professional_medicine', 'professional_psychology', 'public_relations', 'security_study', 'sociology', 'sports_science', 'traditional_chinese_medicine', 'virology', 'world_history', 'world_religions']\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    count = 0\n",
    "    choices_head = ['A', 'B', 'C', 'D']\n",
    "    for task in task_list:\n",
    "        model_pre = model_info[0]\n",
    "        model = model_info[1]\n",
    "        with open(f\"/data/home/chensh/projects/LLM_router/output/cmmlu/{model}/pretrained____data__home__chensh__data__huggingface_model__{model_pre}__{model}_cmmlu_{task}.jsonl\", 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        for i in range(len(json_file)):\n",
    "            resps = np.array(json_file[i]['resps'])\n",
    "            log_probability = resps[:,0,0]\n",
    "            probability = np.exp(log_probability)\n",
    "            prediction = np.argmax(probability)\n",
    "            answer = json_file[i]['target']\n",
    "\n",
    "            if model_index == 0:\n",
    "                output_data.append(\n",
    "                    {\"question\":json_file[i]['doc']['Question'].strip() + \"\".join([\"\\n\" + choices_head[j] + \". \" + json_file[i]['doc'][choices_head[j]] for j in range(4)]) + \"\\n答案：\", \"predictions\": {model_pre+\"/\" + model: prediction}, \"answer\": answer}\n",
    "                )\n",
    "            else:\n",
    "                output_data[count][\"predictions\"][model_pre+\"/\" + model] = prediction\n",
    "                count += 1\n",
    "\n",
    "train_split_index = random.sample(range(len(output_data)), len(output_data))\n",
    "output_data = [output_data[index] for index in train_split_index]\n",
    "train_split = output_data[:int(0.7* len(output_data))]\n",
    "test_split = output_data[int(0.7* len(output_data)):]\n",
    "                \n",
    "from collections import Counter\n",
    "\n",
    "correct = 0\n",
    "data_len = len(test_split)\n",
    "for item_dict in test_split:\n",
    "    predictions = item_dict['predictions']\n",
    "    predictions_list = list(predictions.values())\n",
    "    prediction_counter = Counter(predictions_list)\n",
    "    final_predict, _ = prediction_counter.most_common()[0]\n",
    "    if final_predict == int(item_dict['answer']):\n",
    "        correct += 1\n",
    "\n",
    "print(correct)\n",
    "print(correct / data_len * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "50.85227272727273\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(100)\n",
    "\n",
    "# for arc (multiple choice problem)\n",
    "\n",
    "# task = \"arc_easy\"\n",
    "task = \"arc_challenge\"\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    count = 0\n",
    "    model_pre = model_info[0]\n",
    "    model = model_info[1]\n",
    "    with open(f\"/data/home/chensh/projects/LLM_router/output/{task}/{model}/pretrained____data__home__chensh__data__huggingface_model__{model_pre}__{model}_{task}.jsonl\", 'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    for i in range(len(json_file)):\n",
    "        resps = np.array(json_file[i]['resps'])\n",
    "        log_probability = resps[:,0,0]\n",
    "        probability = np.exp(log_probability)\n",
    "        prediction = np.argmax(probability)\n",
    "        answer = json_file[i]['target']\n",
    "\n",
    "        if model_index == 0:\n",
    "            output_data.append(\n",
    "                {\"question\": \"Question:\" + json_file[i]['doc'][\"question\"] + \"\\nAnswer:\", \"predictions\": {model_pre+\"/\" + model: prediction}, \"answer\": answer}\n",
    "            )\n",
    "        else:\n",
    "            output_data[count][\"predictions\"][model_pre+\"/\" + model] = prediction\n",
    "            count += 1\n",
    "\n",
    "train_split_index = random.sample(range(len(output_data)), len(output_data))\n",
    "output_data = [output_data[index] for index in train_split_index]\n",
    "train_split = output_data[:int(0.7* len(output_data))]\n",
    "test_split = output_data[int(0.7* len(output_data)):]\n",
    "\n",
    "                \n",
    "from collections import Counter\n",
    "\n",
    "correct = 0\n",
    "data_len = len(test_split)\n",
    "for item_dict in test_split:\n",
    "    predictions = item_dict['predictions']\n",
    "    predictions_list = list(predictions.values())\n",
    "    prediction_counter = Counter(predictions_list)\n",
    "    final_predict, _ = prediction_counter.most_common()[0]\n",
    "    if final_predict == int(item_dict['answer']):\n",
    "        correct += 1\n",
    "\n",
    "print(correct)\n",
    "print(correct / data_len * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "654\n",
      "48.588410104011885\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random       \n",
    "\n",
    "# for ceval (multiple choice problem)\n",
    "task_list = ['computer_network', 'operating_system', 'computer_architecture', 'college_programming', 'college_physics', 'college_chemistry', 'advanced_mathematics', 'probability_and_statistics', 'discrete_mathematics', 'electrical_engineer', 'metrology_engineer', 'high_school_mathematics', 'high_school_physics', 'high_school_chemistry', 'high_school_biology', 'middle_school_mathematics', 'middle_school_biology', 'middle_school_physics', 'middle_school_chemistry', 'veterinary_medicine', 'college_economics', 'business_administration', 'marxism', 'mao_zedong_thought', 'education_science', 'teacher_qualification', 'high_school_politics', 'high_school_geography', 'middle_school_politics', 'middle_school_geography', 'modern_chinese_history', 'ideological_and_moral_cultivation', 'logic', 'law', 'chinese_language_and_literature', 'art_studies', 'professional_tour_guide', 'legal_professional', 'high_school_chinese', 'high_school_history', 'middle_school_history', 'civil_servant', 'sports_science', 'plant_protection', 'basic_medicine', 'clinical_medicine', 'urban_and_rural_planner', 'accountant', 'fire_engineer', 'environmental_impact_assessment_engineer', 'tax_accountant', 'physician']\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    count = 0\n",
    "    choices_head = ['A', 'B', 'C', 'D']\n",
    "    for task in task_list:\n",
    "        model_pre = model_info[0]\n",
    "        model = model_info[1]\n",
    "        with open(f\"/data/home/chensh/projects/LLM_router/output/ceval-validation/{model}/pretrained____data__home__chensh__data__huggingface_model__{model_pre}__{model}_ceval-valid_{task}.jsonl\", 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        for i in range(len(json_file)):\n",
    "            resps = np.array(json_file[i]['resps'])\n",
    "            log_probability = resps[:,0,0]\n",
    "            probability = np.exp(log_probability)\n",
    "            prediction = np.argmax(probability)\n",
    "            answer = json_file[i]['target']\n",
    "\n",
    "            if model_index == 0:\n",
    "                output_data.append(\n",
    "                    {\"question\":json_file[i]['doc'][\"question\"] + \n",
    "                                \"\".join([\"\\n\" + choices_head[j] + \". \" + json_file[i]['doc'][choices_head[j]] for j in range(4)]) + \"答案：\", \"predictions\": {model_pre+\"/\" + model: prediction}, \"answer\": answer}\n",
    "                )\n",
    "            else:\n",
    "                output_data[count][\"predictions\"][model_pre+\"/\" + model] = prediction\n",
    "                count += 1\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "correct = 0\n",
    "data_len = len(output_data)\n",
    "for item_dict in output_data:\n",
    "    predictions = item_dict['predictions']\n",
    "    predictions_list = list(predictions.values())\n",
    "    prediction_counter = Counter(predictions_list)\n",
    "    final_predict, _ = prediction_counter.most_common()[0]\n",
    "    if final_predict == int(item_dict['answer']):\n",
    "        correct += 1\n",
    "\n",
    "print(correct)\n",
    "print(correct / data_len * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889\n",
      "67.39954510993176\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# for gsm8k\n",
    "\n",
    "# task = \"gsm8k_train\"\n",
    "task = \"gsm8k-repeat10\"\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    model_pre = model_info[0]\n",
    "    model = model_info[1]\n",
    "    with open(f\"/data/home/chensh/projects/LLM_router/output/gsm8k_test-t0.2/{model}/pretrained____data__home__chensh__data__huggingface_model__{model_pre}__{model}_{task}.jsonl\", 'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    for i in range(len(json_file)):\n",
    "        # extract answer_list & answer\n",
    "        resps = json_file[i]['resps'][0]\n",
    "        pattern = \"#### (\\\\-?[0-9\\\\.\\\\,]+)\"\n",
    "        ans_list = [re.search(pattern, resp).group(1) if re.search(pattern, resp) else None for resp in resps]\n",
    "        target = re.search(pattern, json_file[i]['target']).group(1)\n",
    "        \n",
    "        # compare with origin answer & get score\n",
    "        if model_index == 0:\n",
    "            output_data.append({\"question\":json_file[i]['doc'][\"question\"], \"predictions\": {model_pre+\"/\" + model: ans_list[0]}, \"answer\": target})\n",
    "        else:\n",
    "            output_data[i][\"predictions\"][model_pre+\"/\" + model] = ans_list[0]\n",
    "            \n",
    "output_data = output_data[:int(len(output_data)/2)]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "correct = 0\n",
    "data_len = len(output_data)\n",
    "for item_dict in output_data:\n",
    "    predictions = item_dict['predictions']\n",
    "    predictions_list = list(predictions.values())\n",
    "    prediction_counter = Counter(predictions_list)\n",
    "    final_predict, _ = prediction_counter.most_common()[0]\n",
    "    if final_predict == (item_dict['answer']):\n",
    "        correct += 1\n",
    "\n",
    "print(correct)\n",
    "print(correct / data_len * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340\n",
      "39.03559127439725\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SUBSTITUTIONS = [\n",
    "    (\"an \", \"\"),\n",
    "    (\"a \", \"\"),\n",
    "    (\".$\", \"$\"),\n",
    "    (\"\\\\$\", \"\"),\n",
    "    (r\"\\ \", \"\"),\n",
    "    (\" \", \"\"),\n",
    "    (\"mbox\", \"text\"),\n",
    "    (\",\\\\text{and}\", \",\"),\n",
    "    (\"\\\\text{and}\", \",\"),\n",
    "    (\"\\\\text{m}\", \"\\\\text{}\"),\n",
    "]\n",
    "REMOVED_EXPRESSIONS = [\n",
    "    \"square\",\n",
    "    \"ways\",\n",
    "    \"integers\",\n",
    "    \"dollars\",\n",
    "    \"mph\",\n",
    "    \"inches\",\n",
    "    \"ft\",\n",
    "    \"hours\",\n",
    "    \"km\",\n",
    "    \"units\",\n",
    "    \"\\\\ldots\",\n",
    "    \"sue\",\n",
    "    \"points\",\n",
    "    \"feet\",\n",
    "    \"minutes\",\n",
    "    \"digits\",\n",
    "    \"cents\",\n",
    "    \"degrees\",\n",
    "    \"cm\",\n",
    "    \"gm\",\n",
    "    \"pounds\",\n",
    "    \"meters\",\n",
    "    \"meals\",\n",
    "    \"edges\",\n",
    "    \"students\",\n",
    "    \"childrentickets\",\n",
    "    \"multiples\",\n",
    "    \"\\\\text{s}\",\n",
    "    \"\\\\text{.}\",\n",
    "    \"\\\\text{\\ns}\",\n",
    "    \"\\\\text{}^2\",\n",
    "    \"\\\\text{}^3\",\n",
    "    \"\\\\text{\\n}\",\n",
    "    \"\\\\text{}\",\n",
    "    r\"\\mathrm{th}\",\n",
    "    r\"^\\circ\",\n",
    "    r\"^{\\circ}\",\n",
    "    r\"\\;\",\n",
    "    r\",\\!\",\n",
    "    \"{,}\",\n",
    "    '\"',\n",
    "    \"\\\\dots\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_unnormalized_answer(text: str) -> str:\n",
    "    INVALID_ANSWER = \"[invalidanswer]\"\n",
    "    end_seq = \"I hope it is correct.\"\n",
    "    text += end_seq\n",
    "    match = re.search(\n",
    "        r\"Final Answer: The final answer is(.*?). I hope it is correct.\",\n",
    "        text,\n",
    "    )\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return INVALID_ANSWER\n",
    "    \n",
    "\n",
    "def normalize_final_answer(final_answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a final answer to a quantitative reasoning question.\n",
    "\n",
    "    Copied character for character from appendix D of Lewkowycz et al. (2022)\n",
    "    \"\"\"\n",
    "    final_answer = final_answer.split(\"=\")[-1]\n",
    "\n",
    "    for before, after in SUBSTITUTIONS:\n",
    "        final_answer = final_answer.replace(before, after)\n",
    "    for expr in REMOVED_EXPRESSIONS:\n",
    "        final_answer = final_answer.replace(expr, \"\")\n",
    "\n",
    "    # Extract answer that is in LaTeX math, is bold,\n",
    "    # is surrounded by a box, etc.\n",
    "    final_answer = re.sub(r\"(.*?)(\\$)(.*?)(\\$)(.*)\", \"$\\\\3$\", final_answer)\n",
    "    final_answer = re.sub(r\"(\\\\text\\{)(.*?)(\\})\", \"\\\\2\", final_answer)\n",
    "    final_answer = re.sub(r\"(\\\\textbf\\{)(.*?)(\\})\", \"\\\\2\", final_answer)\n",
    "    final_answer = re.sub(r\"(\\\\overline\\{)(.*?)(\\})\", \"\\\\2\", final_answer)\n",
    "    final_answer = re.sub(r\"(\\\\boxed\\{)(.*)(\\})\", \"\\\\2\", final_answer)\n",
    "\n",
    "    # Normalize shorthand TeX:\n",
    "    #  \\fracab -> \\frac{a}{b}\n",
    "    #  \\frac{abc}{bef} -> \\frac{abc}{bef}\n",
    "    #  \\fracabc -> \\frac{a}{b}c\n",
    "    #  \\sqrta -> \\sqrt{a}\n",
    "    #  \\sqrtab -> sqrt{a}b\n",
    "    final_answer = re.sub(r\"(frac)([^{])(.)\", \"frac{\\\\2}{\\\\3}\", final_answer)\n",
    "    final_answer = re.sub(r\"(sqrt)([^{])\", \"sqrt{\\\\2}\", final_answer)\n",
    "    final_answer = final_answer.replace(\"$\", \"\")\n",
    "\n",
    "    # Normalize 100,000 -> 100000\n",
    "    if final_answer.replace(\",\", \"\").isdigit():\n",
    "        final_answer = final_answer.replace(\",\", \"\")\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(52)\n",
    "\n",
    "\n",
    "# for cmmlu (multiple choice problem)\n",
    "\n",
    "# task_list = [\"minerva_math_precalc\", \"minerva_math_prealgebra\", \"minerva_math_num_theory\", \"minerva_math_intermediate_algebra\", \"minerva_math_geometry\", \"minerva_math_counting_and_prob\", \"minerva_math_algebra\"]\n",
    "\n",
    "task_list = [\"minerva_math_prealgebra\"]\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    count = 0\n",
    "    for task in task_list:\n",
    "        model_pre = model_info[0]\n",
    "        model = model_info[1]\n",
    "        with open(f\"/data/home/chensh/projects/LLM_router/output/MATH/{model}/pretrained____data__home__chensh__data__huggingface_model__{model_pre}__{model}_{task}.jsonl\", 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        for i in range(len(json_file)):\n",
    "            resps = json_file[i]['resps'][0][0]\n",
    "            unnormalized_answer = get_unnormalized_answer(resps)\n",
    "            prediction = normalize_final_answer(unnormalized_answer)\n",
    "            answer = json_file[i]['target']\n",
    "            if model_index == 0:\n",
    "                score = json_file[i]['exact_match']\n",
    "                output_data.append(\n",
    "                    {\"question\": json_file[i]['doc']['problem'], \"predictions\": {model_pre+\"/\" + model: prediction}, 'answer': answer}\n",
    "                )\n",
    "            else:\n",
    "                output_data[count][\"predictions\"][model_pre+\"/\" + model] = prediction\n",
    "                count += 1\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "correct = 0\n",
    "data_len = len(output_data)\n",
    "for item_dict in output_data:\n",
    "    predictions = item_dict['predictions']\n",
    "    predictions_list = list(predictions.values())\n",
    "    prediction_counter = Counter(predictions_list)\n",
    "    final_predict, _ = prediction_counter.most_common()[0]\n",
    "    if final_predict == (item_dict['answer']):\n",
    "        correct += 1\n",
    "\n",
    "print(correct)\n",
    "print(correct / data_len * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the smallest multiple of $5$ which is greater than $-32$?',\n",
       " 'predictions': {'mistralai/Mistral-7B-v0.1': '-160',\n",
       "  'meta-math/MetaMath-Mistral-7B': '-25',\n",
       "  'itpossible/Chinese-Mistral-7B-v0.1': '35',\n",
       "  'HuggingFaceH4/zephyr-7b-beta': '-30',\n",
       "  'cognitivecomputations/dolphin-2.6-mistral-7b': '35',\n",
       "  'meta-llama/Meta-Llama-3-8B': '5',\n",
       "  'cognitivecomputations/dolphin-2.9-llama3-8b': '5',\n",
       "  'meta-llama/Llama-2-13b-hf': '5'}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "42.857142857142854\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu import sentence_bleu\n",
    "import ast\n",
    "\n",
    "def cut_def_question(func_code, question):\n",
    "    len_question = len(question)\n",
    "    return func_code[len_question:]\n",
    "\n",
    "\n",
    "def most_similar_code(clist):\n",
    "    cmp_res = lambda x, y: sentence_bleu(x, [y], lowercase=True).score\n",
    "    if len(clist) == 1:\n",
    "        return 0, clist[0], 0\n",
    "    bleu_scores = []\n",
    "    for idx, agent in enumerate(clist):\n",
    "        total_score = 0\n",
    "        for idx_o, otheragent in enumerate(clist):\n",
    "            if idx == idx_o:\n",
    "                continue\n",
    "            total_score += cmp_res(agent, otheragent)\n",
    "        bleu_scores.append(total_score)\n",
    "    max_index, max_value = max(enumerate(bleu_scores), key=lambda x: x[1])\n",
    "    return max_index, clist[max_index], max_value\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "task = \"humaneval\"\n",
    "\n",
    "humaneval_data = pd.read_parquet(\"/data/home/chensh/data/LLM_datasets/openai_humaneval/openai_humaneval/test-00000-of-00001.parquet\",engine='pyarrow')\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    model_pre = model_info[0]\n",
    "    model = model_info[1]\n",
    "    with open(f\"/data/home/chensh/projects/LLM_router/output/humaneval/{model}/log_samples.json\",'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    with open(f\"/data/home/chensh/projects/LLM_router/output/humaneval/{model}/_humaneval.json\",'r') as f:\n",
    "        generation_file = json.load(f)\n",
    "    json_file = json_file['humaneval']\n",
    "    for index, instance in json_file.items():\n",
    "        correct = 1 if instance[0][1]['passed'] else 0\n",
    "        question = humaneval_data.iloc[int(index)]['prompt']\n",
    "        prediction = generation_file[int(index)][0]\n",
    "        prediction = cut_def_question(prediction, question)\n",
    "        \n",
    "        if model_index == 0:\n",
    "            output_data.append({\"question\": question, \"predictions\": {model_pre + \"/\" + model: prediction}, \"corrects\": {model_pre + \"/\" + model: correct}} )\n",
    "        else:\n",
    "            output_data[int(index)][\"predictions\"][model_pre+\"/\" + model] = prediction\n",
    "            output_data[int(index)][\"corrects\"][model_pre+\"/\" + model] = correct\n",
    "            \n",
    "\n",
    "train_split_index = random.sample(range(len(output_data)), len(output_data))\n",
    "output_data = [output_data[index] for index in train_split_index]\n",
    "train_split = output_data[:115]\n",
    "test_split = output_data[115:]\n",
    "# test_split = output_data[100:]\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "correct = 0\n",
    "data_len = len(test_split)\n",
    "for item_dict in test_split:\n",
    "    predictions = item_dict['predictions']\n",
    "    predictions_list = list(predictions.values())\n",
    "    max_index, _, _ =  most_similar_code(predictions_list)\n",
    "    keys = list(item_dict['corrects'].keys())\n",
    "    correct += item_dict['corrects'][keys[max_index]]\n",
    "\n",
    "print(correct)\n",
    "print(correct / data_len * 100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n",
      "41.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# /data/home/chensh/projects/LLM_router/output/humaneval/Arithmo2-Mistral-7B_humaneval.json\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "task = \"mbpp\"\n",
    "\n",
    "mbpp_data = pd.read_parquet(\"/data/home/chensh/data/LLM_datasets/mbpp/full/test-00000-of-00001.parquet\",engine='pyarrow')\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    model_pre = model_info[0]\n",
    "    model = model_info[1]\n",
    "    with open(f\"/data/home/chensh/projects/LLM_router/output/mbpp/{model}/log_samples.json\",'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    with open(f\"/data/home/chensh/projects/LLM_router/output/mbpp/{model}/_mbpp.json\",'r') as f:\n",
    "        generation_file = json.load(f)\n",
    "    json_file = json_file['mbpp']\n",
    "    for index, instance in json_file.items():\n",
    "        correct = 1 if instance[0][1]['passed'] else 0\n",
    "        description = mbpp_data.iloc[int(index)][\"text\"]\n",
    "        test_example = mbpp_data.iloc[int(index)][\"test_list\"][0]\n",
    "        question = f'\"\"\"\\n{description}\\n{test_example}\\n\"\"\"\\n'\n",
    "        prediction = generation_file[int(index)][0]\n",
    "        prediction = cut_def_question(prediction, question)\n",
    "        \n",
    "        if model_index == 0:\n",
    "            output_data.append({\"question\": question, \"predictions\": {model_pre + \"/\" + model: prediction}, \"corrects\": {model_pre + \"/\" + model: correct}} )\n",
    "        else:\n",
    "            output_data[int(index)][\"predictions\"][model_pre+\"/\" + model] = prediction\n",
    "            output_data[int(index)][\"corrects\"][model_pre+\"/\" + model] = correct\n",
    "            \n",
    "\n",
    "from collections import Counter\n",
    "correct = 0\n",
    "data_len = len(output_data)\n",
    "for item_dict in output_data:\n",
    "    predictions = item_dict['predictions']\n",
    "    predictions_list = list(predictions.values())\n",
    "    max_index, _, _ =  most_similar_code(predictions_list)\n",
    "    keys = list(item_dict['corrects'].keys())\n",
    "    correct += item_dict['corrects'][keys[max_index]]\n",
    "\n",
    "print(correct)\n",
    "print(correct / data_len * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n\\n\\n    for i in range(len(numbers)):\\n        for j in range(i+1, len(numbers)):\\n            if abs(numbers[i] - numbers[j]) < threshold:\\n                return True\\n    return False\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"/data/home/chensh/projects/LLM_router/output/humaneval/Meta-Llama-3-8B/_humaneval.json\",'r') as f:\n",
    "    generation_file = json.load(f)\n",
    "generation_file[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.03034716144347"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(test_split[0]['predictions']['mistralai/Mistral-7B-v0.1'])\n",
    "# answer = cut_def_question(test_split[0]['predictions']['mistralai/Mistral-7B-v0.1'], test_split[0]['question'])\n",
    "# print(answer)\n",
    "\n",
    "sentence_bleu(test_split[0]['predictions']['mistralai/Mistral-7B-v0.1'], [test_split[0]['predictions']['itpossible/Chinese-Mistral-7B-v0.1']], lowercase=True).score\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.880375741396136"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu(test_split[0]['predictions']['mistralai/Mistral-7B-v0.1'], [test_split[0]['predictions']['meta-llama/Meta-Llama-3-8B']], lowercase=True).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
